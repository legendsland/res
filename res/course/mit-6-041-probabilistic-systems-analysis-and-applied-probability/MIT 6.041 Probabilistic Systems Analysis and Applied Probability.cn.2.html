<!DOCTYPE html><html class="translated-ltr" style=""><!--
 Page saved with SingleFile 
 url: file:///home/zy/ws/res/res/course/mit-6-041-probabilistic-systems-analysis-and-applied-probability/MIT%206.041%20Probabilistic%20Systems%20Analysis%20and%20Applied%20Probability.html 
 saved date: Mon Sep 23 2024 23:20:19 GMT+0800 (Hong Kong Standard Time)
--><head>
<meta name="dc.identifier" content="res/329001d9f79947bfea50cb6e2b6363995b013cab">
<meta charset="utf-8"><style>.VIpgJd-ZVi9od-ORHb-OEVmcd{left:0;top:0;height:39px;width:100%;z-index:10000001;position:fixed;border:none;border-bottom:1px solid #6B90DA;margin:0;box-shadow:0 0 8px 1px #999}.VIpgJd-ZVi9od-xl07Ob-OEVmcd{z-index:10000002;border:none;position:fixed;box-shadow:0 3px 8px 2px #999}.VIpgJd-ZVi9od-SmfZ-OEVmcd{z-index:10000000;border:none;margin:0}.goog-te-gadget{font-family:arial;font-size:11px;color:#666;white-space:nowrap}.goog-te-gadget img{vertical-align:middle;border:none}.goog-te-gadget-simple{background-color:#FFF;border-left:1px solid #D5D5D5;border-top:1px solid #9B9B9B;border-bottom:1px solid #E8E8E8;border-right:1px solid #D5D5D5;font-size:10pt;display:inline-block;padding-top:1px;padding-bottom:2px;cursor:pointer}.goog-te-gadget-icon{margin-left:2px;margin-right:2px;width:19px;height:19px;border:none;vertical-align:middle}.goog-te-combo{margin-left:4px;margin-right:4px;vertical-align:baseline}.goog-te-gadget .goog-te-combo{margin:4px 0}.VIpgJd-ZVi9od-l4eHX-hSRGPd,.VIpgJd-ZVi9od-l4eHX-hSRGPd:link,.VIpgJd-ZVi9od-l4eHX-hSRGPd:visited,.VIpgJd-ZVi9od-l4eHX-hSRGPd:hover,.VIpgJd-ZVi9od-l4eHX-hSRGPd:active{font-size:12px;font-weight:bold;color:#444;text-decoration:none}.VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd,.VIpgJd-ZVi9od-TvD9Pc-hSRGPd{display:block;margin:0 10px}.VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd{padding-top:2px;padding-left:4px}.goog-te-combo,.VIpgJd-ZVi9od-ORHb *,.VIpgJd-ZVi9od-SmfZ *,.VIpgJd-ZVi9od-xl07Ob *,.VIpgJd-ZVi9od-vH1Gmf *,.VIpgJd-ZVi9od-l9xktf *{font-family:arial;font-size:10pt}.VIpgJd-ZVi9od-ORHb{margin:0;background-color:#E4EFFB;overflow:hidden}.VIpgJd-ZVi9od-ORHb img{border:none}.VIpgJd-ZVi9od-ORHb-bN97Pc{color:#000}.VIpgJd-ZVi9od-ORHb-bN97Pc img{vertical-align:middle}.VIpgJd-ZVi9od-ORHb-Tswv1b{color:#666;vertical-align:top;margin-top:0;font-size:7pt}.VIpgJd-ZVi9od-ORHb-KE6vqe{width:8px}.VIpgJd-ZVi9od-LgbsSe{border-color:#E7E7E7;border-style:none solid solid none;border-width:0 1px 1px 0}.VIpgJd-ZVi9od-LgbsSe div{border-color:#CCC #999 #999 #CCC;border-right:1px solid #999;border-style:solid;border-width:1px;height:20px}.VIpgJd-ZVi9od-LgbsSe button{background:transparent;border:none;cursor:pointer;height:20px;overflow:hidden;margin:0;vertical-align:top;white-space:nowrap}.VIpgJd-ZVi9od-LgbsSe button:active{background:none repeat scroll 0 0#CCC}.VIpgJd-ZVi9od-SmfZ{margin:0;background-color:#FFF;white-space:nowrap}.VIpgJd-ZVi9od-SmfZ-hSRGPd{text-decoration:none;font-weight:bold;font-size:10pt;border:1px outset #888;padding:6px 10px;white-space:nowrap;position:absolute;left:0;top:0}.VIpgJd-ZVi9od-SmfZ-hSRGPd img{margin-left:2px;margin-right:2px;width:19px;height:19px;border:none;vertical-align:middle}.VIpgJd-ZVi9od-SmfZ-hSRGPd span{text-decoration:underline;margin-left:2px;margin-right:2px;vertical-align:middle}.goog-te-float-top .VIpgJd-ZVi9od-SmfZ-hSRGPd{padding:2px;border-top-width:0}.goog-te-float-bottom .VIpgJd-ZVi9od-SmfZ-hSRGPd{padding:2px;border-bottom-width:0}.VIpgJd-ZVi9od-xl07Ob-lTBxed{text-decoration:none;color:#00C;white-space:nowrap;margin-left:4px;margin-right:4px}.VIpgJd-ZVi9od-xl07Ob-lTBxed span{text-decoration:underline}.VIpgJd-ZVi9od-xl07Ob-lTBxed img{margin-left:2px;margin-right:2px}.goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed{color:#000}.goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed span{text-decoration:none}.VIpgJd-ZVi9od-xl07Ob{background-color:#FFF;text-decoration:none;border:2px solid #C3D9FF;overflow-y:scroll;overflow-x:hidden;position:absolute;left:0;top:0}.VIpgJd-ZVi9od-xl07Ob-ibnC6b{padding:3px;text-decoration:none}.VIpgJd-ZVi9od-xl07Ob-ibnC6b,.VIpgJd-ZVi9od-xl07Ob-ibnC6b:link{color:#00C;background:#FFF}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:visited{color:#551A8B}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:hover{background:#C3D9FF}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:active{color:#00C}.VIpgJd-ZVi9od-vH1Gmf{background-color:#FFF;text-decoration:none;border:1px solid #6B90DA;overflow:hidden;padding:4px}.VIpgJd-ZVi9od-vH1Gmf-KrhPNb{width:16px}.VIpgJd-ZVi9od-vH1Gmf-hgDUwe{margin:6px 0;height:1px;background-color:#aaa;overflow:hidden}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div{padding:4px}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b .uDEFge{display:none}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd .uDEFge{display:auto}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd .fmcmS{padding-left:4px;padding-right:4px}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd{text-decoration:none}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:link div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:visited div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:active div{color:#00C;background:#FFF}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:hover div{color:#FFF;background:#36C}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:link div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:visited div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:hover div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:active div{color:#000;font-weight:bold}.VIpgJd-ZVi9od-l9xktf{background-color:#FFF;overflow:hidden;padding:8px;border:none;border-radius:10px}.VIpgJd-ZVi9od-l9xktf-OEVmcd{background-color:#FFF;border:1px solid #6B90DA;box-shadow:0 3px 8px 2px #999;border-radius:8px}.VIpgJd-ZVi9od-l9xktf img{border:none}.VIpgJd-ZVi9od-l9xktf-fmcmS{margin-top:6px}.VIpgJd-ZVi9od-l9xktf-VgwJlc{margin-top:6px;white-space:nowrap}.VIpgJd-ZVi9od-l9xktf-VgwJlc *{vertical-align:middle}.VIpgJd-ZVi9od-l9xktf-VgwJlc .DUGJie{background-image:url(data:,)}.VIpgJd-ZVi9od-l9xktf-VgwJlc .TdyTDe{background-image:url(data:,)}.VIpgJd-ZVi9od-l9xktf-VgwJlc span{color:#00C;text-decoration:underline;cursor:pointer;margin:0 4px}.VIpgJd-ZVi9od-l9xktf-I9GLp{margin:6px 0 0}.VIpgJd-ZVi9od-l9xktf-I9GLp form{margin:0}.VIpgJd-ZVi9od-l9xktf-I9GLp form textarea{margin-bottom:4px;width:100%}.VIpgJd-ZVi9od-l9xktf-yePe5c{margin:6px 0 4px}.VIpgJd-ZVi9od-aZ2wEe-wOHMyf{z-index:1000;position:fixed;-webkit-transition-delay:.6s;transition-delay:.6s;left:-1000px;top:-1000px}.VIpgJd-ZVi9od-aZ2wEe-wOHMyf-ti6hGc{-webkit-transition-delay:0s;transition-delay:0s;left:-14px;top:-14px}.VIpgJd-ZVi9od-aZ2wEe-OiiCO{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-align:center;-webkit-align-items:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;justify-content:center;width:104px;height:104px;border-radius:50px;background:#FFF url(data:image/png;base64,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)50% 50%no-repeat;-webkit-transition:all .6s ease-in-out;transition:all .6s ease-in-out;-webkit-transform:scale(.4);transform:scale(.4);opacity:0}.VIpgJd-ZVi9od-aZ2wEe-OiiCO-ti6hGc{-webkit-transform:scale(.5);transform:scale(.5);opacity:1}.VIpgJd-ZVi9od-aZ2wEe{margin:2px 0 0 2px;-webkit-animation:spinner-rotator 1.4s linear infinite;animation:spinner-rotator 1.4s linear infinite}@-webkit-keyframes spinner-rotator{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(270deg);transform:rotate(270deg)}}@keyframes spinner-rotator{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(270deg);transform:rotate(270deg)}}.VIpgJd-ZVi9od-aZ2wEe-Jt5cK{stroke-dasharray:187;stroke-dashoffset:0;stroke:#4285F4;-webkit-transform-origin:center;transform-origin:center;-webkit-animation:spinner-dash 1.4s ease-in-out infinite;animation:spinner-dash 1.4s ease-in-out infinite}@-webkit-keyframes spinner-dash{0%{stroke-dashoffset:187}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);transform:rotate(135deg)}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);transform:rotate(450deg)}}@keyframes spinner-dash{0%{stroke-dashoffset:187}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);transform:rotate(135deg)}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);transform:rotate(450deg)}}.VIpgJd-yAWNEb-L7lbkb html,.VIpgJd-yAWNEb-L7lbkb body,.VIpgJd-yAWNEb-L7lbkb div,.VIpgJd-yAWNEb-L7lbkb span,.VIpgJd-yAWNEb-L7lbkb iframe,.VIpgJd-yAWNEb-L7lbkb h1,.VIpgJd-yAWNEb-L7lbkb h2,.VIpgJd-yAWNEb-L7lbkb h3,.VIpgJd-yAWNEb-L7lbkb h4,.VIpgJd-yAWNEb-L7lbkb h5,.VIpgJd-yAWNEb-L7lbkb h6,.VIpgJd-yAWNEb-L7lbkb p,.VIpgJd-yAWNEb-L7lbkb a,.VIpgJd-yAWNEb-L7lbkb img,.VIpgJd-yAWNEb-L7lbkb ol,.VIpgJd-yAWNEb-L7lbkb ul,.VIpgJd-yAWNEb-L7lbkb li,.VIpgJd-yAWNEb-L7lbkb table,.VIpgJd-yAWNEb-L7lbkb form,.VIpgJd-yAWNEb-L7lbkb tbody,.VIpgJd-yAWNEb-L7lbkb tr,.VIpgJd-yAWNEb-L7lbkb td{margin:0;padding:0;border:0;font:inherit;font-size:100%;vertical-align:baseline;text-align:left;line-height:normal}.VIpgJd-yAWNEb-L7lbkb ol,.VIpgJd-yAWNEb-L7lbkb ul{list-style:none}.VIpgJd-yAWNEb-L7lbkb table{border-collapse:collapse;border-spacing:0}.VIpgJd-yAWNEb-L7lbkb caption,.VIpgJd-yAWNEb-L7lbkb th,.VIpgJd-yAWNEb-L7lbkb td{text-align:left;font-weight:normal}.VIpgJd-yAWNEb-L7lbkb input::-moz-focus-inner{border:0}div>.VIpgJd-yAWNEb-L7lbkb{padding:10px 14px}.VIpgJd-yAWNEb-L7lbkb{color:#222;background-color:#fff;border:1px solid #eee;box-shadow:0 4px 16px rgba(0,0,0,.2);-moz-box-shadow:0 4px 16px rgba(0,0,0,.2);-webkit-box-shadow:0 4px 16px rgba(0,0,0,.2);display:none;font-family:arial;font-size:10pt;width:420px;padding:12px;position:absolute;z-index:10000}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-nVMfcd-fmcmS,.VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe{clear:both;font-size:10pt;position:relative;text-align:justify;width:100%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-r4nke{color:#999;font-family:arial,sans-serif;margin:4px 0;text-align:left}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TvD9Pc-LgbsSe{display:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-l4eHX{float:left;margin:0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-PLDbbf{display:inline-block}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-fw42Ze-Z0Arqf-haAclf{display:none;width:100%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-H9tDt{margin-top:20px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-LK5yu{float:left}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-qwU8Me{float:right}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-cGMI2b{min-height:15px;position:relative;height:1%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-jOfkMb-Ne3sFf{background:-webkit-linear-gradient(top,#29910d 0,#20af0e 100%);background:-webkit-gradient(linear,left top,left bottom,from(#29910d),to(#20af0e));background:linear-gradient(top,#29910d 0,#20af0e 100%);background:#29910d;border-radius:4px;-moz-border-radius:4px;-webkit-border-radius:4px;box-shadow:inset 0 2px 2px #1e6609;-moz-box-shadow:inset 0 2px 2px #1e6609;-webkit-box-shadow:inset 0 2px 2px #1e6609;color:white;font-size:9pt;font-weight:bolder;margin-top:12px;padding:6px;text-shadow:1px 1px 1px #1e6609}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-hSRGPd{color:#15c;cursor:pointer;font-family:arial;font-size:11px;margin-right:15px;text-decoration:none}.VIpgJd-yAWNEb-L7lbkb>textarea{font-family:arial;resize:vertical;width:100%;margin-bottom:10px;border-radius:1px;border:1px solid #d9d9d9;border-top:1px solid silver;font-size:13px;height:auto;overflow-y:auto;padding:1px}.VIpgJd-yAWNEb-L7lbkb textarea:focus{box-shadow:inset 0 1px 2px rgba(0,0,0,.3);border:1px solid #4d90fe;outline:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-IbE0S{margin-right:10px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp{min-height:25px;vertical-align:middle;padding-top:8px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp{margin-bottom:5px;margin-bottom:0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input{display:inline-block;min-width:54px;*min-width:70px;border:1px solid #dcdcdc;border:1px solid rgba(0,0,0,.1);text-align:center;color:#444;font-size:11px;font-weight:bold;height:27px;outline:0;padding:0 8px;vertical-align:middle;line-height:27px;margin:0 16px 0 0;box-shadow:0 1px 2px rgba(0,0,0,.1);-moz-box-shadow:0 1px 2px rgba(0,0,0,.1);-webkit-box-shadow:0 1px 2px rgba(0,0,0,.1);border-radius:2px;-webkit-transition:all .218s;transition:all .218s;background-color:#f5f5f5;background-image:-webkit-gradient(linear,left top,left bottom,from(#f5f5f5),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f5f5f5,#f1f1f1);background-image:linear-gradient(top,#f5f5f5,#f1f1f1);-webkit-user-select:none;-moz-user-select:none;cursor:default}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover{border:1px solid #c6c6c6;color:#222;-webkit-transition:all 0s;transition:all 0s;background-color:#f8f8f8;background-image:-webkit-gradient(linear,left top,left bottom,from(#f8f8f8),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f8f8f8,#f1f1f1);background-image:linear-gradient(top,#f8f8f8,#f1f1f1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active{border:1px solid #c6c6c6;color:#333;background-color:#f6f6f6;background-image:-webkit-gradient(linear,left top,left bottom,from(#f6f6f6),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f6f6f6,#f1f1f1);background-image:linear-gradient(top,#f6f6f6,#f1f1f1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus:active{box-shadow:inset 0 0 0 1px rgba(255,255,255,.5);-webkit-box-shadow:inset 0 0 0 1px rgba(255,255,255,.5);-moz-box-shadow:inset 0 0 0 1px rgba(255,255,255,.5)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe{outline:none;border:1px solid #4d90fe;z-index:4!important}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.gk6SMd{background-color:#eee;background-image:-webkit-gradient(linear,left top,left bottom,from(#eee),to(#e0e0e0));background-image:-webkit-linear-gradient(top,#eee,#e0e0e0);background-image:linear-gradient(top,#eee,#e0e0e0);box-shadow:inset 0 1px 2px rgba(0,0,0,.1);border:1px solid #ccc;color:#333}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf{color:white;border-color:#3079ed;background-color:#4d90fe;background-image:-webkit-gradient(linear,left top,left bottom,from(#4d90fe),to(#4787ed));background-image:-webkit-linear-gradient(top,#4d90fe,#4787ed);background-image:linear-gradient(top,#4d90fe,#4787ed)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active{border-color:#3079ed;background-color:#357ae8;background-image:-webkit-gradient(linear,left top,left bottom,from(#4d90fe),to(#357ae8));background-image:-webkit-linear-gradient(top,#4d90fe,#357ae8);background-image:linear-gradient(top,#4d90fe,#357ae8)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover{box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1);-webkit-box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1);-moz-box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover{border-color:#3079ed}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-mrxPge{color:#999;font-family:arial,sans-serif}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-W0vJo-fmcmS{color:#999;font-size:11px;font-family:arial,sans-serif;margin:15px 0 5px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-u0pjoe-fmcmS{color:#800;display:none;font-size:9pt}.VIpgJd-yAWNEb-VIpgJd-fmcmS-sn54Q{background-color:#c9d7f1;box-shadow:2px 2px 4px #99a;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;position:relative}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-xl07Ob{background:#fff;border:1px solid #ddd;box-shadow:0 2px 4px #99a;min-width:0;outline:none;padding:0;position:absolute;z-index:2000}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb{cursor:pointer;padding:2px 5px 5px;margin-right:0;border-style:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb:hover{background:#ddd}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb h1{font-size:100%;font-weight:bold;margin:4px 0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb strong{color:#345aad}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo{text-align:right;position:absolute;right:0;left:auto}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-j7LFlb-SIsrTd .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo{text-align:left;position:absolute;left:0;right:auto}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-fmcmS,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{background-color:#f1ea00;border-radius:4px;-webkit-border-radius:4px;-moz-border-radius:4px;box-shadow:rgba(0,0,0,.5) 3px 3px 4px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;color:#f1ea00;cursor:pointer;margin:-2px -2px -2px -3px;padding:2px 2px 2px 3px;position:relative}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{color:#222}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe{color:white;position:absolute!important}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{background-color:#c9d7f1;border-radius:4px 4px 0 0;-webkit-border-radius:4px 4px 0 0;-moz-border-radius:4px 4px 0 0;box-shadow:rgba(0,0,0,.5) 3px 3px 4px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;cursor:pointer;margin:-2px -2px -2px -3px;padding:2px 2px 3px 3px;position:relative}.VIpgJd-yAWNEb-L7lbkb span:focus{outline:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-DyVDA{background-color:transparent;border:1px solid #4d90fe;border-radius:0;-webkit-border-radius:0;-moz-border-radius:0;margin:-2px;padding:1px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-LzX3ef{border-left:2px solid red;margin-left:-2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-YIAiIb{border-right:2px solid red;margin-right:-2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf{padding:2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS{font-size:11px;padding:2px 2px 3px;margin:0;background-color:#fff;color:#333;border:1px solid #d9d9d9;border-top:1px solid #c0c0c0;display:inline-block;vertical-align:top;height:21px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;-webkit-border-radius:1px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:hover{border:1px solid #b9b9b9;border-top:1px solid #a0a0a0;box-shadow:inset 0 1px 2px rgba(0,0,0,.1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:focus{box-shadow:inset 0 1px 2px rgba(0,0,0,.3);outline:none;border:1px solid #4d90fe}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-sFeBqf{font-size:11px;padding:2px 6px 3px;margin:0 0 0 2px;height:21px}.VIpgJd-yAWNEb-hvhgNd{font-family:"Google Sans",Arial,sans-serif}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-l4eHX-i3jM8c{position:absolute;top:10px;left:14px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-l4eHX-SIsrTd{position:absolute;top:10px;right:14px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif-i3jM8c,.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif-SIsrTd{margin:16px;padding:0}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-IuizWc{margin:0 0 0 36px;padding:0;color:#747775;font-size:14px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif-SIsrTd .VIpgJd-yAWNEb-hvhgNd-IuizWc{text-align:right;margin:0 36px 0 0}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1{width:auto;padding:12px 0 0;color:#1f1f1f;font-size:16px;text-align:initial}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1 .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid{border-radius:0 0 12px 12px;margin:0;background:#f1f4f9;position:relative;min-height:50px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od{display:inline-block;width:77%;padding:12px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-UTujCb{color:#1f1f1f;font-size:12px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-UTujCb{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-eO9mKe{color:#444746;font-size:12px;padding-top:4px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-eO9mKe{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5{position:absolute;top:10px;right:5px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5 .VIpgJd-yAWNEb-SIsrTd{left:5px;right:auto}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-THI6Vb{fill:#0b57d0}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf{margin:-4px 2px 0 0;padding:2px 0 0;width:48px;height:48px;border:none;border-radius:24px;cursor:pointer;background:none}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf:hover{background:#e8ebec}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-aXYTce{display:none}sentinel{}</style><meta name="referrer" content="no-referrer"><style>img[src="data:,"],source[src="data:,"]{display:none!important}</style><link id="res-style" rel="stylesheet" href="/res/dist/res/style.css" type="text/css">
<title>MIT 6.041 Probabilistic Systems Analysis and Applied Probability</title>
</head>
<body>
    <div id="book-container">
        <h1 id="probability-models-and-axioms">1.概率模型和公理</h1><h1>1. Probability Models and Axioms</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:11 <a href="https://youtube.com/watch?v=j9WZyLZCBzs">https://youtube.com/watch?v=j9WZyLZCBzs</a></p><p> 12 years ago (Nov 10, 2012) — 51:11 <a href="https://youtube.com/watch?v=j9WZyLZCBzs">https://youtube.com/watch?v=j9WZyLZCBzs</a></p>
        <h2 id="intro">简介</h2><h2>Intro</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：好的，欢迎来到 6.041/6.431，这是关于概率模型等的课程。我是 John Tsitsiklis。我将教授这门课，我期待这将是一次愉快而有用的经历。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: OK, so welcome to
            6.041/6.431,
            the class on probability models and the like. I’m John Tsitsiklis. I will be teaching this class, and I’m
            looking forward to this being an enjoyable and also useful experience.</p>
        <p>我们有相当多的教职员工参与这门课程，包括你的复习老师和一群助教，但我想特别指出我们的首席助教 Uzoma，他是这门课的关键人物。一切都必须通过他。如果他不知道你在哪个复习部分，那么你根本就不存在，所以请记住这一点。
        </p><p>We have a fair amount of staff involved in this course, your recitation instructors and also a bunch of TAs,
            but
            I want to single out our head TA, Uzoma, who is the key person in this class. Everything has to go through
            him.
            If he doesn’t know in which recitation section you are, then simply you do not exist, so keep that in mind.
        </p>
        <h2 id="administrative-details">管理细节</h2><h2>Administrative Details</h2>
        <p>好的。</p><p>All right.</p>
        <p>所以我们想直接进入主题，但我只花几分钟来谈谈一些行政细节以及课程的运行方式。所以我们每周会讲两次课，我会使用老式的透明胶片。现在，您可以获得这些幻灯片的副本，上面有足够的空间供您做笔记。</p><p>So we want to jump right into the subject, but I’m going to take just a few minutes to talk about a few
            administrative details and how the course is run. So we’re going to have lectures twice a week and I’m going
            to
            use old fashioned transparencies. Now, you get copies of these slides with plenty of space for you to keep
            notes
            on them.</p>
        <p>充分利用幻灯片的一个有用方法是将其用作课堂内容的助记摘要。当然，幻灯片上不会有我要讲的所有内容，但通过查看幻灯片，您可以了解当前正在发生的事情。在进行背诵之前，复习一下幻灯片可能是个好主意。那么背诵中会发生什么呢？</p><p>A useful way of making good use of the slides is to use them as a sort of mnemonic summary of what happens in
            lecture. Not everything that I’m going to say is, of course, on the slides, but by looking them you get the
            sense of what’s happening right now. And it may be a good idea to review them before you go to recitation.
            So
            what happens in recitation?</p>
        <p>在复习课上，你的复习老师可能会复习一些理论，然后为你解决一些问题。然后你参加辅导课，在辅导课上，你会和你的助教一起在非常小的小组里见面。在辅导课上，你实际上是在助教和辅导课上同学的帮助下解决问题的。现在概率是一个棘手的话题。</p><p>In recitation, your recitation instructor is going to maybe review some of the theory and then solve some
            problems for you. And then you have tutorials where you meet in very small groups together with your TA. And
            what happens in tutorials is that you actually do the problem solving with the help of your TA and the help
            of
            your classmates in your tutorial section. Now probability is a tricky subject.</p>
        <p>你可能正在阅读课文、听讲座，一切都很有道理，等等，但直到你真正坐下来尝试解决问题时，你才会真正体会到其中的微妙之处和困难。所以解决问题是这门课的关键部分。</p><p>You may be reading the text, listening to lectures, everything makes perfect sense, and so on, but until you
            actually sit down and try to solve problems, you don’t quite appreciate the subtleties and the difficulties
            that
            are involved. So problem solving is a key part of this class.</p>
        <p>正是由于这个原因，教程才非常有用，因为在那里你实际上可以练习自己解决问题，而不是看着别人为你解决问题。</p><p>And tutorials are extremely useful just for this reason because that’s where you actually get the practice of
            solving problems on your own, as opposed to seeing someone else who’s solving them for you.</p>
        <h2 id="mechanics">机制</h2><h2>Mechanics</h2>
        <p>好的，但是，机械师，今天要发生的一个关键部分是，您将提交手中讲义末尾的日程表。</p><p>OK but, mechanics, a key part of what’s going to happen today is that you will turn in your schedule forms
            that
            are at the end of the handout that you have in your hands.</p>
        <p>然后，助教们会通宵加班，列出谁分到哪个班。当这种情况发生时，这个班的任何人，90% 的概率都会对他们的作业感到满意，10% 的概率会感到不满意。不过，现在不满意的人有一个选择。</p><p>Then, the TAs will be working frantically through the night, and they’re going to be producing a list of who
            goes
            into what section. And when that happens, any person in this class, with probability 90%, is going to be
            happy
            with their assignment and, with probability 10%, they’re going to be unhappy. Now, unhappy people have an
            option, though.</p>
        <p>你可以重新提交你的表格，并附上你的完整日程安排和约束条件，将其交还给助教主管，他会进一步调整并重新分配人员，之后，90% 的不开心的人会变得开心。10% 的人会不那么不开心。好的。那么，在这个过程结束时，随机一个人不开心的概率是多少？是 1%。太好了。很好。也许你不需要这门课。</p><p>You can resubmit your form together with your full schedule and constraints, give it back to the head TA, who
            will then do some further juggling and reassign people, and after that happens, 90% of those unhappy people
            will
            become happy. And 10% of them will be less unhappy. OK. So what’s the probability that a random person is
            going
            to be unhappy at the end of this process? It’s 1%. Excellent. Good. Maybe you don’t need this class.</p>
        <p>好的，1%。我们班大约有 100 人，所以大概会有 1 人不开心。我的意思是，无论你看生活中的任何地方，无论你看哪个群体，总会有一个人不开心，对吧？那么，我们能做些什么呢？好的。关于力学的另一个重要部分是仔细阅读我们关于合作、学术诚信等的声明。</p><p>OK, so 1%. We have about 100 people in this class, so there’s going to be about one unhappy person. I mean,
            anywhere you look in life, in any group you look at, there’s always one unhappy person, right? So, what can
            we
            do about it? All right. Another important part about mechanics is to read carefully the statement that we
            have
            about collaboration, academic honesty, and all that.</p>
        <p>鼓励你，与其他学生合作是个好主意。你可以参考现有的资料，但当你坐下来写解决方案时，你必须把其他东西放在一边，自己写。你不能抄袭别人给你的东西。</p><p>You’re encouraged, it’s a very good idea to work with other students. You can consult sources that are out
            there,
            but when you sit down and write your solutions you have to do that by setting things aside and just write
            them
            on your own. You cannot copy something that somebody else has given to you.</p>
        <p>一个原因是我们不喜欢这种情况发生，另一个原因是你不会对自己有任何好处。实际上，在这门课上取得好成绩的唯一方法是通过自己解决问题来获得大量练习。所以如果你不自己做这件事，那么当测验和考试时间到来时，事情就会变得困难。</p><p>One reason is that we’re not going to like it when it happens, and then another reason is that you’re not
            going
            to do yourself any favor. Really the only way to do well in this class is to get a lot of practice by
            solving
            problems yourselves. So if you don’t do that on your own, then when quiz and exam time comes, things are
            going
            to be difficult.</p>
        <h2 id="sections">章节</h2><h2>Sections</h2>
        <p>因此，正如我在这里提到的，我们将开设复习课，其中一些是针对 6.041 学生的，一些是针对 6.431 学生的，即该班的研究生。现在本科生可以参加研究生复习课。那里的情况是，课程可能会快一点，你可能会遇到本科生课程中没有涉及的更高级的问题。</p><p>So, as I mentioned here, we’re going to have recitation sections, that some of them are for 6.041 students,
            some
            are for 6.431 students, the graduate section of the class. Now undergraduates can sit in the graduate
            recitation
            sections. What’s going to happen there is that things may be just a little faster and you may be covering a
            problem that’s a little more advanced and is not covered in the undergrad sections.</p>
        <p>但如果你读的是研究生班，而你又是本科生，你仍然只负责本科生的课程。也就是说，你可以在课堂上完成本科生的课程，但可能会在不同班接触到相关知识。</p><p>But if you sit in the graduate section, and you’re an undergraduate, you’re still just responsible for the
            undergraduate material. That is, you can just do the undergraduate work in the class, but maybe be exposed
            at
            the different section.</p>
        <h2 id="style">风格</h2><h2>Style</h2>
        <p>好的。关于这堂课的风格，我想说几句。我们想集中讨论基本思想和概念。</p><p>OK. A few words about the style of this class. We want to focus on basic ideas and concepts.</p>
        <p>会有很多公式，但我们在这门课上试图让你真正理解这些公式的含义。一年后，当几乎所有公式都从你的记忆中消失时，你仍然掌握着基本概念。你可以理解它们，所以当你再次查找时，它们仍然有意义。</p><p>There’s going to be lots of formulas, but what we try to do in this class is to actually have you understand
            what
            those formulas mean. And, in a year from now when almost all of the formulas have been wiped out from your
            memory, you still have the basic concepts. You can understand them, so when you look things up again, they
            will
            still make sense.</p>
        <p>这不是那种插上电源就能得到的课程，在那种课程中，你会得到一串公式和数字，然后你插入公式就能得到答案。真正困难的部分通常是选择要使用哪些公式。你需要判断力，你需要直觉。许多概率问题，至少是那些有趣的问题，通常有很多不同的解决方案。有些非常长，有些非常短。</p><p>It’s not the plug and chug kind of class where you’re given a list of formulas, you’re given numbers, and you
            plug in and you get answers. The really hard part is usually to choose which formulas you’re going to use.
            You
            need judgment, you need intuition. Lots of probability problems, at least the interesting ones, often have
            lots
            of different solutions. Some are extremely long, some are extremely short.</p>
        <p>极短的课程通常需要你对正在发生的事情有更深的理解，这样你就可以选一条捷径并使用它。希望你能在这门课上培养这种技能。</p><p>The extremely short ones usually involve some kind of deeper understanding of what’s going on so that you can
            pick a shortcut and use it. And hopefully you are going to develop this skill during this class.</p>
        <h2 id="why-probability">为什么是概率</h2><h2>Why Probability</h2>
        <p>现在，我可以在这次讲座上花很多时间来谈论为什么这个主题很重要。我会简短地说，因为我认为这几乎是显而易见的。生活中发生的任何事情都是不确定的。</p><p>Now, I could spend a lot of time in this lecture talking about why the subject is important. I’ll keep it
            short
            because I think it’s almost obvious. Anything that happens in life is uncertain.</p>
        <p>任何地方都存在不确定性，所以无论你尝试做什么，你都需要有某种方法来处理或思考这种不确定性。而系统地做到这一点的方法是使用概率论提供给我们的模型。所以如果你是一名工程师，正在处理通信系统或信号处理，那么你基本上要面对的是与噪音的斗争。噪音是随机的，是不确定的。你如何对它进行建模？
        </p><p>There’s uncertainty anywhere, so whatever you try to do, you need to have some way of dealing or thinking
            about
            this uncertainty. And the way to do that in a systematic way is by using the models that are given to us by
            probability theory. So if you’re an engineer and you’re dealing with a communication system or signal
            processing, basically you’re facing a fight against noise. Noise is random, is uncertain. How do you model
            it?
        </p>
        <p>你如何处理它？如果你是一名经理，我想你会处理客户需求，这当然是随机的。或者你在处理股票市场，这肯定是随机的。或者你玩赌场，这同样是随机的，等等。你能想到的任何其他领域也是如此。但是，无论你来自哪个领域，基本概念和工具实际上都是一样的。</p><p>How do you deal with it? If you’re a manager, I guess you’re dealing with customer demand, which is, of
            course,
            random. Or you’re dealing with the stock market, which is definitely random. Or you play the casino, which
            is,
            again, random, and so on. And the same goes for pretty much any other field that you can think of. But,
            independent of which field you’re coming from, the basic concepts and tools are really all the same.</p>
        <p>因此，您可能会在书店看到各种各样的书籍，有针对科学家的概率、针对工程师的概率、针对社会科学家的概率、针对占星家的概率。好吧，所有这些书里面都有完全相同的模型、相同的方程式、相同的问题。它们只是使它们成为略有不同的应用问题。基本概念都是相同的，我们将以此为借口，不再过多地讨论特定领域的应用。</p><p>So you may see in bookstores that there are books, probability for scientists, probability for engineers,
            probability for social scientists, probability for astrologists. Well, what all those books have inside them
            is
            exactly the same models, the same equations, the same problems. They just make them somewhat different word
            problems. The basic concepts are just one and the same, and we’ll take this as an excuse for not going too
            much
            into specific domain applications.</p>
        <p>我们会有一些问题和例子，从某种意义上说，它们都源自现实世界的情况。但我们在这门课中并不是真的想培养解决特定领域问题的技能。相反，我们会尝试坚持对这个主题的一般理解。</p><p>We will have problems and examples that are motivated, in some loose sense, from real world situations. But
            we’re
            not really trying in this class to develop the skills for domain specific problems. Rather, we’re going to
            try
            to stick to general understanding of the subject.</p>
        <h2 id="class-details">课程详情</h2><h2>Class Details</h2>
        <p>好的。下一张幻灯片（在讲义中）提供了有关该课程的更多详细信息。
        </p><p>OK. So the next slide, of which you do have in your handout, gives you a few more details about the class.
        </p>
        <p>也许有一点需要在这里评论一下，那就是你确实需要阅读课文。对于微积分书籍，也许你只需要两页的总结就可以理解微积分中所有有趣的公式，而且你只需要这些公式就可以了。但是在这里，因为我们想要发展概念和直觉，所以实际上阅读文字，而不是仅仅浏览方程式，确实会有所不同。</p><p>Maybe one thing to comment here is that you do need to read the text. And with calculus books, perhaps you
            can
            live with a just a two page summary of all of the interesting formulas in calculus, and you can get by just
            with
            those formulas. But here, because we want to develop concepts and intuition, actually reading words, as
            opposed
            to just browsing through equations, does make a difference.</p>
        <p>一开始，这门课比较简单。当我们处理离散概率时，这就是我们第一次测验之前的内容，你们中的一些人可能不需要太系统地遵循这些内容就可以过关。但之后确实会变得困难得多。我要反复强调的是，你必须阅读文本才能真正理解这些内容。</p><p>In the beginning, the class is kind of easy. When we deal with discrete probability, that’s the material
            until
            our first quiz, and some of you may get by without being too systematic about following the material. But it
            does get substantially harder afterwards. And I would keep restating that you do have to read the text to
            really
            understand the material.</p>
        <h2 id="goals">目标</h2><h2>Goals</h2>
        <p>好的。现在我们可以开始讲座的正式部分了。</p><p>OK. So now we can start with the real part of the lecture.</p>
        <p>让我们设定今天的目标。概率或概率论是一个处理不确定性的框架，用于处理存在某种随机性的情况。因此，我们想要做的是，在今天的讲座结束时，让你知道你需要知道的一切，如何建立概率模型。</p><p>Let us set the goals for today. So probability, or probability theory, is a framework for dealing with
            uncertainty, for dealing with situations in which we have some kind of randomness. So what we want to do is,
            by
            the end of today’s lecture, to give you anything that you need to know how to set up what does it take to
            set up
            a probabilistic model.</p>
        <p>那么处理概率模型的基本规则是什么呢？那么，到这堂课结束时，您基本上已经收回了本学期一半的学费，对吧？所以我们将更详细地讨论概率模型。</p><p>And what are the basic rules of the game for dealing with probabilistic models? So, by the end of this
            lecture,
            you will have essentially recovered half of this semester’s tuition, right? So we’re going to talk about
            probabilistic models in more detail.</p>
        <p>样本空间基本上是对随机实验中可能发生的所有事情的描述，概率定律则描述了我们对哪些结果比其他结果更有可能发生的信念。概率定律必须遵循某些我们称之为概率公理的属性。所以今天讲座的主要部分是描述这些公理，也就是游戏规则，并考虑一些非常简单的例子。</p><p>The sample space, which is basically a description of all the things that may happen during a random
            experiment,
            and the probability law, which describes our beliefs about which outcomes are more likely to occur compared
            to
            other outcomes. Probability laws have to obey certain properties that we call the axioms of probability. So
            the
            main part of today’s lecture is to describe those axioms, which are the rules of the game, and consider a
            few
            really trivial examples.</p>
        <h2 id="sample-space">样本空间</h2><h2>Sample Space</h2>
        <p>好的，让我们开始讨论我们的议程。概率模型的第一部分是对实验样本空间的描述。所以我们做一个实验，实验的意思只是说外面发生了一些事情。发生的事情可能是抛硬币，可能是掷骰子，也可能是在玩纸牌游戏。所以我们确定了一个特定的实验。</p><p>OK, so let’s start with our agenda. The first piece in a probabilistic model is a description of the sample
            space
            of an experiment. So we do an experiment, and by experiment we just mean that just something happens out
            there.
            And that something that happens, it could be flipping a coin, or it could be rolling a dice, or it could be
            doing something in a card game. So we fix a particular experiment.</p>
        <p>我们列出了实验过程中可能发生的所有事情。所以我们写下了所有可能结果的列表。所以这是实验所有可能结果的列表。我使用“列表”这个词，但是，如果你想更正式一点，最好把这个列表看作一个集合。所以我们有一个集合。这个集合就是我们的样本空间。</p><p>And we come up with a list of all the possible things that may happen during this experiment. So we write
            down a
            list of all the possible outcomes. So here’s a list of all the possible outcomes of the experiment. I use
            the
            word “list,” but, if you want to be a little more formal, it’s better to think of that list as a set. So we
            have
            a set. That set is our sample space.</p>
        <p>它是一个集合，其元素是实验的可能结果。例如，如果你要抛硬币，你的样本空间将是正面，这是一个结果，反面也是一个结果。这个有两个元素的集合是实验的样本空间。好的。我们在设置样本空间时需要考虑什么？首先，列表应该是互斥的，总体上是详尽的。这是什么意思？</p><p>And it’s a set whose elements are the possible outcomes of the experiment. So, for example, if you’re dealing
            with flipping a coin, your sample space would be heads, this is one outcome, tails is one outcome. And this
            set,
            which has two elements, is the sample space of the experiment. OK. What do we need to think about when we’re
            setting up the sample space? First, the list should be mutually exclusive, collectively exhaustive. What
            does
            that mean?</p>
        <p>总体详尽意味着，无论实验中发生什么，你都会得到其中的一个结果。所以你没有忘记实验中可能发生的任何可能性。互斥意味着如果发生这种情况，那么就不会发生那种情况。所以在实验结束时，你应该能够向我指出这些结果中的一个，恰好一个，并说，这就是发生的结果。</p><p>Collectively exhaustive means that, no matter what happens in the experiment, you’re going to get one of the
            outcomes inside here. So you have not forgotten any of the possibilities of what may happen in the
            experiment.
            Mutually exclusive means that if this happens, then that cannot happen. So at the end of the experiment, you
            should be able to point out to me just one, exactly one, of these outcomes and say, this is the outcome that
            happened.</p>
        <p>好的。这些是基本要求。还有另一个要求稍微宽松一些。当你设置样本空间时，有时你确实可以自由选择描述它的细节。问题是，你要包含多少细节？让我们以这个抛硬币实验为例，思考以下样本空间。</p><p>OK. So these are sort of basic requirements. There’s another requirement which is a little more loose. When
            you
            set up your sample space, sometimes you do have some freedom about the details of how you’re going to
            describe
            it. And the question is, how much detail are you going to include? So let’s take this coin flipping
            experiment
            and think of the following sample space.</p>
        <p>一种可能的结果是正面，第二种可能的结果是反面并且下雨，第三种可能的结果是反面并且不下雨。所以这是我只抛一次硬币的实验的另一个可能的样本空间。这是一个合法的样本空间。这三种可能性是相互排斥的，并且是全面的。哪一个是正确的样本空间？是这个还是那个？</p><p>One possible outcome is heads, a second possible outcome is tails and it’s raining, and the third possible
            outcome is tails and it’s not raining. So this is another possible sample space for the experiment where I
            flip
            a coin just once. It’s a legitimate one. These three possibilities are mutually exclusive and collectively
            exhaustive. Which one is the right sample space? Is it this one or that one?</p>
        <p>好吧，如果你认为我在这个房间里抛硬币与外面的天气完全无关，那么你将坚持使用这个样本空间。另一方面，如果你有迷信的想法，认为雨水可能会对我的硬币产生影响，那么你可能会使用这种样本空间。所以你可能不会这么做，但严格来说，这是一个合理的选择。</p><p>Well, if you think that my coin flipping inside this room is completely unrelated to the weather outside,
            then
            you’re going to stick with this sample space. If, on the other hand, you have some superstitious belief that
            maybe rain has an effect on my coins, you might work with the sample space of this kind. So you probably
            wouldn’t do that, but it’s a legitimate option, strictly speaking.</p>
        <p>这个例子有点轻率，但这里出现的问题是一个基本问题，在科学和工程领域随处可见。每当你处理一个模型或一个情况时，该情况中都有无数的细节。当你想出一个模型时，你会选择一些细节保留在模型中，而有些细节则无关紧要。</p><p>Now this example is a little bit on the frivolous side, but the issue that comes up here is a basic one that
            shows up anywhere in science and engineering. Whenever you’re dealing with a model or with a situation,
            there
            are zillions of details in that situation. And when you come up with a model, you choose some of those
            details
            that you keep in your model, and some that you say, well, these are irrelevant.</p>
        <p>或者也许有一些小的影响，我可以忽略它们，而你将它们排除在模型之外。所以当你进入现实世界时，你肯定需要一些艺术元素和一些判断来设置一个合适的样本空间。</p><p>Or maybe there are small effects, I can neglect them, and you keep them outside your model. So when you go to
            the
            real world, there’s definitely an element of art and some judgment that you need to do in order to set up an
            appropriate sample space.</p>
        <h2 id="example">例子</h2><h2>Example</h2>
        <p>现在来看一个简单的例子。当然，基本的例子是硬币、纸牌和骰子。让我们来处理骰子。</p><p>So, an easy example now. So of course, the elementary examples are coins, cards, and dice. So let’s deal with
            dice.</p>
        <p>但为了使图表保持简洁，我们不会考虑六面骰子，而是考虑只有四个面的骰子。因此，你可以用四面体来实现这一点，其实这并不重要。基本上，这是一个骰子，当你掷它时，你会得到一、二、三或四的结果。然而，我要考虑的实验将由两次掷骰子组成。这里的关键点就在这里。</p><p>But to keep the diagram small, instead of a six sided die, we’re going to think about the die that only has
            four
            faces. So you can do that with a tetrahedron, doesn’t really matter. Basically, it’s a die that when you
            roll
            it, you get a result which is one, two, three or four. However, the experiment that I’m going to think about
            will consist of two rolls of a dice. A crucial point here.</p>
        <p>我掷了两次骰子，但我认为这只是一个实验，而不是两个不同的实验，也不是同一个实验的两次重复。所以这是一个大实验。在这个大实验中，各种事情都可能发生，比如我掷了一次骰子，然后我又掷了两次骰子。好的。那么这个实验的样本空间是什么？嗯，样本空间由可能的结果组成。</p><p>I’m rolling the die twice, but I’m thinking of this as just one experiment, not two different experiments,
            not a
            repetition twice of the same experiment. So it’s one big experiment. During that big experiment various
            things
            could happen, such as I’m rolling the die once, and then I’m rolling the die twice. OK. So what’s the sample
            space for that experiment? Well, the sample space consists of the possible outcomes.</p>
        <p>一种可能的结果是，第一次掷骰子结果是 2，第二次掷骰子结果是 3。在这种情况下，你得到的结果就是这个，先是 2，然后是 3。这是一种可能的结果。我描述的方式是，这个结果与这里的这个结果不同，这里的 3 之后是 2。如果你在玩西洋双陆棋，那么结果如何并不重要。</p><p>One possible outcome is that your first roll resulted in two and the second roll resulted in three. In which
            case, the outcome that you get is this one, a two followed by three. This is one possible outcome. The way
            I’m
            describing things, this outcome is to be distinguished from this outcome here, where a three is followed by
            two.
            If you’re playing backgammon, it doesn’t matter which one of the two happened.</p>
        <p>但是如果你正在处理一个概率模型，并且想要跟踪这个复合实验中发生的所有事情，那么就有充分的理由区分这两种结果。我的意思是，当这种情况发生时，它肯定与那种情况不同。2 后跟 3 与 3 后跟 2 不同。所以这是这个实验的正确样本空间，在这个实验中我们掷两次骰子。
        </p><p>But if you’re dealing with a probabilistic model that you want to keep track of everything that happens in
            this
            composite experiment, there are good reasons for distinguishing between these two outcomes. I mean, when
            this
            happens, it’s definitely something different from that happening. A two followed by a three is different
            from a
            three followed by a two. So this is the correct sample space for this experiment where we roll the die
            twice.
        </p>
        <p>它总共有 16 个元素，当然是一个有限集。有时，与其用列表、集合或此类图表来描述样本空间，不如用某种顺序的方式来描述实验。每当您的实验包含多个阶段时，给出一个图表来展示这些阶段是如何演变的，至少在视觉上是有用的。</p><p>It has a total of 16 elements and it’s, of course, a finite set. Sometimes, instead of describing sample
            spaces
            in terms of lists, or sets, or diagrams of this kind, it’s useful to describe the experiment in some
            sequential
            way. Whenever you have an experiment that consists of multiple stages, it might be useful, at least
            visually, to
            give a diagram that shows you how those stages evolve.</p>
        <p>这就是我们使用顺序描述或基于树的描述所做的，即绘制实验过程中可能演变的树。因此，在这棵树中，我考虑第一阶段，我掷第一个骰子，有四个可能的结果，一、二、三、四和 4。并且，考虑到发生的情况，假设在第一次掷骰子中，我得到了一个 1。</p><p>And that’s what we do by using a sequential description or a tree based description by drawing a tree of the
            possible evolutions during our experiment. So in this tree, I’m thinking of a first stage in which I roll
            the
            first die, and there are four possible results, one, two, three and four.and 4. And, given what happened,
            let’s
            say in the first roll, suppose I got a one.</p>
        <p>然后我掷第二个骰子，第二个骰子的结果有四种可能。可能的结果分别是一、二、三和四。那么这两个图之间的关系是什么呢？例如，结果二和三对应于树上的这条路径。所以这条路径对应于二和三。</p><p>Then I’m rolling the second dice, and there are four possibilities for what may happen to the second die. And
            the
            possible results are one, tow, three and four again. So what’s the relation between the two diagrams? Well,
            for
            example, the outcome two followed by three corresponds to this path on the tree. So this path corresponds to
            two
            followed by a three.</p>
        <p>任何路径都与特定结果相关联，任何结果都与特定路径相关联。并且，除了路径之外，您可能还想从该图的叶子的角度来思考。同样，将每个叶子视为一个可能的结果。当然，我们有 16 种结果，我们有 16 种结果。也许您注意到了我使用的语言的微妙之处。</p><p>Any path is associated to a particular outcome, any outcome is associated to a particular path. And, instead
            of
            paths, you may want to think in terms of the leaves of this diagram. Same thing, think of each one of the
            leaves
            as being one possible outcome. And of course we have 16 outcomes here, we have 16 outcomes here. Maybe you
            noticed the subtlety that I used in my language.</p>
        <p>我说我掷了第一个骰子，结果是 2。我没有使用“结果”这个词。我想保留“结果”这个词来表示整个实验结束时的总体结果。所以“2,3”是实验的结果。实验由几个阶段组成。2 是第一阶段的结果，3 是第二阶段的结果。</p><p>I said I rolled the first dice and the result that I get is a two. I didn’t use the word “outcome.” I want to
            reserve the word “outcome” to mean the overall outcome at the end of the overall experiment. So “2,3” is the
            outcome of the experiment. The experiment consisted of stages. Two was the result in the first stage, three
            was
            the result in the second stage.</p>
        <p>把所有这些结果放在一起，就得到了结果。好吧，也许我们在这里吹毛求疵，但保持概念正确是有用的。这个例子的特别之处在于，除了微不足道之外，它的样本空间也是有限的。总共有 16 种可能的结果。并非每个实验都有有限的样本空间。这是一个样本空间无限的实验。所以你在玩飞镖，目标是这个方块。</p><p>You put all those results together, and you get your outcome. OK, perhaps we are splitting hairs here, but
            it’s
            useful to keep the concepts right. What’s special about this example is that, besides being trivial, it has
            a
            sample space which is finite. There’s 16 possible total outcomes. Not every experiment has a finite sample
            space. Here’s an experiment in which the sample space is infinite. So you are playing darts and the target
            is
            this square.</p>
        <p>而且你玩这个游戏很熟练，所以你确信你的飞镖总是会落在方格内。但你的飞镖究竟会落在方格的哪个位置，这本身就是随机的。我们不知道它会落在哪里。这是不确定的。所以方格内的所有可能点都是实验的可能结果。</p><p>And you’re perfect at that game, so you’re sure that your darts will always fall inside the square. So, but
            where
            exactly your dart would fall inside that square, that itself is random. We don’t know what it’s going to be.
            It’s uncertain. So all the possible points inside the square are possible outcomes of the experiment.</p>
        <p>因此，实验的典型结果是一对数字 x、y，其中 x 和 y 是介于 0 和 1 之间的实数。现在有无数个实数，正方形中有无数个点，所以这是样本空间是无限集的一个例子。好的，我们稍后会重新讨论这个例子。</p><p>So a typical outcome of the experiment is going to a pair of numbers, x,y, where x and y are real numbers
            between
            zero and one. Now there’s infinitely many real numbers, there’s infinitely many points in the square, so
            this is
            an example in which our sample space is an infinite set. OK, so we’re going to revisit this example a little
            later.</p>
        <h2 id="assigning-probabilities">分配概率</h2><h2>Assigning probabilities</h2>
        <p>这两个例子说明了简单实验中的样本空间。</p><p>So these are two examples of what the sample space might be in simple experiments.</p>
        <p>现在，更重要的任务是研究这些可能的结果，并对它们的相对可能性做出一些陈述。与其他结果相比，哪种结果更有可能发生？我们这样做的方法是为结果分配概率。嗯，不完全是。假设您要做的只是为各个结果分配概率。如果您回到这个例子，并考虑一个特定的结果。</p><p>Now, the more important order of business is now to look at those possible outcomes and to make some
            statements
            about their relative likelihoods. Which outcome is more likely to occur compared to the others? And the way
            we
            do this is by assigning probabilities to the outcomes. Well, not exactly. Suppose that all you were to do
            was to
            assign probabilities to individual outcomes. If you go back to this example, and you consider one particular
            outcome.</p>
        <p>假设这个点。无限精确地精确击中这个点的概率是多少？直观地说，这个概率是零。因此，在任何合理的模型中，此图中的任何单个点的概率都应该为零。所以如果你只是告诉我任何单个结果的概率为零，你并没有真正告诉我很多可以参考的信息。</p><p>Let’s say this point. what would be the probability that you hit exactly this point to infinite precision?
            Intuitively, that probability would be zero. So any individual point in this diagram in any reasonable model
            should have zero probability. So if you just tell me that any individual outcome has zero probability,
            you’re
            not really telling me much to work with.</p>
        <p>因此，我们要做的是将概率分配给样本空间的子集，而不是将概率分配给单个结果。所以情况是这样的。我们有样本空间，即 omega，我们考虑样本空间的某个子集。称之为 A。我想为这个特定的子集分配一个数字，一个数值概率，它代表我对这个集合发生可能性的信念。好的。</p><p>For that reason, what instead we’re going to do is to assign probabilities to subsets of the sample space, as
            opposed to assigning probabilities to individual outcomes. So here’s the picture. We have our sample space,
            which is omega, and we consider some subset of the sample space. Call it A. And I want to assign a number, a
            numerical probability, to this particular subset which represents my belief about how likely this set is to
            occur. OK.</p>
        <p>“发生”是什么意思？我在这里介绍一种概率论中使用的语言。当我们谈论样本空间的子集时，我们通常称它们为事件，而不是子集。原因是它与描述正在发生的事情的语言配合得很好。所以结果是一个点。结果是随机的。</p><p>What do we mean “to occur?” And I’m introducing here a language that’s being used in probability theory. When
            we
            talk about subsets of the sample space, we usually call them events, as opposed to subsets. And the reason
            is
            because it works nicely with the language that describes what’s going on. So the outcome is a point. The
            outcome
            is random.</p>
        <p>结果可能在这个集合内，在这种情况下，如果我们得到的结果在这个集合内，我们就说事件 A 发生了。或者结果可能在集合之外，在这种情况下，我们说事件 A 没有发生。所以我们要给事件分配概率。现在，我们应该如何分配这个概率呢？嗯，概率是为了描述你对哪些集合比其他集合更有可能发生的信念。</p><p>The outcome may be inside this set, in which case we say that event A occurred, if we get an outcome inside
            here.
            Or the outcome may fall outside the set, in which case we say that event A did not occur. So we’re going to
            assign probabilities to events. And now, how should we do this assignment? Well, probabilities are meant to
            describe your beliefs about which sets are more likely to occur versus other sets.</p>
        <p>因此，有很多方法可以分配这些概率。但是这个游戏有一些基本规则。首先，我们希望概率是介于 0 和 1 之间的数字，因为这是通常的惯例。因此，概率为零意味着我们确定某事不会发生。概率为一意味着我们基本上确定某事会发生。因此，我们希望数字介于 0 和 1 之间。我们还希望有其他一些事情。</p><p>So there’s many ways that you can assign those probabilities. But there are some ground rules for this game.
            First, we want probabilities to be numbers between zero and one because that’s the usual convention. So a
            probability of zero means we’re certain that something is not going to happen. Probability of one means that
            we’re essentially certain that something’s going to happen. So we want numbers between zero and one. We also
            want a few other things.</p>
        <p>而其他一些事情将被封装在一组公理中。在这种情况下，“公理”的含义是任何合法的概率模型都应遵循的基本规则。您可以选择使用哪种概率。但是，无论您使用什么，它们都应该遵循某些一致性属性，因为如果它们遵循这些属性，那么您就可以继续进行有用的计算并进行一些有用的推理。
        </p><p>And those few other things are going to be encapsulated in a set of axioms. What “axioms” means in this
            context,
            it’s the ground rules that any legitimate probabilistic model should obey. You have a choice of what kind of
            probabilities you use. But, no matter what you use, they should obey certain consistency properties because
            if
            they obey those properties, then you can go ahead and do useful calculations and do some useful reasoning.
        </p>
        <p>那么这些属性是什么呢？首先，概率应该是非负的。好吗？这是我们的惯例。我们希望概率是介于 0 和 1 之间的数字。所以它们当然应该是非负的。事件 A 发生的概率应该是非负数。第二条公理是什么？整个样本空间的概率等于一。为什么这是有道理的？</p><p>So what are these properties? First, probabilities should be non negative. OK? That’s our convention. We want
            probabilities to be numbers between zero and one. So they should certainly be non negative. The probability
            that
            event A occurs should be a non negative number. What’s the second axiom? The probability of the entire
            sample
            space is equal to one. Why does this make sense?</p>
        <p>嗯，结果肯定是样本空间的一个元素，因为我们设置了一个样本空间，它是整体上详尽的。无论结果是什么，它都将成为样本空间的一个元素。我们确定事件 omega 将会发生。因此，我们通过说 omega 的概率等于一来表示这种确定性。到目前为止相当简单。</p><p>Well, the outcome is certain to be an element of the sample space because we set up a sample space, which is
            collectively exhaustive. No matter what the outcome is, it’s going to be an element of the sample space.
            We’re
            certain that event omega is going to occur. Therefore, we represent this certainty by saying that the
            probability of omega is equal to one. Pretty straightforward so far.</p>
        <h2 id="intersection-and-union">交集与并集</h2><h2>Intersection and Union</h2>
        <p>更有趣的公理是第三条规则。</p><p>The more interesting axiom is the third rule.</p>
        <p>在开始之前，先简单回顾一下。如果你有两个集合 A 和 B，那么 A 和 B 的交集由那些同时属于 A 和 B 的元素组成。我们这样表示。当你以概率的方式思考时，思考交集的方式是使用“和”这个词。这个事件，这个交集，就是 A 发生和 B 发生的事件。</p><p>Before getting into it, just a quick reminder. If you have two sets, A and B, the intersection of A and B
            consists of those elements that belong both to A and B. And we denote it this way. When you think
            probabilistically, the way to think of intersection is by using the word “and.” This event, this
            intersection,
            is the event that A occurred and B occurred.</p>
        <p>如果我在这里得到一个结果，A 发生了，B 也同时发生了。所以你可能会发现“和”这个词比“交集”这个词更方便一点。同样，我们对两个事件的并集也有一些符号，我们这样写。</p><p>If I get an outcome inside here, A has occurred and B has occurred at the same time. So you may find the word
            “and” to be a little more convenient than the word “intersection.” And similarly, we have some notation for
            the
            union of two events, which we write this way.</p>
        <p>两个集合或两个事件的并集是属于第一个集合、第二个集合或两者的所有元素的集合。当你谈论事件时，你可以使用“或”这个词。所以这是 A 发生或 B 发生的事件。这个“或”意味着它们也可能都发生了。好的。现在我们有了这个符号，那么第三条公理说了什么？</p><p>The union of two sets, or two events, is the collection of all the elements that belong either to the first
            set,
            or to the second, or to both. When you talk about events, you can use the word “or.” So this is the event
            that A
            occurred or B occurred. And this “or” means that it could also be that both of them occurred. OK. So now
            that we
            have this notation, what does the third axiom say?</p>
        <p>第三条公理说，如果我们有两个事件 A 和 B，它们没有共同元素。所以这是 A，这是 B，也许这就是我们的大样本空间。这两个事件没有共同元素。所以这两个事件的交集是空集。它们的交集中什么都没有。那么，A 和 B 的总概率必须等于各个概率的总和。</p><p>The third axiom says that if we have two events, A and B, that have no common elements. so here’s A, here’s
            B,
            and perhaps this is our big sample space. The two events have no common elements. So the intersection of the
            two
            events is the empty set. There’s nothing in their intersection. Then, the total probability of A together
            with B
            has to be equal to the sum of the individual probabilities.</p>
        <p>因此，A 发生或 B 发生的概率等于 A 发生的概率加上 B 发生的概率。因此，将概率想象成奶油奶酪。您有一磅奶油奶酪，总概率分配给整个样本空间。奶油奶酪分布在这个集合中。A 的概率是 A 上面有多少奶油奶酪。</p><p>So the probability that A occurs or B occurs is equal to the probability that A occurs plus the probability
            that
            B occurs. So think of probability as being cream cheese. You have one pound of cream cheese, the total
            probability assigned to the entire sample space. And that cream cheese is spread out over this set. The
            probability of A is how much cream cheese sits on top of A.</p>
        <p>B 的概率是 B 上面有多少东西。A 和 B 结合的概率是上面有多少奶油奶酪，显然是这里有多少东西和那里有多少东西的总和。所以概率就像奶油奶酪，或者就像质量一样。</p><p>Probability of B is how much sits on top of B. The probability of A union B is the total amount of cream
            cheese
            sitting on top of this and that, which is obviously the sum of how much is sitting here and how much is
            sitting
            there. So probabilities behave like cream cheese, or they behave like mass.</p>
        <p>例如，如果你考虑某个物体，这个由两部分组成的集合的质量显然是两个质量的总和。所以这个属性非常直观。这是一个很自然的属性。</p><p>For example, if you think of some material object, the mass of this set consisting of two pieces is obviously
            the
            sum of the two masses. So this property is a very intuitive one. It’s a pretty natural one to have.</p>
        <h2 id="are-these-axioms-enough">这些公理足够吗</h2><h2>Are these axioms enough</h2>
        <p>好的。这些公理足以满足我们的要求吗？我之前提到过，我们希望概率是 0 到 1 之间的数字。这里有一个公理告诉你概率是非负的。</p><p>OK. Are these axioms enough for what we want to do? I mentioned a while ago that we want probabilities to be
            numbers between zero and one. Here’s an axiom that tells you that probabilities are non negative.</p>
        <p>我们是否应该有另一个公理告诉我们概率小于或等于一？这是一个理想的特性。我们希望掌握它。好吧，为什么它不在那份名单上？好吧，从事公理制定工作的人是数学家，数学家往往非常简洁。如果你不必说的话，你就不会说。这就是这种情况。</p><p>Should we have another axiom that tells us that probabilities are less than or equal to one? It’s a desirable
            property. We would like to have it in our hands. OK, why is it not in that list? Well, the people who are in
            the
            axiom making business are mathematicians and mathematicians tend to be pretty laconic. You don’t say
            something
            if you don’t have to say it. And this is the case here.</p>
        <p>我们不需要那个额外的公理，因为我们可以从现有的公理中推导出它。事情是这样的。一个是整个样本空间的概率。这里我们使用第二个公理。现在样本空间由 A 和 A 的补集组成。好吗？当我写 A 的补集时，我指的是集合 omega 内的 A 的补集。</p><p>We don’t need that extra axiom because we can derive it from the existing axioms. Here’s how it goes. One is
            the
            probability over the entire sample space. Here we’re using the second axiom. Now the sample space consists
            of A
            together with the complement of A. OK? When I write the complement of A, I mean the complement of A inside
            of
            the set omega.</p>
        <p>所以我们有 omega，这是 A，这是 A 的补集，整个集合是 omega。好的。现在，下一步是什么？我下一步该做什么？我应该使用哪个公理？我们使用公理三，因为集合和该集合的补集是不相交的。它们没有任何共同元素。所以公理三适用并告诉我这是 A 的概率加上 A 补集的概率。</p><p>So we have omega, here’s A, here’s the complement of A, and the overall set is omega. OK. Now, what’s the
            next
            step? What should I do next? Which axiom should I use? We use axiom three because a set and the complement
            of
            that set are disjoint. They don’t have any common elements. So axiom three applies and tells me that this is
            the
            probability of A plus the probability of A complement.</p>
        <p>具体来说，A 的概率等于一减去 A 补集的概率，并且小于或等于一。为什么？因为根据第一条公理，概率是非负的。</p><p>In particular, the probability of A is equal to one minus the probability of A complement, and this is less
            than
            or equal to one. Why? Because probabilities are non negative, by the first axiom.</p>
        <h2 id="union-of-3-sets">3 个集合的并集</h2><h2>Union of 3 sets</h2>
        <p>好的。所以我们得到了我们想要的结论。概率总是小于或等于一，这是我们已有的三个公理的简单推论。</p><p>OK. So we got the conclusion that we wanted. Probabilities are always less than or equal to one, and this is
            a
            simple consequence of the three axioms that we have.</p>
        <p>这是一个非常好的论证，因为它实际上使用了这些公理中的每一个。这个论证很简单，但你必须使用所有这三个属性才能得到你想要的结论。好的。所以我们可以从公理中得到有趣的东西。我们能得到一些更有趣的东西吗？三个集合的并集怎么样？它应该有什么样的概率？所以这是一个由三部分组成的事件。</p><p>This is a really nice argument because it actually uses each one of those axioms. The argument is simple, but
            you
            have to use all of these three properties to get the conclusion that you want. OK. So we can get interesting
            things out of our axioms. Can we get some more interesting ones? How about the union of three sets? What
            kind of
            probability should it have? So here’s an event consisting of three pieces.</p>
        <p>我想说一下 A 合并 B 合并 C 的概率。我想说的是，这个概率等于三个单独概率的总和。我该怎么做呢？我有一个公理告诉我，我可以对两个事件做到这一点。我没有三个事件的公理。好吧，也许我可以修改一些东西，但仍然能够使用那个公理。</p><p>And I want to say something about the probability of A union B union C. What I would like to say is that this
            probability is equal to the sum of the three individual probabilities. How can I do it? I have an axiom that
            tells me that I can do it for two events. I don’t have an axiom for three events. Well, maybe I can massage
            things and still be able to use that axiom.</p>
        <p>这里有诀窍。三个集合的并集，你可以把它看作是形成前两个集合的并集，然后取与第三个集合的并集。好吗？所以取并集时，你可以按你想要的任何顺序取并集。所以这里我们有两个集合的并集。现在，根据假设，ABC 是不相交的，或者这就是我画它的方式。
        </p><p>And here’s the trick. The union of three sets, you can think of it as forming the union of the first two sets
            and
            then taking the union with the third set. OK? So taking unions, you can take the unions in any order that
            you
            want. So here we have the union of two sets. Now, ABC are disjoint, by assumption or that’s how I drew it.
        </p>
        <p>因此，如果 A、B 和 C 不相交，则 A 的并集 B 与 C 不相交。因此，这里我们得到了两个不相交集合的并集。因此，根据可加性公理，并集的概率将是第一个集合的概率加上第二个集合的概率。</p><p>So if A, B, and C are disjoint, then A union B is disjoint from C. So here we have the union of two disjoint
            sets. So by the additivity axiom, the probability of that the union is going to be the probability of the
            first
            set plus the probability of the second set.</p>
        <p>现在我可以再次使用可加性公理来写出这是 A 的概率加上 B 的概率加上 C 的概率。因此，通过使用这个针对两个集合陈述的公理，我们实际上可以为三个不相交集合的并集得出类似的属性。然后你可以根据需要多次重复此论证。
        </p><p>And now I can use the additivity axiom once more to write that this is probability of A plus probability of B
            plus probability of C. So by using this axiom which was stated for two sets, we can actually derive a
            similar
            property for the union of three disjoint sets. And then you can repeat this argument as many times as you
            want.
        </p>
        <p>它适用于十个不相交集合的并集、一百个不相交集合的并集以及任何有限个集合的并集。因此，如果 A1 到 An 不相交，则 A1 并 An 的概率等于各个集合概率的总和。好的。当我们处理有限集时，这是特殊情况。假设我只有一组有限的结果。</p><p>It’s valid for the union of ten disjoint sets, for the union of a hundred disjoint sets, for the union of any
            finite number of sets. So if A1 up to An are disjoint, then the probability of A1 union An is equal to the
            sum
            of the probabilities of the individual sets. OK. Special case of this is when we’re dealing with finite
            sets.
            Suppose I have just a finite set of outcomes.</p>
        <p>我把它们放在一个集合中，我对这个集合的概率很感兴趣。这就是我们的样本空间。有很多结果，但我从中选取一些，并将它们组合成一个集合。</p><p>I put them together in a set and I’m interested in the probability of that set. So here’s our sample space.
            There’s lots of outcomes, but I’m taking a few of these and I form a set out of them.</p>
        <h2 id="union-of-finite-sets">有限集的并集</h2><h2>Union of finite sets</h2>
        <p>在这张图中，这是一个由三个元素组成的集合。一般来说，它由 k 个元素组成。现在，一个有限集，我可以将其写成单个元素集的并集。</p><p>This is a set consisting of, in this picture, three elements. In general, it consists of k elements. Now, a
            finite set, I can write it as a union of single element sets.</p>
        <p>所以这里的集合是这个元素集与这个元素集和那个元素集的并集。所以这个集合的总概率将是这些元素集的概率之和。现在，对于一个元素集的概率，你需要在这里使用括号，因为概率是分配给集合的。</p><p>So this set here is the union of this one element set, together with this one element set together with that
            one
            element set. So the total probability of this set is going to be the sum of the probabilities of the one
            element
            sets. Now, probability of a one element set, you need to use the brackets here because probabilities are
            assigned to sets.</p>
        <p>但这有点乏味，所以这里我们稍微滥用符号，去掉那些括号，只写出这个单一、个别结果的概率。无论如何，从这个练习中得出的结论是，有限个可能结果集合的总概率，总概率等于各个元素概率的总和。所以这些基本上是概率论的公理。或者，它们几乎就是公理。</p><p>But this gets kind of tedious, so here one abuses notation a little bit and we get rid of those brackets and
            just
            write probability of this single, individual outcome. In any case, conclusion from this exercise is that the
            total probability of a finite collection of possible outcomes, the total probability is equal to the sum of
            the
            probabilities of individual elements. So these are basically the axioms of probability theory. Or, well,
            they’re
            almost the axioms.</p>
        <p>这里涉及一些微妙之处。其中一个微妙之处是，这个公理并不能完全满足我们想要做的所有事情。我们将在讲座结束时回顾这一点。</p><p>There are some subtleties that are involved here. One subtlety is that this axiom here doesn’t quite do the
            job
            for everything we would like to do. And we’re going to come back to this at the end of the lecture.</p>
        <h2 id="weird-sets">奇怪的布景</h2><h2>Weird sets</h2>
        <p>第二个微妙之处与怪异集合有关。我们说事件是样本空间的一个子集，并为事件分配概率。</p><p>A second subtlety has to do with weird sets. We said that an event is a subset of the sample space and we
            assign
            probabilities to events.</p>
        <p>这是否意味着我们要为样本空间的每个可能子集分配概率？理想情况下，我们希望这样做。不幸的是，这并不总是可行的。如果你取一个样本空间，比如正方形，正方形有很好的子集，你可以通过用线切割它来描述它们，等等。</p><p>Does this mean that we are going to assign probability to every possible subset of the sample space? Ideally,
            we
            would wish to do that. Unfortunately, this is not always possible. If you take a sample space, such as the
            square, the square has nice subsets, those that you can describe by cutting it with lines and so on.</p>
        <p>但它也有一些非常丑陋的子集，这些子集是无法想象的，无法想象的，但它们确实存在。这些非常奇怪的集合是无法以符合概率公理的方式为它们分配概率的。好的。所以这是一个非常非常好的观点，你可以立即忘记这堂课的其余部分。</p><p>But it does have some very ugly subsets, as well, that are impossible to visualize, impossible to imagine,
            but
            they do exist. And those very weird sets are such that there’s no way to assign probabilities to them in a
            way
            that’s consistent with the axioms of probability. OK. So this is a very, very fine point that you can
            immediately forget for the rest of this class.</p>
        <p>只有当你最终从事概率论理论方面的博士研究时，你才会遇到这些集合。因此，一些非常奇怪的集合没有分配概率，这只是一个数学上的微妙之处。但我们不会遇到这些集合，它们也不会出现在任何应用程序中。好的。现在让我们重新回顾我们的例子。让我们回到骰子的例子。我们有样本空间。</p><p>You will only encounter these sets if you end up doing doctoral work on the theoretical aspects of
            probability
            theory. So it’s just a mathematical subtlety that some very weird sets do not have probabilities assigned to
            them. But we’re not going to encounter these sets and they do not show up in any applications. OK. So now
            let’s
            revisit our examples. Let’s go back to the die example. We have our sample space.</p>
        <p>现在我们需要指定一个概率定律。你可以指定许多可能的概率定律。我在这里任意选择一个，其中我说每个可能结果的概率都是 1/16。好的。我为什么要建立这个模型？嗯，从经验上讲，如果你有制作精良的骰子，它们往往会表现出那样的表现。我们将在本课的后面部分回顾这种故事。</p><p>Now we need to assign a probability law. There’s lots of possible probability laws that you can assign. I’m
            picking one here, arbitrarily, in which I say that every possible outcome has the same probability of 1/16.
            OK.
            Why do I make this model? Well, empirically, if you have well manufactured dice, they tend to behave that
            way.
            We will be coming back to this kind of story later in this class.</p>
        <p>但我并不是说这是唯一的概率定律。你可能掷出一些奇怪的骰子，其中某些结果出现的可能性比其他结果更大。但为了简单起见，我们假设每个结果的概率都是 1/16。好的。现在我们有了样本空间和概率定律，我们实际上可以解决任何问题。我们可以回答可能向我们提出的任何问题。</p><p>But I’m not saying that this is the only probability law that there can be. You might have weird dice in
            which
            certain outcomes are more likely than others. But to keep things simple, let’s take every outcome to have
            the
            same probability of 1/16. OK. Now that we have in our hands a sample space and the probability law, we can
            actually solve any problem there is. We can answer any question that could be posed to us.</p>
        <p>例如，结果（即这对结果）是 1,1 或 1,2 的概率是多少。我们在这里讨论的是这个特定事件，1,1 或 1,2。所以这是一个由这两个项目组成的事件。根据我们刚才讨论的内容，有限结果集合的概率是它们各自概率的总和。</p><p>For example, what’s the probability that the outcome, which is this pair, is either 1,1 or 1,2. We’re talking
            here about this particular event, 1,1 or 1,2. So it’s an event consisting of these two items. According to
            what
            we were just discussing, the probability of a finite collection of outcomes is the sum of their individual
            probabilities.</p>
        <p>它们每个的概率都是 1/16，所以这个概率是 2/16。那么 x 等于 1 的事件的概率呢？x 是第一次掷骰子，所以这是第一次掷骰子等于 1 的概率。注意这里使用的语法。概率被分配给子集，也就是集合，所以我们认为这意味着 x 等于 1 的所有结果的集合。</p><p>Each one of them has probability of 1/16, so the probability of this is 2/16. How about the probability of
            the
            event that x is equal to one. x is the first roll, so that’s the probability that the first roll is equal to
            one. Notice the syntax that’s being used here. Probabilities are assigned to subsets, to sets, so we think
            of
            this as meaning the set of all outcomes such that x is equal to one.</p>
        <p>你怎么回答这个问题？你回到图片，试着想象或识别这个感兴趣的事件。x 等于 1 对应于此处的事件。这些都是 x 等于 1 的结果。有四种结果。每种结果的概率为 1/16，所以答案是 4/16。好的。x 加 y 为奇数的概率如何？好的。这需要多做一点工作。</p><p>How do you answer this question? You go back to the picture and you try to visualize or identify this event
            of
            interest. x is equal to one corresponds to this event here. These are all the outcomes at which x is equal
            to
            one. There’s four outcomes. Each one has probability 1/16, so the answer is 4/16. OK. How about the
            probability
            that x plus y is odd? OK. That will take a little bit more work.</p>
        <p>但是，如果你进入样本空间，你就会找出所有奇数和的结果。所以，这就是奇数和的某个地方，还有其他地方，我想，这已经穷尽了所有奇数和的可能结果。我们数一数。有多少个奇数和？一共有八个。每个奇数的概率是 1/16，总概率是 8/16。这是一个更难的问题。</p><p>But you go to the sample space and you identify all the outcomes at which the sum is an odd number. So that’s
            a
            place where the sum is odd, these are other places, and I guess that exhausts all the possible outcomes at
            which
            we have an odd sum. We count them. How many are there? There’s a total of eight of them. Each one has
            probability 1/16, total probability is 8/16. And harder question.</p>
        <p>两次掷骰结果中最小值为 2 的概率是多少？如果没有图表的帮助，你可能无法在脑海中计算出这个结果。但是一旦你有了图表，事情就变得简单了。你问这个问题。好吧，这是一个事件，两次掷骰结果中最小值为 2。这可能以几种方式发生。它可能发生的方式有哪些？</p><p>What is the probability that the minimum of the two rolls is equal to 2? This is something that you probably
            couldn’t do in your head without the help of a diagram. But once you have a diagram, things are simple. You
            ask
            the question. OK, this is an event, that the minimum of the two rolls is equal to two. This can happen in
            several ways. What are the several ways that it can happen?</p>
        <p>转到图表并尝试识别它们。因此，如果它们都是 2，则最小值等于 2。或者可能是 x 为 2 且 y 更大，或者 y 为 2 且 x 更大。好的。我想我们重新发现黄色和蓝色等于绿色，所以我们在这里看到总共有五种可能的结果。</p><p>Go to the diagram and try to identify them. So the minimum is equal to two if both of them are two’s. Or it
            could
            be that x is two and y is bigger, or y is two and x is bigger. OK. I guess we rediscover that yellow and
            blue
            make green, so we see here that there’s a total of five possible outcomes.</p>
        <p>这个事件的概率是 5/16。这是一个简单的例子，但我们在这个例子中遵循的程序实际上适用于您可能遇到的任何概率模型。你设置样本空间，你做出一个描述该样本空间的概率规律的陈述，然后有人问你关于各种事件的问题。</p><p>The probability of this event is 5/16. Simple example, but the procedure that we followed in this example
            actually applies to any probability model you might ever encounter. You set up your sample space, you make a
            statement that describes the probability law over that sample space, then somebody asks you questions about
            various events.</p>
        <p>你查看图片，识别这些事件，将它们记录下来，然后开始计数并计算你所考虑的结果的总概率。</p><p>You go to your pictures, identify those events, pin them down, and then start kind of counting and
            calculating
            the total probability for those outcomes that you’re considering.</p>
        <h2 id="discrete-uniform-law">离散均匀律</h2><h2>Discrete uniform law</h2>
        <p>这个例子是所谓的离散均匀定律的一个特例。如果所有结果的可能性都相同，则模型遵循离散均匀定律。但不一定非得如此。这只是概率定律的一个例子。</p><p>This example is a special case of what is called the discrete uniform law. The model obeys the discrete
            uniform
            law if all outcomes are equally likely. It doesn’t have to be that way. That’s just one example of a
            probability
            law.</p>
        <p>但是，如果所有结果都具有同等可能性，并且有 N 个结果，并且您有一个包含 n 个元素的集合 A，那么由于所有结果都具有同等可能性，因此每个元素的概率都是 N 的 1/1。为了让我们的概率加起来等于 1，每个结果都必须具有这么多的概率，并且有 n 个元素。这给出了您感兴趣的事件的概率。</p><p>But when things are that way, if all outcomes are equally likely and we have N of them, and you have a set A
            that
            has little n elements, then each one of those elements has probability one over capital N since all outcomes
            are
            equally likely. And for our probabilities to add up to one, each one must have this much probability, and
            there’s little n elements. That gives you the probability of the event of interest.</p>
        <p>因此，像上一张幻灯片中的问题以及更普遍的离散均匀定律下这里描述的这类问题，这些问题可以归结为计数。我的样本空间中有多少个元素？感兴趣的事件中有多少个元素？计数通常很简单，但对于某些问题来说，计数会变得相当复杂。</p><p>So problems like the one in the previous slide and more generally of the type described here under discrete
            uniform law, these problems reduce to just counting. How many elements are there in my sample space? How
            many
            elements are there inside the event of interest? Counting is generally simple, but for some problems it gets
            pretty complicated.</p>
        <p>几周后，我们将不得不用整堂课来讨论如何系统地计数。现在我们在上一个例子中遵循的程序与你在连续概率问题中遵循的程序相同。所以，回到我们的飞镖问题，我们得到正方形内的随机点。这就是我们的样本空间。我们需要指定一个概率定律。</p><p>And in a couple of weeks, we’re going to have to spend the whole lecture just on the subject of how to count
            systematically. Now the procedure we followed in the previous example is the same as the procedure you would
            follow in continuous probability problems. So, going back to our dart problem, we get the random point
            inside
            the square. That’s our sample space. We need to assign a probability law.</p>
        <p>由于缺乏想象力，我将概率定律视为子集的面积。因此，如果我们有两个面积相等的样本空间子集，那么我假设它们发生的可能性相同。它们落在这里的可能性与它们落在那里的概率相同。模型不必如此。
        </p><p>For lack of imagination, I’m taking the probability law to be the area of a subset. So if we have two subsets
            of
            the sample space that have equal areas, then I’m postulating that they are equally likely to occur. The
            probably
            that they fall here is the same as the probability that they fall there. The model doesn’t have to be that
            way.
        </p>
        <p>但如果我完全不知道哪些点比其他点更有可能，那么使用这个模型可能是合理的。因此面积相等意味着概率相等。如果面积是原来的两倍，概率也会是原来的两倍。这就是我们的模型。现在我们可以回答问题了。让我们回答一个简单的问题。结果恰好是这个点的概率是多少？</p><p>But if I have sort of complete ignorance of which points are more likely than others, that might be the
            reasonable model to use. So equal areas mean equal probabilities. If the area is twice as large, the
            probability
            is going to be twice as big. So this is our model. We can now answer questions. Let’s answer the easy one.
            What’s the probability that the outcome is exactly this point?</p>
        <p>当然，这个概率为零，因为单个点的面积为零。由于这个概率等于面积，所以概率为零。那么我们得到的点的坐标总和小于或等于 1/2 的概率呢？你如何处理它？好吧，你再看看这幅图，看看你的样本空间，试着描述你正在谈论的事件。</p><p>That of course is zero because a single point has zero area. And since this probability is equal to area,
            that’s
            zero probability. How about the probability that the sum of the coordinates of the point that we got is less
            than or equal to 1/2? How do you deal with it? Well, you look at the picture again, at your sample space,
            and
            try to describe the event that you’re talking about.</p>
        <p>总和小于 1/2 对应于得到低于这条线的结果，这条线是 x 加 y 等于 1/2 的线。因此，该线与轴的截距为 1/2 和 1/2。因此，您可以直观地描述事件，然后使用概率定律。我们的概率定律是，集合的概率等于该集合的面积。</p><p>The sum being less than 1/2 corresponds to getting an outcome that’s below this line, where this line is the
            line
            where x plus y equals to 1/2. So the intercepts of that line with the axis are 1/2 and 1/2. So you describe
            the
            event visually and then you use your probability law. The probability law that we have is that the
            probability
            of a set is equal to the area of that set.</p>
        <p>所以我们需要求这个三角形的面积，也就是 1/2 乘以 1/2 乘以 1/2，一半等于 1/8。好的。这两个例子告诉我们，有一幅图并用它把正在讨论的事件形象化总是有用的。一旦你掌握了概率定律，那么找到感兴趣事件的概率就只是计算的问题了。</p><p>So all we need to find is the area of this triangle, which is 1/2 times 1/2 times 1/2, half, equals to 1/8.
            OK.
            Moral from these two examples is that it’s always useful to have a picture and work with a picture to
            visualize
            the events that you’re talking about. And once you have a probability law in your hands, then it’s a matter
            of
            calculation to find the probabilities of an event of interest.</p>
        <p>当然，我们在这两个例子中所做的计算非常简单。有时计算可能要困难得多，但这是另一回事。例如，这是微积分的事情，或者擅长代数等等。就概率而言，你要做什么是很清楚的，然后你可能会面临更难的代数部分来实际进行计算。三角形的面积很容易计算。</p><p>The calculations we did in these two examples, of course, were very simple. Sometimes calculations may be a
            lot
            harder, but it’s a different business. It’s a business of calculus, for example, or being good in algebra
            and so
            on. As far as probability is concerned, it’s clear what you will be doing, and then maybe you’re faced with
            a
            harder algebraic part to actually carry out the calculations. The area of a triangle is easy to compute.</p>
        <p>如果我写下一个非常复杂的形状，那么您可能需要解决一个难以解决的积分问题才能找到该形状的面积，但这是属于另一类的内容，您现在可能已经掌握了。
        </p><p>If I had put down a very complicated shape, then you might need to solve a hard integration problem to find
            the
            area of that shape, but that’s stuff that belongs to another class that you have presumably mastered by now.
        </p>
        <h2 id="an-example">一个例子</h2><h2>An example</h2>
        <p>很好，好的。现在让我花几分钟时间回到我之前提出的观点。我说的是，我们关于可加性的公理可能还不够。</p><p>Good, OK. So now let me spend just a couple of minutes to return to a point that I raised before. I was
            saying
            that the axiom that we had about additivity might not quite be enough.</p>
        <p>让我们用下面的例子来说明我的意思。想象一下你不断抛硬币，直到第一次掷出正面的实验。这个实验的样本空间是什么？第一次抛硬币可能发生，也可能在第十次抛硬币时发生。第一次掷出正面可能在第百万次抛硬币时发生。
        </p><p>Let’s illustrate what I mean by the following example. Think of the experiment where you keep flipping a coin
            and
            you wait until you obtain heads for the first time. What’s the sample space of this experiment? It might
            happen
            the first flip, it might happen in the tenth flip. Heads for the first time might occur in the millionth
            flip.
        </p>
        <p>因此，这个实验的结果将是一个整数，并且没有限制。你可能要等很长时间才能得到结果。因此，自然样本空间是所有可能整数的集合。有人告诉你一些关于概率定律的信息。你必须等待 n 次翻转的概率等于 2 的负 n 次方。这是从哪里来的？那是另一个故事。</p><p>So the outcome of this experiment is going to be an integer and there’s no bound to that integer. You might
            have
            to wait very much until that happens. So the natural sample space is the set of all possible integers.
            Somebody
            tells you some information about the probability law. The probability that you have to wait for n flips is
            equal
            to two to the minus n.&nbsp;Where did this come from? That’s a separate story.</p>
        <p>它是从哪里来的？有人告诉我们这一点，这些概率在这里被绘制成 n 的函数。你被要求找出结果为偶数的概率。你如何计算这个概率？所以，结果为偶数的概率就是仅由偶数组成的子集的概率。</p><p>Where did it come from? Somebody tells this to us, and those probabilities are plotted here as a function of
            n.&nbsp;And you’re asked to find the probability that the outcome is an even number. How do you go about
            calculating
            that probability? So the probability of being an even number is the probability of the subset that consists
            of
            just the even numbers.</p>
        <p>所以它是这种类型的一个子集，包括 2、4 等等。所以任何理性的人都会说，得到 2、4 或 6 等等结果的概率等于得到 2 的概率加上得到 4 的概率加上得到 6 的概率等等。这些概率是已知的。所以在这里我必须做代数运算。</p><p>So it would be a subset of this kind, that includes two, four, and so on. So any reasonable person would say,
            well the probability of obtaining an outcome that’s either two or four or six and so on is equal to the
            probability of obtaining a two, plus the probability of obtaining a four, plus the probability of obtaining
            a
            six, and so on. These probabilities are given to us. So here I have to do my algebra.</p>
        <p>我加上这个几何级数，得到的答案是 1/3。任何理性的人都会这么做。但是只知道他们之前发布的公理的人可能会陷入困境。他们会在这一点上陷入困​​境。我们如何证明这一点？</p><p>I add this geometric series and I get an answer of 1/3. That’s what any reasonable person would do. But the
            person who only knows the axioms that they posted just a little earlier may get stuck. They would get stuck
            at
            this point. How do we justify this?</p>
        <p>我们有不相交集并集的这个属性，以及相应的属性，它告诉我们有限多个事物（结果）的总概率是它们各自概率的总和。但在这里我们将其应用于无限集合。无限多个点的概率等于每个点的概率的总和。</p><p>We had this property for the union of disjoint sets and the corresponding property that tells us that the
            total
            probability of finitely many things, outcomes, is the sum of their individual probabilities. But here we’re
            using it on an infinite collection. The probability of infinitely many points is equal to the sum of the
            probabilities of each one of these.</p>
        <p>为了证明这一步的合理性，我们需要引入一条额外的规则，即一条额外的公理，它告诉我们这一步实际上是合法的。这就是可数可加性公理，它比我们之前的可加性公理强一点，或者说强很多。它告诉我们，如果我们有一系列不相交的集合，并且我们想找到它们的总概率，那么我们可以将它们各自的概率相加。
        </p><p>To justify this step we need to introduce one additional rule, an additional axiom, that tells us that this
            step
            is actually legitimate. And this is the countable additivity axiom, which is a little stronger, or quite a
            bit
            stronger, than the additivity axiom we had before. It tells us that if we have a sequence of sets that are
            disjoint and we want to find their total probability, then we are allowed to add their individual
            probabilities.
        </p>
        <p>因此，情况可能如下所示。我们有一系列集合，A1、A2、A3 等等。我猜，为了将它们放入样本空间中，这些集合可能需要越来越小。它们是不相交的。我们有一系列这样的集合。落入这些集合中任意位置的总概率是它们各自概率的总和。</p><p>So the picture might be such as follows. We have a sequence of sets, A1, A2, A3, and so on. I guess in order
            to
            fit them inside the sample space, the sets need to get smaller and smaller perhaps. They are disjoint. We
            have a
            sequence of such sets. The total probability of falling anywhere inside one of those sets is the sum of
            their
            individual probabilities.</p>
        <p>这里涉及的一个关键微妙之处是，我们谈论的是一系列事件。我们所说的“序列”是指这些事件可以按顺序排列。我可以告诉你第一个事件、第二个事件、第三个事件等等。因此，如果你有这样一个事件集合，可以按第一、第二、第三等等的顺序排列，那么你可以将它们的概率相加，以找到它们合并的概率。</p><p>A key subtlety that’s involved here is that we’re talking about a sequence of events. By “sequence” we mean
            that
            these events can be arranged in order. I can tell you the first event, the second event, the third event,
            and so
            on. So if you have such a collection of events that can be ordered as first, second, third, and so on, then
            you
            can add their probabilities to find the probability of their union.</p>
        <p>所以这一点实际上比你现在可能意识到的要微妙一些，我将在下一讲开始时再次谈到这一点。现在，享受第一周的课程，祝你周末愉快。谢谢。</p><p>So this point is actually a little more subtle than you might appreciate at this point, and I’m going to
            return
            to it at the beginning of the next lecture. For now, enjoy the first week of classes and have a good
            weekend.
            Thank you.</p>
        <h1 id="conditioning-and-bayes-rule">2. 条件反射和贝叶斯规则</h1><h1>2. Conditioning and Bayes’ Rule</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:11 <a href="https://youtube.com/watch?v=TluTv5V0RmE">https://youtube.com/watch?v=TluTv5V0RmE</a></p><p> 12 years ago (Nov 10, 2012) — 51:11 <a href="https://youtube.com/watch?v=TluTv5V0RmE">https://youtube.com/watch?v=TluTv5V0RmE</a></p>
        <h2 id="unknown">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu JOHN TSISIKLIS：今天的议程如下。我们将进行快速回顾。然后我们将介绍一些非常重要的概念。这个想法是，所有信息都是。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu JOHN TSISIKLIS: So here’s the agenda
            for
            today. We’re going to do a very quick review. And then we’re going to introduce some very important
            concepts.
            The idea is that all information is.</p>
        <p>信息总是部分的。问题是，如果我们有一些关于随机实验的部分信息，我们该如何处理概率。我们将介绍条件概率的重要概念。然后我们将看到三种非常有用的使用它的方法。这些方法基本上对应于将问题分解为更简单部分的分而治之方法。</p><p>Information is always partial. And the question is what do we do to probabilities if we have some partial
            information about the random experiments. We’re going to introduce the important concept of conditional
            probability. And then we will see three very useful ways in which it is used. And these ways basically
            correspond to divide and conquer methods for breaking up problems into simpler pieces.</p>
        <p>还有一个更基本的工具，它允许我们使用条件概率进行推理，也就是说，如果我们获得一些关于某种现象的信息，我们可以推断出我们未见过的事物的什么？我们快速回顾一下。在建立随机实验模型时，要做的第一件事是列出实验的所有可能结果。</p><p>And also one more fundamental tool which allows us to use conditional probabilities to do inference, that is,
            if
            we get a little bit of information about some phenomenon, what can we infer about the things that we have
            not
            seen? So our quick review. In setting up a model of a random experiment, the first thing to do is to come up
            with a list of all the possible outcomes of the experiment.</p>
        <h2 id="unknown-1">未知</h2><h2>Unknown</h2>
        <p>所以这个列表就是我们所说的样本空间。它是一个集合。样本空间的元素是所有可能的结果。这些可能的结果必须彼此区分。它们是互斥的。要么发生一个，要么发生另一个，但不会同时发生。它们是详尽无遗的，也就是说，无论实验结果如何，它们都将成为样本空间的一个元素。</p><p>So that list is what we call the sample space. It’s a set. And the elements of the sample space are all the
            possible outcomes. Those possible outcomes must be distinguishable from each other. They’re mutually
            exclusive.
            Either one happens or the other happens, but not both. And they are collectively exhaustive, that is no
            matter
            what the outcome of the experiment is going to be an element of the sample space.</p>
        <p>然后我们上次讨论过，如何选择样本空间也需要一定的技巧，这取决于你想捕捉多少细节。这通常是比较容易的部分。然后更有趣的部分是为我们的模型分配概率，也就是对我们认为可能发生的事情和我们认为不可能发生的事情做出一些陈述。</p><p>And then we discussed last time that there’s also an element of art in how to choose your sample space,
            depending
            on how much detail you want to capture. This is usually the easy part. Then the more interesting part is to
            assign probabilities to our model, that is to make some statements about what we believe to be likely and
            what
            we believe to be unlikely.</p>
        <p>我们这样做的方法是将概率分配给样本空间的子集。因此，由于我们有样本空间，因此我们可能有一个子集 A。我们为该子集分配一个数字 P(A)，这是该事件发生的概率。或者说，当我们进行实验并得到结果时，这个概率就是结果恰好落在该事件内的概率。</p><p>The way we do that is by assigning probabilities to subsets of the sample space. So as we have our sample
            space
            here, we may have a subset A. And we assign a number to that subset P(A), which is the probability that this
            event happens. Or this is the probability that when we do the experiment and we get an outcome it’s the
            probability that the outcome happens to fall inside that event.</p>
        <h2 id="unknown-2">未知</h2><h2>Unknown</h2>
        <p>我们有某些规则，概率应该满足这些规则。它们是非负的。整个样本空间的概率等于一，这表示我们确信，无论如何，结果都会成为样本空间的一个元素。好吧，如果我们将右上角设置为穷尽所有可能性，情况应该就是这样。</p><p>We have certain rules that probabilities should satisfy. They’re non negative. The probability of the overall
            sample space is equal to one, which expresses the fact that we’re are certain, no matter what, the outcome
            is
            going to be an element of the sample space. Well, if we set the top right so that it exhausts all
            possibilities,
            this should be the case.</p>
        <p>然后，概率还有另一个有趣的性质，即如果我们有两个事件或两个不相交的子集，并且我们感兴趣的是其中一个发生的概率，即结果属于 A 或属于 B。对于不相交事件，这两个事件的总概率加在一起就是它们各自概率的总和。因此，概率就像质量一样。</p><p>And then there’s another interesting property of probabilities that says that, if we have two events or two
            subsets that are disjoint, and we’re interested in the probability, that one or the other happens, that is
            the
            outcome belongs to A or belongs to B. For disjoint events the total probability of these two, taken
            together, is
            just the sum of their individual probabilities. So probabilities behave like masses.</p>
        <p>由 A 和 B 组成的物体的质量是这两个物体质量的总和。或者你可以把概率看作面积。它们又具有相同的属性。A 和 B 的面积之和等于 A 的面积加上 B 的面积。</p><p>The mass of the object consisting of A and B is the sum of the masses of these two objects. Or you can think
            of
            probabilities as areas. They have, again, the same property. The area of A together with B is the area of A
            plus
            the area B.</p>
        <h2 id="unknown-3">未知</h2><h2>Unknown</h2>
        <p>但正如我们在上节课结束时所讨论的那样，掌握这个可加性性质的更通用版本是很有用的，如果我们取一个集合序列，它表示如下。A1、A2、A3、A4 等等。我们将所有这些集合放在一起。这是一个无限序列。</p><p>But as we discussed at the end of last lecture, it’s useful to have in our hands a more general version of
            this
            additivity property, which says the following, if we take a sequence of sets. A1, A2, A3, A4, and so on. And
            we
            put all of those sets together. It’s an infinite sequence.</p>
        <p>我们要求结果落在这个无限联合中的某个位置的概率，也就是说，我们要求结果属于这些集合之一的概率，并假设这些集合是不相交的，我们可以通过将各个集合的概率相加来再次找到整个集合的概率。所以这是一个很好且简单的属性。但它比你想象的要微妙一些。</p><p>And we ask for the probability that the outcome falls somewhere in this infinite union, that is we are asking
            for
            the probability that the outcome belongs to one of these sets, and assuming that the sets are disjoint, we
            can
            again find the probability for the overall set by adding up the probabilities of the individual sets. So
            this is
            a nice and simple property. But it’s a little more subtle than you might think.</p>
        <p>让我们通过以下示例来了解发生了什么。上次我们举了一个例子，我们将样本空间设为单位正方形。我们说让我们考虑一个概率定律，该定律说子集的概率就是该子集的面积。所以让我们考虑这个概率定律。好的。现在单位正方形是集合，让我这样画出来。</p><p>And let’s see what’s going on by considering the following example. We had an example last time where we take
            our
            sample space to be the unit square. And we said let’s consider a probability law that says that the
            probability
            of a subset is just the area of that subset. So let’s consider this probability law. OK. Now the unit square
            is
            the set let me just draw it this way.</p>
        <h2 id="unknown-4">未知</h2><h2>Unknown</h2>
        <p>单位正方形是由所有点组成的一个元素集的并集。因此，单位正方形由正方形内各个点的并集组成。因此，所有 x 和 y 的并集。好吗？因此，正方形由其中包含的所有点组成。现在让我们进行计算。一个是整个样本空间的概率，即单位正方形。</p><p>The unit square is the union of one element set consisting all of the points. So the unit square is made up
            by
            the union of the various points inside the square. So union over all x’s and y’s. OK? So the square is made
            up
            out of all the points that this contains. And now let’s do a calculation. One is the probability of our
            overall
            sample space, which is the unit square.</p>
        <p>现在单位正方形是这些东西的并集，根据我们的可加性公理，它是所有这些单元素集的概率之和。那么单元素集的概率是多少？这个单元素集的概率是多少？我们的结果恰好是那个特定点的概率是多少？嗯，它是那个集合的面积，也就是零。所以它只是零的和。</p><p>Now the unit square is the union of these things, which, according to our additivity axiom, is the sum of the
            probabilities of all of these one element sets. Now what is the probability of a one element set? What is
            the
            probability of this one element set? What’s the probability that our outcome is exactly that particular
            point?
            Well, it’s the area of that set, which is zero. So it’s just the sum of zeros.</p>
        <p>按照任何合理的定义，零的和都是零。所以我们刚刚证明了一等于零。好的。要么是概率论已经过时了，要么是我推导过程中出了点错误。好的，这个错误非常微妙，它就出现在这一步。我们通过说单位平方是所有这些集合的并集，在某种程度上应用了可加性公理。我们真的可以应用我们的可加性公理吗？</p><p>And by any reasonable definition the sum of zeros is zero. So we just proved that one is equal to zero. OK.
            Either probability theory is dead or there is some mistake in the derivation that I did. OK, the mistake is
            quite subtle and it comes at this step. We’re sort of applied the additivity axiom by saying that the unit
            square is the union of all those sets. Can we really apply our additivity axiom.</p>
        <h2 id="unknown-5">未知</h2><h2>Unknown</h2>
        <p>问题就在这里。可加性公理适用于我们有一系列不相交事件并取其并集的情况。这是一个集合序列吗？你能通过取其中的元素序列来构成整个单位正方形并覆盖整个单位正方形吗？好吧，如果你尝试，如果你开始查看一个元素点的序列，该序列将永远无法耗尽整个单位正方形。</p><p>Here’s the catch. The additivity axiom applies to the case where we have a sequence of disjoint events and we
            take their union. Is this a sequence of sets? Can you make up the whole unit square by taking a sequence of
            elements inside it and cover the whole unit square? Well if you try, if you start looking at the sequence of
            one
            element points, that sequence will never be able to exhaust the whole unit square.</p>
        <p>所以这背后有一个更深层次的原因。原因是无限集的大小并不完全相同。整数是一个无限集。你可以将整数按序列排列。但像单位平方这样的连续集是一个更大的集合。它被称为不可数。它拥有的元素比任何序列都要多。所以这里的并集不是这种类型的，在那里我们会有一个事件序列。</p><p>So there’s a deeper reason behind that. And the reason is that infinite sets are not all of the same size.
            The
            integers are an infinite set. And you can arrange the integers in a sequence. But the continuous set like
            the
            units square is a bigger set. It’s so called uncountable. It has more elements than any sequence could have.
            So
            this union here is not of this kind, where we would have a sequence of events.</p>
        <p>这是一种不同类型的并集。它涉及许多更多集合的并集。因此可数可加性公理不适用于这种情况。因为我们处理的不是集合序列。所以这是错误的步骤。因此，在某种程度上，您可能会认为这令人费解且非常混乱。</p><p>It’s a different kind of union. It’s a Union that involves a union of many, many more sets. So the countable
            additivity axiom does not apply in this case. Because, we’re not dealing with a sequence of sets. And so
            this is
            the incorrect step. So at some level you might think that this is puzzling and awfully confusing.</p>
        <h2 id="unknown-6">未知</h2><h2>Unknown</h2>
        <p>另一方面，如果你按照微积分中熟悉的方式来思考面积，那就没什么神秘的了。单位正方形上的每个点的面积为零。当你把所有的点放在一起时，它们就构成了一个面积有限的物体。所以这背后应该没有什么神秘之处。现在，这个讨论告诉我们一件有趣的事情，尤其是单个元素集的面积为零这一事实，如下。
        </p><p>On the other hand, if you think about areas of the way you’re used to them from calculus, there’s nothing
            mysterious about it. Every point on the unit square has zero area. When you put all the points together,
            they
            make up something that has finite area. So there shouldn’t be any mystery behind it. Now, one interesting
            thing
            that this discussion tells us, especially the fact that the single elements set has zero area, is the
            following.
        </p>
        <p>单个点的概率为零。在你做完实验并观察结果后，它将是一个单独的点。所以在那个实验中发生的事情是你最初认为发生概率为零的事情。所以如果你碰巧得到了一些特定的数字，你会说，“嗯，一开始，我对这些特定数字的看法是什么？我认为它们的概率为零。</p><p>Individual points have zero probability. After you do the experiment and you observe the outcome, it’s going
            to
            be an individual point. So what happened in that experiment is something that initially you thought had zero
            probability of occurring. So if you happen to get some particular numbers and you say, "Well, in the
            beginning,
            what did I think about those specific numbers? I thought they had zero probability.</p>
        <p>但这些特定的数字却确实发生了。”因此，从中可以得出一个教训，即零概率并不意味着不可能。它本身只是意味着极不可能。因此，零概率的事情确实会发生。在这样的连续模型中，实际上零概率结果就是发生的一切。而保险杠贴纸版本就是总是期待意想不到的事情。是吗？观众：约翰·齐斯克利斯：嗯，概率应该是一个实数。</p><p>But yet those particular numbers did occur." So one moral from this is that zero probability does not mean
            impossible. It just means extremely, extremely unlikely by itself. So zero probability things do happen. In
            such
            continuous models, actually zero probability outcomes are everything that happens. And the bumper sticker
            version of this is to always expect the unexpected. Yes? AUDIENCE: JOHN TSISIKLIS: Well, probability is
            supposed
            to be a real number.</p>
        <h2 id="unknown-7">未知</h2><h2>Unknown</h2>
        <p>所以它要么是零，要么是正数。所以你可以考虑接近该点的概率，这些概率很小，接近于零。这就是我们解释连续模型中概率的方法。但这是前面两章的内容。是吗？听众：我们如何解释零概率？如果我们可以这样使用模型，那么一概率呢？</p><p>So it’s either zero or it’s a positive number. So you can think of the probability of things just close to
            that
            point and those probabilities are tiny and close to zero. So that’s how we’re going to interpret
            probabilities
            in continuous models. But this is two chapters ahead. Yeah? AUDIENCE: How do we interpret probability of
            zero?
            If we can use models that way, then how about probability of one?</p>
        <p>那是极有可能但不一定一定发生？约翰·齐西克利斯：情况也是如此。例如，如果你在这个连续模型中询问 x、y 不等于零的概率，那么这个是整个正方形，除了一个点。所以这个的面积将是 1。但这个事件并不完全确定，因为零也是可能的。</p><p>That it’s extremely likely but not necessarily for certain? JOHN TSISIKLIS: That’s also the case. For
            example, if
            you ask in this continuous model, if you ask me for the probability that x, y, is different than the zero,
            zero
            this is the whole square, except for one point. So the area of this is going to be one. But this event is
            not
            entirely certain because the zero, zero outcome is also possible.</p>
        <p>所以再说一次，概率为 1 意味着基本确定性。但它仍然允许结果可能超出该集合的可能性。所以这些是当你有连续模型时发生的一些奇怪的事情。这就是为什么我们从离散模型开始这堂课，接下来的几周我们会花在离散模型上。好的。</p><p>So again, probability of one means essential certainty. But it still allows the possibility that the outcome
            might be outside that set. So these are some of the weird things that are happening when you have continuous
            models. And that’s why we start to this class with discrete models, on which would be spending the next
            couple
            of weeks. OK.</p>
        <h2 id="unknown-8">未知</h2><h2>Unknown</h2>
        <p>所以现在，一旦我们建立了概率模型，并且我们有了具有这些属性的合法概率定律，那么剩下的通常就很简单了。有人问你一个计算某个事件概率的问题。当你被告知一些关于概率定律的事情时，比如概率等于面积，然后你只需要计算。</p><p>So now once we have set up our probability model and we have a legitimate probability law that has these
            properties, then the rest is usually simple. Somebody asks you a question of calculating the probability of
            some
            event. While you were told something about the probability law, such as for example the probabilities are
            equal
            to areas, and then you just need to calculate.</p>
        <p>在这类例子中，有人会给你一个集合，你必须计算该集合的面积。所以剩下的只是计算，很简单。好了，现在是时候开始我们今天的主要工作了。起点如下。你对世界有所了解。根据你所知道的，你建立一个概率模型，并写下不同结果的概率。</p><p>In these type of examples somebody would give you a set and you would have to calculate the area of that set.
            So
            the rest is just calculation and simple. Alright, so now it’s time to start with our main business for
            today.
            And the starting point is the following. You know something about the world. And based on what you know when
            you
            set up a probability model and you write down probabilities for the different outcomes.</p>
        <p>然后发生了一些事情，有人告诉你更多关于这个世界的事情，给你一些新的信息。一般来说，这些新信息应该会改变你对已经发生或可能发生的事情的信念。所以每当我们得到新的信息，一些关于实验结果的部分信息时，我们都应该修正我们的信念。条件概率就是在我们得到一些信息后，在修正我们的信念后适用的概率。</p><p>Then something happens, and somebody tells you a little more about the world, gives you some new information.
            This new information, in general, should change your beliefs about what happened or what may happen. So
            whenever
            we’re given new information, some partial information about the outcome of the experiment, we should revise
            our
            beliefs. And conditional probabilities are just the probabilities that apply after the revision of our
            beliefs,
            when we’re given some information.</p>
        <h2 id="unknown-9">未知</h2><h2>Unknown</h2>
        <p>让我们将其变成一个数值示例。因此，在样本空间中，假设样本空间的这一部分概率为 3/6，这一部分概率为 2/6，而那部分概率为 1/6。我想这意味着在这里我们有零概率。这些就是我们对实验结果的初始信念。现在假设有人来告诉你事件 B 发生了。所以他们不会告诉你实验的全部结果。</p><p>So lets make this into a numerical example. So inside the sample space, this part of the sample space, let’s
            say
            has probability 3/6, this part has 2/6, and that part has 1/6. I guess that means that out here we have zero
            probability. So these were our initial beliefs about the outcome of the experiment. Suppose now that someone
            comes and tells you that event B occurred. So they don’t tell you the full outcome with the experiment.</p>
        <p>但他们只是告诉你，结果已知位于集合 B 中。那么，你当然应该以某种方式改变你的信念。你对可能发生和不可能发生的新信念将用这种符号表示出来。</p><p>But they just tell you that the outcome is known to lie inside this set B. Well then, you should certainly
            change
            your beliefs in some way. And your new beliefs about what is likely to occur and what is not is going to be
            denoted by this notation.</p>
        <p>这是事件 A 发生的条件概率，即假设我们被告知并且确信事件位于事件 B 内，则结果位于集合 A 内的概率。现在，一旦您被告知结果位于事件 B 内，那么我们以前的样本空间在某种程度上就无关紧要了。然后我们就有了样本空间，也就是集合 B。</p><p>This is the conditional probability that the event A is going to occur, the probability that the outcome is
            going
            to fall inside the set A given that we are told and we’re sure that the event lies inside the event B Now
            once
            you’re told that the outcome lies inside the event B, then our old sample space in some ways is irrelevant.
            We
            have then you sample space, which is just the set B.</p>
        <h2 id="unknown-10">未知</h2><h2>Unknown</h2>
        <p>我们确信结果将在 B 之内。例如，这个条件概率是多少？应该是 1。鉴于我告诉你 B 发生了，你确信 B 发生了，所以这具有单位概率。所以这里我们看到了信念修正的一个例子。最初，事件 B 的概率为 (2+1)/6</p><p>We are certain that the outcome is going to be inside B. For example, what is this conditional probability?
            It
            should be one. Given that I told you that B occurred, you’re certain that B occurred, so this has unit
            probability. So here we see an instance of revision of our beliefs. Initially, event B had the probability
            of
            (2+1)/6</p>
        <p>那是 1/2。最初，我们认为 B 的概率是 1/2。一旦我们被告知 B 发生了，B 的新概率就等于 1。好的。我们如何修改 A 发生的概率？所以我们将得到实验的结果。我们知道它在 B 内部。所以我们要么在这里得到一些东西，A 不会发生。要么在这里得到一些东西，A 确实发生了。</p><p>That’s 1/2. Initially, we thought B had probability 1/2. Once we’re told that B occurred, the new probability
            of
            B is equal to one. OK. How do we revise the probability that A occurs? So we are going to have the outcome
            of
            the experiment. We know that it’s inside B. So we will either get something here, and A does not occur. Or
            something inside here, and A does occur.</p>
        <p>假设我们在 B 中，结果出现在这里的可能性有多大？我们将这样考虑。在我们的初始模型中，集合 B 的这一部分（其中也出现了 A）出现的可能性是 B 的那部分的两倍。因此，这里出现的结果总体上是外面出现的结果的两倍。</p><p>What’s the likelihood that, given that we’re inside B, the outcome is inside here? Here’s how we’re going to
            think about. This part of this set B, in which A also occurs, in our initial model was twice as likely as
            that
            part of B. So outcomes inside here collectively were twice as likely as outcomes out there.</p>
        <h2 id="unknown-11">未知</h2><h2>Unknown</h2>
        <p>因此，我们将保持相同的比例，假设我们在集合 B 中，我们仍然希望这里的结果比那里的结果的概率高出一倍。因此，概率的比例应该是二比一。这些概率应该加起来等于一，因为它们加在一起构成了 B 的条件概率。因此，条件概率应该是 2/3 出现在这里，1/3 出现在那里。</p><p>So we’re going to keep the same proportions and say, that given that we are inside the set B, we still want
            outcomes inside here to be twice as likely outcomes there. So the proportion of the probabilities should be
            two
            versus one. And these probabilities should add up to one because together they make the conditional
            probability
            of B. So the conditional probabilities should be 2/3 probability of being here and 1/3 probability of being
            there.</p>
        <p>这就是我们修改概率的方法。这是一种合理的、直观合理的修改方法。让我们将我们所做的转化为定义。定义如下，假设 B 发生，A 的条件概率计算如下。我们看看 B 的总概率。从这里的概率中，有多少比例分配给事件 A 也发生的点？</p><p>That’s how we revise our probabilities. That’s a reasonable, intuitively reasonable, way of doing this
            revision.
            Let’s translate what we did into a definition. The definition says the following, that the conditional
            probability of A given that B occurred is calculated as follows. We look at the total probability of B. And
            out
            of that probability that was inside here, what fraction of that probability is assigned to points for which
            the
            event A also occurs?</p>
        <p>它给出的数字和我们用这个启发式论证得到的数字一样吗？在这个例子中，A 与 B 相交的概率是 2/6，除以 B 的总概率，即 3/6，所以结果是 2/3，这与我们之前得到的答案一致。所以前者确实符合我们试图做的事情。一个小技术细节。</p><p>Does it give us the same numbers as we got with this heuristic argument? Well in this example, probability of
            A
            intersection B is 2/6, divided by total probability of B, which is 3/6, and so it’s 2/3, which agrees with
            this
            answer that’s we got before. So the former indeed matches what we were trying to do. One little technical
            detail.</p>
        <h2 id="unknown-12">未知</h2><h2>Unknown</h2>
        <p>如果事件 B 的概率为零，那么这里的比例就毫无意义了。所以在这种情况下，我们说条件概率没有定义。现在你可以把这个定义解开，然后把它写成这种形式。A 和 B 相交的概率是 B 的概率乘以条件概率。所以这只是定义的结果，但它有一个很好的解释。把概率看作频率。</p><p>If the event B has zero probability, and then here we have a ratio that doesn’t make sense. So in this case,
            we
            say that conditional probabilities are not defined. Now you can take this definition and unravel it and
            write it
            in this form. The probability of A intersection B is the probability of B times the conditional probability.
            So
            this is just consequence of the definition but it has a nice interpretation. Think of probabilities as
            frequencies.</p>
        <p>如果我反复做实验，A 和 B 同时发生的情况会占多少比例？嗯，B 发生的时间会占一定比例。在 B 发生的那些时间中，还有一部分实验中 A 也会发生。因此，请按如下方式解释条件概率。</p><p>If I do the experiment over and over, what fraction of the time is it going to be the case that both A and B
            occur? Well, there’s going to be a certain fraction of the time at which B occurs. And out of those times
            when B
            occurs, there’s going to be a further fraction of the experiments in which A also occurs. So interpret the
            conditional probability as follows.</p>
        <p>你只需观察那些恰好发生 B 的实验。再观察那些已经发生 B 的实验中，事件 A 也发生的概率是多少。这个等式有一个对称版本。事件 B 和 A 之间存在对称性。所以你也有这种反向关系。好的，那么我们用这些条件概率来做什么呢？首先，我要说一句。条件概率就像普通概率一样。</p><p>You only look at those experiments at which B happens to occur. And look at what fraction of those
            experiments
            where B already occurred, event A also occurs. And there’s a symmetrical version of this equality. There’s
            symmetry between the events B and A. So you also have this relation that goes the other way. OK, so what do
            we
            use these conditional probabilities for? First, one comment. Conditional probabilities are just like
            ordinary
            probabilities.</p>
        <h2 id="unknown-13">未知</h2><h2>Unknown</h2>
        <p>它们是适用于已知事件 B 发生的新宇宙的新概率。所以我们有一个原始概率模型。我们被告知 B 会发生。我们修改我们的模型。我们的新模型应该仍然是合法的概率模型。因此它应该满足普通概率所满足的各种属性。</p><p>They’re the new probabilities that apply in a new universe where event B is known to have occurred. So we had
            an
            original probability model. We are told that B occurs. We revise our model. Our new model should still be
            legitimate probability model. So it should satisfy all sorts of properties that ordinary probabilities do
            satisfy.</p>
        <p>例如，如果 A 和 B 是不相交事件，那么我们知道 A 和 B 合并的概率等于 A 的概率加上 B 的概率。现在如果我告诉你发生了某个事件 C，我们就会置身于一个发生事件 C 的新宇宙中。我们有该宇宙的新概率。这些是条件概率。条件概率也满足这种属性。</p><p>So for example, if A and B are disjoint events, then we know that the probability of A union B is equal to
            the
            probability of A plus probability of B. And now if I tell you that a certain event C occurred, we’re placed
            in a
            new universe where event C occurred. We have new probabilities for that universe. These are the conditional
            probabilities. And conditional probabilities also satisfy this kind of property.</p>
        <p>因此，这只是我们通常的可加性公理，但应用于一个新模型，在这个模型中，我们被告知事件 C 发生了。因此，条件概率的味道或气味与普通概率没有任何不同。给定特定事件 B，条件概率只是在我们的样本空间上形成一个概率定律。这是一个不同的概率定律，但它仍然是一个具有所有所需属性的概率定律。</p><p>So this is just our usual additivity axiom but the applied in a new model, in which we were told that event C
            occurred. So conditional probabilities do not taste or smell any different than ordinary probabilities do.
            Conditional probabilities, given a specific event B, just form a probability law on our sample space. It’s a
            different probability law but it’s still a probability law that has all of the desired properties.</p>
        <h2 id="unknown-14">未知</h2><h2>Unknown</h2>
        <p>好的，那么条件概率在哪里出现呢？它们确实出现在测验中，也确实出现在愚蠢的问题中。那么我们就从这个开始吧。我们有上次的这个例子。掷两次骰子，所有可能的角色对都具有相同的可能性，因此这个方格中的每个元素都有 1/16 的概率。所以所有元素都具有相同的可能性。这是我们最初的模型。</p><p>OK, so where do conditional probabilities come up? They do come up in quizzes and they do come up in silly
            problems. So let’s start with this. We have this example from last time. Two rolls of a die, all possible
            pairs
            of roles are equally likely, so every element in this square has probability of 1/16. So all elements are
            equally likely. That’s our original model.</p>
        <p>然后有人过来告诉我们，两次掷骰结果中最小的数等于零。那是什么事件？最小数等于零可以以多种方式发生，如果我们掷出两个零，或者如果我们掷出一个零和……抱歉，如果我们掷出两个，或者掷出一个两个和更大的数。所以这是我们的新事件 B。红色事件是事件 B。</p><p>Then somebody comes and tells us that the minimum of the two rolls is equal to zero. What’s that event? The
            minimum equal to zero can happen in many ways, if we get two zeros or if we get a zero and. sorry, if we get
            two’s, or get a two and something larger. And so the is our new event B. The red event is the event B.</p>
        <p>现在我们要计算这个新宇宙中的概率。例如，你可能对这个问题感兴趣，关于两次掷骰中最大值的问题。在新宇宙中，最大值等于一的概率是多少？最大值等于一就是这个黑色事件。鉴于我们被告知 B 发生了，这个黑色事件不可能发生。所以这个概率等于零。</p><p>And now we want to calculate probabilities inside this new universe. For example, you may be interested in
            the
            question, questions about the maximum of the two rolls. In the new universe, what’s the probability that the
            maximum is equal to one? The maximum being equal to one is this black event. And given that we’re told that
            B
            occurred, this black events cannot happen. So this probability is equal to zero.</p>
        <h2 id="unknown-15">未知</h2><h2>Unknown</h2>
        <p>如果事件 B 发生，最大值等于 2，结果会怎样？好的，我们可以在这里使用这个定义。它将是最大值等于 2 且 B 发生的概率除以 B 发生的概率。最大值等于 2 的概率。好的，最大值等于 2 的事件是什么？让我们画出来。这将是蓝色事件。</p><p>How about the maximum being equal to two, given that event B? OK, we can use the definition here. It’s going
            to
            be the probability that the maximum is equal to two and B occurs divided by the probability of B. The
            probability that the maximum is equal to two. OK, what’s the event that the maximum is equal to two? Let’s
            draw
            it. This is going to be the blue event.</p>
        <p>如果我们得到任何蓝点，最大值就等于 2。所以两个事件的交点就是红色事件和蓝色事件的交点。它们的交点只有一个点。所以发生交点的概率是 1/16。这是分子。分母呢？</p><p>The maximum is equal to two if we get any of those blue points. So the intersection of the two events is the
            intersection of the red event and the blue event. There’s only one point in their intersection. So the
            probability of that intersection happening is 1/16. That’s the numerator. How about the denominator?</p>
        <p>事件 B 由五个元素组成，每个元素的概率为 1/16。所以是 5/16。所以答案是 1/5。我们能以更快的方式得到这个答案吗？是的。事情是这样的。我们试图找到在 B 发生的情况下得到这一点的条件概率。B 由五个元素组成。当我们开始时，这五个元素的概率都是相等的，所以它们在之后仍然保持相等的概率。</p><p>The event B consists of five elements, each one of which had probability of 1/16. So that’s 5/16. And so the
            answer is 1/5. Could we have gotten this answer in a faster way? Yes. Here’s how it goes. We’re trying to
            find
            the conditional probability that we get this point, given that B occurred. B consist of five elements. All
            of
            those five elements were equally likely when we started, so they remain equally likely afterwards.</p>
        <h2 id="unknown-16">未知</h2><h2>Unknown</h2>
        <p>因为当我们定义条件概率时，我们会在集合内保持相同的比例。所以五个红色元素的概率是相等的。它们在条件世界中的概率是相等的。所以条件事件 B 发生后，这五个元素中的每一个都有相同的概率。所以我们实际得到这个点的概率将是 1/5。这就是捷径。</p><p>Because when we define conditional probabilities, we keep the same proportions inside the set. So the five
            red
            elements were equally likely. They remain equally likely in the conditional world. So conditional event B
            having
            happened, each one of these five elements has the same probability. So the probability that we actually get
            this
            point is going to be 1/5. And so that’s the shortcut.</p>
        <p>更一般地，只要在初始样本空间中有一个均匀分布，当你以某个事件为条件时，你的新分布仍将是均匀的，但对于我们考虑的较小事件而言。因此，我们从大正方形上的均匀分布开始，最终只在红点上得到均匀分布。然而，现在除了愚蠢的问题之外，条件概率还出现在真实而有趣的情况中。</p><p>More generally, whenever you have a uniform distribution on your initial sample space, when you condition on
            an
            event, your new distribution is still going to be uniform, but on the smaller events of that we considered.
            So
            we started with a uniform distribution on the big square and we ended up with a uniform distribution just on
            the
            red point. Now besides silly problems, however, conditional probabilities show up in real and interesting
            situations.</p>
        <p>这个例子将让你了解这是如何发生的。好吧。实际上，在这个例子中，我实际上不是从常规概率的概率模型开始，而是从条件概率的角度来定义模型。我们将看看这是如何做到的。故事是这样的。在你所观察的天空的特定区域，可能有一架飞机在天空中飞行。</p><p>And this example is going to give you some idea of how that happens. OK. Actually, in this example, instead
            of
            starting with a probability model in terms of regular probabilities, I’m actually going to define the model
            in
            terms of conditional probabilities. And we’ll see how this is done. So here’s the story. There may be an
            airplane flying up in the sky, in a particular sector of the sky that you’re watching.</p>
        <h2 id="unknown-17">未知</h2><h2>Unknown</h2>
        <p>有时有，有时没有。根据经验，当你抬头看时，飞机在上面飞行的概率为 5%，而上面没有飞机的概率为 95%。所以事件 A 就是飞机在上面飞行的事件。现在你买了这个可以抬头看的神奇雷达。
        </p><p>Sometimes there is one sometimes there isn’t. And from experience you know that when you look up, there’s
            five
            percent probability that the plane is flying above there and 95% probability that there’s no plane up there.
            So
            event A is the event that the plane is flying out there. Now you bought this wonderful radar that’s looks
            up.
        </p>
        <p>制造商的规格说明中会告诉你，如果飞机在空中，你的雷达有 99% 的概率会记录下一些东西，屏幕上会出现一个光点。而有 1% 的概率它不会记录任何东西。所以这幅图的特定部分是一个独立的概率模型，它描述了在飞机在空中的情况下你的雷达会做什么。所以我告诉你，飞机就在空中。</p><p>And you’re told in the manufacturer’s specs that, if there is a plane out there, your radar is going to
            register
            something, a blip on the screen with probability 99%. And it will not register anything with probability one
            percent. So this particular part of the picture is a self contained probability model of what your radar
            does in
            a world where a plane is out there. So I’m telling you that the plane is out there.</p>
        <p>所以我们现在要处理的是条件概率，因为我给了你一些特定的信息。如果知道飞机就在那儿，你的雷达就会这样运行，99% 的概率会探测到它，1% 的概率会错过它。所以这幅图是一个独立的概率模型。概率加起来等于 1。但它是一个更大模型的一部分。同样，还有另一种可能性。</p><p>So we’re now dealing with conditional probabilities because I gave you some particular information. Given
            this
            information that the plane is out there, that’s how your radar is going to behave with probability 99% is
            going
            to detect it, with probability one percent is going to miss it. So this piece of the picture is a self
            contained
            probability model. The probabilities add up to one. But it’s a piece of a larger model. Similarly, there’s
            the
            other possibility.</p>
        <h2 id="unknown-18">未知</h2><h2>Unknown</h2>
        <p>也许飞机不在那里，制造商的规格会告诉你一些关于误报的信息。误报是指飞机不在那里，但由于某种原因，你的雷达接收到了一些噪音或其他东西，并在屏幕上显示了一个光点。假设这种情况发生的概率为 10%。而你的雷达给出正确答案的概率为 90%。</p><p>Maybe a plane is not up there and the manufacturer specs tell you something about false alarms. A false alarm
            is
            the situation where the plane is not there, but for some reason your radar picked up some noise or whatever
            and
            shows a blip on the screen. And suppose that this happens with probability ten percent. Whereas with
            probability
            90% your radar gives the correct answer.</p>
        <p>因此，这有点像是飞机将要发生什么的模型，我们给出了关于飞机的概率，也给出了关于雷达行为的概率。因此，我在这里通过从条件概率开始而不是从普通概率开始，间接地指定了我们模型中的概率定律。我们能从条件数 1 开始推导出普通概率吗？是的，我们当然可以。</p><p>So this is sort of a model of what’s going to happen with respect to both the plane we’re given probabilities
            about this and we’re given probabilities about how the radar behaves. So here I have indirectly specified
            the
            probability law in our model by starting with conditional probabilities as opposed to starting with ordinary
            probabilities. Can we derive ordinary probabilities starting from the conditional number ones? Yeah, we
            certainly can.</p>
        <p>让我们看看这个事件，A 和 B 的交汇点，也就是上面的事件，有一架飞机，我们的雷达探测到了它。我们如何计算这个概率？我们使用条件概率的定义，这是 A 的概率乘以给定 A 的 B 的条件概率。所以它是 0.05 乘以 0.99。如果你感兴趣的话，答案是 0.0495。好的。</p><p>Let’s look at this event, A intersection B, which is the event up here, that there is a plane and our radar
            picks
            it up. How can we calculate this probability? Well we use the definition of conditional probabilities and
            this
            is the probability of A times the conditional probability of B given A. So it’s 0.05 times 0.99. And the
            answer,
            in case you care. It’s 0.0495. OK.</p>
        <h2 id="unknown-19">未知</h2><h2>Unknown</h2>
        <p>因此，我们可以使用树枝上的概率来计算最终结果（即树叶）的概率。因此，本质上，我们最终要做的就是将这个树枝的概率乘以那个树枝的概率。现在，这个问题的答案是什么？我们的雷达记录到某个东西的概率是多少？</p><p>So we can calculate the probabilities of final outcomes, which are the leaves of the tree, by using the
            probabilities that we have along the branches of the tree. So essentially, what we ended up doing was to
            multiply the probability of this branch times the probability of that branch. Now, how about the answer to
            this
            question. What is the probability that our radar is going to register something?</p>
        <p>好的，这是一个可以以多种方式发生的事件。它是由这个结果组成的事件。有一架飞机，雷达记录了一些与这个结果相关的信息，没有飞机，但雷达仍然记录了一些信息。因此，要找到这个事件的概率，我们需要两种结果的单独概率。对于第一个结果，我们已经计算过了。</p><p>OK, this is an event that can happen in multiple ways. It’s the event that consists of this outcome. There is
            a
            plane and the radar registers something together with this outcome, there is no plane but the radar still
            registers something. So to find the probability of this event, we need the individual probabilities of the
            two
            outcomes. For the first outcome, we already calculated it.</p>
        <p>对于第二种结果，发生这种情况的概率将是这个概率的 95% 乘以 0.10，这是假设那里没有飞机的情况下采取这个分支的条件概率。所以我们只需将数字相加。0.05 乘以 0.99 加上 0.95 乘以 0.1，最终答案是 0.1445。好的。现在有一个有趣的问题。假设你的雷达记录了一些东西，那里有一架飞机的可能性有多大？</p><p>For the second outcome, the probability that this happens is going to be this probability 95% times 0.10,
            which
            is the conditional probability for taking this branch, given that there was no plane out there. So we just
            add
            the numbers. 0.05 times 0.99 plus 0.95 times 0.1 and the final answer is 0.1445. OK. And now here’s the
            interesting question. Given that your radar recorded something, how likely is it that there is an airplane
            up
            there?</p>
        <h2 id="unknown-20">未知</h2><h2>Unknown</h2>
        <p>您的雷达记录到的事件可能由两种原因引起。要么那里有一架飞机，您的雷达发挥了作用。要么那里什么也没有，但您的雷达发出了误报。这种情况相对于那种情况的概率是多少？好的。直观的捷径是它应该是概率。
        </p><p>Your radar registering something that can be caused by two things. Either there’s a plane there, and your
            radar
            did its job. Or there was nothing, but your radar fired a false alarm. What’s the probability that this is
            the
            case as opposed to that being the case? OK. The intuitive shortcut would be that it should be the
            probability.
        </p>
        <p>你查看这两个元素的相对几率，然后利用它们找出它们出现的可能性与出现的可能性相比有多大。但我们不这样做，而是直接写下定义并使用它。它是 A 和 B 发生的概率除以 B 发生的概率。这只是我们对条件概率的定义。现在我们已经找到了分子。</p><p>You look at their relative odds of these two elements and you use them to find out how much more likely it is
            to
            be there as opposed to being there. But instead of doing this, let’s just write down the definition and just
            use
            it. It’s the probability of A and B happening, divided by the probability of B. This is just our definition
            of
            conditional probabilities. Now we have already found the numerator.</p>
        <p>我们已经计算了分母。所以我们取这两个数字的比值，得出最终答案，即 0.34。好的。这个例子中发生了一件有点奇怪的事情。这个数字是不是感觉有点太低了？我的雷达所以这是一个条件概率，假设我的雷达说那里有东西，那里确实有东西。</p><p>We have already calculated the denominator. So we take the ratio of these two numbers and we find the final
            answer which is 0.34. OK. There’s this slightly curious thing that’s happened in this example. Doesn’t this
            number feel a little too low? My radar So this is a conditional probability, given that my radar said there
            is
            something out there, that there is indeed something there.</p>
        <h2 id="unknown-21">未知</h2><h2>Unknown</h2>
        <p>所以这有点像是我们的雷达给出正确答案的概率。现在，我们的雷达的规格相当不错。在这种情况下，它有 99% 的时间给出正确答案。在这种情况下，它有 90% 的时间给出正确答案。所以你会认为你的雷达非常可靠。</p><p>So it’s sort of the probability that our radar gave the correct answer. Now, the specs of our radar we’re
            pretty
            good. In this situation, it gives you the correct answer 99% of the time. In this situation, it gives you
            the
            correct answer 90% of the time. So you would think that your radar there is really reliable.</p>
        <p>但这里雷达记录了一些东西，但你得到的答案是正确的，假设它记录了一些东西，那么飞机在那里的概率只有 30%。所以你不能真正依赖雷达的测量结果，即使雷达的规格非常好。原因是什么？嗯，原因是误报很常见。</p><p>But yet here the radar recorded something, but the chance that the answer that you get out of this is the
            right
            one, given that it recorded something, the chance that there is an airplane out there is only 30%. So you
            cannot
            really rely on the measurements from your radar, even though the specs of the radar were really good. What’s
            the
            reason for this? Well, the reason is that false alarms are pretty common.</p>
        <p>大多数时候什么都没有。误报的概率为 10%。因此，在任何给定的实验中，误报的概率大约为 10%。大约有 5% 的概率是外面有东西，你的雷达会捕捉到它。因此，当你的雷达记录到某个东西时，它实际上更有可能是误报，而不是真的有飞机。这个概率大约为 10%。
        </p><p>Most of the time there’s nothing. And there’s a ten percent probability of false alarms. So there’s roughly a
            ten
            percent probability that in any given experiment, you have a false alarm. And there is about the five
            percent
            probability that something out there and your radar gets it. So when your radar records something, it’s
            actually
            more likely to be a false alarm rather than being an actual airplane. This has probability ten percent
            roughly.
        </p>
        <h2 id="unknown-22">未知</h2><h2>Unknown</h2>
        <p>这个概率大约是 5%。因此，条件概率有时在得到的答案方面是违反直觉的。你可以编造类似的故事，讲述医生如何解释测试结果。所以你对某种疾病的检测呈阳性。这是否意味着你一定患有这种疾病？</p><p>This has probability roughly five percent So conditional probabilities are sometimes counter intuitive in
            terms
            of the answers that they get. And you can make similar stories about doctors interpreting the results of
            tests.
            So you tested positive for a certain disease. Does it mean that you have the disease necessarily?</p>
        <p>如果这种疾病已经从地球上根除，那么检测结果呈阳性并不意味着你患有这种疾病，即使检测结果设计得相当好。不幸的是，医生有时也会出错。这种情况下的推理非常微妙。现在，在讲座的剩余部分，我们要做的是拿这个我们做了三件事的例子来抽象它们。</p><p>Well if that disease has been eradicated from the face of the earth, testing positive doesn’t mean that you
            have
            the disease, even if the test was designed to be a pretty good one. So unfortunately, doctors do get it
            wrong
            also sometimes. And the reasoning that comes in such situations is pretty subtle. Now for the rest of the
            lecture, what we’re going to do is to take this example where we did three things and abstract them.</p>
        <p>我们刚才做的这三个简单的计算是解决更一般的概率问题的三个非常重要、非常基本的工具。那么第一个是什么呢？我们通过将概率和条件概率相乘来找到复合事件（两件事发生）的概率。更一般的版本是，看看任何可能涉及大量事件的情况。所以这里有一个故事，事件 A 可能发生，也可能不会发生。</p><p>These three trivial calculations that’s we just did are three very important, very basic tools that you use
            to
            solve more general probability problems. So what’s the first one? We find the probability of a composite
            event,
            two things happening, by multiplying probabilities and conditional probabilities. More general version of
            this,
            look at any situation, maybe involving lots and lots of events. So here’s a story that event A may happen or
            may
            not happen.</p>
        <h2 id="unknown-23">未知</h2><h2>Unknown</h2>
        <p>假设 A 发生了，那么 B 也可能发生，或者 B 不发生。假设 B 也发生了，那么 C 也可能发生，或者 C 不发生。有人通过一路提供所有这些条件概率来为你指定一个模型。注意我们在树进展过程中沿着分支移动的内容。树中的任何点都对应于某些已发生的事件。</p><p>Given that A occurred, it’s possible that B happens or that B does not happen. Given that B also happens,
            it’s
            possible that the event C also happens or that event C does not happen. And somebody specifies for you a
            model
            by giving you all these conditional probabilities along the way. Notice what we move along the branches as
            the
            tree progresses. Any point in the tree corresponds to certain events having happened.</p>
        <p>然后，假设这件事已经发生，我们指定条件概率。假设这件事已经发生，C 也发生的可能性有多大？给定这种模型，我们如何找到这个事件的概率？答案非常简单。你所做的就是沿着树移动，并在此过程中乘以条件概率。那么就频率而言，A、B 和 C 这三件事发生的频率是多少？
        </p><p>And then, given that this has happened, we specify conditional probabilities. Given that this has happened,
            how
            likely is it for that C also occurs? Given a model of this kind, how do we find the probability or for this
            event? The answer is extremely simple. All that you do is move along with the tree and multiply conditional
            probabilities along the way. So in terms of frequencies, how often do all three things happen, A, B, and C?
        </p>
        <p>首先，您会看到 A 出现的频率是多少。在 A 出现的次数中，B 出现的频率是多少？在 A 和 B 都出现的次数中，C 出现的频率是多少？您可以将这三个频率相乘。这方面的正式证明是什么？好吧，我们手中唯一拥有的就是条件概率的定义。所以我们就用这个吧。然后。好的。</p><p>You first see how often does A occur. Out of the times that A occurs, how often does B occur? And out of the
            times where both A and B have occurred, how often does C occur? And you can just multiply those three
            frequencies with each other. What is the formal proof of this? Well, the only thing we have in our hands is
            the
            definition of conditional probabilities. So let’s just use this. And. OK.</p>
        <h2 id="unknown-24">未知</h2><h2>Unknown</h2>
        <p>现在，条件概率的定义告诉我们，两件事的概率是其中一件事的概率乘以条件概率。不幸的是，这里我们有三件事的概率。我能做什么？我可以在这里加一个括号，把它看作这个和那个的概率，并在这里应用我们对条件概率的定义。</p><p>Now, the definition of conditional probabilities tells us that the probability of two things is the
            probability
            of one of them times a conditional probability. Unfortunately, here we have the probability of three things.
            What can I do? I can put a parenthesis in here and think of this as the probability of this and that and
            apply
            our definition of conditional probabilities here.</p>
        <p>两件事发生的概率是第一件事发生的概率乘以第二件事发生的条件概率（给定 A 和 B，假设第一件事发生）。所以这只是给定另一个事件时事件的条件概率的定义。另一个事件是一个复合事件，但这不是问题。它只是一个事件。</p><p>The probability of two things happening is the probability that the first happens times the conditional
            probability that the second happens, given A and B, given that the first one happened. So this is just the
            definition of the conditional probability of an event, given another event. That other event is a composite
            one,
            but that’s not an issue. It’s just an event.</p>
        <p>然后我们再次使用条件概率的定义来分解它，使其成为 P(A)、P(给定 A 的 B)，最后是最后一个项。好的。这证明了我在幻灯片上列出的公式。如果你想计算此图中的任何其他概率。例如，如果你想计算这个概率，你仍然需要沿着树的不同分支乘以条件概率。
        </p><p>And then we use the definition of conditional probabilities once more to break this apart and make it P(A),
            P(B
            given A) and then finally, the last term. OK. So this proves the formula that I have up there on the slides.
            And
            if you wish to calculate any other probability in this diagram. For example, if you want to calculate this
            probability, you would still multiply the conditional probabilities along the different branches of the
            tree.
        </p>
        <h2 id="unknown-25">未知</h2><h2>Unknown</h2>
        <p>具体来说，在这个分支中，给定 A 交集 B 补集，您将得到 C 补集的条件概率，依此类推。因此，您可以沿着所有这些树分支写下概率，然后边走边将它们相乘。所以这是我们要介绍的第一个技能。第二个是什么？我们所做的是计算由某个事件 B 组成的总概率。</p><p>In particular, here in this branch, you would have the conditional probability of C complement, given A
            intersection B complement, and so on. So you write down probabilities along all those tree branches and just
            multiply them as you go. So this was the first skill that we are covering. What was the second one? What we
            did
            was to calculate the total probability of a certain event B that consisted of.</p>
        <p>由不同的可能性组成，这些可能性对应于不同的场景。所以我们想计算由这两个元素组成的事件 B 的概率。让我们概括一下。所以我们有一个大模型。这个样本空间被划分为多个集合。在我们的雷达示例中，我们将其划分为两个集合。要么有一架飞机在那里，要么有一架飞机不在那里。</p><p>Was made up from different possibilities, which corresponded to different scenarios. So we wanted to
            calculate
            the probability of this event B that consisted of those two elements. Let’s generalize. So we have our big
            model. And this sample space is partitioned in a number of sets. In our radar example, we had a partition in
            two
            sets. Either a plane is there, or a plane is not there.</p>
        <p>因为我们想要概括，现在我将为三种可能性或三种可能情景的情况画一幅图。所以无论世界上发生什么，都有三种可能情景，A1、A2、A3。想象一下空中什么都没有，空中有一架飞机，或者空中有一群鹅在飞。所以有三种可能情景。</p><p>Since we’re trying to generalize, now I’m going to give you a picture for the case of three possibilities or
            three possible scenarios. So whatever happens in the world, there are three possible scenarios, A1, A2, A3.
            So
            think of these as there’s nothing in the air, there’s an airplane in the air, or there’s a flock of geese
            flying
            in the air. So there’s three possible scenarios.</p>
        <h2 id="unknown-26">未知</h2><h2>Unknown</h2>
        <p>然后有一个感兴趣的事件 B，例如雷达记录了某事或没有记录某事。我们通过给出 Ai 的概率来指定此模型。这是不同场景的概率。有人还给出了假设第 Ai 个场景已经发生的情况下，事件 B 发生的概率。将 Ai 视为场景。我们想要计算事件 B 的总体概率。
        </p><p>And then there’s a certain event B of interest, such as a radar records something or doesn’t record
            something. We
            specify this model by giving probabilities for the Ai’s. That’s the probability of the different scenarios.
            And
            somebody also gives us the probabilities that this event B is going to occur, given that the Ai th scenario
            has
            occurred. Think of the Ai’s as scenarios. And we want to calculate the overall probability of the event B.
        </p>
        <p>这个例子中发生了什么？也许，如果我回到之前使用的图片，而不是这张图片，会更容易想象。我们有三种可能的情况，A1、A2、A3。在每种情况下，B 可能发生或 B 可能不会发生。依此类推。所以这里有 A2 与 B 的交点。这里有 A3 与 B 的交点。</p><p>What’s happening in this example? Perhaps, instead of this picture, it’s easier to visualize if I go back to
            the
            picture I was using before. We have three possible scenarios, A1, A2, A3. And under each scenario, B may
            happen
            or B may not happen. And so on. So here we have A2 intersection B. And here we have A3 intersection B.</p>
        <p>在上一张幻灯片中，我们找到了如何计算此类事件的概率的方法，即在这里乘以概率，在那里乘以条件概率。现在我们被要求计算事件 B 的总概率。事件 B 可能以三种可能的方式发生。它可以在这里发生。它可以在那里发生。它可以在这里发生。所以这就是我们的事件 B。它由三个元素组成。</p><p>In the previous slide, we found how to calculate the probability of any event of this kind, which is done by
            multiplying probabilities here and conditional probabilities there. Now we are asked to calculate the total
            probability of the event B. The event B can happen in three possible ways. It can happen here. It can happen
            there. And it can happen here. So this is our event B. It consists of three elements.</p>
        <h2 id="unknown-27">未知</h2><h2>Unknown</h2>
        <p>要计算事件 B 的总概率，我们需要做的就是将这三个概率相加。因此，B 是一个由这三个元素组成的事件。B 有三种发生方式。要么 B 与 A1 同时发生，要么 B 与 A2 同时发生，要么 B 与 A3 同时发生。因此，我们需要将这三个偶然事件的概率相加。</p><p>To calculate the total probability of our event B, all we need to do is to add these three probabilities. So
            B is
            an event that consists of these three elements. There are three ways that B can happen. Either B happens
            together with A1, or B happens together with A2, or B happens together with A3. So we need to add the
            probabilities of these three contingencies.</p>
        <p>对于每一种偶然事件，我们都可以使用乘法规则来计算其概率。因此，A1 和 B 发生的概率是这样的。它是在 A1 发生的情况下，A1 和 B 发生的概率。这种偶然事件的概率是通过将 A2 发生的概率乘以在 B 发生的情况下 A2 的条件概率得出的。对于第三个偶然事件，方法类似。因此，这是我们这里的一般规则。</p><p>For each one of those contingencies, we can calculate its probability by using the multiplication rule. So
            the
            probability of A1 and B happening is this. It’s the probability of A1 and then B happening given that A1
            happens. The probability of this contingency is found by taking the probability that A2 happens times the
            conditional probability of A2, given that B happened. And similarly for the third one. So this is the
            general
            rule that we have here.</p>
        <p>这条规则是针对三种情景编写的。但显然，它对四种或五种或更多情景也有概括。它提供了一种方法，通过考虑事件发生方式的不同概率，来分解可能以多种方式发生的事件的计算。好的。那么。是吗？观众：对于无限样本空间，这是否必须改变？约翰·齐西克利斯：不。&nbsp;</p><p>The rule is written for the case of three scenarios. But obviously, it has a generalization for the case of
            four
            or five or more scenarios. It gives you a way of breaking up the calculation of an event that can happen in
            multiple ways by considering individual probabilities for the different ways that the event can happen. OK.
            So.
            Yes? AUDIENCE: Does this have to change for infinite sample space? JOHN TSISIKLIS: No.&nbsp;</p>
        <h2 id="unknown-28">未知</h2><h2>Unknown</h2>
        <p>无论你的样本空间是无限的还是有限的，这都是正确的。我在这个论点中使用的是，我们只划分了三种场景，三种事件。所以它是对有限数量事件的划分。如果它是无限事件序列的划分，这也是正确的。但我认为这是本章末尾的理论问题之一。你现在可能不需要它。</p><p>This is true whether your sample space is infinite or finite. What I’m using in this argument that we have a
            partition into just three scenarios, three events. So it’s a partition to a finite number of events. It’s
            also
            true if it’s a partition into an infinite sequence of events. But that’s, I think, one of the theoretical
            problems at the end of the chapter. You probably may not need it for now.</p>
        <p>好的，回到这里的故事。这里记录了世界上可能发生的三种情景。在每种情景下，事件 B 可能会发生，也可能不会发生。因此，这些概率告诉我们不同情景的可能性。这些条件概率告诉我们，在一种情景、另一种情景或另一种情景下，B 发生的可能性有多大。</p><p>OK, going back to the story here. There are three possible scenarios about what could happen in the world
            that
            are captured here. Event, under each scenario, event B may or may not happen. And so these probabilities
            tell us
            the likelihoods of the different scenarios. These conditional probabilities tell us how likely is it for B
            to
            happen under one scenario, or the other scenario, or the other scenario.</p>
        <p>B 的总体概率是通过对不同可能世界、不同可能情景中 B 的概率进行某种组合得出的。在某些情况下，B 可能非常有可能。在另一种情况下，B 可能非常不可能。我们将所有这些都考虑在内，并根据情景的可能性对它们进行加权。现在请注意，由于 A1、A2 和三形成一个分区，这三个概率具有什么属性？添加到什么？</p><p>The overall probability of B is found by taking some combination of the probabilities of B in the different
            possible worlds, in the different possible scenarios. Under some scenario, B may be very likely. Under
            another
            scenario, it may be very unlikely. We take all of these into account and weigh them according to the
            likelihood
            of the scenarios. Now notice that since A1, A2, and three form a partition, these three probabilities have
            what
            property? Add to what?</p>
        <h2 id="unknown-29">未知</h2><h2>Unknown</h2>
        <p>它们相加为一。所以这是这个分支的概率，加上这个分支，再加上这个分支。所以我们这里得到的是 B 在不同世界或不同场景中的概率的加权平均值。特殊情况。假设这三种场景的可能性相同。所以 A1 的 P 等于 1/3，等于 A2 的 P，A3 的 P。我们在这里说什么？</p><p>They add to one. So it’s the probability of this branch, plus this branch, plus this branch. So what we have
            here
            is a weighted average of the probabilities of the B’s into the different worlds, or in the different
            scenarios.
            Special case. Suppose the three scenarios are equally likely. So P of A1 equals 1/3, equals to P of A2, P of
            A3.
            what are we saying here?</p>
        <p>在同样可能的情况下，B 的概率是 B 在三个不同单词或三个不同场景中的概率的平均值。好的。最后一步。如果我们再往前翻两张幻灯片，我们做的最后一件事是计算这种条件概率，即给定 B 时 A 的概率，这是一个与推理问题本质上相关的概率。</p><p>In that case of equally likely scenarios, the probability of B is the average of the probabilities of B in
            the
            three different words, or in the three different scenarios. OK. So to finally, the last step. If we go back
            again two slides, the last thing that we did was to calculate a conditional probability of this kind,
            probability of A given B, which is a probability associated essentially with an inference problem.</p>
        <p>假设我们的雷达记录了一些东西，飞机在空中的可能性有多大？所以我们试图根据我们得到的信息推断飞机是否在空中。让我们再概括一下。我们将重写我们在该示例中所做的操作，但使用一般符号而不是具体数字。因此，我们拥有的模型再次涉及不同场景的概率。</p><p>Given that our radar recorded something, how likely is it that the plane was up there? So we’re trying to
            infer
            whether a plane was up there or not, based on the information that we’ve got. So let’s generalize once more.
            And
            we’re just going to rewrite what we did in that example, but in terms of general symbols instead of the
            specific
            numbers. So once more, the model that we have involves probabilities of the different scenarios.</p>
        <h2 id="unknown-30">未知</h2><h2>Unknown</h2>
        <p>我们称它们为先验概率。它们是我们对每种情景发生可能性的初始信念。我们还有一个测量设备模型，它告诉我们在那种情景下我们的雷达记录某事物的可能性有多大。所以我们再次得到了这些条件概率。我们得到了这些分支的条件概率。然后我们被告知事件 B 发生了。</p><p>These we call them prior probabilities. They’re are our initial beliefs about how likely each scenario is to
            occur. We also have a model of our measuring device that tells us under that scenario how likely is it that
            our
            radar will register something or not. So we’re given again these conditional probabilities. We’re given the
            conditional probabilities for these branches. Then we are told that event B occurred.</p>
        <p>基于这些新信息，我们希望对不同情景的相对可能性形成一些新信念。回到雷达示例，飞机出现的概率为 5%。鉴于雷达记录了某些东西，我们将改变我们的信念。现在，飞机出现的概率为 34%。</p><p>And on the basis of this new information, we want to form some new beliefs about the relative likelihood of
            the
            different scenarios. Going back again to our radar example, an airplane was present with probability 5%.
            Given
            that the radar recorded something, we’re going to change our beliefs. Now, a plane is present with
            probability
            34%.</p>
        <p>雷达，因为我们看到了一些东西，所以我们将修正我们对飞机是否在那里的判断。所以我们需要做的是根据我们得到的信息计算不同情景的条件概率。所以最初，我们有不同情景的这些概率。
        </p><p>The radar, since we saw something, we are going to revise our beliefs as to whether the plane is out there or
            is
            not there. And so what we need to do is to calculate the conditional probabilities of the different
            scenarios,
            given the information that we got. So initially, we have these probabilities for the different scenarios.
        </p>
        <h2 id="unknown-31">未知</h2><h2>Unknown</h2>
        <p>一旦我们获得信息，我们就会更新它们，并根据我们所做的观察计算修正概率或条件概率。好的。那么我们该怎么做？我们只需使用条件概率的定义两次。根据定义，条件概率是两件事发生的概率除以条件事件的概率。现在，我再次使用条件概率的定义，或者更确切地说，我使用乘法规则。</p><p>Once we get the information, we update them and we calculate our revised probabilities or conditional
            probabilities given the observation that we made. OK. So what do we do? We just use the definition of
            conditional probabilities twice. By definition the conditional probability is the probability of two things
            happening divided by the probability of the conditioning event. Now, I’m using the definition of conditional
            probabilities once more, or rather I use the multiplication rule.</p>
        <p>两件事发生的概率是第一件事和第二件事的概率。所以这些是已知的东西。它们是不同情景的概率。这是我们测量设备的模型，我们假设它是可用的。分母呢？这是事件 B 的总概率。但我们发现使用上一张幻灯片中的公式很容易计算。</p><p>The probability of two things happening is the probability of the first and the second. So these are things
            that
            are given to us. They’re the probabilities of the different scenarios. And it’s the model of our measuring
            device, which we assume to be available. And how about the denominator? This is total probability of the
            event
            B. But we just found that’s it’s easy to calculate using the formula in the previous slide.</p>
        <p>为了找到事件 B 发生的总体概率，我们查看不同情景下 B 发生的概率，并根据所有情景的概率对它们进行加权。因此，最终，我们根据问题的数据（即不同情景的概率和给定 A 的 B 的条件概率）得出条件概率（给定 B 的 A）的公式。</p><p>To find the overall probability of event B occurring, we look at the probabilities of B occurring under the
            different scenario and weigh them according to the probabilities of all the scenarios. So in the end, we
            have a
            formula for the conditional probability, A’s given B, based on the data of the problem, which were
            probabilities
            of the different scenarios and conditional probabilities of B, given the A’s.</p>
        <h2 id="unknown-32">未知</h2><h2>Unknown</h2>
        <p>所以这个计算的作用基本上就是颠倒条件的顺序。我们得到了这种条件概率，其中 A 给定 B，然后我们产生新的条件概率，其中事情反过来。所以从示意图上看，这里发生的事情是，我们有一个因果模型。所以一个场景发生，这可能会导致 B 发生，也可能不会导致 B 发生。所以这是一个因果模型。
        </p><p>So what this calculation does is, basically, it reverses the order of conditioning. We are given conditional
            probabilities of these kind, where it’s B given A and we produce new conditional probabilities, where things
            go
            the other way. So schematically, what’s happening here is that we have model of cause and effect and. So a
            scenario occurs and that may cause B to happen or may not cause it to happen. So this is a cause/effect
            model.
        </p>
        <p>它使用概率建模，例如给定 Ai 时 B 的概率。我们想要做的是推理，我们被告知 B 会发生，我们想要推断 Ai 是否也发生了。而适当的概率是假设 B 发生，A 发生的条件概率。所以我们从我们的情况的因果模型开始。</p><p>And it’s modeled using probabilities, such as probability of B given Ai. And what we want to do is inference
            where we are told that B occurs, and we want to infer whether Ai also occurred or not. And the appropriate
            probabilities for that are the conditional probabilities that A occurred, given that B occurred. So we’re
            starting with a causal model of our situation.</p>
        <p>它根据给定的原因建模观察到某种结果的可能性。然后我们进行推理，这回答了这样一个问题：假设观察到了这种结果，世界处于这种特定情况或状态或场景的可能性有多大。因此，贝叶斯规则的名称来自 18 世纪的英国神学家托马斯·贝叶斯。它实际上。</p><p>It models from a given cause how likely is a certain effect to be observed. And then we do inference, which
            answers the question, given that the effect was observed, how likely is it that the world was in this
            particular
            situation or state or scenario. So the name of the Bayes rule comes from Thomas Bayes, a British theologian
            back
            in the 1700s. It actually.</p>
        <h2 id="unknown-33">未知</h2><h2>Unknown</h2>
        <p>这个计算解决了一个基本问题，一个基本的哲学问题，即如何从经验或实验数据中学习，以及以某种系统的方式学习。所以当时的英国人专注于这类问题。有没有一个关于如何将新知识与先前知识相结合的基本理论。这个计算证明了，是的，以系统的方式做到这一点是可能的。</p><p>This calculation addresses a basic problem, a basic philosophical problem, how one can learn from experience
            or
            from experimental data and some systematic way. So the British at that time were preoccupied with this type
            of
            question. Is there a basic theory that about how we can incorporate new knowledge to previous knowledge. And
            this calculation made an argument that, yes, it is possible to do that in a systematic way.</p>
        <p>因此，这种哲学基础有着非常悠久的历史，也有很多讨论。但就我们的目的而言，它只是一个非常有用的工具。当你试图根据部分观察进行推理时，它几乎是所有事情的基础。很好。下次见。</p><p>So the philosophical underpinnings of this have a very long history and a lot of discussion around them. But
            for
            our purposes, it’s just an extremely useful tool. And it’s the foundation of almost everything that gets
            done
            when you try to do inference based on partial observations. Very well. Till next time.</p>
        <h1 id="independence">3.独立性</h1><h1>3. Independence</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 46:30 <a href="https://youtube.com/watch?v=19Ql_Q3l0GA">https://youtube.com/watch?v=19Ql_Q3l0GA</a></p><p> 12 years ago (Nov 10, 2012) — 46:30 <a href="https://youtube.com/watch?v=19Ql_Q3l0GA">https://youtube.com/watch?v=19Ql_Q3l0GA</a></p>
        <h2 id="unknown-34">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：让我们开始吧。和往常一样，我们将快速回顾一下上次讨论的内容。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Let us start. So as
            always,
            we’re to have a quick review of what we discussed last time.</p>
        <p>今天我们只介绍一个新概念，即两个事件的独立性概念。我们将用这个概念来做实验。那么我们上次讨论的是什么？这个想法是，我们有一个实验，实验有一个样本空间 omega。然后有人来告诉我们，你知道实验的结果恰好位于这个特定事件 B 中。</p><p>And then today we’re going to introduce just one new concept, the notion of independence of two events. And
            we
            will play with that concept. So what did we talk about last time? The idea is that we have an experiment,
            and
            the experiment has a sample space omega. And then somebody comes and tells us you know the outcome of the
            experiments happens to lie inside this particular event B.</p>
        <p>有了这些信息，我们对情况的了解就会有所改变。它告诉我们结果会在这里面的某个地方。所以这基本上就是我们的新样本空间。现在我们需要重新分配各种可能结果的概率，因为，例如，这些结果，即使它们事先有正概率，现在我们被告知 B 发生了，这些结果的概率将为零。</p><p>Given this information, it kind of changes what we know about the situation. It tells us that the outcome is
            going to be somewhere inside here. So this is essentially our new sample space. And now we need to we
            reassign
            probabilities to the various possible outcomes, because, for example, these outcomes, even if they had
            positive
            probability beforehand, now that we’re told that B occurred, those outcomes out there are going to have zero
            probability.</p>
        <h2 id="unknown-35">未知</h2><h2>Unknown</h2>
        <p>所以我们需要修改我们的概率。新的概率被称为条件概率，它们是这样定义的。假设我们被告知 B 发生了，A 发生的条件概率是通过这个公式计算的，它告诉我们以下内容。在最初分配给事件 B 的总概率中，有多少比例的概率被分配给也导致 A 发生的结果？</p><p>So we need to revise our probabilities. The new probabilities are called conditional probabilities, and
            they’re
            defined this way. The conditional probability that A occurs given that we’re told that B occurred is
            calculated
            by this formula, which tells us the following. out of the total probability that was initially assigned to
            the
            event B, what fraction of that probability is assigned to outcomes that also make A to happen?</p>
        <p>因此，在分配给 B 的总概率中，我们可以看到分配给此处也将导致 A 发生的那些元素的总概率的比例。如果此处的分母为零，则条件概率未定义。</p><p>So out of the total probability assigned to B, we see what fraction of that total probability is assigned to
            those elements here that will also make A happen. Conditional probabilities are left undefined if the
            denominator here is zero.</p>
        <p>定义得出的一个简单结论是，如果我们将该术语带到另一边，那么我们可以通过取第一件事发生的概率，然后，假设第一件事发生，再取第二件事发生的条件概率，来找到两件事发生的概率。然后我们上次看到，我们可以通过将稍微复杂的事件分解为不同的场景来分而治之，从而计算出它们发生的概率。</p><p>An easy consequence of the definition is if we bring that term to the other side, then we can find the
            probability of two things happening by taking the probability that the first thing happens, and then, given
            that
            the first thing happened, the conditional probability that the second one happens. Then we saw last time
            that we
            can divide and conquer in calculating probabilities of mildly complicated events by breaking it down into
            different scenarios.</p>
        <h2 id="unknown-36">未知</h2><h2>Unknown</h2>
        <p>因此，事件 B 可以以两种方式发生。它可以与 A 一起发生，即这个概率，也可以与 A 的互补项一起发生，即这个概率。因此，基本上，我们说 B 的总概率是这个概率（即 A 与 B 的交集）加上那个概率（即 A 与 B 的互补项交集）。</p><p>So event B can happen in two ways. It can happen either together with A, which is this probability, or it can
            happen together with A complement, which is this probability. So basically what we’re saying that the total
            probability of B is the probability of this, which is A intersection B, plus the probability of that, which
            is A
            complement intersection B.</p>
        <p>因此，这两个事实，乘法规则和总概率定理，是将概率计算分解为更简单部分的基本工具。因此，我们通过一次查看每一件事来找到两件事发生的概率。这就是我们将一种情况分解为两种不同可能情景的方法。然后我们还有贝叶斯规则，它执行以下操作。</p><p>So these two facts here, multiplication rule and the total probability theorem, are basic tools that one uses
            to
            break down probability calculations into a simpler parts. So we find probabilities of two things happening
            by
            looking at each one at a time. And this is what we do to break up a situation with two different possible
            scenarios. Then we also have the Bayes rule, which does the following.</p>
        <p>给定一个具有此类条件概率的模型，贝叶斯规则允许我们计算事件以不同顺序出现的条件概率。你可以将这些概率视为描述特定情况的因果模型，而这些是你根据现有信息进行推断后得到的概率。现在我们推导出贝叶斯规则，这是一个简单的半线计算。</p><p>Given a model that has conditional probabilities of this kind, the Bayes rule allows us to calculate
            conditional
            probabilities in which the events appear in different order. You can think of these probabilities as
            describing
            a causal model of a certain situation, whereas these are the probabilities that you get after you do some
            inference based on the information that you have available. Now the Bayes rule, we derived it, and it’s a
            trivial half line calculation.</p>
        <h2 id="unknown-37">未知</h2><h2>Unknown</h2>
        <p>但它是现实世界中很多有用事物的基础。上次我们举了雷达的例子。你可以想象更复杂的情况，其中有一堆或很多关于环境的不同假设。给定环境中的任何特定设置，你有一个可以产生许多不同结果的测量设备。你从测量设备中观察最终结果，并试图猜测发生了哪个特定分支。</p><p>But it underlies lots and lots of useful things in the real world. We had the radar example last time. You
            can
            think of more complicated situations in which there’s a bunch or lots of different hypotheses about the
            environment. Given any particular setting in the environment, you have a measuring device that can produce
            many
            different outcomes. And you observe the final outcome out of your measuring device, and you’re trying to
            guess
            which particular branch occurred.</p>
        <p>也就是说，你试图根据特定的测量结果猜测世界的状态。这就是推理的全部内容。因此，现实世界的问题与我们上次看到的简单示例的唯一不同之处在于，这种树稍微复杂一些。你可能有无数种可能的结果，等等。</p><p>That is, you’re trying to guess the state of the world based on a particular measurement. That’s what
            inference
            is all about. So real world problems only differ from the simple example that we saw last time in that this
            kind
            of tree is a little more complicated. You might have infinitely many possible outcomes here and so on.</p>
        <p>因此，建立模型可能更加复杂，但基于贝叶斯规则进行的基本计算与我们看到的基本计算基本相同。现在我们要讨论的是，有时我们使用条件概率来描述模型，让我们通过查看一个抛硬币三次的模型来做到这一点。我们如何使用条件概率来描述这种情况？所以我们有一个实验。</p><p>So setting up the model may be more elaborate, but the basic calculation that’s done based on the Bayes rule
            is
            essentially the same as the one that we saw. Now something that we discuss is that sometimes we use
            conditional
            probabilities to describe models, and let’s do this by looking at a model where we toss a coin three times.
            And
            how do we use conditional probabilities to describe the situation? So we have one experiment.</p>
        <h2 id="unknown-38">未知</h2><h2>Unknown</h2>
        <p>但是那个实验包括三次连续的抛硬币。因此，可能的结果，也就是我们的样本空间，由长度为 3 的字符串组成，这些字符串告诉我们掷出的是正面还是反面，以及顺序如何。因此，连续三次掷出正面是一个特定的结果。那么，分支前面的那些标签是什么意思呢？这里的 P 当然代表第一次抛硬币结果为正面的概率。</p><p>But that one experiment consists of three consecutive coin tosses. So the possible outcomes, our sample
            space,
            consists of strings of length 3 that tell us whether we had heads, tails, and in what sequence. So three
            heads
            in a row is one particular outcome. So what is the meaning of those labels in front of the branches? So this
            P
            here, of course, stands for the probability that the first toss resulted in heads.</p>
        <p>让我用这个符号来表示第一次是正面。我在 toss one 中放了一个 H。这个概率在这里是什么意思呢？这个概率的含义是条件概率。它是在第一次掷出正面的情况下，第二次掷出正面的条件概率。</p><p>And let me use this notation to denote that the first was heads. I put an H in toss one. How about the
            meaning of
            this probability here? Well the meaning of this probability is a conditional one. It’s the conditional
            probability that the second toss resulted in heads, given that the first one resulted in heads.</p>
        <p>同样，这里的标签对应于在第一次和第二次掷硬币都是正面的情况下，第三次掷硬币出现正面的概率。因此，在我写下的这个特定模型中，无论前一次掷硬币结果如何，掷出正面的概率 P 都保持不变。</p><p>And similarly this label here corresponds to the probability that the third toss resulted in heads, given
            that
            the first one and the second one resulted in heads. So in this particular model that I wrote down here,
            those
            probabilities, P, of obtaining heads remain the same no matter what happened in the previous toss.</p>
        <h2 id="unknown-39">未知</h2><h2>Unknown</h2>
        <p>例如，即使第一次抛掷的结果为反面，如果第一次抛掷的结果为反面，则第二次抛掷的结果为正面的概率仍为 P。因此，我们假设无论第一次抛掷的结果如何，第二次抛掷的结果仍将具有等于 P 的条件概率。因此，该条件概率与第一次抛掷的结果无关。</p><p>For example, even if the first toss was tails, we still have the same probability, P, that the second one is
            heads, given that the first one was tails. So we’re assuming that no matter what happened in the first toss,
            the
            second toss will still have a conditional probability equal to P. So that conditional probability does not
            depend on what happened in the first toss.</p>
        <p>我们将看到，这是一种非常特殊的情况，这就是我们即将介绍的独立性概念。但在讨论独立性之前，让我们再练习一下上次在这个例子中讲到的三个技能。第一个技能是乘法规则。你如何找到几件事发生的概率？那就是先出现反面，然后出现正面，然后出现反面的概率。
        </p><p>And we will see that this is a very special situation, and that’s really the concept of independence that we
            are
            going to introduce shortly. But before we get to independence, let’s practice once more the three skills
            that we
            covered last time in this example. So first skill was multiplication rule. How do you find the probability
            of
            several things happening? That is the probability that we have tails followed by heads followed by tails.
        </p>
        <p>所以这里我们讨论的是这个特定结果，先是反面，然后是正面，然后是反面。我们计算这种概率的方法是沿着导致这个结果的路径乘以条件概率。这些条件概率记录在这里。所以它将是 (1 减 P) 乘以 P 乘以 (1 减 P)。所以这是乘法规则。</p><p>So here we’re talking about this particular outcome here, tails followed by heads followed by tails. And the
            way
            we calculate such a probability is by multiplying conditional probabilities along the path that takes us to
            this
            outcome. And so these conditional probabilities are recorded here. So it’s going to be (1 minus P) times P
            times
            (1 minus P). So this is the multiplication rule.</p>
        <h2 id="unknown-40">未知</h2><h2>Unknown</h2>
        <p>第二个问题是，我们如何找到一个稍微复杂一些的事件的概率？所以我写下的感兴趣的事件是三次投掷中总共出现一次正面的概率。正好一次正面。这个事件可以以多种方式发生。它发生在这里。它发生在这里。它也发生在这里。所以我们想要找到由这三种结果组成的事件的总概率。</p><p>Second question is how do we find the probability of a mildly complicated event? So the event of interest
            here
            that I wrote down is the probability that in the three tosses, we had a total of one head. Exactly one head.
            This is an event that can happen in multiple ways. It happens here. It happens here. And it also happens
            here.
            So we want to find the total probability of the event consisting of these three outcomes.</p>
        <p>我们该怎么做？我们只需将每个单独结果的概率相加。我们如何找到单个结果的概率？好吧，这就是我们刚才所做的。现在请注意，这个结果的概率为 P 乘以 (1 减 P) 平方。那个不应该在那里。那么它在哪里？啊。就是这个。好的，所以这个结果的概率是 (1 减 P 乘以 P) 乘以 (1 减 P)，相同的概率。</p><p>What do we do? We just add the probabilities of each individual outcome. How do we find the probability of an
            individual outcome? Well, that’s what we just did. Now notice that this outcome has probability P times (1
            minus
            P) squared. That one should not be there. So where is it? Ah. It’s this one. OK, so the probability of this
            outcome is (1 minus P times P) times (1 minus P), the same probability.</p>
        <p>最后，这个数又是 (1 减 P) 平方乘以 P。所以，一次正面事件有三种发生方式。这三种方式发生的概率都一样。这就是答案。最后，我们学到的最后一件事是使用贝叶斯规则来计算和推断。所以有人告诉你，你三次投掷中只有一次是正面。</p><p>And finally, this one is again (1 minus P) squared times P. So this event of one head can happen in three
            ways.
            And each one of those three ways has the same probability of occurring. And this is the answer. And finally,
            the
            last thing that we learned how to do is to use the Bayes rule to calculate and make an inference. So
            somebody
            tells you that there was exactly one head in your three tosses.</p>
        <h2 id="unknown-41">未知</h2><h2>Unknown</h2>
        <p>第一次抛掷结果为正面的概率是多少？好吧，如果我告诉你抛掷了三次，我想你就能猜出答案了。其中一次是正面。第一次、第二次还是第三次抛掷结果为正面？嗯，根据对称性，它们应该都是等概率的。所以第一次抛掷结果为正面的概率应该只有 1/3。让我们用定义来检验一下我们的直觉。</p><p>What is the probability that the first toss resulted in heads? OK, I guess you can guess the answer here if I
            tell you that there were three tosses. One of them was heads. Where was that head in the first, the second,
            or
            the third? Well, by symmetry, they should all be equally likely. So there should be probably just 1/3 that
            head
            occurred in the first toss. Let’s check our intuition using the definitions.</p>
        <p>因此，条件概率的定义告诉我们，条件概率是两种情况都发生的概率。第一次抛掷是正面，并且恰好有一次正面，除以一次正面的概率。第一次抛掷是正面，并且恰好有一次正面的概率是多少？这与事件正面、反面、反面相同。</p><p>So the definition of conditional probability tells us the conditional probability is the probability of both
            things happening. First toss is heads, and we have exactly one head divided by the probability of one head.
            What
            is the probability that the first toss is heads, and we have exactly one head? This is the same as the event
            heads, tails, tails.</p>
        <p>如果我告诉你第一个是正面，而且只有一个正面，那就意味着其他都是反面。所以这是正面、反面、反面的概率除以一个正面的概率。我们知道所有这些量都是正面、反面、反面的概率是 P 乘以 (1 减 P) 平方。一个正面的概率是 3 乘以 P 乘以 (1 减 P) 平方。</p><p>If I tell you that the first is heads, and there’s only one head, it means that the others are tails. So this
            is
            the probability of heads, tails, tails divided by the probability of one head. And we know all of these
            quantities probability of heads, tails, tails is P times (1 minus P) squared. Probability of one head is 3
            times
            P times (1 minus P) squared.</p>
        <h2 id="unknown-42">未知</h2><h2>Unknown</h2>
        <p>所以最终答案是 1/3，这是你应该凭直觉猜到的。非常好。所以我们对上次讲过的内容进行了练习。再说一遍，思考。我们在这里练习和锻炼的基本技能基本上有三种。</p><p>So the final answer is 1/3, which is what you should have a guessed on intuitive grounds. Very good. So we
            got
            our practice on the material that we did cover last time. Again, think. There’s basically three basic skills
            that we are practicing and exercising here.</p>
        <p>在问题、测验和现实生活中，你可能必须在更复杂的环境中运用这三种技能，但最终通常就是如此。现在让我们关注一下我之前讨论过的这个特定模型的这个特殊功能。想想第二次投掷中的事件正面。
        </p><p>In the problems, quizzes, and in the real life, you may have to apply those three skills in somewhat more
            complicated settings, but in the end that’s what it boils down to usually. Now let’s focus on this special
            feature of this particular model that I discussed a little earlier. Think of the event heads in the second
            toss.
        </p>
        <p>最初，第二次抛硬币正面的概率是 P，即硬币投出的概率。如果我告诉你第一次抛硬币是正面，那么第二次抛硬币正面的概率是多少？同样是 P。如果我告诉你第一次抛硬币是反面，那么第二次抛硬币正面的概率是多少？同样是 P。</p><p>Initially, the probability of heads in the second toss, you know, that it’s P, the probability of success of
            your
            coin. If I tell you that the first toss resulted in heads, what’s the probability that the second toss is
            heads?
            It’s again P. If I tell you that the first toss was tails, what’s the probability that the second toss is
            heads?
            It’s again P.</p>
        <h2 id="unknown-43">未知</h2><h2>Unknown</h2>
        <p>因此，无论我告诉你第一次抛掷的结果，还是不告诉你，对你来说都没有任何区别。无论第一次抛掷的结果如何，你总会说第二次抛掷出现正面的概率为 P。这是一种特殊情况，我们将给它命名，称之为属性独立性。</p><p>So whether I tell you the result of the first toss, or I don’t tell you, it doesn’t make any difference to
            you.
            You would always say the probability of heads in the second toss is going to P, no matter what happened in
            the
            first toss. This is a special situation to which we’re going to give a name, and we’re going to call that
            property independence.</p>
        <p>基本上，两件事之间的独立性代表这样的事实：无论第一件事是否发生，它都不会给你提供任何信息，不会导致你改变对第二件事的信念。这是直觉。让我们试着把它转化为数学。我们有两个事件，如果我告诉你发生了 A，你对 B 的初始信念不会改变，那么我们就说它们是独立的。</p><p>Basically independence between two things stands for the fact that the first thing, whether it occurred or
            not,
            doesn’t give you any information, does not cause you to change your beliefs about the second event. This is
            the
            intuition. Let’s try to translate this into mathematics. We have two events, and we’re going to say that
            they’re
            independent if your initial beliefs about B are not going to change if I tell you that A occurred.</p>
        <p>所以你相信某件事 B 发生的可能性有多大。然后有人来告诉你，A 已经发生了。你会改变你的信念吗？不，我不会改变它们。每当你处于这种情况时，你就会说这两个事件是独立的。直观地说，A 发生的事实并没有向你传达任何有关事件 B 发生可能性的信息。</p><p>So you believe something how likely B is. Then somebody comes and tells you, you know, A has happened. Are
            you
            going to change your beliefs? No, I’m not going to change them. Whenever you are in such a situation, then
            you
            say that the two events are independent. Intuitively, the fact that A occurred does not convey any
            information
            to you about the likelihood of event B.</p>
        <h2 id="unknown-44">未知</h2><h2>Unknown</h2>
        <p>A 提供的信息不是那么有用，不相关。A 与其他事情有关。它对于您猜测 B 是否会发生没有用。所以我们可以将此作为对独立性定义的首次尝试。现在请记住，我们有这个属性，两件事发生的概率是第一件事的概率乘以第二件事的条件概率。</p><p>The information that A provides is not so useful, is not relevant. A has to do with something else. It’s not
            useful for your guessing whether B is going to occur or not. So we can take this as a first attempt into a
            definition of independence. Now remember that we have this property, the probability of two things happening
            is
            the probability of the first times the conditional probability of the second.</p>
        <p>如果存在独立性，则该条件概率与无条件概率相同。因此，如果根据该定义存在独立性，我们就会得到这样的属性，即只需将两件事各自的概率相乘，就可以找到两件事发生的概率。第一次抛硬币正面的概率是 1/2。第二次抛硬币正面的概率是 1/2。正面的概率是 1/4。如果两次抛硬币彼此独立，就会发生这种情况。</p><p>If we have independence, this conditional probability is the same as the unconditional probability. So if we
            have
            independence according to that definition, we get this property that you can find the probability of two
            things
            happening by just multiplying their individual probabilities. Probability of heads in the first toss is 1/2.
            Probability of heads in the second toss is 1/2. Probability of heads is 1/4. That’s what happens if your two
            tosses are independent of each other.</p>
        <p>因此，这里的这个属性是这个定义的结果，但实际上，用这个定义而不是那个定义会更好、更好、更简单、更干净、更漂亮。这两个定义等价吗？嗯，它们几乎相同，除了一件事。条件概率仅在以具有正概率的事件为条件时才有定义。</p><p>So this property here is a consequence of this definition, but it’s actually nicer, better, simpler, cleaner,
            more beautiful to take this as our definition instead of that one. Are the two definitions equivalent? Well,
            they’re are almost the same, except for one thing. Conditional probabilities are only defined if you
            condition
            on an event that has positive probability.</p>
        <h2 id="unknown-45">未知</h2><h2>Unknown</h2>
        <p>因此，这个定义仅限于事件 A 具有正概率的情况，而这个定义是可以随时写下来的东西。我们会说两个事件是独立的，当且仅当它们同时发生的概率等于它们两个单独概率的乘积。特别是，我们可以有零概率的事件。这没什么错。</p><p>So this definition would be limited to cases where event A has positive probability, whereas this definition
            is
            something that you can write down always. We will say that two events are independent if and only if their
            probability of happening simultaneously is equal to the product of their two individual probabilities. And
            in
            particular, we can have events of zero probability. There’s nothing wrong with that.</p>
        <p>如果 A 的概率为 0，那么 A 与 B 的交集的概率也将为 0，因为这是一个更小的事件。所以我们会得到零等于零。我刚才所说的推论是，如果事件 A 的概率为零，它实际上独立于我们模型中的任何其他事件，因为我们会得到零等于零。并且定义将得到满足。</p><p>If A has 0 probability, then A intersection B will also have zero probability, because it’s an even smaller
            event. And so we’re going to get zero is equal to zero. A corollary of what I just said, if an event A has
            zero
            probability, it’s actually independent of any other event in our model, because we’re going to get zero is
            equal
            to zero. And the definition is going to be satisfied.</p>
        <p>这有点难以与我们对独立性的直觉相协调，但话又说回来，这是数学定义的一部分。因此，我希望你们记住这个概念，即独立性是可以使用此定义正式检查的东西，但也可以直观地检查，在某些情况下，你可以推断出，无论发生什么并决定 A 是否会发生，都与无论发生什么并决定 B 是否会发生没有任何绝对关系。</p><p>This is a little bit harder to reconcile with the intuition we have about independence, but then again, it’s
            part
            of the mathematical definition. So what I want you to retain is this notion that the independence is
            something
            that you can check formally using this definition, but also you can check intuitively by if, in some cases,
            you
            can reason that whatever happens and determines whether A is going to occur or not, has nothing absolutely
            to do
            with whatever happens and determines whether B is going to occur or not.</p>
        <h2 id="unknown-46">未知</h2><h2>Unknown</h2>
        <p>所以如果我在这个房间里做科学实验，实验受到一些噪音的影响，从而产生随机性。五年后，其他人在其他地方做同样的科学实验，实验也受到其他噪音的影响，你通常会说这些实验是独立的。</p><p>So if I’m doing a science experiment in this room, and it gets hit by some noise that’s causes randomness.
            And
            then five years later, somebody somewhere else does the same science experiment somewhere else, it gets hit
            by
            other noise, you would usually say that these experiments are independent.</p>
        <p>因此，一个实验中发生的事件不会改变你对另一个实验中可能发生事件的信念，因为这两个实验中的噪声源完全不相关。它们彼此之间没有任何关系。所以，如果我今天在这里抛硬币，明天在办公室抛硬币，其中一个不会影响另一个。所以我从这两个实验中得到的事件应该是独立的。</p><p>So what events happen in one experiment are not going to change your beliefs about what might be happening in
            the
            other, because the sources of noise in these two experiments are completely unrelated. They have nothing to
            do
            with each other. So if I flip a coin here today, and I flip a coin in my office tomorrow, one shouldn’t
            affect
            the other. So the events that I get from these should be independent.</p>
        <p>所以独立性通常就是这样产生的。通过存在不相互作用的不同物理现象。有时即使存在物理相互作用，你也会获得独立性，但你恰好发生了数值事故。A 和 B 可能在物理上紧密相关，但发生了数值事故，你在这里得到了相等，这是我们获得独立性的另一种情况。现在假设我们有两个像这样布局的事件。</p><p>So that’s usually how independence arises. By having distinct physical phenomena that do not interact.
            Sometimes
            you also get independence even though there is a physical interaction, but you just happen to have a
            numerical
            accident. A and B might be physically related very tightly, but a numerical accident happens and you get
            equality here, that’s another case where we do get independence. Now suppose that we have two events that
            are
            laid out like this.</p>
        <h2 id="unknown-47">未知</h2><h2>Unknown</h2>
        <p>这两个事件是独立的吗？这幅图告诉你一个事件与另一个事件是分开的。但分开与独立无关。事实上，这两个事件就像连体双胞胎一样相互依赖。为什么会这样？如果我告诉你发生了 A，那么你肯定不会发生 B。因此，有关 A 发生的信息肯定会影响你对 B 可能发生或不发生的信念。</p><p>Are these two events independent or not? The picture kind of tells you that one is separate from the other.
            But
            separate has nothing to do with independent. In fact, these two events are as dependent as Siamese twins.
            Why is
            that? If I tell you that A occurred, then you are certain that B did not occur. So information about the
            occurrence of A definitely affects your beliefs about the possible occurrence or non occurrence of B.</p>
        <p>当图像是那样的时候，知道 A 发生了会极大地改变我对 B 的信念，因为现在我突然确信 B 没有发生。所以像这样的图像实际上是极端依赖的情况。所以不要混淆独立性和不相交性。它们是非常不同的属性类型。听众：问题。教授：是的？听众：所以我理解解释，但 A 与 B 相交的概率为零，因为它们是不相交的。教授：是的。</p><p>When the picture is like that, knowing that A occurred will change drastically my beliefs about B, because
            now I
            suddenly become certain that B did not occur. So a picture like this is a case actually of extreme
            dependence.
            So don’t confuse independence with disjointness. They’re very different types of properties. AUDIENCE:
            Question.
            PROFESSOR: Yes? AUDIENCE: So I understand the explanation, but the probability of A intersect B to zero,
            because
            they’re disjoint. PROFESSOR: Yes.</p>
        <p>听众：但是概率 A 和概率 B 的乘积中，其中一个会是 1。教授：不，假设概率是 1/3、1/4，其余的都在那里。你检查一下独立性的定义。A 和 B 相交的概率是零。A 的概率乘以 B 的概率是 1/12。两者不相等。因此，我们没有独立性。听众：对。</p><p>AUDIENCE: But then the product of probability A and probability B, one of them is going to be 1. PROFESSOR:
            No,
            suppose that the probabilities are 1/3,1/4, and the rest is out there. You check the definition of
            independence.
            Probability of A intersection B is zero. Probability of A times the probability of B is 1/12. The two are
            not
            equal. Therefore we do not have independence. AUDIENCE: Right.</p>
        <h2 id="unknown-48">未知</h2><h2>Unknown</h2>
        <p>那么，A 的概率为 1，而 B 的概率为 0，这种直觉有​​什么问题呢？教授：没有。B 发生时 A 的概率等于 0。A 的概率等于 1/3。所以，这两者是不同的。因此，我们对 A 有一些最初的信念，但一旦我们被告知 B 发生了，我们对 A 的信念就改变了。
        </p><p>So what’s wrong with the intuition of the probability of A being 1, and the other one being 0? PROFESSOR:
            No.&nbsp;The
            probability of A given B is equal to 0. Probability of A is equal to 1/3. So again, these two are different.
            So
            we had some initial beliefs about A, but as soon as we are told that B occurred, our beliefs about A
            changed.
        </p>
        <p>因此，由于我们的信念发生了变化，这意味着 B 传达了有关 A 的信息。听众：那么，您不能在维恩图上画出独立性吗？教授：我听不见。听众：您能在维恩图上画出独立性吗？教授：不，维恩图永远不足以决定独立性。因此，您将获得独立性的典型图景是，一个事件以这种方式发生，另一个事件以这种方式发生。</p><p>And so since our beliefs changed, that means that B conveys information about A. AUDIENCE: So can you not
            draw
            independent on a Venn diagram? PROFESSOR: I can’t hear you. AUDIENCE: Can you draw independence on a Venn
            diagram? PROFESSOR: No, the Venn diagram is never enough to decide independence. So the typical picture in
            which
            you’re going to have independence would be one event this way, and another event this way.</p>
        <p>你需要用这个的概率乘以那个的概率，然后从数值上检查它是否等于这个交点的概率。所以它不仅仅是一个维恩图。数字需要正确显示。现在我们前段时间确实说过，条件概率就像普通概率一样，我们在概率论中所做的任何事情也可以在条件宇宙中完成。谈论条件概率。
        </p><p>You need to take the probability of this times the probability of that, and check that, numerically, it’s
            equal
            to the probability of this intersection. So it’s more than a Venn diagram. Numbers need to come out right.
            Now
            we did say some time ago that conditional probabilities are just like ordinary probabilities, and whatever
            we do
            in probability theory can also be done in conditional universes. Talking about conditional probabilities.
        </p>
        <h2 id="unknown-49">未知</h2><h2>Unknown</h2>
        <p>因此，既然我们有独立性的概念，那么也应该有条件独立性的概念。因此，独立性的定义是 A 与 B 相交的概率等于 A 的概率乘以 B 的概率。条件独立性的合理定义是什么？条件独立性意味着，同样的属性可能为真，但在一个条件宇宙中，我们被告知某个事件会发生。</p><p>So since we have a notion of independence, then there should be also a notion of conditional independence. So
            independence was defined by the probability that A intersection B is equal to the probability of A times the
            probability of B. What would be a reasonable definition of conditional independence? Conditional
            independence
            would mean that this same property could be true, but in a conditional universe where we are told that the
            certain event happens.</p>
        <p>因此，如果我们被告知事件 C 已经发生，那么我们将被传送到一个条件宇宙中，其中唯一重要的是条件概率。这只是与之前对独立性相同的简单定义，但应用于条件宇宙。所以这是条件独立性的定义。所以它是独立性，但参考条件概率。</p><p>So if we’re told that the event C has happened, then were transported in a conditional universe where the
            only
            thing that matters are conditional probabilities. And this is just the same plain, previous definition of
            independence, but applied in a conditional universe. So this is the definition of conditional independence.
            So
            it’s independence, but with reference to the conditional probabilities.</p>
        <p>从直觉上看，它的含义也是一样的，在有条件的世界中，如果我告诉你 A 发生了，那么这不会改变你对 B 的信念。假设你有这样的图景。有人告诉你事件 A 和 B 是无条件独立的。然后有人来告诉你事件 C 确实发生了，所以我们现在生活在这个新的宇宙中。</p><p>And intuitively it has, again, the same meaning, that in the conditional world, if I tell you that A
            occurred,
            then that doesn’t change your beliefs about B. So suppose you had a picture like this. And somebody told you
            that events A and B are independent unconditionally. Then somebody comes and tells you that event C actually
            has
            occurred, so we now live in this new universe.</p>
        <h2 id="unknown-50">未知</h2><h2>Unknown</h2>
        <p>在这个新的宇宙中，A 和 B 的独立性是否会被保留？A 和 B 在这个新的宇宙中是独立的吗？答案是否定的，因为在新的宇宙中，事件 A 剩下的就是这一部分。事件 B 剩下的就是这一部分。这两部分是不相交的。所以我们又回到了这种境地。所以在条件宇宙中，A 和 B 是不相交的。</p><p>In this new universe, is the independence of A and B going to be preserved or not? Are A and B independent in
            this new universe? The answer is no, because in the new universe, whatever is left of event A is this piece.
            Whatever is left of event B is this piece. And these two pieces are disjoint. So we are back in a situation
            of
            this kind. So in the conditional universe, A and B are disjoint.</p>
        <p>因此，一般来说，它们不会是独立的。这个例子的寓意是什么？原始模型中的独立性并不意味着条件模型中的独立性。相反的情况也是可能的。让我们通过另一个例子来说明。我有两枚硬币，它们都严重偏向正面。一枚硬币更倾向于正面。另一枚硬币更倾向于反面。所以概率是 90%。</p><p>And therefore, generically, they’re not going to be independent. What’s the moral of this example? Having
            independence in the original model does not imply independence in a conditional model. The opposite is also
            possible. And let’s illustrate by another example. So I have two coins, and both of them are badly biased.
            One
            coin is much biased in favor of heads. The other coin is much biased in favor of tails. So the probabilities
            being 90%.</p>
        <p>让我们考虑硬币 A 的独立翻转。这是相关模型。这是第一枚硬币两次独立翻转的模型。将会有两次翻转，每次正面的概率为 0.9。所以这是一个描述硬币 A 的模型。你可以将其视为条件模型，即以他们选择硬币 A 为条件的硬币翻转模型。</p><p>Let’s consider independent flips of coin A. This is the relevant model. This is a model of two independent
            flips
            of the first coin. There’s going to be two flips, and each one has probability 0.9 of being heads. So that’s
            a
            model that describes coin A. You can think of this as a conditional model which is a model of the coin flips
            conditioned on the fact that they have chosen coin A.</p>
        <h2 id="unknown-51">未知</h2><h2>Unknown</h2>
        <p>或者，我们可以在一个条件世界中处理硬币 B，我们选择硬币 B 并将其抛两次，这就是相关模型。例如，两次正面的概率是第一次正面的概率，第二次正面的概率，每个概率都是 0.1。现在我要将其构建成一个更大的实验，首先我随机选择两枚硬币中的一枚。所以我有这两枚硬币。</p><p>Alternatively we could be dealing with coin B in a conditional world where we chose coin B and flip it twice,
            this is the relevant model. The probability of two heads, for example, is the probability of heads the first
            time, heads the second time, and each one is 0.1. Now I’m building this into a bigger experiment in which I
            first start by choosing one of the two coins at random. So I have these two coins.</p>
        <p>我盲目地挑选其中一个。然后我开始抛硬币。现在的问题是，抛硬币或抛硬币是否相互独立？如果我们只停留在这个子模型中，抛硬币是否独立？它们是独立的，因为无论第一次抛硬币结果如何，第二次抛硬币正面的概率都是相同的 0.9。</p><p>I blindly pick one of them. And then I start flipping them. So the question now is, are the coin flips, or
            the
            coin tosses, are they independent of each other? If we just stay inside this sub model here, are the coin
            flips
            independent? They are independent, because the probability of heads in the second toss is the same, 0.9, no
            matter what happened in the first toss.</p>
        <p>因此，第二次抛硬币结果的条件概率不受第一次抛硬币结果的影响。因此，第二次抛硬币和第一次抛硬币是独立的。因此，这里我们只处理普通的、独立的抛硬币。类似地，这个子模型中的抛硬币也是独立的。</p><p>So the conditional probabilities of what happens in the second toss are not affected by the outcome of the
            first
            toss. So the second toss and the first toss are independent. So here we’re just dealing with plain,
            independent
            coin flips. Similarity the coin flips within this sub model are also independent.</p>
        <h2 id="unknown-52">未知</h2><h2>Unknown</h2>
        <p>现在的问题是，如果我们将大模型视为一个概率模型，而不是查看条件子模型，那么抛硬币是否彼此独立？几次抛硬币的结果是否会为您提供有关后续抛硬币的信息？如果我连续观察到十次正面朝上。因此，现在让我们考虑进行更多次抛硬币，而不是两次抛硬币，以便扩大树。</p><p>Now the question is, if we look at the big model as just one probability model, instead of looking at the
            conditional sub models, are the coin flips independent of each other? Does the outcome of a few coin flips
            give
            you information about subsequent coin flips? Well if I observe ten heads in a row. So instead of two coin
            flips,
            now let’s think of doing more of them so that the tree gets expanded.</p>
        <p>那么让我们从这个问题开始。我不知道是哪一枚硬币。第 11 次抛硬币正面的概率是多少？这里完全对称，所以答案只能是 1/2。那么让我们解释一下，为什么是 1/2？那么，第 11 次抛硬币正面的概率，这个结果怎么可能发生呢？可能有两种情况。</p><p>So let’s start with this. I don’t know which coin it is. What’s the probability that the 11th coin toss is
            going
            to be heads? There’s complete symmetry here, so the answer could not be anything other than 1/2. So let’s
            justify it, why is it 1/2? Well, the probability that the 11th toss is heads, how can that outcome happen?
            It
            can happen in two ways.</p>
        <p>你可以选择硬币 A，概率是 1/2。选择硬币 A 后，第 11 次抛掷正面的概率是 0.9。或者你也可以选择硬币 B。如果抛掷结果是硬币 B，那么正面的概率是 0.1。所以最终答案是 1/2。所以每枚硬币都有偏差，但偏差的方式不同。</p><p>You can choose coin A, which happens with probability 1/2. And having chosen coin A, there’s probability 0.9
            that
            it results in that you get heads in the 11th toss. Or you can choose coin B. And if it’s coin B when you
            flip
            it, there’s probably 0.1 that you have heads. So the final answer is 1/2. So each one of the coins is
            biased,
            but they’re biased in different ways.</p>
        <h2 id="unknown-53">未知</h2><h2>Unknown</h2>
        <p>如果我不知道是哪枚硬币，它们的两个偏差就会抵消，而掷出正面的概率正好在中间，那么就是 1/2。现在如果有人告诉你前十次抛掷都是正面，这会改变你对第 11 次抛掷的看法吗？一个理性的人会这样想。如果是硬币 B，连续掷出 10 次正面的概率可以忽略不计。</p><p>If I don’t know which coin it is, their two biases kind of cancel out, and the probability of obtaining heads
            is
            just in the middle, then it’s 1/2. Now if someone tells you that the first ten tosses were heads, is that
            going
            to change your beliefs about the 11th toss? Here’s how a reasonable person would think about it. If it’s
            coin B
            the probability of obtaining 10 heads in a row is negligible.</p>
        <p>它将是 0.1 的 10 次方。如果是硬币 A，连续 10 次出现正面的概率是一个更合理的数字。它是 0.9 的 10 次方。因此，这种事件更有可能发生在硬币 A 上，而不是硬币 B 上。连续看到十次正面的合理解释是，我实际上选择了硬币 A。</p><p>It’s going to be 0.1 to the 10th. If it’s coin A. The probability of 10 heads in a row is a more reasonable
            number. It’s 0.9 to the 10th. So this event is a lot more likely to occur with coin A, rather than coin B.
            The
            plausible explanation of having seen ten heads in a row is that I actually chose coin A.</p>
        <p>当你连续看到十次正面朝上时，你就可以肯定我们面对的是硬币 A。一旦你确定我们面对的是硬币 A，下一次抛掷正面朝上的概率是多少？这个概率是 0.9。所以本质上，我在这里进行的是推理计算。根据这些信息，我可以推断出我面对的是哪枚硬币。</p><p>When you see ten heads in a row, you are pretty certain that it’s coin A that we’re dealing with. And once
            you’re
            pretty certain that it’s coin A that we’re dealing with, what’s the probability that the next toss is heads?
            It’s going to be 0.9. So essentially here I’m doing an inference calculation. Given this information, I’m
            making
            an inference about which coin I’m dealing with.</p>
        <h2 id="unknown-54">未知</h2><h2>Unknown</h2>
        <p>我非常确定它是硬币 A，并且假设它是硬币 A，那么这个概率就是 0.9。我在这里放了一个近似符号，因为我的推断是近似的。我非常确定它是硬币 A。我不能 100% 确定它是硬币 A。但无论如何，这里发生的是，无条件概率与条件概率不同。</p><p>I become pretty certain that it’s coin A, and given that it’s coin A, this probability is going to be 0.9.
            And
            I’m putting an approximate sign here, because the inference that I did is approximate. I’m pretty certain
            it’s
            coin A. I’m not 100% certain that it’s coin A. But in any case what happens here is that the unconditional
            probability is different from the conditional probability.</p>
        <p>这些信息让我改变了对第 11 次抛硬币的信念。这意味着第 11 次抛硬币依赖于之前的抛硬币。因此，抛硬币现在变得具有依赖性。导致这种依赖性的物理联系是什么？物理联系就是硬币的选择。通过选择一枚特定的硬币，我在未来的抛硬币中引入了一种模式。这种模式就是导致依赖性的原因。</p><p>This information here makes me change my beliefs about the 11th toss. And this means that the 11th toss is
            dependent on the previous tosses. So the coin tosses have now become dependent. What is the physical link
            that
            causes this dependence? Well, the physical link is the choice of the coin. By choosing a particular coin,
            I’m
            introducing a pattern in the future coin tosses. And that pattern is what causes dependence.</p>
        <p>好吧，我在这里用词有点太随意了，因为我们定义了两个事件独立性的概念。但在这里我指的是独立抛硬币，我考虑的是多次抛硬币，比如 10 次或 11 次。所以为了恰当起见，我还应该为你定义多个事件独立性的概念，而不仅仅是两个事件独立性的概念。</p><p>OK, so I’ve been playing a little bit too loose with the language here, because we defined the concept of
            independence of two events. But here I have been referring to independent coin tosses, where I’m thinking
            about
            many coin tosses, like 10 or 11 of them. So to be proper, I should have defined for you also the notion of
            independence of multiple events, not just two.</p>
        <h2 id="unknown-55">未知</h2><h2>Unknown</h2>
        <p>我们不想仅仅说第一次抛硬币与第二次抛硬币无关。我们希望能够说，这 10 次抛硬币都是相互独立的。直观地说，这意味着同样的事情。关于一些抛硬币的信息不会改变你对其余抛硬币的看法。我们如何将其转化为数学定义？</p><p>We don’t want to just say coin toss one is independent from coin toss two. We want to be able to say
            something
            like, these 10 then coin tosses are all independent of each other. Intuitively what that means should be the
            same thing. that information about some of the coin tosses doesn’t change your beliefs about the remaining
            coin
            tosses. How do we translate that into a mathematical definition?</p>
        <p>好吧，强加这样的要求是一种丑陋的尝试。想象一下 A1 是第一次抛硬币是正面的事件。A2 是第二次抛硬币是正面的事件。A3 是第三次抛硬币是正面的事件，依此类推。这是一个事件，其发生与否不是由前三次抛硬币决定的。这是一个事件，其发生与否是由第五次和第六次抛硬币决定的。</p><p>Well, an ugly attempt would be to impose requirements such as this. Think of A1 being the event that the
            first
            flip was heads. A2 is the event of that the second flip was heads. A3, the third flip, was heads, and so on.
            Here is an event whose occurrence is not determined by the first three coin flips. And here’s an event whose
            occurrence or not is determined by the fifth and sixth coin flip.</p>
        <p>如果我们从物理角度认为所有这些抛硬币事件之间都毫无关联，那么第五次和第六次抛硬币事件的信息不会改变我们对前三次抛硬币事件的预期。因此，该事件的概率，即条件概率，应该与无条件概率相同。</p><p>If we think physically that all those coin flips have nothing to do with each other, information about the
            fifth
            and sixth coin flip are not going to change what we expect from the first three. So the probability of this
            event, the conditional probability, should be the same as the unconditional probability.</p>
        <h2 id="unknown-56">未知</h2><h2>Unknown</h2>
        <p>我们希望这种关系是真的，无论你写下什么样的公式，只要这里出现的事件与那里出现的事件不同。好的。这是一个丑陋的定义。真正起作用并导致所有此类公式的数学定义如下。</p><p>And we would like a relation of this kind to be true, no matter what kind of formula you write down, as long
            as
            the events that show up here are different from the events that show up there. OK. That’s sort of an ugly
            definition. The mathematical definition that actually does the job, and leads to all the formulas of this
            kind,
            is the following.</p>
        <p>如果我们可以通过乘以概率来找到事件联合发生的概率，那么我们就说事件集合是独立的。即使你查看这些事件的子集合，这也是正确的。让我们更精确一点。如果我们有三个事件，定义告诉我们，如果以下条件为真，那么这三个事件是独立的。概率 A1、A2 和 A3，你可以通过乘以各个概率来计算这个概率。</p><p>We’re going to say that the collection of events are independent if we can find the probability of their
            joint
            occurrence by just multiplying probabilities. And that will be true even if you look at sub collections of
            these
            events. Let’s make that more precise. If we have three events, the definition tells us that the three events
            are
            independent if the following are true. Probability A1 and A2 and A3, you can calculate this probability by
            multiplying individual probabilities.</p>
        <p>但即使你选取的事件更少，情况也是如此。我们只选取可用的索引中的几个索引。所以我们还要求 P(A1 交集 A2) 等于 P(A1) 乘以 P(A2)。对于选择索引的其他可能性也是如此。好的，独立性，数学定义，要求计算我们手中任何事件交集的概率，只需将各个概率相乘即可完成计算。</p><p>But the same is true even if you take fewer events. Just a few indices out of the indices that we have
            available.
            So we also require P(A1 intersection A2) is P(A1) times P(A2). And similarly for the other possibilities of
            choosing the indices. OK, so independence, mathematical definition, requires that calculating probabilities
            of
            any intersection of the events we have in our hands, that calculation can be done by just multiplying
            individual
            probabilities.</p>
        <h2 id="unknown-57">未知</h2><h2>Unknown</h2>
        <p>这必须适用于我们考虑手头上的所有事件或这些事件的子集的情况。现在这些关系本身被称为成对独立性。例如，这个关系告诉我们 A1 独立于 A2。这告诉我们 A2 独立于 A3。这将告诉我们 A1 独立于 A3。但所有事件的独立性实际上需要更多。
        </p><p>And this has to apply to the case where we consider all of the events in our hands or just sub collections of
            those events. Now these relations just by themselves are called pairwise independence. So this relation, for
            example, tells us that A1 is independent from A2. This tells us that A2 is independent from A3. This will
            tell
            us that A1 is independent from A3. But independence of all the events together actually requires a little
            more.
        </p>
        <p>还有一个等式与同时考虑所有三个事件有关。而且这个额外的等式并不是多余的。它确实会产生影响。独立性和成对独立性是不同的东西。让我们用一个例子来说明这种情况。假设我们有两次抛硬币。</p><p>One more equality that has to do with all three events being considered at the same time. And this extra
            equality
            is not redundant. It actually does make a difference. Independence and pairwise independence are different
            things. So let’s illustrate the situation with an example. Suppose we have two coin flips.</p>
        <p>两次抛硬币是独立的，所以偏差是 1/2，所以所有可能结果的概率都是 1/2 乘以 1/2，也就是 1/4。现在让我们考虑一系列不同的事件。一个事件是第一次抛硬币是正面。这是这里的蓝色集合。另一个事件是第二次抛硬币是正面。这是这里的黑色事件。好。这两个事件是独立的吗？如果你用数学方法检查，答案是肯定的。</p><p>The coin tosses are independent, so the bias is 1/2, so all possible outcomes have a probability of 1/2 times
            1/2, which is 1/4. And let’s consider now a bunch of different events. One event is that the first toss is
            heads. This is this blue set here. Another event is the second toss is heads. And this is this black event
            here.
            OK. Are these two events independent? If you check it mathematically, yes.</p>
        <h2 id="unknown-58">未知</h2><h2>Unknown</h2>
        <p>A 的概率是 B 的概率的 1/2。A 的概率乘以 B 的概率是 1/4，这与 A 和 B 相交的概率相同，也就是这个集合。所以我们刚刚从数学上验证了 A 和 B 是独立的。现在让我们考虑第三个事件，即第一次和第二次抛掷给出相同的结果。我将使用不同的颜色。第一次和第二次抛掷给出相同的结果。</p><p>Probability of A is probability of B is 1/2. Probability of A times probability of B is 1/4, which is the
            same as
            the probability of A intersection B, which is this set. So we have just checked mathematically that A and B
            are
            independent. Now lets consider a third event which is that the first and second toss give the same result.
            I’ll
            use a different color. First and second toss to give the same result.</p>
        <p>这就是我们掷出正面或反面的事件。所以这是 C 的概率。C 的概率是多少？好吧，C 由两个结果组成，每个结果的概率都是 1/4，所以 C 的概率是 1/2。C 与 A 相交的概率是多少？C 与 A 相交就是这一个结果，概率为 1/4。A 与 B 相交 C 的概率是多少？</p><p>This is the event that we obtain heads, heads or tails, tails. So this is the probability of C. What’s the
            probability of C? Well, C is made up of two outcomes, each one of which has probability 1/4, so the
            probability
            of C is 1/2. What is the probability of C intersection A? C intersection A is just this one outcome, and has
            probability 1/4. What’s the probability of A intersection B intersection C?</p>
        <p>这三个事件恰好与这个结果相交，所以这个概率也是 1/4。好的。给定 A 和 B，C 的概率是多少？如果 A 发生了，B 也发生了，你肯定这个结果发生了。如果第一次抛掷是 H，第二次抛掷也是 H，那么你肯定第一次和第二次抛掷的结果相同。
        </p><p>The three events intersect just this outcome, so this probability is also 1/4. OK. What’s the probability of
            C
            given A and B? If A has occurred, and B has occurred, you are certain that this outcome here happened. If
            the
            first toss is H and the second toss is H, then you’re certain of the first and second toss gave the same
            result.
        </p>
        <h2 id="unknown-59">未知</h2><h2>Unknown</h2>
        <p>因此，在给定 A 和 B 的情况下，C 的条件概率等于 1。那么在这个例子中，我们是否具有独立性？没有。C，即我们在第一次和第二次抛掷中获得相同结果的概率为 1/2。一半的可能结果会让我们两次抛硬币得到相同的结果。正面，正面或反面，反面。</p><p>So the conditional probability of C given A and B is equal to 1. So do we have independence in this example?
            We
            don’t. C, that we obtain the same result in the first and the second toss, has probability 1/2. Half of the
            possible outcomes give us two coin flips with the same result. heads, heads or tails, tails.</p>
        <p>因此 C 的概率是 1/2。但如果我告诉你事件 A 和 B 都发生了，那么你肯定 C 发生了。如果我告诉你掷出的是正面和正面，那么你肯定结果是一样的。因此，条件概率不同于无条件概率。因此，通过将这两个关系结合在一起，我们得出这三个事件不是独立的。但它们是两两独立的吗？</p><p>So the probability of C is 1/2. But if I tell you that the events A and B both occurred, then you’re certain
            that
            C occurred. If I tell you that we had heads and heads, then you’re certain the outcomes were the same. So
            the
            conditional probability is different from the unconditional probability. So by combining these two relations
            together, we get that the three events are not independent. But are they pairwise independent?</p>
        <p>A 独立于 B 吗？是的，因为 A 的概率乘以 B 的概率是 1/4，也就是 A 与 B 相交的概率。C 独立于 A 吗？嗯，C 和 A 的概率是 1/4。C 的概率是 1/2。A 的概率是 1/2。所以它符合条件。1/4 等于 1/2 和 1/2，所以事件 C 和事件 A 是独立的。
        </p><p>Is A independent from B? Yes, because probability of A times probability of B is 1/4, which is probability of
            A
            intersection B. Is C independent from A? Well, the probability of C and A is 1/4. The probability of C is
            1/2.
            The probability of A is 1/2. So it checks. 1/4 is equal to 1/2 and 1/2, so event C and event A are
            independent.
        </p>
        <h2 id="unknown-60">未知</h2><h2>Unknown</h2>
        <p>知道第一次掷硬币是正面并不会改变你对两次掷硬币结果是否相同的信念。知道第一次掷硬币是正面，那么第二次掷硬币是正面或反面的概率也是相同的。因此，事件 C 发生的概率也是 1/2。换个说法，如果我告诉你两次结果相同。</p><p>Knowing that the first toss was heads does not change your beliefs about whether the two tosses are going to
            have
            the same outcome or not. Knowing that the first was heads, well, the second is equally likely to be heads or
            tails. So event C has just the same probability, again, 1/2, to occur. To put it the opposite way, if I tell
            you
            that the two results were the same.</p>
        <p>所以要么是正面，要么是反面。这告诉你第一次抛掷的结果是什么？是正面，还是反面？嗯，这什么也没告诉你。两次抛掷的结果都可能是正面，所以第一次抛掷正面的概率等于 1/2，告诉你 C 发生了，这并没有改变任何事情。</p><p>So it’s either heads, heads or tails, tails. what does that tell you about the first toss? Is it heads, or is
            it
            tails? Well, it doesn’t tell you anything. It could be either over the two, so the probability of heads in
            the
            first toss is equal to 1/2, and telling you C occurred does not change anything.</p>
        <p>所以这是一个例子，说明了我们有三个事件的情况，我们检查其中两个事件的任意组合是否具有成对独立性。它们的交集概率等于它们概率的乘积。另一方面，这三个事件加在一起并不是独立的。A 没有告诉我任何有用的信息，无论 C 是否会发生。B 没有告诉我任何有用的信息。</p><p>So this is an example that illustrates the case where we have three events in which we check that pairwise
            independence holds for any combination of two of these events. We have the probability of their intersection
            is
            equal to the product of their probabilities. On the other hand, the three events taken all together are not
            independent. A doesn’t tell me anything useful, whether C is going to occur or not. B doesn’t tell me
            anything
            useful.</p>
        <h2 id="unknown-61">未知</h2><h2>Unknown</h2>
        <p>但如果我告诉你 A 和 B 都发生了，那么它们两个结合起来会告诉我一些关于 C 的有用信息。也就是说，它们告诉我 C 肯定发生了。很好。所以独立性是一个有点微妙的概念。一旦你掌握了它的真正含义，那么事情可能就会变得顺理成章。但这是一个很容易产生误解的概念。所以花点时间去消化一下。</p><p>But if I tell you that both A and B occurred, the two of them together tell me something useful about C.
            Namely,
            they tell me that C certainly has occurred. Very good. So independence is this somewhat subtle concept. Once
            you
            grasp the intuition of what it really means, then things perhaps fall in place. But it’s a concept where
            it’s
            easy to get some misunderstanding. So just take some time to digest.</p>
        <p>因此，为了让大家轻松一点，我将用剩下的四分钟来讨论一个非常简单的问题，它涉及条件概率等。这就是问题，其表述方式与各种教科书中的表述方式完全相同。其表述方式如下。好吧，考虑一个不合时宜的地方，那里仍然有国王或王后，而且男孩实际上比女孩优先。所以如果有一个男孩。</p><p>So to lighten things up, I’m going to spend the remaining four minutes talking about the very nice, simple
            problem that involves conditional probabilities and the like. So here’s the problem, formulated exactly as
            it
            shows up in various textbooks. And the formulation says the following. Well, consider one of those
            anachronistic
            places where they still have kings or queens, and where actually boys take precedence over girls. So if
            there is
            a boy.</p>
        <p>如果王室有一个男孩，那么他就会成为国王，即使他有一个姐姐可能是王后。所以我们有一个这样的王室。那个王室有两个孩子，我们知道有一个国王。有一个国王，这意味着两个孩子中至少有一个是男孩。否则我们就不会有国王了。</p><p>If the royal family has a boy, then he will become the king even if he has an older sister who might be the
            queen. So we have one of those royal families. That royal family had two children, and we know that there is
            a
            king. There is a king, which means that at least one of the two children was a boy. Otherwise we wouldn’t
            have a
            king.</p>
        <h2 id="unknown-62">未知</h2><h2>Unknown</h2>
        <p>国王的兄弟姐妹是女性的概率是多少？好的。我想我们需要对遗传学做一些假设。我们假设每个孩子是男孩或女孩的概率为 1/2，并且不同的孩子，他们的性格与其他孩子是独立的。所以每个孩子的出生基本上都是抛硬币。好的，所以如果你接受这个，你会说，好吧，国王是个孩子。他的兄弟姐妹是另一个孩子。</p><p>What is the probability that the king’s sibling is female? OK. I guess we need to make some assumptions about
            genetics. Let’s assume that every child is a boy or a girl with probability 1/2, and that different
            children,
            what they are is independent from what the other children were. So every childbirth is basically a coin
            flip.
            OK, so if you take that, you say, well, the king is a child. His sibling is another child.</p>
        <p>孩子们彼此独立。所以兄弟姐妹是女孩的概率是 1/2。这是天真的答案。现在让我们试着正式地做一下。让我们建立一个实验模型。据说，皇室有两个孩子，所以有四种结果。男孩、男孩、女孩、女孩、男孩和女孩。现在，我们被告知有一位国王，这意味着什么？这个结果没有发生。这是不可能的。</p><p>Children are independent of each other. So the probability that the sibling is a girl is 1/2. That’s the
            naive
            answer. Now let’s try to do it formally. Let’s set up a model of the experiment. The royal family had two
            children, as we’re told, so there’s four outcomes. boy, boy girl, girl boy, and girl. Now, we are told that
            there is a king, which means what? This outcome here did not happen. It is not possible.</p>
        <p>仍有三种可能的结果。因此，在存在国王的情况下，这是我们的条件样本空间。原始模型的概率是多少？好吧，在模型中，我们假设每个孩子都是男孩或女孩的概率为 1/2，那么这四种结果的概率是相等的，它们就像这样。这些是原始概率。</p><p>There are three outcomes that remain possible. So this is our conditional sample space given that there is
            king.
            What are the probabilities for the original model? Well with the model that we assume that every child is a
            boy
            or a girl independently with probability 1/2, then the four outcomes would be equally likely, and they’re
            like
            this. These are the original probabilities.</p>
        <h2 id="unknown-63">未知</h2><h2>Unknown</h2>
        <p>但是一旦我们被告知这个结果不会发生，因为我们有一位国王，那么我们就会被带到更小的样本空间。在这个样本空间中，兄弟姐妹是女孩的概率是多少？好吧，在三个结果中有两个兄弟姐妹是女孩。所以兄弟姐妹是女孩的概率实际上是 2/3。所以这应该是正确的答案。也许有点违反直觉。</p><p>But once we are told that this outcome did not happen, because we have a king, then we are transported to the
            smaller sample space. In this sample space, what’s the probability that the sibling is a girl? Well the
            sibling
            is a girl in two out of the three outcomes. So the probability that the sibling is a girl is actually 2/3.
            So
            that’s supposed to be the right answer. Maybe a little counter intuitive.</p>
        <p>所以你可以耍小聪明说，哦，我比你更了解这类问题，这是一个棘手的问题，这就是为什么答案是 2/3。但实际上，我没有充分的理由说答案是 2/3。当我提出这个模型时，我做了很多隐藏的假设，我还没有说出来。所以为了对这个答案进行逆向工程，让我们真正思考一下这个答案应该是正确答案的概率模型是什么。</p><p>So you can play smart and say, oh I understand such problems better than you, here is a trick problem and
            here’s
            why the answer is 2/3. But actually I’m not fully justified in saying that the answer is 2/3. I made lots of
            hidden assumptions when I put this model down, which I didn’t yet state. So to reverse engineer this answer,
            let’s actually think what’s the probability model for which this would have been the right answer.</p>
        <p>这是概率模型。皇室。皇室父母决定只生两个孩子。他们生下了他们。结果至少有一个是男孩，并成为了国王。在这种情况下。他们决定只生两个孩子。那么这就是大样本空间。结果一个是男孩。这就排除了这个结果。那么这幅图是正确的，这就是正确答案。</p><p>And here’s the probability model. The royal family. the royal parents decided to have exactly two children.
            They
            went and had them. It turned out that at least one was a boy and became a king. Under this scenario. that
            they
            decide to have exactly two children. then this is the big sample space. It turned out that one was a boy.
            That
            eliminates this outcome. And then this picture is correct and this is the right answer.</p>
        <h2 id="unknown-64">未知</h2><h2>Unknown</h2>
        <p>但其中隐藏着一些假设。如果王室遵循以下策略会怎么样？我们会生孩子，直到生出男孩，这样我们就会生出国王，然后我们就停止。好吧，假设他们有两个孩子，兄弟姐妹是女孩的概率是多少？是 1。他们有两个孩子的原因是因为第一个是女孩，所以他们必须生第二个。</p><p>But there’s hidden assumptions being there. How about if the royal family had followed the following
            strategy?
            We’re going to have children until we get a boy, so that we get a king, and then we’ll stop. OK, given they
            have
            two children, what’s the probability that the sibling is a girl? It’s 1. The reason that they had two
            children
            was because the first was a girl, so they had to have a second.</p>
        <p>因此，关于生育习惯的假设确实需要被纳入考虑范围，而这些假设将会影响决策。或者，如果这是一个古老的王国，国王总是会确保扼杀他的任何一个兄弟，那么兄弟姐妹是女孩的概率实际上又是 1，依此类推。</p><p>So assumptions about reproductive practices actually need to come in, and they’re going to affect the
            decisions.
            Or, if it’s one of those ancient kingdoms where a king would always make sure too strangle any of his
            brothers,
            then the probability that the sibling is a girl is actually 1 again, and so on.</p>
        <p>所以这意味着当你开始处理措辞松散的问题时，需要小心谨慎，确保它到底意味着什么，以及你做出了什么假设。好的，下周见。</p><p>So it means that one needs to be careful when you start with loosely worded problems to make sure exactly
            what it
            means and what assumptions you’re making. All right, see you next week.</p>
        <h1 id="counting">4. 计数</h1><h1>4. Counting</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:34 <a href="https://youtube.com/watch?v=6oV3pKLgW2I">https://youtube.com/watch?v=6oV3pKLgW2I</a></p><p> 12 years ago (Nov 10, 2012) — 51:34 <a href="https://youtube.com/watch?v=6oV3pKLgW2I">https://youtube.com/watch?v=6oV3pKLgW2I</a></p>
        <h2 id="unknown-65">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：好的。今天的讲座将讨论计数。所以我认为计数在概念上是一件非常简单的事情，但它也是一个相当棘手的话题。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation, or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: OK. So today’s lecture
            will be
            on the subject of counting. So counting, I guess, is a pretty simple affair conceptually, but it’s a topic
            that
            can also get to be pretty tricky.</p>
        <p>我们之所以要讨论计数，是因为有很多概率问题的解决方案实际上可以归结为成功计算各种集合的基数。所以我们将看到人们可以在各种情况下系统地计数的基本、最简单的方法。因此，与之前的讲座不同，我们不会介绍任何重要的概率性质的新概念。我们只会使用我们已经知道的概率工具。</p><p>The reason we’re going to talk about counting is that there’s a lot of probability problems whose solution
            actually reduces to successfully counting the cardinalities of various sets. So we’re going to see the
            basic,
            simplest methods that one can use to count systematically in various situations. So in contrast to previous
            lectures, we’re not going to introduce any significant new concepts of a probabilistic nature. We’re just
            going
            to use the probability tools that we already know.</p>
        <p>我们将在涉及计数的情况下应用它们。今天我们只是触及这个主题的表面。数学中有一门叫做组合学的学科，研究组合学的人实际上一生都在计算越来越复杂的集合。</p><p>And we’re going to apply them in situations where there’s also some counting involved. Now, today we’re going
            to
            just touch the surface of this subject. There’s a whole field of mathematics called combinatorics who are
            people
            who actually spend their whole lives counting more and more complicated sets.</p>
        <h2 id="unknown-66">未知</h2><h2>Unknown</h2>
        <p>我们不可能接近该领域的全部复杂性，但我们会获得足够的工具，使我们能够解决在最常见情况下遇到的问题。因此，基本思想、基本原则是我们已经讨论过的。因此，计数方法适用于我们有有限数量结果的概率实验，并且每个结果、每个可能结果的情况。
        </p><p>We were not going to get anywhere close to the full complexity of the field, but we’ll get just enough tools
            that
            allow us to address problems of the type that one encounters in most common situations. So the basic idea,
            the
            basic principle is something that we’ve already discussed. So counting methods apply in situations where we
            have
            probabilistic experiments with a finite number of outcomes and where every outcome. every possible outcome.
        </p>
        <p>发生概率相同。所以我们有样本空间 omega，里面有一堆离散点。集合 omega 的基数是大写 N。因此，具体来说，我们假设样本点发生概率相等，这意味着样本空间的每个元素都具有相同的概率，即 1/N。然后我们对样本空间的一个子集感兴趣，称之为 A。</p><p>Has the same probability of occurring. So we have our sample space, omega, and it’s got a bunch of discrete
            points in there. And the cardinality of the set omega is some capital N. So, in particular, we assume that
            the
            sample points are equally likely, which means that every element of the sample space has the same
            probability
            equal to 1 over N. And then we are interested in a subset of the sample space, call it A.</p>
        <p>该子集由多个元素组成。假设该子集的基数等于小 n。然后，要找到该集合的概率，我们需要做的就是将各个元素的概率相加。有小 n 个元素，每个元素的概率为大 N 之 1。这就是答案。
        </p><p>And that subset consists of a number of elements. Let the cardinality of that subset be equal to little
            n.&nbsp;And
            then to find the probability of that set, all we need to do is to add the probabilities of the individual
            elements. There’s little n elements, and each one has probability one over capital N. And that’s the answer.
        </p>
        <h2 id="unknown-67">未知</h2><h2>Unknown</h2>
        <p>因此，这意味着要解决此上下文中的问题，我们需要做的就是找出大写 N 和小写 n。现在，如果有人给你一个集合，只是给你一个列表，然后给你另一个集合，同样，给你一个列表，那么计算元素就很容易了。你只需计算列表上有多少元素即可。</p><p>So this means that to solve problems in this context, all that we need to be able to do is to figure out the
            number capital N and to figure out the number little n.&nbsp;Now, if somebody gives you a set by just giving you
            a
            list and gives you another set, again, giving you a list, it’s easy to count there element. You just count
            how
            much there is on the list.</p>
        <p>但有时集合的描述方式更为隐晦，我们可能需要做更多工作。正确计数涉及各种技巧。最常见的技巧是。当您考虑一组可能的结果时，通过顺序过程描述这些可能结果的构造。</p><p>But sometimes the sets are described in some more implicit way, and we may have to do a little bit more work.
            There’s various tricks that are involved in counting properly. And the most common one is to. when you
            consider
            a set of possible outcomes, to describe the construction of those possible outcomes through a sequential
            process.</p>
        <p>想象一下一个概率实验，它涉及多个阶段，每个阶段都可能存在多种可能的选择。整个实验包括执行所有阶段直至结束。样本空间中的点数就是这个多阶段实验中可能有多少个最终结果。</p><p>So think of a probabilistic experiment that involves a number of stages, and in each one of the stages
            there’s a
            number of possible choices that there may be. The overall experiment consists of carrying out all the stages
            to
            the end. And the number of points in the sample space is how many final outcomes there can be in this multi
            stage experiment.</p>
        <h2 id="unknown-68">未知</h2><h2>Unknown</h2>
        <p>因此，在这张图中，我们进行了一个实验，在第一阶段我们有四个选择。在第二阶段，无论第一阶段发生了什么，按照这个图的方式，我们都有三个选择。无论我们最终是在这里、那里还是那里，在第二阶段我们都有三个选择。</p><p>So in this picture we have an experiment in which of the first stage we have four choices. In the second
            stage,
            no matter what happened in the first stage, the way this is drawn we have three choices. No matter whether
            we
            ended up here, there, or there, we have three choices in the second stage.</p>
        <p>然后是第三阶段，至少在这张图中，无论前两个阶段发生了什么，在第三阶段我们都会有两个可能的选择。那么这棵树的末端有多少片叶子呢？这很简单。它只是这三个数字的乘积。</p><p>And then there’s a third stage and at least in this picture, no matter what happened in the first two stages,
            in
            the third stage we’re going to have two possible choices. So how many leaves are there at the end of this
            tree?
            That’s simple. It’s just the product of these three numbers.</p>
        <p>我们可能拥有的叶子数量是 4 乘以 3 乘以 2。每个阶段的选择数量都会相乘，这给出了总体选择的数量。所以这是一般规则，我们将反复使用的一般技巧。所以让我们将它应用于一些非常简单的问题作为热身。</p><p>The number of possible leaves that we have out there is 4 times 3 times 2. Number of choices at each stage
            gets
            multiplied, and that gives us the number of overall choices. So this is the general rule, the general trick
            that
            we are going to use over and over. So let’s apply it to some very simple problems as a warm up.</p>
        <h2 id="unknown-69">未知</h2><h2>Unknown</h2>
        <p>如果允许使用三个字母，然后是四个数字，那么可以制作多少个车牌？至少如果你使用英文字母，那么第一个字母有 26 种选择。然后第二个字母有 26 种选择。然后第三个字母有 26 种选择。然后我们开始数字。</p><p>How many license plates can you make if you’re allowed to use three letters and then followed by four digits?
            At
            least if you’re dealing with the English alphabet, you have 26 choices for the first letter. Then you have
            26
            choices for the second letter. And then 26 choices for the third letter. And then we start the digits.</p>
        <p>第一个数字有 10 个选择，第二个数字有 10 个选择，第三个数字有 10 个选择，最后一个数字有 10 个选择。让我们让它稍微复杂一点，假设我们对没有字母重复和数字重复的车牌感兴趣。所以你必须使用不同的字母，不同的数字。你能制作多少个车牌？好的，让我们选择第一个字母，我们有 26 个选择。</p><p>We have 10 choices for the first digit, 10 choices for the second digit, 10 choices for the third, 10 choices
            for
            the last one. Let’s make it a little more complicated, suppose that we’re interested in license plates where
            no
            letter can be repeated and no digit can be repeated. So you have to use different letters, different digits.
            How
            many license plates can you make? OK, let’s choose the first letter, and we have 26 choices.</p>
        <p>现在，我准备选择第二个字母，我有多少个选择？我有 25 个，因为我已经用了一个字母。我还有 25 个字母可供选择。对于下一个字母，有多少个选择？好吧，我用完了两个字母，所以我只有 24 个可用。</p><p>Now, I’m ready to choose my second letter, how many choices do I have? I have 25, because I already used one
            letter. I have the 25 remaining letters to choose from. For the next letter, how many choices? Well, I used
            up
            two of my letters, so I only have 24 available.</p>
        <h2 id="unknown-70">未知</h2><h2>Unknown</h2>
        <p>然后我们从数字开始，第一个数字有 10 个选择，第二个数字有 9 个选择，第三个数字有 8 个选择，最后一个数字有 7 个选择。好了。现在，让我们将一些符号带入相关问题中。给定一个由 n 个元素组成的集合，你需要将这 n 个元素按顺序排列。也就是对它们进行排序。这些元素的任何可能排序都称为排列。</p><p>And then we start with the digits, 10 choices for the first digit, 9 choices for the second, 8 for the third,
            7
            for the last one. All right. So, now, let’s bring some symbols in a related problem. You are given a set
            that
            consists of n elements and you’re supposed to take those n elements and put them in a sequence. That is to
            order
            them. Any possible ordering of those elements is called a permutation.</p>
        <p>例如，如果我们有集合 1,2.3,4，那么一个可能的排列就是列表 2,3.4,1。这是一个可能的排列。当然，还有许多可能的排列，问题是有多少种。好的，让我们考虑通过一次选择一个来构建这个排列。这些元素中的哪一个会进入每个位置？进入第一个位置或元素的数字有多少种选择？</p><p>So for example, if we have the set 1,2.3,4, a possible permutation is the list 2,3.4,1. That’s one possible
            permutation. And there’s lots of possible permutations, of course, the question is how many are there. OK,
            let’s
            think about building this permutation by choosing one at a time. Which of these elements goes into each one
            of
            these slots? How many choices for the number that goes into the first slot or the elements?</p>
        <p>好吧，我们可以选择任何一个可用元素，所以我们有 n 个选择。假设这个元素放在这里，用完那个元素后，我们剩下 n-1 个元素，我们可以从中挑选任何一个并将其放入第二个位置。所以这里我们有 n 个选择，这里我们将有 n-1 个选择，那么我们放在那里的元素数量将是 n-2 个选择。</p><p>Well, we can choose any one of the available elements, so we have n choices. Let’s say this element goes
            here,
            having used up that element, we’re left with n minus 1 elements and we can pick any one of these and bring
            it
            into the second slot. So here we have n choices, here we’re going to have n minus 1 choices, then how many
            we
            put there will have n minus 2 choices.</p>
        <h2 id="unknown-71">未知</h2><h2>Unknown</h2>
        <p>然后你一直往下直到最后。当你要选择最后一个元素时会发生什么？嗯，你已经使用了 n 减去它们，你的包里只剩下一个了。你不得不使用那个。所以在最后一个阶段，你只有一个选择。所以，基本上，可能的排列数是从 n 到 1 的所有整数的乘积，或者从 1 到 n 的所有整数的乘积。&nbsp;</p><p>And you go down until the end. What happens at this point when you are to pick the last element? Well, you’ve
            used n minus of them, there’s only one left in your bag. You’re forced to use that one. So the last stage,
            you’re going to have only one choice. So, basically, the number of possible permutations is the product of
            all
            integers from n down to one, or from one up to n.&nbsp;</p>
        <p>我们用一个符号来表示这个数字，它被称为 n 阶乘。所以 n 阶乘是 n 个对象的排列数。你可以对给定的 n 个对象进行排序的方式数。现在，一个不同的等式。我们有 n 个元素。假设元素为 1、1、2，直到 n。这是一个集合。我们想创建一个子集。有多少个可能的子集？</p><p>And there’s a symbol that we use for this number, it’s called n factorial. So n factorial is the number of
            permutations of n objects. The number of ways that you can order n objects that are given to you. Now, a
            different equation. We have n elements. Let’s say the elements are 1,1,2, up to n.&nbsp;And it’s a set. And we
            want
            to create a subset. How many possible subsets are there?</p>
        <p>因此，说到子集，就意味着要查看每个元素，并决定是否将其放入子集中。例如，我可以选择放入 1，但不放入 2，不放入 3，放入 4，等等。这就是创建子集的方法。</p><p>So speaking of subsets means looking at each one of the elements and deciding whether you’re going to put it
            in
            to subsets or not. For example, I could choose to put 1 in, but 2 I’m not putting it in, 3 I’m not putting
            it
            in, 4 I’m putting it, and so on. So that’s how you create a subset.</p>
        <h2 id="unknown-72">未知</h2><h2>Unknown</h2>
        <p>你看着每一个元素，然后说，好吧，我要把它放在子集中，或者我不放它。所以把它们想象成由几个阶段组成。在每个阶段，你都会看一个元素，然后做出一个二元决策。我要把它放在子集中吗？还是不放？因此，有多少个子集？嗯，对于第一个元素，我有两个选择。</p><p>You look at each one of the elements and you say, OK, I’m going to put it in the subset, or I’m not going to
            put
            it. So think of these as consisting of stages. At each stage you look at one element, and you make a binary
            decision. Do I put it in the subset, or not? So therefore, how many subsets are there? Well, I have two
            choices
            for the first element.</p>
        <p>我是否要放入子集？对于下一个元素，我有两个选择，依此类推。对于每个元素，我们都有两个选择。因此，选择总数是 2 的 n 次方。因此，结论是，子集（通常是 n 个元素的集合）的数量是 2 的 n 次方。因此，具体来说，如果我们取 n 等于 1，让我们检查一下我们的答案是否合理。</p><p>Am I going to put in the subset, or not? I have two choices for the next element, and so on. For each one of
            the
            elements, we have two choices. So the overall number of choices is 2 to the power n.&nbsp;So, conclusion. the
            number
            of subsets, often n element set, is 2 to the n.&nbsp;So in particular, if we take n equal to 1, let’s check that
            our
            answer makes sense.</p>
        <p>如果 n 等于 1，那么它有多少个子集？所以我们处理的是只有一个元素的集合。子集是什么？一个子集就是这个。我们有一个元素集的其他子集吗？是的，我们有空集。这是第二个。这是这个特定集合的两个可能子集。所以当 n 等于 1 时有 2 个子集，这可以检查答案。好的。</p><p>If we have n equal to one, how many subsets does it have? So we’re dealing with a set of just one. What are
            the
            subsets? One subset is this one. Do we have other subsets of the one element set? Yes, we have the empty
            set.
            That’s the second one. These are the two possible subsets of this particular set. So 2 subsets when n is
            equal
            to 1, that checks the answer. All right.</p>
        <h2 id="unknown-73">未知</h2><h2>Unknown</h2>
        <p>好了，既然已经讲了这么多，我们现在可以做第一个例子了。我们拿到一个骰子，然后要掷 6 次。好了，让我们对掷骰子做出一些假设。我们假设掷骰子是独立的，而且骰子也是公平的。这意味着掷骰子的任何特定结果的概率。</p><p>OK, so having gone so far, we can do our first example now. So we are given a die and we’re going to roll it
            6
            times. OK, let’s make some assumptions about the rolls. Let’s assume that the rolls are independent, and
            that
            the die is also fair. So this means that the probability of any particular outcome of the die rolls.</p>
        <p>例如，我们有 6 次投掷，一个特定结果可能是 3、3、1、6、5。所以这是一个可能的结果。这个结果的概率是多少？发生这种情况的概率是 1/6，发生这种情况的概率是 1/6，发生这种情况的概率是 1/6，等等。所以结果为这个的概率是 1/6 的六分之一。我使用什么来得出这个答案？</p><p>For example, so we have 6 rolls, one particular outcome could be 3,3,1,6,5. So that’s one possible outcome.
            What’s the probability of this outcome? There’s probability 1/6 that this happens, 1/6 that this happens,
            1/6
            that this happens, and so on. So the probability that the outcome is this is 1/6 to the sixth. What did I
            use to
            come up with this answer?</p>
        <p>我使用了独立性，所以我将第一次掷骰子得到 2 的概率乘以第二次掷骰子得到 3 的概率，以此类推。然后我假设骰子是公平的，所以 2 的概率是 1/6，3 的概率是 1/6，以此类推。</p><p>I used independence, so I multiplied the probability of the first roll gives me a 2, times the probability
            that
            the second roll gives me a 3, and so on. And then I used the assumption that the die is fair, so that the
            probability of 2 is 1/6, the probably of 3 is 1/6, and so on.</p>
        <h2 id="unknown-74">未知</h2><h2>Unknown</h2>
        <p>所以如果我要把它说清楚，那就是我们在第一轮掷骰子中得到 2 的概率，乘以在第二轮掷骰子中得到 3 的概率，再乘以在最后一轮掷骰子中得到 5 的概率。所以通过独立性，我可以乘以概率。而且因为骰子是公平的，所以每个数字都是 1/6 的六分之一。所以无论我在这里输入什么数字，同样的计算方法都适用。</p><p>So if I were to spell it out, it’s the probability that we get the 2 in the first roll, times the probability
            of
            3 in the second roll, times the probability of the 5 in the last roll. So by independence, I can multiply
            probabilities. And because the die is fair, each one of these numbers is 1/6 to the sixth. And so the same
            calculation would apply no matter what numbers I would put in here.</p>
        <p>所以所有可能的结果都是同样可能出现的。让我们从这个开始。既然所有可能的结果都同样可能找到概率问题的答案，如果我们要处理某个特定事件，那么事件就是所有掷骰子的结果都不同。这就是我们的事件 A。我们的样本空间是某个大写欧米茄集合。
        </p><p>So all possible outcomes are equally likely. Let’s start with this. So since all possible outcomes are
            equally
            likely to find an answer to a probability question, if we’re dealing with some particular event, so the
            event is
            that all rolls give different numbers. That’s our event A. And our sample space is some set capital omega.
        </p>
        <p>我们知道答案是集合 A 的基数除以集合 omega 的基数。所以我们先处理简单的问题。样本空间中有多少个元素？掷骰子 6 次会有多少种可能的结果？第一次掷骰子有 6 种选择。第二次掷骰子有 6 种选择，依此类推。</p><p>We know that the answer is going to be the cardinality of the set A, divided by the cardinality of the set
            omega.
            So let’s deal with the easy one first. How many elements are there in the sample space? How many possible
            outcomes are there when you roll a dice 6 times? You have 6 choices for the first roll. You have 6 choices
            for
            the second roll and so on.</p>
        <h2 id="unknown-75">未知</h2><h2>Unknown</h2>
        <p>因此，结果的总数将是 6 的六次方。因此，样本空间中的元素数是 6 的六次方。我想这与此相符。我们有 6 的六次方个结果，每个结果都有这么多的概率，因此总概率等于一。对吗？因此，单个结果的概率是可能结果数除以一，就是这个。好的。</p><p>So the overall number of outcomes is going to be 6 to the sixth. So number of elements in the sample space is
            6
            to the sixth power. And I guess this checks with this. We have 6 to the sixth outcomes, each one has this
            much
            probability, so the overall probability is equal to one. Right? So the probability of an individual outcome
            is
            one over how many possible outcomes we have, which is this. All right.</p>
        <p>那么分子呢？我们感兴趣的是所有数字都不同的结果。那么所有数字都不同的结果是什么呢？所以骰子有 6 个面。我们掷 6 次。我们会得到 6 个不同的数字。这意味着我们将用尽所有可能的数字，但它们可以以任何可能的顺序出现。</p><p>So how about the numerator? We are interested in outcomes in which the numbers that we get are all different.
            So
            what is an outcome in which the numbers are all different? So the die has 6 faces. We roll it 6 times. We’re
            going to get 6 different numbers. This means that we’re going to exhaust all the possible numbers, but they
            can
            appear in any possible sequence.</p>
        <p>因此，使该事件发生的结果是 1 到 6 的数字列表，但以任意顺序排列。因此，使事件 A 发生的可能结果只是 1 到 6 的数字的排列。使事件发生的一个可能结果。就是这个。</p><p>So an outcome that makes this event happen is a list of the numbers from 1 to 6, but arranged in some
            arbitrary
            order. So the possible outcomes that make event A happen are just the permutations of the numbers from 1 to
            6.
            One possible outcome that makes our events to happen. it would be this.</p>
        <h2 id="unknown-76">未知</h2><h2>Unknown</h2>
        <p>这里我们有 6 个可能的数字，但任何其他没有重复数字的此类列表也可以。因此，使事件发生的结果数是 6 个元素的排列数。所以它是 6 的阶乘。因此最终答案将是 6 的阶乘除以 6 的 6 次方。好吧，这就是解决此类问题的典型方法。</p><p>Here we have 6 possible numbers, but any other list of this kind in which none of the numbers is repeated
            would
            also do. So number of outcomes that make the event happen is the number of permutations of 6 elements. So
            it’s 6
            factorial. And so the final answer is going to be 6 factorial divided by 6 to the sixth. All right, so
            that’s a
            typical way that’s one solves problems of this kind.</p>
        <p>我们知道如何计算某些事物。例如，在这里我们知道如何计算排列，并且我们利用我们的知识来计算我们需要处理的集合的元素。现在让我们来处理一个稍微困难一点的问题。我们再次给出一个包含 n 个元素的集合。我们已经知道该集合有多少个子集，但现在我们感兴趣的是其中恰好有 k 个元素的子集。</p><p>We know how to count certain things. For example, here we knew how to count permutations, and we used our
            knowledge to count the elements of the set that we need to deal with. So now let’s get to a slightly more
            difficult problem. We’re given once more a set with n elements. We already know how many subsets that set
            has,
            but now we would be interested in subsets that have exactly k elements in them.</p>
        <p>因此，我们从包含 n 个元素的大集合开始，然后我们想要构造一个包含 k 个元素的子集。从这 n 个元素中，我将选择 k 个元素并将它们放入其中。有多少种方法可以做到这一点？更具体的方式来思考这个问题。</p><p>So we start with our big set that has n elements, and we want to construct a subset that has k elements. Out
            of
            those n I’m going to choose k and put them in there. In how many ways can I do this? More concrete way of
            thinking about this problem.</p>
        <h2 id="unknown-77">未知</h2><h2>Unknown</h2>
        <p>某个小组中有 n 个人，你想从该小组中挑选人员组成一个委员会，你想组成一个有 k 个人的委员会。其中 k 是给定数字。例如，一个 5 人委员会。如果从 100 人开始，有多少个 5 人委员会可能？这就是我们想要计算的。有多少个 k 元素子集？</p><p>You have n people in some group and you want to form a committee by picking people from that group, and you
            want
            to form a committee with k people. Where k is a given number. For example, a 5 person committee. How many 5
            person committees are possible if you’re starting with 100 people? So that’s what we want to count. How many
            k
            element subsets are there?</p>
        <p>我们还不知道答案，但让我们给它起个名字。名字将是这个特定的符号，我们读作 n 选择 k。从 n 个元素中，我们要选择 k 个。好的。这可能有点棘手。所以我们要做的是找出一个稍微容易一些的问题，也就是。</p><p>We don’t yet know the answer, but let’s give a name to it. And the name is going to be this particular
            symbol,
            which we read as n choose k. Out of n elements, we want to choose k of them. OK. That may be a little
            tricky. So
            what we’re going to do is to instead figure out a somewhat easier problem, which is going to be.</p>
        <p>我可以用多少种方法从这些人中挑选出 k 个人，并按特定顺序排列他们？那么我可以制作多少个包含 k 个人的有序列表？所谓有序，是指我们挑选出这 k 个人，并称这是社区中的第一个人。这是委员会中的第二个人。这是委员会中的第三个人，依此类推。</p><p>In how many ways can I pick k out of these people and puts them in a particular order? So how many possible
            ordered lists can I make that consist of k people? By ordered, I mean that we take those k people and we say
            this is the first person in the community. That’s the second person in the committee. That’s the third
            person in
            the committee and so on.</p>
        <h2 id="unknown-78">未知</h2><h2>Unknown</h2>
        <p>那么我们有多少种方法可以做到这一点？从这 n 个人中，我们只想选择其中的 k 个，并将它们放入槽中。一个接一个。所以这很像我们之前解决的车牌问题。所以我们有 n 个选择，可以选择谁作为社区中的顶级人物。我们可以选择任何人并让他们成为第一人。</p><p>So in how many ways can we do this? Out of these n, we want to choose just k of them and put them in slots.
            One
            after the other. So this is pretty much like the license plate problem we solved just a little earlier. So
            we
            have n choices for who we put as the top person in the community. We can pick anyone and have them be the
            first
            person.</p>
        <p>然后我要选择委员会中的第二个人。我已经用完了 1 个人。所以这里我有 n 减 1 个选择。现在，在这个阶段我已经用完了 2 个人，所以这里我有 n 减 2 个选择。这样继续下去。那么，最后一个数字是多少？是 n 减 k 吗？其实不是。</p><p>Then I’m going to choose the second person in the committee. I’ve used up 1 person. So I’m going to have n
            minus
            1 choices here. And now, at this stage I’ve used up 2 people, so I have n minus 2 choices here. And this
            keeps
            going on. Well, what is going to be the last number? Is it’s n minus k? Well, not really.</p>
        <p>我从第二个数字开始减去数字，所以最后我将减去 k - 1。这就是最后一个人的选择数。这就是方法数。这些数字的乘积给出了创建由我们开始时的 n 个人中的 k 个人组成的有序列表的方法数。</p><p>I’m starting subtracting numbers after the second one, so by the end I will have subtracted k minus 1. So
            that’s
            how many choices I will have for the last person. So this is the number of ways. the product of these
            numbers
            there gives me the number of ways that I can create ordered lists consisting of k people out of the n that
            we
            started with.</p>
        <h2 id="unknown-79">未知</h2><h2>Unknown</h2>
        <p>现在，您可以进行一些代数运算，并检查此处的表达式是否与该表达式相同。为什么会这样？这个阶乘包含从 1 到 n 的所有乘积。这个阶乘包含从 1 到 n 减 k 的所有乘积。因此，您会得到取消。剩下的是从此处之后的下一个数字开始的所有乘积，也就是这个特定的数字。</p><p>Now, you can do a little bit of algebra and check that this expression here is the same as that expression.
            Why
            is this? This factorial has all the products from 1 up to n.&nbsp;This factorial has all the products from 1 up
            to n
            minus k. So you get cancellations. And what’s left is all the products starting from the next number after
            here,
            which is this particular number.</p>
        <p>因此，创建这种有序列表的可能方法数为 n 阶乘除以 n 减 k 阶乘。现在，我可以用另一种方式创建有序列表。我可以先选择 k 个将加入委员会的人，然后按顺序排列，而不是一次挑选一个人。</p><p>So the number of possible ways of creating such ordered lists is n factorial divided by n minus k factorial.
            Now,
            a different way that I could make an ordered list. instead of picking the people one at a time, I could
            first
            choose my k people who are going to be in the committee, and then put them in order.</p>
        <p>并告诉他们这 k 个人中，你是第一，你是第二，你是第三。从这 k 个人开始，我可以用多少种方式对他们进行排序？这就是排列数。从一个包含 k 个对象的集合开始，我可以用多少种方式将它们按特定顺序排列？有多少种特定顺序？这基本上就是问题所在。我可以用多少种方式对这 k 个人进行排列和安排。</p><p>And tell them out of these k, you are the first, you are the second, you are the third. Starting with this k
            people, in how many ways can I order them? That’s the number of permutations. Starting with a set with k
            objects, in how many ways can I put them in a specific order? How many specific orders are there? That’s
            basically the question. In how many ways can I permute these k people and arrange them.</p>
        <h2 id="unknown-80">未知</h2><h2>Unknown</h2>
        <p>因此，执行此步骤的方法数是 k 的阶乘。那么，有多少种方法可以从一个包含 n 个元素的集合开始，经过此过程，最后得到一个包含 k 个元素的排序列表？根据规则。当我们有阶段时，阶段总数是第一阶段有多少个选择，乘以第二阶段有多少个选择。</p><p>So the number of ways that you can do this step is k factorial. So in how many ways can I start with a set
            with n
            elements, go through this process, and end up with a sorted list with k elements? By the rule that. when we
            have
            stages, the total number of stages is how many choices we had in the first stage, times how many choices we
            had
            in the second stage.</p>
        <p>这个过程发生的方式数是这个乘以那个。这是这个过程可能发生的另一种方式。可能的方式数是这个数字。无论我们以哪种方式执行这个过程，最终我们都会有从我们开始的 n 个人中安排 k 个人的可能方式。所以我们计算时得到的最终答案应该是这个，或者是这个乘以那个。</p><p>The number of ways that this process can happen is this times that. This is a different way that process
            could
            happen. And the number of possible of ways is this number. No matter which way we carry out that process, in
            the
            end we have the possible ways of arranging k people out of the n that we started with. So the final answer
            that
            we get when we count should be either this, or this times that.</p>
        <p>这两种计数方法都同样有效，所以它们应该会给出相同的答案。所以我们在这里得到了这个等式。因此，这两个表达式对应于两种不同的构建 k 人有序列表的方法，最初从 n 人开始。现在我们有了这个关系，我们可以将 k 阶乘发送给分母。这告诉我们，n 选 k，这个数字将是多少。所以这个公式。</p><p>Both are equally valid ways of counting, so both should give us the same answer. So we get this equality
            here. So
            these two expressions corresponds to two different ways of constructing ordered lists of k people starting
            with
            n people initially. And now that we have this relation, we can send the k factorial to the denominator. And
            that
            tells us what that number, n choose k, is going to be. So this formula.</p>
        <h2 id="unknown-81">未知</h2><h2>Unknown</h2>
        <p>这里用红色写着，因为到学期结束前你们会看到无数次。它们被称为二项式系数。它们告诉我们从具有 n 个元素的集合开始，创建 k 个元素子集的可能方法的数量。考虑极端情况来对公式进行健全性检查总是好的。所以让我们以 k 等于 n 的情况为例。&nbsp;</p><p>It’s written here in red, because you’re going to see it a zillion times until the end of the semester. they
            are
            called the binomial coefficients. And they tell us the number of possible ways that we can create a k
            element
            subset, starting with a set that has n elements. It’s always good to do a sanity check to formulas by
            considering extreme cases. So let’s take the case where k is equal to n.&nbsp;</p>
        <p>在这种情况下，正确的答案是什么？元素集合中有多少个 n 个元素子集？好吧，你的子集需要包含每一个。你没有任何选择。只有一个选择。那就是集合本身。所以答案应该等于 1。这是 n 个元素子集的数量，从包含 n 个元素的集合开始。让我们看看公式是否给出了正确的答案。</p><p>What’s the right answer in this case? How many n elements subsets are there out of an element set? Well, your
            subset needs to include every one. You don’t have any choices. There’s only one choice. It’s the set itself.
            So
            the answer should be equal to 1. That’s the number of n element subsets, starting with a set with n
            elements.
            Let’s see if the formula gives us the right answer.</p>
        <p>我们有 n 的阶乘除以 k，在我们的例子中是 n。n 的阶乘。然后 n 减去 k 等于 0 的阶乘。所以如果我们的公式是正确的，我们应该得到这个等式。那么如何才能使它正确呢？嗯，这取决于我们赋予这个符号什么样的含义？我们如何定义零阶乘？我想在某些方面它是任意的。</p><p>We have n factorial divided by k, which is n in our case. n factorial. And then n minus k is 0 factorial. So
            if
            our formula is correct, we should have this equality. And what’s the way to make that correct? Well, it
            depends
            what kind of meaning do we give to this symbol? How do we define zero factorial? I guess in some ways it’s
            arbitrary.</p>
        <h2 id="unknown-82">未知</h2><h2>Unknown</h2>
        <p>我们将以一种使这个公式正确的方式来定义它。因此，我们将使用的定义是，只要阶乘为 0，它就代表数字 1。因此，让我们检查一下，在另一个极端情况下，这是否也正确。如果我们让 k 等于 0，公式会给出什么？它再次给出 n 阶乘除以 0 阶乘乘以 n 阶乘。</p><p>We’re going to define it in a way that makes this formula right. So the definition that we will be using is
            that
            whenever you have 0 factorial, it’s going to stand for the number 1. So let’s check that this is also
            correct,
            at the other extreme case. If we let k equal to 0, what does the formula give us? It gives us, again, n
            factorial divided by 0 factorial times n factorial.</p>
        <p>根据我们的惯例，这又等于 1。所以我们开始时的集合中有一个子集有零个元素。它是哪个子集？它是空集。所以空集是我们开始时的集合中恰好有零个元素的唯一子集。所以这个公式也适用于这种极端情况。所以我们可以放心使用它。</p><p>According to our convention, this again is equal to 1. So there is one subset of our set that we started with
            that has zero elements. Which subset is it? It’s the empty set. So the empty set is the single subset of the
            set
            that we started with that happens to have exactly zero elements. So the formula checks in this extreme case
            as
            well. So we’re comfortable using it.</p>
        <p>现在这些阶乘和系数是真正混乱的代数对象。它们满足许多漂亮的恒等式，有时你可以用归纳法和到处进行取消来代数证明它们。但这真的很混乱。有时你可以聪明地利用你对这些系数所代表含义的理解来绕过这些计算。这是一个典型的例子。这些二项式系数的总和是多少？</p><p>Now these factorials and these coefficients are really messy algebraic objects. There’s lots of beautiful
            identities that they satisfy, which you can prove algebraically sometimes by using induction and having
            cancellations happen all over the place. But it’s really messy. Sometimes you can bypass those calculations
            by
            being clever and using your understanding of what these coefficients stand for. So here’s a typical example.
            What is the sum of those binomial coefficients?</p>
        <h2 id="unknown-83">未知</h2><h2>Unknown</h2>
        <p>我固定 n，然后对所有可能的情况求和。所以如果你是一个代数天才，你会把这个表达式取在这里，把它代入这里，然后开始疯狂地做代数运算。半小时后，你可能会得到正确的答案。但现在让我们试着聪明一点。这到底有什么用？那个公式算什么？我们正在考虑 k 个元素子集。这就是这个数字。</p><p>I fix n, and sum over all possible cases. So if you’re an algebra genius, you’re going to take this
            expression
            here, plug it in here, and then start doing algebra furiously. And half an hour later, you may get the right
            answer. But now let’s try to be clever. What does this really do? What does that formula count? We’re
            considering k element subsets. That’s this number.</p>
        <p>我们正在考虑不同 k 选择下的 k 个元素子集的数量。这个和中的第一项计算我们有多少个 0 元素子集。这个和中的下一项计算我们有多少个 1 元素子集。下一项计算我们有多少个 2 元素子集。那么最后，我们计算了什么？我们计算了子集的总数。我们考虑了所有可能的基数。我们计算了大小为 k 的子集的数量。</p><p>And we’re considering the number of k element subsets for different choices of k. The first term in this sum
            counts how many 0 element subsets we have. The next term in this sum counts how many 1 element subsets we
            have.
            The next term counts how many 2 element subsets we have. So in the end, what have we counted? We’ve counted
            the
            total number of subsets. We’ve considered all possible cardinalities. We’ve counted the number of subsets of
            size k.</p>
        <p>我们已经考虑了所有可能的大小 k。总计数将是子集的总数。我们知道这是多少。在几张幻灯片之前，我们讨论了这个数字等于 2 的 n 次方。所以，答案很好、很清晰、很简单，一旦你对你面前的代数表达式做出解释，就很容易猜到。好的。</p><p>We’ve considered all possible sizes k. The overall count is going to be the total number of subsets. And we
            know
            what this is. A couple of slides ago, we discussed that this number is equal to 2 to the n.&nbsp;So, nice, clean
            and
            simple answer, which is easy to guess once you give an interpretation to the algebraic expression that you
            have
            in front of you. All right.</p>
        <h2 id="unknown-84">未知</h2><h2>Unknown</h2>
        <p>让我们再来看一个二项式系数将要出现的例子。背景是这样的。n 次独立的抛硬币，每次抛硬币都有 P 个概率，即正面朝上。这就是我们的概率实验。假设我们抛了 6 次硬币。得到这个特定结果序列的概率是多少？由于独立性，我们可以乘以概率。</p><p>So let’s move again to sort of an example in which those binomial coefficients are going to show up. So
            here’s
            the setting. n independent coin tosses, and each coin toss has a probability, P, of resulting in heads. So
            this
            is our probabilistic experiment. Suppose we do 6 tosses. What’s the probability that we get this particular
            sequence of outcomes? Because of independence, we can multiply probability.</p>
        <p>因此，它将是第一次抛掷结果为正面的概率，乘以第二次抛掷结果为反面的概率，乘以第三次抛掷结果为反面的概率，乘以正面的概率，乘以正面的概率，乘以正面的概率，也就是 P 的四次方乘以 (1 减 P) 的平方。这就是这个特定序列的概率。如果是其他序列呢？</p><p>So it’s going to be the probability that the first toss results in heads, times the probability that the
            second
            toss results in tails, times the probability that the third one results in tails, times probability of
            heads,
            times probability of heads, times probability of heads, which is just P to the fourth times (1 minus P)
            squared.
            So that’s the probability of this particular sequence. How about a different sequence?</p>
        <p>如果我有 4 次反面和 2 次正面，但顺序不同。假设我们考虑这个特定结果。答案会有所不同吗？我们仍然有 P，乘以 P，乘以 P，乘以 P，乘以 (1 减 P)，乘以 (1 减 P)。我们会再次得到相同的答案。</p><p>If I had 4 tails and 2 heads, but in a different order. let’s say if we considered this particular outcome.
            would
            the answer be different? We would still have P, times P, times P, times P, times (1 minus P), times (1 minus
            P).
            We would get again, the same answer.</p>
        <h2 id="unknown-85">未知</h2><h2>Unknown</h2>
        <p>因此，从这个例子中，您可以观察到，更一般地讲，获得特定正面和反面序列的概率是 P 的幂，等于正面的次数。所以这里出现了 4 次正面。因此出现了 P 的第四次方。然后是（1 减 P）的反面次数的幂。因此每个 k 次正面序列都是如此。</p><p>So what you observe from just this example is that, more generally, the probability of obtaining a particular
            sequence of heads and tails is P to a power, equal to the number of heads. So here we had 4 heads. So
            there’s P
            to the fourth showing up. And then (1 minus P) to the power number of tails. So every k head sequence.</p>
        <p>每一个恰好有 k 次正面的结果都有相同的概率，即 P 的 k 次方（1 减 p），n 的 k 次方。这是任何特定序列恰好有 k 次正面的概率。所以这就是特定序列有 k 次正面的概率。现在让我们问一个问题，我的实验结果恰好有 k 次正面，但顺序任意的概率是多少？</p><p>Every outcome in which we have exactly k heads, has the same probability, which is going to be P to the k, (1
            minus p), to the (n minus k). This is the probability of any particular sequence that has exactly k heads.
            So
            that’s the probability of a particular sequence with k heads. So now let’s ask the question, what is the
            probability that my experiment results in exactly k heads, but in some arbitrary order?</p>
        <p>所以正面可能出现在任何地方。所以这种情况有多种不同的发生方式。这个事件发生的总体概率是多少？所以一个事件发生的概率是该事件可能发生的所有单独方式的概率之和。所以它是导致该事件发生的所有结果的概率之和。</p><p>So the heads could show up anywhere. So there’s a number of different ways that this can happen. What’s the
            overall probability that this event takes place? So the probability of an event taking place is the sum of
            the
            probabilities of all the individual ways that the event can occur. So it’s the sum of the probabilities of
            all
            the outcomes that make the event happen.</p>
        <h2 id="unknown-86">未知</h2><h2>Unknown</h2>
        <p>我们可以获得 k 个正面的不同方式是包含 k 个正面的不同序列的数量。我们刚刚发现，任何包含 k 个正面的序列都有这个概率。因此，要进行这个求和，我们只需要取每个单独的 k 个正面序列的共同概率，乘以这个求和中有多少个项。</p><p>The different ways that we can obtain k heads are the number of different sequences that contain exactly k
            heads.
            We just figured out that any sequence with exactly k heads has this probability. So to do this summation, we
            just need to take the common probability of each individual k head sequence, times how many terms we have in
            this sum.</p>
        <p>所以我们现在要做的就是找出有多少个 k 次正面的序列。有多少次结果恰好有 k 次正面。好的。那么我可以用什么方式向你描述一个有 k 次正面的序列？我可以取与不同投掷相对应的 n 个位置。我对恰好有 k 次正面的特定序列感兴趣。</p><p>So what we’re left to do now is to figure out how many k head sequences are there. How many outcomes are
            there in
            which we have exactly k heads. OK. So what are the ways that I can describe to you a sequence with k heads?
            I
            can take my n slots that corresponds to the different tosses. I’m interested in particular sequences that
            have
            exactly k heads.</p>
        <p>所以我需要做的是选择 k 个位置并给它们分配正面。因此，指定一个恰好有 k 个正面的序列与绘制此图并告诉您哪 k 个位置恰好有正面是一样的。所以我需要从这 n 个位置中选择 k 个，并为它们分配正面。我有多少种方法可以选择这 k 个位置？</p><p>So what I need to do is to choose k slots and assign heads to them. So to specify a sequence that has exactly
            k
            heads is the same thing as drawing this picture and telling you which are the k slots that happened to have
            heads. So I need to choose out of those n slots, k of them, and assign them heads. In how many ways can I
            choose
            this k slots?</p>
        <h2 id="unknown-87">未知</h2><h2>Unknown</h2>
        <p>嗯，问题是从一组 n 个位置开始，并从 n 个可用位置中选择 k 个位置。因此，k 个头序列的数量与我们开始的一组位置的 k 个元素子集的数量相同，这些位置是从 1 到 n 的 n 个位置。我们知道这个数字是多少。我们之前计算过，从一组有 n 个元素开始，k 个元素子集的数量。</p><p>Well, it’s the question of starting with a set of n slots and choosing k slots out of the n available. So the
            number of k head sequences is the same as the number of k element subsets of the set of slots that we
            started
            with, which are the n slots 1 up to n.&nbsp;We know what that number is. We counted, before, the number of k
            element
            subsets, starting with a set with n elements.</p>
        <p>我们给这个数字赋予一个符号，也就是 n 选 k。这就是我们得到的最终答案。这就是所谓的二项式概率。它们给出了从一枚公平硬币开始抛出不同次数的正面的概率。当然，对于合理的 k 值，这个公式是正确的，也就是说，对于 k 等于 0、1 直到 n，它都是正确的。&nbsp;</p><p>And we gave a symbol to that number, which is that thing, n choose k. So this is the final answer that we
            obtain.
            So these are the so called binomial probabilities. And they gave us the probabilities for different numbers
            of
            heads starting with a fair coin that’s being tossed a number of times. This formula is correct, of course,
            for
            reasonable values of k, meaning its correct for k equals 0,1, up to n.&nbsp;</p>
        <p>如果 k 大于 n，那么 k 次出现正面的概率是多少？如果 k 大于 n，则不可能出现 k 次正面，因此该概率当然为零。因此，这些概率只有在我们进行了 n 次投掷的情况下，才对可能的数字 k 有意义。现在有一个问题类似于我们在上一张幻灯片中遇到的问题。如果我写下这个总和。</p><p>If k is bigger than n, what’s the probability of k heads? If k is bigger than n, there’s no way to obtain k
            heads, so that probability is, of course, zero. So these probabilities only makes sense for the numbers k
            that
            are possible, given that we have n tosses. And now a question similar to the one we had in the previous
            slide.
            If I write down this summation.</p>
        <h2 id="unknown-88">未知</h2><h2>Unknown</h2>
        <p>比上一张幻灯片中的代数更糟糕。你认为这个数字会是多少？它应该是 1，因为这是获得 k 个正面的概率。当我们进行求和时，我们所做的是考虑 0 个正面的概率，加上 1 个正面的概率，加上 2 个正面的概率，加上 n 个正面的概率。我们已经在实验中穷尽了所有可能性。</p><p>Even worse algebra than the one in the previous slide. what do you think this number will turn out to be? It
            should be 1 because this is the probability of obtaining k heads. When we do the summation, what we’re doing
            is
            we’re considering the probability of 0 heads, plus the probability of 1 head, plus the probability of 2
            heads,
            plus the probability of n heads. We’ve exhausted all the possibilities in our experiment.</p>
        <p>所以，当你穷尽所有可能性时，总概率必须等于 1。所以这是另一个漂亮的公式，它可以计算出非常简单的东西。当然，如果你试图用代数方法证明这个恒等式，那你就得付出很多努力。所以现在有了二项式概率，我们就可以解决更难的问题了。让我们再做一次同样的实验。我们独立抛硬币 10 次。所以这 10 次抛硬币是独立的。</p><p>So the overall probability, when you exhaust all possibilities, must be equal to 1. So that’s yet another
            beautiful formula that evaluates into something really simple. And if you tried to prove this identity
            algebraically, of course, you would have to suffer quite a bit. So now armed with the binomial
            probabilities, we
            can do the harder problems. So let’s take the same experiment again. We flip a coin independently 10 times.
            So
            these 10 tosses are independent.</p>
        <p>我们抛了 10 次。我们看不到结果，但有人过来告诉我们，你知道，你抛了 10 次，正好有 3 次是正面。好吗？所以某个事件发生了。现在你被要求找出另一个事件的概率，即前两次抛掷都是正面。我们称该事件为事件 A。好。那么我们是否处于离散均匀概率定律的环境中？</p><p>We flip it 10 times. We don’t see the result, but somebody comes and tells us, you know, there were exactly 3
            heads in the 10 tosses that you had. OK? So a certain event happened. And now you’re asked to find the
            probability of another event, which is that the first 2 tosses were heads. Let’s call that event A. OK. So
            are
            we in the setting of discrete uniform probability laws?</p>
        <h2 id="unknown-89">未知</h2><h2>Unknown</h2>
        <p>当我们多次抛硬币时，是否所有结果都具有相同的概率？所有序列都具有相同的概率？如果你有一枚公平的硬币，情况就是这样。所有序列都具有相同的概率。但如果你的硬币不公平，那么正面/正面的概率当然会与反面/反面的概率不同。如果你的硬币偏向正面，那么正面/正面的概率就会更大。所以我们并不完全处于统一的设置中。
        </p><p>When we toss a coin multiple times, is it the case that all outcomes are equally likely? All sequences are
            equally likely? That’s the case if you have a fair coin. that all sequences are equally likely. But if your
            coin
            is not fair, of course, heads/heads is going to have a different probability than tails/tails. If your coin
            is
            biased towards heads, then heads/heads is going to be more likely. So we’re not quite in the uniform
            setting.
        </p>
        <p>我们的整体样本空间 omega 不包含等概率元素。我们会关心这个吗？不一定。现在所有动作都发生在我们被告知已经发生的事件 B 中。所以我们有我们的大样本空间 omega。该样本空间的元素不具有等概率。我们被告知某个事件 B 发生了。并且在该事件 B 中，我们被要求找到 A 也发生的条件概率。</p><p>Our overall sample space, omega, does not have equally likely elements. Do we care about that? Not
            necessarily.
            All the action now happens inside the event B that we are told has occurred. So we have our big sample
            space,
            omega. Elements of that sample space are not equally likely. We are told that a certain event B occurred.
            And
            inside that event B, we’re asked to find the conditional probability that A has also occurred.</p>
        <p>现在幸运的是，在事件 B 中，所有结果的概率都是相等的。B 中的结果是 10 次投掷的序列，其中恰好有 3 次正面。每个 3 次正面的序列都有这个概率。因此，B 中的元素彼此之间的概率是相等的。一旦我们以事件 B 发生为条件，这里不同结果的概率会发生什么变化？嗯，条件概率定律与无条件概率定律保持相同的比例。</p><p>Now here’s the lucky thing, inside the event B, all outcomes are equally likely. The outcomes inside B are
            the
            sequences of 10 tosses that have exactly 3 heads. Every 3 head sequence has this probability. So the
            elements of
            B are equally likely with each other. Once we condition on the event B having occurred, what happens to the
            probabilities of the different outcomes inside here? Well, conditional probability laws keep the same
            proportions as the unconditional ones.</p>
        <h2 id="unknown-90">未知</h2><h2>Unknown</h2>
        <p>当我们开始时，B 的元素是等概率的，因此一旦我们被告知 B 已经发生，它们也是等概率的。因此，为了解决这个问题，我们只需要将我们带到这个较小的宇宙，思考一下那个小宇宙中发生了什么。在那个小宇宙中，B 的所有元素都是等概率的。</p><p>The elements of B were equally likely when we started, so they’re equally likely once we are told that B has
            occurred. So to do with this problem, we need to just transport us to this smaller universe and think about
            what’s happening in that little universe. In that little universe, all elements of B are equally likely.</p>
        <p>因此，要找到该集合中某个子集的概率，我们只需要计算 B 的基数，并计算 A 的基数。让我们这样做。B 中的结果数。有多少种方法可以在 10 次投掷中得到 3 次正面？这是我们之前考虑的数字，即 10 选 3。这是投掷 10 次时 3 次正面序列的数量。现在让我们看看事件 A。</p><p>So to find the probability of some subset of that set, we only need to count the cardinality of B, and count
            the
            cardinality of A. So let’s do that. Number of outcomes in B. in how many ways can we get 3 heads out of 10
            tosses? That’s the number we considered before, and it’s 10 choose 3. This is the number of 3 head sequences
            when you have 10 tosses. Now let’s look at the event A.</p>
        <p>事件 A 是前两次抛掷都是正面，但我们现在生活在这个宇宙 B 中。假设 B 发生了，A 中有多少元素？A 在 B 宇宙中有多少种发生方式。如果你被告知前两次都是正面。抱歉。那么在 B 中有 3 次正面的结果中，有多少以正面/正面开头？</p><p>The event A is that the first 2 tosses where heads, but we’re living now inside this universe B. Given that B
            occurred, how many elements does A have in there? In how many ways can A happen inside the B universe. If
            you’re
            told that the first 2 were heads. sorry. So out of the outcomes in B that have 3 heads, how many start with
            heads/heads?</p>
        <h2 id="unknown-91">未知</h2><h2>Unknown</h2>
        <p>好吧，如果以正面/正面开始，那么唯一不确定的就是第三个头的位置。所以我们以正面/正面开始，我们将有三个头，问题是，第三个头会在哪里。它有八种可能性。所以插槽 1 是正面，插槽 2 是正面，第三个头可以位于其他任何地方。所以第三个头的位置有 8 种可能性。好的。</p><p>Well, if it starts with heads/heads, then the only uncertainty is the location of the third head. So we
            started
            with heads/heads, we’re going to have three heads, the question is, where is that third head going to be. It
            has
            eight possibilities. So slot 1 is heads, slot 2 is heads, the third heads can be anywhere else. So there’s 8
            possibilities for where the third head is going to be. OK.</p>
        <p>因此，我们这里计算的其实是 A 交集 B 的基数，也就是 B 中的元素中有多少个元素使 A 发生，除以 B 的基数。这样我们就可以得到答案，也就是 10 选 3，除以 8。我可能应该重新绘制一下他们这里的图。集合 A 不一定包含在 B 中。</p><p>So what we have counted here is really the cardinality of A intersection B, which is out of the elements in
            B,
            how many of them make A happen, divided by the cardinality of B. And that gives us the answer, which is
            going to
            be 10 choose 3, divided by 8. And I should probably redraw a little bit of the picture that they have here.
            The
            set A is not necessarily contained in B.</p>
        <p>它也可能有 B 之外的东西。因此，前两次抛掷都是正面的事件可能发生在总共 3 次正面的情况下，但也可能发生在不同的正面总数的情况下。但是一旦我们被传送到集合 B 内部，我们需要计算的只是 A 的这一部分。它是 A 与 B 的交集，并将其与集合 B 中的元素总数进行比较。</p><p>It could also have stuff outside B. So the event that the first 2 tosses are heads can happen with a total of
            3
            heads, but it can also happen with a different total number of heads. But once we are transported inside the
            set
            B, what we need to count is just this part of A. It’s A intersection B and compare it with the total number
            of
            elements in the set B.</p>
        <h2 id="unknown-92">未知</h2><h2>Unknown</h2>
        <p>我是不是写反了？是的。所以这是 8/10 选 3。好的。我们现在要用一个更难的问题来结束。好的。n 选 k 这件事与从一个集合开始并挑选 k 个元素的子集有关。</p><p>Did I write it the opposite way? Yes. So this is 8 over 10 choose 3. OK. So we’re going to close with a more
            difficult problem now. OK. This business of n choose k has to do with starting with a set and picking a
            subset
            of k elements.</p>
        <p>另一种思考方式是，我们从一个包含 n 个元素的集合开始，然后选择一个包含 k 个元素的子集，这意味着剩下 n 减 k 个元素。选择子集与将集合分成两部分相同。现在让我们概括这个问题并开始计算分区。有人给你一个包含 n 个元素的集合。有人还给你一些数字。</p><p>Another way of thinking of that is that we start with a set with n elements and you choose a subset that has
            k,
            which means that there’s n minus k that are left. Picking a subset is the same as partitioning our set into
            two
            pieces. Now let’s generalize this question and start counting partitions in general. Somebody gives you a
            set
            that has n elements. Somebody gives you also certain numbers.</p>
        <p>N1、n2、n3，比如说 n4，这些数字加起来是 n。你被要求将这个集合分成四个子集，每个子​​集都有特定的基数。所以你要求将它分成四块，每块都有规定的基数。我们有多少种方法可以进行这种划分？当我们分成两块时，n 选择 k 是答案，更一般的答案是什么？</p><p>N1, n2, n3, let’s say, n4, where these numbers add up to n.&nbsp;And you’re asked to partition this set into four
            subsets where each one of the subsets has this particular cardinality. So you’re asking to cut it into four
            pieces, each one having the prescribed cardinality. In how many ways can we do this partitioning? n choose k
            was
            the answer when we partitioned in two pieces, what’s the answer more generally?</p>
        <h2 id="unknown-93">未知</h2><h2>Unknown</h2>
        <p>举一个具体的分割示例，您有一副 52 张牌的牌组，像打桥牌一样，您给每位玩家发 13 张牌。假设发牌过程公平，且牌组洗得整齐，那么将 52 张牌分成四手（即每手 13 张的四个子集）的概率应该是相等的。</p><p>For a concrete example of a partition, you have your 52 card deck and you deal, as in bridge, by giving 13
            cards
            to each one of the players. Assuming that the dealing is done fairly and with a well shuffled deck of cards,
            every particular partition of the 52 cards into four hands, that is four subsets of 13 each, should be
            equally
            likely.</p>
        <p>因此，我们取出 52 张牌，并将它们分成 13、13、13 和 13 的子集。我们假设所有可能的划分，所有可能的发牌方式都是同样可能的。因此，我们又一次处于可以使用计数的环境中，因为所有可能的结果都是同样可能的。因此，实验的结果就是每个玩家最终拿到的牌。</p><p>So we take the 52 cards and we partition them into subsets of 13,13.13, and 13. And we assume that all
            possible
            partitions, all possible ways of dealing the cards are equally likely. So we are again in a setting where we
            can
            use counting, because all the possible outcomes are equally likely. So an outcome of the experiment is the
            hands
            that each player ends up getting.</p>
        <p>当你拿到牌时，你拿到牌的顺序并不重要。重要的是你手里有什么牌。所以重要的是你拿到了哪些牌子集。好的。那么在这个实验中，样本空间的基数是多少？让我们用我们得到的具体数字来计算分割 52 张牌的问题。那么考虑如下发牌。</p><p>And when you get the cards in your hands, it doesn’t matter in which order that you got them. It only matters
            what cards you have on you. So it only matters which subset of the cards you got. All right. So what’s the
            cardinality of the sample space in this experiment? So let’s do it for the concrete numbers that we have for
            the
            problem of partitioning 52 cards. So think of dealing as follows.</p>
        <h2 id="unknown-94">未知</h2><h2>Unknown</h2>
        <p>你完美地洗牌，然后拿出最上面的 13 张牌，把它们交给一个人。这个人可能有多少种手牌？从 52 张牌中，我随机选择 13 张，把它们交给第一个人。做完这些之后，接下来会发生什么？我还剩下 39 张牌。从这 39 张牌中，我挑选了 13 张，把它们交给第二个人。</p><p>You shuffle the deck perfectly, and then you take the top 13 cards and give them to one person. In how many
            possible hands are there for that person? Out of the 52 cards, I choose 13 at random and give them to the
            first
            person. Having done that, what happens next? I’m left with 39 cards. And out of those 39 cards, I pick 13 of
            them and give them to the second person.</p>
        <p>现在我剩下 26 张牌。从这 26 张牌中，我选出 13 张，把它们给第三个人。对于最后一个人，实际上没有任何选择。我必须从这 13 张牌中把全部 13 张都给那个人。这个数字正好等于 1。所以我们不关心它。好的。接下来你要做的就是写下这些数字的公式。</p><p>Now I’m left with 26 cards. Out of those 26, I choose 13, give them to the third person. And for the last
            person
            there isn’t really any choice. Out of the 13, I have to give that person all 13. And that number is just
            equal
            to 1. So we don’t care about it. All right. So next thing you do is to write down the formulas for these
            numbers.</p>
        <p>例如，这里你将得到 52 的阶乘，除以 13 的阶乘，再乘以 39 的阶乘，然后继续。然后会发生很好的消去。这个 39 的阶乘将消去由此产生的 39 的阶乘，依此类推。完成消去和所有代数运算后，你就会得到这个特定的答案，也就是将 52 张牌分成四个玩家的可能数量，每个玩家恰好有 13 手牌。</p><p>So, for example, here you would have 52 factorial, divided by 13 factorial, times 39 factorial, and you
            continue.
            And then there are nice cancellations that happen. This 39 factorial is going to cancel the 39 factorial
            that
            comes from there, and so on. After you do the cancellations and all the algebra, you’re left with this
            particular answer, which is the number of possible partitions of 52 cards into four players where each
            player
            gets exactly 13 hands.</p>
        <h2 id="unknown-95">未知</h2><h2>Unknown</h2>
        <p>如果你要将此公式推广到我们这里的设置，更通用的公式是。你有 n 个阶乘，其中 n 是你要分配的对象数，除以阶乘的乘积。好的，这里我针对将其分成四个集合的情况进行计算。所以当我们将一个集合分成四个规定基数的子集时，这就是答案。</p><p>If you were to generalize this formula to the setting that we have here, the more general formula is. you
            have n
            factorial, where n is the number of objects that you are distributing, divided by the product of the
            factorials
            of the. OK, here I’m doing it for the case where we split it into four sets. So that would be the answer
            when we
            partition a set into four subsets of prescribed cardinalities.</p>
        <p>你可以猜一下，如果要将其分成五组或六组，该公式将如何推广。好的。到目前为止，我们只是确定了样本空间的大小。现在我们需要查看我们的事件，即每个玩家都拿到一张 A 牌的事件，我们将其称为事件 A。该事件有多少种发生方式？有多少种可能的牌型，其中每个玩家都恰好有一张 A 牌？</p><p>And you can guess how that formula would generalize if you want to split it into five sets or six sets. OK.
            So
            far we just figured out the size of the sample space. Now we need to look at our event, which is the event
            that
            each player gets an ace, let’s call that event A. In how many ways can that event happens? How many possible
            hands are there in which every player has exactly one ace?</p>
        <p>因此，我需要考虑如何按顺序分发牌，以便每个人都拿到一张 A，然后试着思考有多少种方式可以实现这个顺序过程。因此，确保每个人都拿到一张 A 的一种方法如下。我拿出四张 A，随机分配给四名玩家，但要确保每人只拿到一张 A。</p><p>So I need to think about the sequential process by which I distribute the cards so that everybody gets
            exactly
            one ace, and then try to think in how many ways can that sequential process happen. So one way of making
            sure
            that everybody gets exactly one ace is the following. I take the four aces and I distribute them randomly to
            the
            four players, but making sure that each one gets exactly one ace.</p>
        <h2 id="unknown-96">未知</h2><h2>Unknown</h2>
        <p>有多少种方法可以实现这一点？我拿出黑桃 A，然后从四个人中随机挑选一个人来发。因此，有 4 种选择。然后，我剩下 3 张 A 可以分发。那个人已经拿到了一张 A。我拿出下一张 A，然后把它发给剩下的 3 个人中的一个人。因此，有 3 种选择。</p><p>In how many ways can that happen? I take the ace of spades and I send it to a random person out of the four.
            So
            there’s 4 choices for this. Then I’m left with 3 aces to distribute. That person already gotten an ace. I
            take
            the next ace, and I give it to one of the 3 people remaining. So there’s 3 choices for how to do that.</p>
        <p>然后对于下一个 A，有 2 个人还没有拿到 A，他们会随机地把 A 给其中一个人。所以这些就是 4 个 A 的可能分配方式，这样每个人就能拿到一个。这实际上和这个问题是一样的。</p><p>And then for the next ace, there’s 2 people who have not yet gotten an ace, and they give it randomly to one
            of
            them. So these are the possible ways of distributing for the 4 aces, so that each person gets exactly one.
            It’s
            actually the same as this problem.</p>
        <p>从一组四样东西开始，有多少种方法可以将它们分成四个子集，其中第一组有一个元素，第二组有一个元素，第三组有另一个元素，依此类推。所以它符合那个公式，给了我们 4 的阶乘。好的。所以有不同的方法来分配 A。然后有不同的方法来分配剩下的 48 张牌。有多少种方法？</p><p>Starting with a set of four things, in how many ways can I partition them into four subsets where the first
            set
            has one element, the second has one element, the third one has another element, and so on. So it agrees with
            that formula by giving us 4 factorial. OK. So there are different ways of distributing the aces. And then
            there’s different ways of distributing the remaining 48 cards. How many ways are there?</p>
        <h2 id="unknown-97">未知</h2><h2>Unknown</h2>
        <p>好吧，我有 48 张牌，我要给四个玩家每人发 12 张。这和我们这里的问题完全一样，只不过现在是 48 张牌，每人 12 张。这就给了我们这个特定的计数。所以把所有这些放在一起，我们就可以得到不同的方式，我们可以把牌发给四个玩家，这样每个人都能得到一张 A。</p><p>Well, I have 48 cards that I’m going to distribute to four players by giving 12 cards to each one. It’s
            exactly
            the same question as the one we had here, except that now it’s 48 cards, 12 to each person. And that gives
            us
            this particular count. So putting all that together gives us the different ways that we can distribute the
            cards
            to the four players so that each one gets exactly one ace.</p>
        <p>可能的方式数将是这个四阶乘，从这里开始，乘以这个数字。这给了我们感兴趣的事件可能发生的方式数。然后分母是我们的样本空间的基数，也就是这个数字。所以这看起来一团糟。事实证明，这个表达式确实简化成了一个非常非常简单的东西。</p><p>The number of possible ways is going to be this four factorial, coming from here, times this number. this
            gives
            us the number of ways that the event of interest can happen. and then the denominator is the cardinality of
            our
            sample space, which is this number. So this looks like a horrible mess. It turns out that this expression
            does
            simplify to something really, really simple.</p>
        <p>如果你看一下这个问题的教科书，你会看到另一种推导方法，它能让你快速得到相同的数字答案。好的。所以这基本上结束了第一章。从下次开始，我们将考虑引入随机变量，让这个主题更加有趣。</p><p>And if you look at the textbook for this problem, you will see an alternative derivation that gives you a
            short
            cut to the same numerical answer. All right. So that basically concludes chapter one. From next time we’re
            going
            to consider introducing random variables and make the subject even more interesting.</p>
        <h1 id="discrete-random-variables-i">5. 离散随机变量 I</h1><h1>5. Discrete Random Variables I</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 50:35 <a href="https://youtube.com/watch?v=3MOahpLxj6A">https://youtube.com/watch?v=3MOahpLxj6A</a></p><p> 12 years ago (Nov 10, 2012) — 50:35 <a href="https://youtube.com/watch?v=3MOahpLxj6A">https://youtube.com/watch?v=3MOahpLxj6A</a></p>
        <h2 id="intro-1">简介</h2><h2>Intro</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。让我们开始吧。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation, or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu, OK. So let us start.</p>
        <h2 id="outline">大纲</h2><h2>Outline</h2>
        <p>好的。今天我们将开始本课程的一个新单元。到目前为止，我们已经讲完了概率论的基础知识。</p><p>All right. So today we’re starting a new unit in this class. We have covered, so far, the basics of
            probability
            theory.</p>
        <p>就概率而言，主要概念和工具就是这些。但如果这门学科只有这些，那么它就不够丰富。使概率论更加有趣和丰富的是，我们还可以讨论随机变量，它们是将数值结果分配给实验结果的方法。</p><p>The main concepts and tools, as far as just probabilities are concerned. But if that was all that there is in
            this subject, the subject would not be rich enough. What makes probability theory a lot more interesting and
            richer is that we can also talk about random variables, which are ways of assigning numerical results to the
            outcomes of an experiment.</p>
        <p>因此，我们将定义什么是随机变量，然后使用所谓的概率质量函数来描述它们。基本上，某些数值比其他数值更有可能出现，我们通过以通常的方式为它们分配概率来捕获这一点。我们使用所谓的概率质量函数以紧凑的方式表示它们。</p><p>So we’re going to define what random variables are, and then we’re going to describe them using so called
            probability mass functions. Basically some numerical values are more likely to occur than other numerical
            values, and we capture this by assigning probabilities to them the usual way. And we represent these in a
            compact way using the so called probability mass functions.</p>
        <p>我们将看到几个随机变量的例子，其中一些我们已经见过，但术语不同。到目前为止，这只是一些你已经知道如何进行的定义和计算。但接下来我们将介绍今天的一个新概念。所以到目前为止，这主要是符号和定义的练习。</p><p>We’re going to see a couple of examples of random variables, some of which we have already seen but with
            different terminology. And so far, it’s going to be just a couple of definitions and calculations of the
            type
            that you already know how to do. But then we’re going to introduce the one new, big concept of the day. So
            up to
            here it’s going to be mostly an exercise in notation and definitions.</p>
        <p>但后来我们得到了一个大概念，即随机变量的期望值，它是随机变量的某种平均值。然后我们还要非常简短地讨论一下期望的近距离，即随机变量的方差的概念。</p><p>But then we got our big concept which is the concept of the expected value of a random variable, which is
            some
            kind of average value of the random variable. And then we’re going to also talk, very briefly, about close
            distance of the expectation, which is the concept of the variance of a random variable.</p>
        <h2 id="random-variable">随机变量</h2><h2>Random Variable</h2>
        <p>好的。那么什么是随机变量？它是对实验的每个可能结果分配的数值。</p><p>OK. So what is a random variable? It’s an assignment of a numerical value to every possible outcome of the
            experiment.</p>
        <p>这是图片。样本空间是这个班级，我们这里有很多学生。这是我们的样本空间，omega。我对随机学生的身高感兴趣。所以我将使用一条实线来记录身高，假设这是以英寸为单位的身高。实验开始后，我随机挑选一名学生。</p><p>So here’s the picture. The sample space is this class, and we’ve got lots of students in here. This is our
            sample
            space, omega. I’m interested in the height of a random student. So I’m going to use a real line where I
            record
            height, and let’s say this is height in inches. And the experiment happens, I pick a random student.</p>
        <p>然后我去测量那个随机学生的身高，然后得到一个具体的数字。那么以英寸为单位的合适数字是多少？假设是 60 英寸。好的。或者我选择另一个学生，该学生的身高为 71 英寸，依此类推。这就是实验。这些是结果。这些是我们称为身高的随机变量的数值。好的。那么从数学上讲，我们在这里处理的是什么？</p><p>And I go and measure the height of that random student, and that gives me a specific number. So what’s a good
            number in inches? Let’s say 60. OK. Or I pick another student, and that student has a height of 71 inches,
            and
            so on. So this is the experiment. These are the outcomes. These are the numerical values of the random
            variable
            that we call height. OK. So mathematically, what are we dealing with here?</p>
        <p>我们基本上是在处理从样本空间到实数的函数。该函数以实验结果（即典型学生）作为参数，并产生该函数的值，即该特定学生的身高。因此，我们想到一个抽象对象，用大写字母 H 表示，它是称为身高的随机变量。而该随机变量本质上就是我们在这里讨论的这个特定函数。</p><p>We’re basically dealing with a function from the sample space into the real numbers. That function takes as
            argument, an outcome of the experiment, that is a typical student, and produces the value of that function,
            which is the height of that particular student. So we think of an abstract object that we denote by a
            capital H,
            which is the random variable called height. And that random variable is essentially this particular function
            that we talked about here.</p>
        <p>好的。所以我们在这里做了一个区分。H 是抽象的身高。它是一个函数。这里的数字是当您选择实验的一个特定结果时，此函数采用的特定数值。现在，当您进行单一概率实验时，您可以有多个随机变量。所以也许，除了身高之外，我还对典型学生的体重感兴趣。所以当实验发生时，我会随机选择那个学生。</p><p>OK. So there’s a distinction that we’re making here. H is height in the abstract. It’s the function. These
            numbers here are particular numerical values that this function takes when you choose one particular outcome
            of
            the experiment. Now, when you have a single probability experiment, you can have multiple random variables.
            So
            perhaps, instead of just height, I’m also interested in the weight of a typical student. And so when the
            experiment happens, I pick that random student.</p>
        <p>这是学生的身高。但那个学生也会有体重，我可以在这里记录下来。同样，每个学生都会有自己特定的体重。所以体重函数与样本空间和实数函数不同，它是一个不同的随机变量。所以我在这里要说的是，一个概率实验可能涉及几个有趣的随机变量。</p><p>This is the height of the student. But that student would also have a weight, and I could record it here. And
            similarly, every student is going to have their own particular weight. So the weight function is a different
            function from the sample space to the real numbers, and it’s a different random variable. So the point I’m
            making here is that a single probabilistic experiment may involve several interesting random variables.</p>
        <p>我可能对某个随机学生的身高或体重感兴趣。这些是不同的随机变量，可能会引起我的兴趣。我还可以做其他事情。假设我定义一个对象，例如 H bar，它是 2.58。这对应什么？嗯，这是以厘米为单位的高度。</p><p>I may be interested in the height of a random student or the weight of the random student. These are
            different
            random variables that could be of interest. I can also do other things. Suppose I define an object such as H
            bar, which is 2.58. What does that correspond to? Well, this is the height in centimeters.</p>
        <p>现在，H bar 是 H 本身的函数，但如果要画出这幅图，图会是这样的。60 被映射到 150，71 被映射到，哦，这对我来说太难了。好的，被映射到某个东西，等等。所以 H bar 也是一个随机变量。为什么？</p><p>Now, H bar is a function of H itself, but if you were to draw the picture, the picture would go this way. 60
            gets
            mapped to 150,71 gets mapped to, oh, that’s too hard for me. OK, gets mapped to something, and so on. So H
            bar
            is also a random variable. Why?</p>
        <p>一旦我选出一个特定的学生，这个特定的结果就完全决定了 H 的数值，也就是这个学生的身高，但以厘米为单位。我们这里得到的实际上是一个随机变量，它被定义为另一个随机变量的函数。这个例子试图说明的一点是，随机变量的函数也是随机变量。实验发生了，实验为这个对象确定了一个数值。</p><p>Once I pick a particular student, that particular outcome determines completely the numerical value of H bar,
            which is the height of that student but measured in centimeters. What we have here is actually a random
            variable, which is defined as a function of another random variable. And the point that this example is
            trying
            to make is that functions of random variables are also random variables. The experiment happens, the
            experiment
            determines a numerical value for this object.</p>
        <p>一旦你有了这个对象的数值，这也决定了那个对象的数值。因此，给定一个结果，这个特定对象的数值就确定了。因此，H bar 本身就是样本空间的一个函数，从结果到数值。根据我们这里的形式定义，这使它成为一个随机变量。</p><p>And once you have the numerical value for this object, that determines also the numerical value for that
            object.
            So given an outcome, the numerical value of this particular object is determined. So H bar is itself a
            function
            from the sample space, from outcomes to numerical values. And that makes it a random variable according to
            the
            formal definition that we have here.</p>
        <p>因此，正式定义是，随机变量不是随机的，它不是变量，它只是从样本空间到实数的函数。这是抽象的、正确的思考方式。现在，随机变量可以是不同类型的。它们可以是离散的，也可以是连续的。假设我用英寸来测量身高，但我四舍五入到最接近的英寸。那么我在这里得到的数值将只是整数。
        </p><p>So the formal definition is that the random variable is not random, it’s not a variable, it’s just a function
            from the sample space to the real numbers. That’s the abstract, right way of thinking about them. Now,
            random
            variables can be of different types. They can be discrete or continuous. Suppose that I measure the heights
            in
            inches, but I round to the nearest inch. Then the numerical values I’m going to get here would be just
            integers.
        </p>
        <p>因此，这将使其成为一个整数值随机变量。这是一个离散随机变量。或者，也许我有一个高度测量秤，它具有无限精度，可以将您的身高记录到无限位精度。在这种情况下，您的身高将只是一个一般的实数。因此，我们将有一个随机变量，其值取自整个实数集。</p><p>So that would make it an integer valued random variable. And this is a discrete random variable. Or maybe I
            have
            a scale for measuring height which is infinitely precise and records your height to an infinite number of
            digits
            of precision. In that case, your height would be just a general real number. So we would have a random
            variable
            that takes values in the entire set of real numbers.</p>
        <p>嗯，我想不是真正的负数，而是非负数的集合。这将是一个连续随机变量。它取连续集合中的值。所以我们将讨论离散和连续随机变量。我们要做的第一件事是花几节课讲离散随机变量，因为离散总是比较容易。然后我们将在连续设置中重复所有内容。</p><p>Well, I guess not really negative numbers, but the set of non negative numbers. And that would be a
            continuous
            random variable. It takes values in a continuous set. So we will be talking about both discrete and
            continuous
            random variables. The first thing we will do will be to devote a few lectures on discrete random variables,
            because discrete is always easier. And then we’re going to repeat everything in the continuous setting.</p>
        <p>因此离散比较简单，它是理解所有概念的正确方法，即使是那些看似基本的概念。然后，当我们讨论连续情况时，你就会明白发生了什么。因此，在连续情况下，你会遇到微积分的所有复杂性以及一些额外的数学知识。</p><p>So discrete is easier, and it’s the right place to understand all the concepts, even those who may appear to
            be
            elementary. And then you will be set to understand what’s going on when we go to the continuous case. So in
            the
            continuous case, you get all the complications of calculus and some extra math that comes in there.</p>
        <p>因此，在简单的离散情况下，所有概念都应牢记在心，这样在处理连续情况时就不会出现概念障碍。现在，有一点很重要，虽然可能看起来微不足道，但实际上非常重要，这样你就不会在不同类型的概念之间纠结。随机变量本身和它所取的数值之间存在根本区别。</p><p>So it’s important to have been down all the concepts very well in the easy, discrete case so that you don’t
            have
            conceptual hurdles when you move on to the continuous case. Now, one important remark that may seem trivial
            but
            it’s actually very important so that you don’t get tangled up between different types of concepts. there’s a
            fundamental distinction between the random variable itself, and the numerical values that it takes.</p>
        <p>从抽象或数学的角度来说，随机变量 x 或本例中的 H 是一个函数。好吧。如果您喜欢编程，那么“程序”或“子程序”可能更合适。那么子程序高度是什么？给定一个学生，我带那个学生，强迫他们站在秤上并测量他们。这就是测量身高的子程序。它实际上是一个以学生为输入并产生数字作为输出的函数。我们用大写 H 表示子程序。</p><p>Abstractly speaking, or mathematically speaking, a random variable, x, or H in this example, is a function.
            OK.
            Maybe if you like programming the words “procedure” or “sub routine” might be better. So what’s the sub
            routine
            height? Given a student, I take that student, force them on the scale and measure them. That’s the sub
            routine
            that measures heights. It’s really a function that takes students as input and produces numbers as output.
            The
            sub routine we denoted by capital H.</p>
        <p>这就是随机变量。但是一旦你将某个学生代入该子程序，你最终会得到一个特定的数字。这是该子程序的数值输出或该函数的数值。该数值是实数的一个元素。因此，数值是一个实数，其中大写 X 是从欧米茄到实数的函数。因此，它们是非常不同类型的对象。</p><p>That’s the random variable. But once you plug in a particular student into that sub routine, you end up
            getting a
            particular number. This is the numerical output of that sub routine or the numerical value of that function.
            And
            that numerical value is an element of the real numbers. So the numerical value is a real number, where this
            capital X is a function from omega to the real numbers. So they are very different types of objects.</p>
        <p>我们随时跟踪讨论内容的方式是使用大写字母表示随机变量，小写字母表示数字。好的。现在，一旦我们手头有一个随机变量，该随机变量就会采用不同的数值。我们想要描述一下随机变量可以采用的不同数值的相对可能性。</p><p>And the way that we keep track of what we’re talking about at any given time is by using capital letters for
            random variables and lower case letters for numbers. OK. So now once we have a random variable at hand, that
            random variable takes on different numerical values. And we want to describe to say something about the
            relative
            likelihoods of the different numerical values that the random variable can take.</p>
        <p>这是我们的样本空间，这是实数线。一系列结果会产生一个特定的数值。如果有这个结果，就会产生另一个数值。如果有这个结果，就会产生另一个数值。我们的样本空间就在这里。实数就在这里。我们想要问的是，这个特定数值出现的可能性有多大？</p><p>So here’s our sample space, and here’s the real line. And there’s a bunch of outcomes that gave rise to one
            particular numerical value. There’s another numerical value that arises if we have this outcome. There’s
            another
            numerical value that arises if we have this outcome. So our sample space is here. The real numbers are here.
            And
            what we want to do is to ask the question, how likely is that particular numerical value to occur?</p>
        <p>因此，我们本质上要问的是，我们得到导致该特定数值的结果的可能性有多大？我们计算该数值的总体概率，并使用条形表示该概率，这样我们最终会生成一个条形图。因此，这可能是与此图片相关的一个可能的条形图。</p><p>So what we’re essentially asking is, how likely is it that we obtain an outcome that leads to that particular
            numerical value? We calculate that overall probability of that numerical value and we represent that
            probability
            using a bar so that we end up generating a bar graph. So that could be a possible bar graph associated with
            this
            picture.</p>
        <p>这个条形的大小是我们的随机变量取这个数值的总概率，也就是导致这个数值的不同结果的概率之和。所以我们在这里绘制的条形图。我们给它起个名字。它是一个函数，我们用小写字母 b 表示，大写字母 X。大写字母 X 表示我们正在讨论哪个随机变量。</p><p>The size of this bar is the total probability that our random variable took on this numerical value, which is
            just the sum of the probabilities of the different outcomes that led to that numerical value. So the thing
            that
            we’re plotting here, the bar graph. we give a name to it. It’s a function, which we denote by lowercase b,
            capital X. The capital X indicates which random variable we’re talking about.</p>
        <p>它是小 x 的函数，小 x 是随机变量取值的范围。所以在数学符号中，PMF 在某个特定数字（小 x）处的值是随机变量取数值（小 x）的概率。如果你想精确地理解它的含义，它是随机变量最终取该值（小 x）的所有结果的总体概率。
        </p><p>And it’s a function of little x, which is the range of values that our random variable is taking. So in
            mathematical notation, the value of the PMF at some particular number, little x, is the probability that our
            random variable takes on the numerical value, little x. And if you want to be precise about what this means,
            it’s the overall probability of all outcomes for which the random variable ends up taking that value, little
            x.
        </p>
        <p>因此，这是所有 omega 导致特定数值 x 的总体概率。那么我们对 PMF 了解多少呢？由于存在概率，因此条形图中的所有这些条目都必须是非负的。此外，如果您穷尽小 x 的所有可能值，那么您将穷尽这里所有可能的结果。因为每个结果都会导致某个特定的 x。</p><p>So this is the overall probability of all omegas that lead to that particular numerical value, x, of
            interest. So
            what do we know about PMFs? Since there are probabilities, all these entries in the bar graph have to be non
            negative. Also, if you exhaust all the possible values of little x’s, you will have exhausted all the
            possible
            outcomes here. Because every outcome leads to some particular x.</p>
        <p>所以这些概率的总和应该等于一。这是这里的第二个关系。所以这个关系告诉我们一些小 x 将会发生。它们发生的概率不同，但是当你把所有可能的小 x 放在一起考虑时，其中一个小 x 将会实现。概率需要加一。好的。让我们来看看第一个非平凡条形图的例子。</p><p>So the sum of these probabilities should be equal to one. This is the second relation here. So this relation
            tell
            us that some little x is going to happen. They happen with different probabilities, but when you consider
            all
            the possible little x’s together, one of those little x’s is going to be realized. Probabilities need to add
            to
            one. OK. So let’s get our first example of a non trivial bar graph.</p>
        <p>考虑这样一个实验：我从一枚硬币开始，然后开始反复抛掷。我一直这样做，直到第一次掷出正面。那么这个实验可能的结果是什么呢？一种可能的结果是，我第一次抛掷时就掷出了正面，然后我就停下来了。在这种情况下，我的随机变量取值为 1。或者，我可能先掷出反面，然后掷出正面。</p><p>Consider the experiment where I start with a coin and I start flipping it over and over. And I do this until
            I
            obtain heads for the first time. So what are possible outcomes of this experiment? One possible outcome is
            that
            I obtain heads at the first toss, and then I stop. In this case, my random variable takes the value 1. Or
            it’s
            possible that I obtain tails and then heads.</p>
        <p>要掷多少次才能掷出正面？这将是 x 等于 2。或者更一般地说，我可能会掷出 k-1 次反面，然后在第 k 次掷出正面，在这种情况下，我们的随机变量取值小写 k。这就是实验。所以大写 X 是一个定义明确的随机变量。它是掷出正面前需要掷的次数。</p><p>How many tosses did it take until heads appeared? This would be x equals to 2. Or more generally, I might
            obtain
            tails for k minus 1 times, and then obtain heads at the k th time, in which case, our random variable takes
            the
            value, little k. So that’s the experiment. So capital X is a well defined random variable. It’s the number
            of
            tosses it takes until I see heads for the first time.</p>
        <p>这些是可能的结果。这些是样本空间的元素。这些是取决于结果的 X 值。显然，X 是结果的函数。你告诉我结果，我就会告诉你 X 是什么。所以我们现在要做的是计算 X 的 PMF。因此，根据定义，k 的 Px 是我们的随机变量取值 k 的概率。</p><p>These are the possible outcomes. These are elements of our sample space. And these are the values of X
            depending
            on the outcome. Clearly X is a function of the outcome. You tell me the outcome, I’m going to tell you what
            X
            is. So what we want to do now is to calculate the PMF of X. So Px of k is, by definition, the probability
            that
            our random variable takes the value k.</p>
        <p>要使随机变量取 k 的值，第一个正面出现在抛掷次数为 k 时。只有当我们获得以下事件序列时，此事件才可能发生。T 是第 k 次抛掷的前 k 次减 1 次、反面和正面。因此，随机变量等于 k ​​的事件与此事件相同，即 k 减 1 次反面后跟 1 次正面。该事件发生的概率是多少？</p><p>For the random variable to take the value of k, the first head appears at toss number k. The only way that
            this
            event can happen is if we obtain this sequence of events. T’s the first k minus 1 times, tails, and heads at
            the
            k th flip. So this event, that the random variable is equal to k, is the same as this event, k minus 1 tails
            followed by 1 head. What’s the probability of that event?</p>
        <p>我们假设抛硬币是独立的。因此，为了找到这个事件的概率，我们需要将反面概率乘以反面概率，再乘以反面概率。我们将 k 减一，再乘以正面概率，这样在末尾就多了一个 p。这就是所谓的几何 PMF 公式。为什么我们称之为几何 PMF？</p><p>We’re assuming that the coin tosses are independent. So to find the probability of this event, we need to
            multiply the probability of tails, times the probability of tails, times the probability of tails. We
            multiply k
            minus one times, times the probability of heads, which puts an extra p at the end. And this is the formula
            for
            the so called geometric PMF. And why do we call it geometric?</p>
        <p>因为如果你绘制这个随机变量 X 的条形图，我们从 1 开始，有一个特定的数字，即 p。然后在 2 处我们得到 p(1 p)。在 3 处我们会得到更小的数字，它是 p 乘以 (1 p) 的平方。并且条形图以几何级数的速度不断下降。每个条形图都比前一个条形图小，因为每次我们都会得到一个额外的 1 p 因子。</p><p>Because if you go and plot the bar graph of this random variable, X, we start at 1 with a certain number,
            which
            is p.&nbsp;And then at 2 we get p(1 p). At 3 we’re going to get something smaller, it’s p times (1 p) squared.
            And
            the bars keep going down at the rate of geometric progression. Each bar is smaller than the previous bar,
            because each time we get an extra factor of 1 p involved.</p>
        <p>因此，这个 PMF 的形状是几何序列的图形。因此，我们称其为几何 PMF，并且我们也称 X 为几何随机变量。因此，直到第一次出现正面为止的抛硬币次数是几何随机变量。因此，这是一个如何计算随机变量的 PMF 的示例。这是一个简单的例子，因为这个事件只能以一种方式实现。</p><p>So the shape of this PMF is the graph of a geometric sequence. For that reason, we say that it’s the
            geometric
            PMF, and we call X also a geometric random variable. So the number of coin tosses until the first head is a
            geometric random variable. So this was an example of how to compute the PMF of a random variable. This was
            an
            easy example, because this event could be realized in one and only one way.</p>
        <p>因此，要找到这个概率，我们只需要找到这个特定结果的概率。更一般地说，会有很多结果会导致相同的数值。我们需要跟踪所有这些结果。例如，在这张图片中，如果我想找到 PMF 的这个值，我需要把导致该值的所有结果的概率加起来。</p><p>So to find the probability of this, we just needed to find the probability of this particular outcome. More
            generally, there’s going to be many outcomes that can lead to the same numerical value. And we need to keep
            track of all of them. For example, in this picture, if I want to find this value of the PMF, I need to add
            up
            the probabilities of all the outcomes that leads to that value.</p>
        <p>所以一般程序就是这幅图所暗示的。要找到这个概率，你需要确定哪些结果会导致这个数值，然后将它们的概率相加。让我们举一个简单的例子。我拿一个四面体骰子。我掷了两次。有很多随机变量可以与同一个实验联系起来。所以第一次掷的结果，我们可以称之为 F。</p><p>So the general procedure is exactly what this picture suggests. To find this probability, you go and identify
            which outcomes lead to this numerical value, and add their probabilities. So let’s do a simple example. I
            take a
            tetrahedral die. I toss it twice. And there’s lots of random variables that you can associate with the same
            experiment. So the outcome of the first throw, we can call it F.</p>
        <p>这是一个随机变量，因为一旦你告诉我实验中发生了什么，它就被确定了。第二次投掷的结果是另一个随机变量。两次投掷中的最小值也是一个随机变量。一旦我进行实验，这个随机变量就会呈现一个特定的数值。</p><p>That’s a random variable because it’s determined once you tell me what happens in the experiment. The outcome
            of
            the second throw is another random variable. The minimum of the two throws is also a random variable. Once I
            do
            the experiment, this random variable takes on a specific numerical value.</p>
        <p>假设我做了一个实验，得到了 2 和 3。那么这个随机变量将取数值 2。这将取数值 3。这将取数值 2。现在假设我想计算这个随机变量的 PMF。我需要做的是计算 Px(0)、Px(1)、Px(2)、Px(3) 等等。</p><p>So suppose I do the experiment and I get a 2 and a 3. So this random variable is going to take the numerical
            value of 2. This is going to take the numerical value of 3. This is going to take the numerical value of 2.
            And
            now suppose that I want to calculate the PMF of this random variable. What I will need to do is to calculate
            Px(0), Px(1), Px(2), Px(3), and so on.</p>
        <p>那么，我们不要进行整个计算，我们只计算 PMF 的一个条目。所以 Px(2)。这是两次投掷中最小的一次得到 2 的概率。这种情况可能以多种方式发生。有五种方式可以发生。</p><p>Let’s not do the entire calculation then, let’s just calculate one of the entries of the PMF. So Px(2).
            that’s
            the probability that the minimum of the two throws gives us a 2. And this can happen in many ways. There are
            five ways that it can happen.</p>
        <p>这些都是两个结果中最小的一个等于 2 的结果。假设四面体骰子是公平的，并且投掷是独立的，那么就有 5 个结果。每个结果的概率都是 1/16。一共有 5 个，所以我们得到的答案是 5/16。从概念上讲，这只是您用来计算 PMF 的过程，就像您构建这个特定的条形图一样。</p><p>Those are all of the outcomes for which the smallest of the two is equal to 2. That’s five outcomes assuming
            that
            the tetrahedral die is fair and the tosses are independent. Each one of these outcomes has probability of
            1/16.
            There’s five of them, so we get an answer, 5/16. Conceptually, this is just the procedure that you use to
            calculate PMFs the way that you construct this particular bar graph.</p>
        <p>你考虑随机变量的所有可能值，然后对于每个随机变量，通过将导致该特定数值的所有可能结果的概率相加，找到随机变量取该值的概率。那么让我们再做一次更有趣的问题。让我们重新回顾上次的抛硬币问题。让我们固定一个数字 n，然后决定连续抛硬币 n 次。</p><p>You consider all the possible values of your random variable, and for each one of those random variables you
            find
            the probability that the random variable takes on that value by adding the probabilities of all the possible
            outcomes that leads to that particular numerical value. So let’s do another, more interesting one. So let’s
            revisit the coin tossing problem from last time. Let us fix a number n, and we decide to flip a coin n
            consecutive times.</p>
        <p>每次抛硬币都是独立的。每次抛硬币都有概率 p，即掷出正面的次数。让我们考虑一下随机变量，即掷出正面的次数。好吧，这是我们上次处理过的问题。我们知道不同掷出正面次数的概率，但现在我们将使用今天的符号来做同样的事情。</p><p>Each time the coin tosses are independent. And each one of the tosses will have a probability, p, of
            obtaining
            heads. Let’s consider the random variable, which is the total number of heads that have been obtained. Well,
            that’s something that we dealt with last time. We know the probabilities for different numbers of heads, but
            we’re just going to do the same now using today’s notation.</p>
        <p>具体来说，我们让 n 等于 4。Px 是随机变量 X 的 PMF。根据定义，Px(2) 是随机变量取值为 2 的概率。所以这就是我们四次投掷中恰好出现两次正面的概率。恰好出现两次正面的事件可以以多种方式发生。我在这里写下了可能发生的不同方式。</p><p>So let’s, for concreteness, n equal to 4. Px is the PMF of that random variable, X. Px(2) is meant to be, by
            definition, it’s the probability that a random variable takes the value of 2. So this is the probability
            that we
            have, exactly two heads in our four tosses. The event of exactly two heads can happen in multiple ways. And
            here
            I’ve written down the different ways that it can happen.</p>
        <p>事实证明，这种情况发生的方式恰好有六种。幸运的是，每一种方式的概率都相同。p 平方乘以 (1 p) 平方。因此，这给出了在 2 处求得的 PMF 值。因此，我们在这里明确地计算出这种情况发生的方式有六种，这导致了这个因子 6。但是这个因子 6 最终与这个 4 选 2 相同。如果您还记得上次的定义，4 选 2 是 4 的阶乘除以 2 的阶乘，再除以 2 的阶乘，这确实等于 6。这是您将使用的更通用的公式。</p><p>It turns out that there’s exactly six ways that it can happen. And each one of these ways, luckily enough,
            has
            the same probability. p squared times (1 p) squared. So that gives us the value for the PMF evaluated at 2.
            So
            here we just counted explicitly that we have six possible ways that this can happen, and this gave rise to
            this
            factor of 6. But this factor of 6 turns out to be the same as this 4 choose 2. If you remember definition
            from
            last time, 4 choose 2 is 4 factorial divided by 2 factorial, divided by 2 factorial, which is indeed equal
            to 6.
            And this is the more general formula that you would be using.</p>
        <p>一般来说，如果你有 n 次投掷，并且你想知道掷出 k 次正面的概率，那么这个事件的概率由这个公式给出。这就是我们上次推导出来的公式。只不过上次我们没有使用这个符号。我们只是说掷出 k 次正面的概率等于这个。今天我们介绍额外的符号。</p><p>In general, if you have n tosses and you’re interested in the probability of obtaining k heads, the
            probability
            of that event is given by this formula. So that’s the formula that we derived last time. Except that last
            time
            we didn’t use this notation. We just said the probability of k heads is equal to this. Today we introduce
            the
            extra notation.</p>
        <p>有了这种符号，我们可能还想为 Px 绘制一个条形图。在本例中，针对抛硬币问题。如果当 n 是一个相当大的数字时，将该条形图绘制为 k 函数，最终将获得一个形状类似于这样的条形图。</p><p>And also having that notation, we may be tempted to also plot a bar graph for the Px. In this case, for the
            coin
            tossing problem. And if you plot that bar graph as a function of k when n is a fairly large number, what you
            will end up obtaining is a bar graph that has a shape of something like this.</p>
        <p>因此，某些 k 值比其他值更有可能，而更可能的值位于范围的中间。而极端值，比如头太少或头太多，都不太可能。现在，神奇的是，当 n 很大时，这条曲线会呈现出相当明确的形状，就像所谓的钟形曲线。</p><p>So certain values of k are more likely than others, and the more likely values are somewhere in the middle of
            the
            range. And extreme values. too few heads or too many heads, are unlikely. Now, the miraculous thing is that
            it
            turns out that this curve gets a pretty definite shape, like a so called bell curve, when n is big.</p>
        <p>这是概率论中一个非常深奥和核心的事实，我们将在几个月后讲到。目前，这可能只是一个有趣的观察。如果你进入 MATLAB，输入这个公式，并让 MATLAB 为你绘制它，你将得到这种形式的有趣形状。</p><p>This is a very deep and central fact from probability theory that we will get to in a couple of months. For
            now,
            it just could be a curious observation. If you go into MATLAB and put this formula in and ask MATLAB to plot
            it
            for you, you’re going to get an interesting shape of this form.</p>
        <p>稍后我们将必须了解这是从何而来的，以及我们得到的渐近形式是否有一个简单、简单的公式。</p><p>And later on we will have to sort of understand where this is coming from and whether there’s a nice, simple
            formula for the asymptotic form that we get.</p>
        <h2 id="expectation">期待</h2><h2>Expectation</h2>
        <p>好的。到目前为止，我基本上没有讲什么新东西，只是讲了一些符号和一些概念，你必须将随机变量视为样本空间中的函数。现在是时候介绍一些新东西了。</p><p>All right. So, so far I’ve said essentially nothing new, just a little bit of notation and this little
            conceptual
            thing that you have to think of random variables as functions in the sample space. So now it’s time to
            introduce
            something new.</p>
        <p>这是当今的重大概念。从某种意义上说，这是一个简单的概念。但它是我们处理随机变量时最核心、最重要的概念。它是随机变量的期望值的概念。因此，期望值应该是，让我们宽泛地说，类似于平均值，其中你将概率解释为频率之类的东西。所以你玩某个游戏，你的奖励将是。
        </p><p>This is the big concept of the day. In some sense it’s an easy concept. But it’s the most central, most
            important
            concept that we have to deal with random variables. It’s the concept of the expected value of a random
            variable.
            So the expected value is meant to be, let’s speak loosely, something like an average, where you interpret
            probabilities as something like frequencies. So you play a certain game and your rewards are going to be.
        </p>
        <p>使用我的标准数字。您的奖励将是 1 美元，概率为 1/6。奖励将是 2 美元，概率为 1/2，奖励将是 4 美元，概率为 1/3。所以这是某个随机变量的 PMF 图。如果您玩那个游戏并以这个概率获得这么多美元，依此类推，如果您玩这个游戏无数次，您预计平均会得到多少钱？</p><p>Use my standard numbers. your rewards are going to be one dollar with probability 1/6. It’s going to be 2
            dollars
            with probability 1/2, and four dollars with probability 1/3. So this is a plot of the PMF of some random
            variable. If you play that game and you get so many dollars with this probability, and so on, how much do
            you
            expect to get on the average if you play the game a zillion times?</p>
        <p>好吧，你可以这样想。六分之一的时间我会得到一美元。一半的时间是那个结果，我会得到两美元。三分之一的时间是另一个结果，我会得到四美元。你计算这个数字，结果是2.5。好的。</p><p>Well, you can think as follows. one sixth of the time I’m going to get one dollar. One half of the time that
            outcome is going to happen and I’m going to get two dollars. And one third of the time the other outcome
            happens, and I’m going to get four dollars. And you evaluate that number and it turns out to be 2.5. OK.</p>
        <p>因此，如果您将这些概率视为获得不同收益的频率，那么这就是计算平均收益的合理方法。粗略地说，当您试图理解各种事物时，将概率视为频率并没有什么坏处。那么我们在这里做了什么？我们取不同结果、不同数值的概率，并将它们乘以相应的数值。</p><p>So that’s a reasonable way of calculating the average payoff if you think of these probabilities as the
            frequencies with which you obtain the different payoffs. And loosely speaking, it doesn’t hurt to think of
            probabilities as frequencies when you try to make sense of various things. So what did we do here? We took
            the
            probabilities of the different outcomes, of the different numerical values, and multiplied them with the
            corresponding numerical value.</p>
        <p>同样，这里我们有一个概率和相应的数值，我们将所有 x 相加。这就是我们所做的。它看起来是一个有趣的量。所以我们要给它起个名字，我们称之为随机变量的期望值。所以这个公式只是捕捉了我们所做的计算。我们如何解释期望值？</p><p>Similarly here, we have a probability and the corresponding numerical value and we added up over all x’s. So
            that’s what we did. It looks like an interesting quantity to deal with. So we’re going to give a name to it,
            and
            we’re going to call it the expected value of a random variable. So this formula just captures the
            calculation
            that we did. How do we interpret the expected value?</p>
        <p>因此，我在这个例子中使用的就是这种解释。你可以把它看作是你通过大量重复实验得到的平均值，其中你将概率解释为不同数值发生的频率。</p><p>So the one interpretation is the one that I used in this example. You can think of it as the average that you
            get
            over a large number of repetitions of an experiment where you interpret the probabilities as the frequencies
            with which the different numerical values can happen.</p>
        <p>还有另一种解释，它更直观，也更有见地。如果你还记得大一物理课上的内容，这种公式可以告诉你这种物体的重心。如果你从字面上理解这幅图，把它想象成六分之一的质量在这里，一半的质量在这里，三分之一的质量在那里，你会问我这个结构的重心是多少。</p><p>There’s another interpretation that’s a little more visual and that’s kind of insightful, if you remember
            your
            freshman physics, this kind of formula gives you the center of gravity of an object of this kind. If you
            take
            that picture literally and think of this as a mass of one sixth sitting here, and the mass of one half
            sitting
            here, and one third sitting there, and you ask me what’s the center of gravity of that structure.</p>
        <p>这个公式可以让你计算出重心。那么重心是什么呢？重心就是如果你把笔放在正下方，图表就会保持在原位，不会掉到一边，也不会掉到另一边。所以在这个东西中，从图中可以看出，由于数字 4 稍微靠右一点，而且更重一点，重心应该在这里附近。</p><p>This is the formula that gives you the center of gravity. Now what’s the center of gravity? It’s the place
            where
            if you put your pen right underneath, that diagram will stay in place and will not fall on one side and will
            not
            fall on the other side. So in this thing, by picture, since the 4 is a little more to the right and a little
            heavier, the center of gravity should be somewhere around here.</p>
        <p>这就是数学给我们的结果。结果是 2.5。一旦你对重心有了这种解释，有时你就可以很快地计算出期望值。所以这是我们的新随机变量。它是均匀随机变量，其中每个数值的可能性都相同。这里总共有 n 加 1 个可能的数值。所以它们中的每一个都有 1/(n + 1) 的概率。</p><p>And that’s what for the math gave us. It turns out to be two and a half. Once you have this interpretation
            about
            centers of gravity, sometimes you can calculate expectations pretty fast. So here’s our new random variable.
            It’s the uniform random variable in which each one of the numerical values is equally likely. Here there’s a
            total of n plus 1 possible numerical values. So each one of them has probability 1 over (n + 1).</p>
        <p>让我们计算一下这个随机变量的期望值。我们可以从字面上理解这个公式，考虑所有可能的结果，或者所有可能的数值，并根据它们对应的概率进行加权，然后进行计算并得到答案。但我给了你重心的直觉。你能用这种直觉来猜答案吗？这种重心的基础设施是什么？我们有对称性。所以它应该在中间。</p><p>Let’s calculate the expected value of this random variable. We can take the formula literally and consider
            all
            possible outcomes, or all possible numerical values, and weigh them by their corresponding probability, and
            do
            this calculation and obtain an answer. But I gave you the intuition of centers of gravity. Can you use that
            intuition to guess the answer? What’s the center of gravity infrastructure of this kind? We have symmetry.
            So it
            should be in the middle.</p>
        <p>那么中间值是多少呢？它是两个端点的平均值。所以，不用做代数运算，你就知道答案是 n/2。所以，当你有 PMF 时，你应该遵循这个原则，PMF 围绕某个点对称。那个特定的点就是与这个特定的 PMF 相关的期望值。好的。</p><p>And what’s the middle? It’s the average of the two end points. So without having to do the algebra, you know
            that’s the answer is going to be n over 2. So this is a moral that you should keep whenever you have PMF,
            which
            is symmetric around a certain point. That certain point is going to be the expected value associated with
            this
            particular PMF. OK.</p>
        <p>那么，既然定义了期望值，我们还剩下什么要做呢？好吧，我们想研究它的行为方式，它具有什么样的属性，以及如何计算复杂随机变量的期望值。因此，我们要开始的第一个复杂问题是处理随机变量函数的情况。好的。那么让我重新绘制与之前相同的图。我们有 omega。</p><p>So having defined the expected value, what is there that’s left for us to do? Well, we want to investigate
            how it
            behaves, what kind of properties does it have, and also how do you calculate expected values of complicated
            random variables. So the first complication that we’re going to start with is the case where we deal with a
            function of a random variable. OK. So let me redraw this same picture as before. We have omega.</p>
        <p>这是我们的样本空间。这是实数直线。我们有一个随机变量，它会产生各种 X 值。所以这个随机变量就是大写 X，每个结果都会为我们的随机变量 X 产生一个特定的数值 x。所以大写 X 实际上是将这些点映射到实数直线上的函数。</p><p>This is our sample space. This is the real line. And we have a random variable that gives rise to various
            values
            for X. So the random variable is capital X, and every outcome leads to a particular numerical value x for
            our
            random variable X. So capital X is really the function that maps these points into the real line.</p>
        <p>然后我考虑这个随机变量的一个函数，称之为大写 Y，它是我之前的随机变量的函数。这个新的随机变量 Y 的数值在我知道大写 X 的数值后就完全确定了。也许你会得到这样的图表。所以 X 是一个随机变量。一旦你有了结果，这就决定了 x 的值。Y 也是一个随机变量。</p><p>And then I consider a function of this random variable, call it capital Y, and it’s a function of my previous
            random variable. And this new random variable Y takes numerical values that are completely determined once I
            know the numerical value of capital X. And perhaps you get a diagram of this kind. So X is a random
            variable.
            Once you have an outcome, this determines the value of x. Y is also a random variable.</p>
        <p>一旦有了结果，就决定了 y 的值。一旦知道了 X，Y 就完全确定了。我们有一个计算 X 期望值的公式。假设你有兴趣计算 Y 的期望值。你会怎么做？好的。你手中唯一拥有的就是定义，所以你可以从使用定义开始。这意味着什么？</p><p>Once you have the outcome, that determines the value of y. Y is completely determined once you know X. We
            have a
            formula for how to calculate the expected value of X. Suppose that you’re interested in calculating the
            expected
            value of Y. How would you go about it? OK. The only thing you have in your hands is the definition, so you
            could
            start by just using the definition. And what does this entail?</p>
        <p>它要求对于 y 的每一个特定值，收集导致该 y 值的所有结果。求出它们的概率。在这里做同样的事情。对于该值，收集这些结果。求出它们的概率和 y 的权重。所以这个公式在这条线上做加法。我们考虑不同的结果并将结果加起来。</p><p>It entails for every particular value of y, collect all the outcomes that leads to that value of y. Find
            their
            probability. Do the same here. For that value, collect those outcomes. Find their probability and weight by
            y.
            So this formula does the addition over this line. We consider the different outcomes and add things up.</p>
        <p>还有另一种方法可以进行同样的计算，我们不是对这些数字进行加法，而是在这里进行加法。我们考虑 x 的不同可能值，并按以下方式思考。对于 x 的每个可能值，该值都会以这个概率出现。如果该值已经出现，这就是我得到的 x 的 g。所以我在考虑这个结果的概率。</p><p>There’s an alternative way of doing the same accounting where instead of doing the addition over those
            numbers,
            we do the addition up here. We consider the different possible values of x, and we think as follows. for
            each
            possible value of x, that value is going to occur with this probability. And if that value has occurred,
            this is
            how much I’m getting, the g of x. So I’m considering the probability of this outcome.</p>
        <p>在这种情况下，y 取这个值。然后我考虑这个结果的概率。在这种情况下，x 的 g 再次取该值。然后我考虑这个特定的 x，它以这个概率发生，在这种情况下，x 的 g 取该值，这里也是一样。我们最终做完全相同的算术，唯一的问题是我们是否将事物捆绑在一起。</p><p>And in that case, y takes this value. Then I’m considering the probabilities of this outcome. And in that
            case, g
            of x takes again that value. Then I consider this particular x, it happens with this much probability, and
            in
            that case, g of x takes that value, and similarly here. We end up doing exactly the same arithmetic, it’s
            only a
            question whether we bundle things together.</p>
        <p>也就是说，如果我们计算这个概率，那么我们就会把这两种情况捆绑在一起。而如果我们在这里进行加法，我们会进行单独的计算。这个概率乘以这个数字，然后这个概率乘以那个数字。所以这只是我们进行计算的方式的简单重新排列，但如果你真的想计算期望值，它在实践中确实会产生很大的不同。</p><p>That is, if we calculate the probability of this, then we’re bundling these two cases together. Whereas if we
            do
            the addition up here, we do a separate calculation. this probability times this number, and then this
            probability times that number. So it’s just a simple rearrangement of the way that we do the calculations,
            but
            it does make a big difference in practice if you actually want to calculate expectations.</p>
        <p>因此，我提到的第二个程序，即通过 x 轴进行加法运算，对应于此公式。考虑 x 的所有可能性，当 x 发生时，您会得到多少钱？这给出了您获得的平均金额。好吧。所以我挥挥手，争辩说这只是一种不同的会计方法，当然需要证明这个公式。幸运的是，它是可以证明的。
        </p><p>So the second procedure that I mentioned, where you do the addition by running over the x axis corresponds to
            this formula. Consider all possibilities for x and when that x happens, how much money are you getting? That
            gives you the average money that you are getting. All right. So I kind of hand waved and argued that it’s
            just a
            different way of accounting, of course one needs to prove this formula. And fortunately it can be proved.
        </p>
        <p>您将在背诵中看到这一点。大多数人一旦对概率概念有了一点了解，实际上就会相信从定义上来说这是正确的。事实上，从定义上来说，它并不正确。这被称为无意识统计学家定律。这是你一直在做的事情，但它确实需要证明。好吧。</p><p>You’re going to see that in recitation. Most people, once they’re a little comfortable with the concepts of
            probability, actually believe that this is true by definition. In fact it’s not true by definition. It’s
            called
            the law of the unconscious statistician. It’s something that you always do, but it’s something that does
            require
            justification. All right.</p>
        <p>因此，这基本上为我们提供了一种计算随机变量函数期望值的捷径，而不必找到该函数的 PMF。我们可以使用原始函数的 PMF。好的。所以我们会反复使用这个属性。在开始使用它之前，请注意一点。一般来说，随机变量函数的平均值与平均值函数不同。</p><p>So this gives us basically a shortcut for calculating expected values of functions of a random variable
            without
            having to find the PMF of that function. We can work with the PMF of the original function. All right. So
            we’re
            going to use this property over and over. Before we start using it, one general word of caution. the average
            of
            a function of a random variable, in general, is not the same as the function of the average.</p>
        <p>因此，取平均值和取函数这两个运算不交换。这个不等式告诉你，一般来说，你不能根据平均值进行推理。所以我们会看到这个属性不成立的例子。你会看到很多这样的情况。我只想说，这在一般情况下是不正确的，但我们感兴趣的是这种关系成立的例外情况。</p><p>So these two operations of taking averages and taking functions do not commute. What this inequality tells
            you is
            that, in general, you can not reason on the average. So we’re going to see instances where this property is
            not
            true. You’re going to see lots of them. Let me just throw it here that it’s something that’s not true in
            general, but we will be interested in the exceptions where a relation like this is true.</p>
        <p>但这些是例外。所以一般来说，期望是平均值，类似于平均值。但平均值的函数与函数的平均值不同。好的。现在让我们来看看期望的性质。假设 alpha 是一个实数，我问你，这个实数的期望值是多少？例如，如果我写下这个表达式。期望值为 2。这是什么？</p><p>But these will be the exceptions. So in general, expectations are average, something like averages. But the
            function of an average is not the same as the average of the function. OK. So now let’s go to properties of
            expectations. Suppose that alpha is a real number, and I ask you, what’s the expected value of that real
            number?
            So for example, if I write down this expression. expected value of 2. What is this?</p>
        <p>好吧，我们定义了随机变量，也定义了随机变量的期望。因此，为了使句法有意义，这里的东西应该是一个随机变量。2 是数字 2 吗？它是随机变量吗？从某种意义上说，是的。它是始终取 2 值的随机变量。假设您有一个实验，并且该实验每次发生时总是输出 2。</p><p>Well, we defined random variables and we defined expectations of random variables. So for this to make
            syntactic
            sense, this thing inside here should be a random variable. Is 2 the number 2 is it a random variable? In
            some
            sense, yes. It’s the random variable that takes, always, the value of 2. So suppose that you have some
            experiment and that experiment always outputs 2 whenever it happens.</p>
        <p>然后你可以说，是的，这是一个随机实验，但它总是给我 2。无论如何，随机变量的值始终是 2。它是一种退化的随机变量，其中没有任何真正的随机性，但将其视为一种特殊情况仍然有用。因此，它对应于从样本空间到实线的函数，该函数仅取一个值。</p><p>Then you can say, yes, it’s a random experiment but it always gives me 2. The value of the random variable is
            always 2 no matter what. It’s kind of a degenerate random variable that doesn’t have any real randomness in
            it,
            but it’s still useful to think of it as a special case. So it corresponds to a function from the sample
            space to
            the real line that takes only one value.</p>
        <p>无论结果如何，它总是给我一个 2。好的。如果你有一个随机变量总是给你一个 2，那么预期值会是多少？这个求和中出现的唯一条目是数字 2。2 的概率等于 1，该随机变量的值等于 2。所以它就是数字本身。</p><p>No matter what the outcome is, it always gives me a 2. OK. If you have a random variable that always gives
            you a
            2, what is the expected value going to be? The only entry that shows up in this summation is that number 2.
            The
            probability of a 2 is equal to 1, and the value of that random variable is equal to 2. So it’s the number
            itself.</p>
        <p>因此，在总是给出 2 的实验中，平均值是 2。好的。这很简单。现在让我们进行实验，其中年龄是您的身高（以英寸为单位）。我知道您的身高（以英寸为单位），但我对以厘米为单位测量的身高感兴趣。这与您的身高（以英寸为单位）有什么关系？
        </p><p>So the average value in an experiment that always gives you 2’s is 2. All right. So that’s nice and simple.
            Now
            let’s go to our experiment where age was your height in inches. And I know your height in inches, but I’m
            interested in your height measured in centimeters. How is that going to be related to your height in inches?
        </p>
        <p>好吧，如果你把你的身高以英寸为单位换算成厘米，我就会得到另一个随机变量，无论如何，它总是比我一开始使用的随机变量大 2.5 倍。如果你取某个量并总是乘以 2.5，那么这个量的平均值会怎样呢？它也会乘以 2.5。</p><p>Well, if you take your height in inches and convert it to centimeters, I have another random variable, which
            is
            always, no matter what, two and a half times bigger than the random variable I started with. If you take
            some
            quantity and always multiplied by two and a half what happens to the average of that quantity? It also gets
            multiplied by two and a half.</p>
        <p>因此，你会得到这样的关系，即以厘米为单位测量的学生平均身高是英寸为单位测量的学生平均身高的 2.5 倍。因此，这完全符合直觉。如果你将其概括，它就会给我们这样的关系，即如果你有一个数字，你可以将它拉出期望范围，从而得到正确的结果。</p><p>So you get a relation like this, which says that the average height of a student measured in centimeters is
            two
            and a half times the average height of a student measured in inches. So that makes perfect intuitive sense.
            If
            you generalize it, it gives us this relation, that if you have a number, you can pull it outside the
            expectation
            and you get the right result.</p>
        <p>所以这是一个可以基于平均值进行推理的情况。如果你取一个数字，比如身高，然后乘以某个数字，你就可以基于平均值进行推理。我把这些数字乘以二，平均值就会增加二。所以这是我上面提到的警告的一个例外。我们如何证明这个事实是正确的？
        </p><p>So this is a case where you can reason on the average. If you take a number, such as height, and multiply it
            by a
            certain number, you can reason on the average. I multiply the numbers by two, the averages will go up by
            two. So
            this is an exception to this cautionary statement that I had up there. How do we prove that this fact is
            true?
        </p>
        <p>好吧，我们可以在这里使用期望值规则，它告诉我们 alpha X 的期望值，也就是我们的 x 的 g，本质上，将是我的函数 x 的 g 的总和，乘以 x 的概率。在我们的特定情况下，x 的 g 是 alpha 乘以 x。我们有这些概率。alpha 超出了总和。</p><p>Well, we can use the expected value rule here, which tells us that the expected value of alpha X, this is our
            g
            of X, essentially, is going to be the sum over all x’s of my function, g of X, times the probability of the
            x’s.
            In our particular case, g of X is alpha times X. And we have those probabilities. And the alpha goes outside
            the
            summation.</p>
        <p>因此我们得到 alpha，即 x 的总和，即 x 的 x Px，即 alpha 乘以 X 的期望值。这就是你使用上面的规则正式证明这个关系的方法。我这里的下一个公式也是用同样的方式证明的。这个公式告诉你什么？如果我取每个人的身高（以厘米为单位）。我们已经乘以了 alpha。上帝会给每个人额外十厘米的奖励。</p><p>So we get alpha, sum over x’s, x Px of x, which is alpha times the expected value of X. So that’s how you
            prove
            this relation formally using this rule up here. And the next formula that I have here also gets proved the
            same
            way. What does this formula tell you? If I take everybody’s height in centimeters. we already multiplied by
            alpha. and the gods give everyone a bonus of ten extra centimeters.</p>
        <p>那么全班的平均身高会怎样呢？嗯，只会增加 10 厘米。所以这个期望会给你带来额外的 beta，也就是在平均身高（以厘米为单位）上增加一个 beta，我们也知道它是 alpha 乘以 X 的期望值，再加上 beta。所以这是期望的线性性质。</p><p>What’s going to happen to the average height of the class? Well, it will just go up by an extra ten
            centimeters.
            So this expectation is going to be giving you the bonus of beta just adds a beta to the average height in
            centimeters, which we also know to be alpha times the expected value of X, plus beta. So this is a linearity
            property of expectations.</p>
        <p>如果你取一个随机变量的线性函数，那么该线性函数的期望值就是期望值的线性函数。所以这是我们对这个警告的一大例外，即如果 g 是线性的，则相等。好的。</p><p>If you take a linear function of a single random variable, the expected value of that linear function is the
            linear function of the expected value. So this is our big exception to this cautionary note, that we have
            equal
            if g is linear. OK.</p>
        <h2 id="variance">方差</h2><h2>Variance</h2>
        <p>好的。那么让我们进入今天的最后一个概念。什么样的随机变量函数可能令人感兴趣？一种可能性可能是 X 平方的平均值。</p><p>All right. So let’s get to the last concept of the day. What kind of functions of random variables may be of
            interest? One possibility might be the average value of X squared.</p>
        <p>为什么它很有趣？嗯，为什么不呢。它是你能想到的最简单的函数。所以如果你想计算X平方的期望值，你可以使用这个计算随机变量函数期望值的一般规则。你考虑所有可能的x。对于每个x，你看看它发生的概率是多少。如果这个x发生了，你考虑并看看x平方有多大。</p><p>Why is it interesting? Well, why not. It’s the simplest function that you can think of. So if you want to
            calculate the expected value of X squared, you would use this general rule for how you can calculate
            expected
            values of functions of random variables. You consider all the possible x’s. For each x, you see what’s the
            probability that it occurs. And if that x occurs, you consider and see how big x squared is.</p>
        <p>现在，您可以计算出更有趣的数量、更有趣的期望，它与 x 平方无关，而是与 x 到均值的平方之间的距离有关。所以让我们试着分析一下我们在这里得到的内容。让我们看看里面的数量。它是什么样的数量？它是一个随机变量。为什么？X 是随机的，随机变量，X 的预期值是一个数字。</p><p>Now, the more interesting quantity, a more interesting expectation that you can calculate has to do not with
            x
            squared, but with the distance of x from the mean and then squared. So let’s try to parse what we’ve got up
            here. Let’s look just at the quantity inside here. What kind of quantity is it? It’s a random variable. Why?
            X
            is random, the random variable, expected value of X is a number.</p>
        <p>从一个随机变量中减去一个数字，你会得到另一个随机变量。取一个随机变量并求其平方，你会得到另一个随机变量。所以这里的东西是一个合法的随机变量。它是什么样的随机变量？假设我们有一个实验，并且我们有不同的 x 可能发生。这张图片中 X 的平均值可能在这里附近。我做了这个实验。</p><p>Subtract a number from a random variable, you get another random variable. Take a random variable and square
            it,
            you get another random variable. So the thing inside here is a legitimate random variable. What kind of
            random
            variable is it? So suppose that we have our experiment and we have different x’s that can happen. And the
            mean
            of X in this picture might be somewhere around here. I do the experiment.</p>
        <p>我得到了 x 的某个数值。假设我得到了这个数值。我查看与平均值的距离，也就是这个长度，然后取它的平方。每次我做实验时，我都会记录与平均值的距离并取它的平方。所以我更强调大的距离。然后我取所有可能结果、所有可能数值的平均值。</p><p>I obtain some numerical value of x. Let’s say I obtain this numerical value. I look at the distance from the
            mean, which is this length, and I take the square of that. Each time that I do the experiment, I go and
            record
            my distance from the mean and square it. So I give more emphasis to big distances. And then I take the
            average
            over all possible outcomes, all possible numerical values.</p>
        <p>所以我试图计算与均值之间的平均平方距离。这对应于这里的公式。所以我画的图与此相对应。对于 x 的每个可能数值，该数值对应于与均值平方的一定距离，我根据 x 的特定值出现的可能性进行加权。所以这测量了与均值之间的平均平方距离。</p><p>So I’m trying to compute the average squared distance from the mean. This corresponds to this formula here.
            So
            the picture that I drew corresponds to that. For every possible numerical value of x, that numerical value
            corresponds to a certain distance from the mean squared, and I weight according to how likely is that
            particular
            value of x to arise. So this measures the average squared distance from the mean.</p>
        <p>现在，由于该期望值规则，当然，这个东西与期望值相同。它是随机变量的平均值，即与均值的平方距离。有了这个概率，随机变量就会取这个数值，而与均值的平方距离最终会取这个特定的数值。好的。那么为什么方差很有趣呢？</p><p>Now, because of that expected value rule, of course, this thing is the same as that expectation. It’s the
            average
            value of the random variable, which is the squared distance from the mean. With this probability, the random
            variable takes on this numerical value, and the squared distance from the mean ends up taking that
            particular
            numerical value. OK. So why is the variance interesting?</p>
        <p>它告诉我们我们预期的平均数与平均值的差距有多大。实际上，我们计算的不是与平均值的距离，而是距离的平方。因此，它更加强调了这里的异常值。但它衡量了分布的分散程度。通常，较大的方差意味着这些条形图向左和向右延伸很远。</p><p>It tells us how far away from the mean we expect to be on the average. Well, actually we’re not counting
            distances from the mean, it’s distances squared. So it gives more emphasis to the kind of outliers in here.
            But
            it’s a measure of how spread out the distribution is. A big variance means that those bars go far to the
            left
            and to the right, typically.</p>
        <p>而小方差则意味着所有这些条形都紧密集中在平均值附近。这是平均平方偏差。小方差意味着我们通常有小偏差。大方差意味着我们通常有大偏差。现在，从实际情况来看，当你想计算方差时，有一个方便的公式，我没有证明，但你会在背诵中看到它。这只是两行代数。
        </p><p>Where as a small variance would mean that all those bars are tightly concentrated around the mean value. It’s
            the
            average squared deviation. Small variance means that we generally have small deviations. Large variances
            mean
            that we generally have large deviations. Now as a practical matter, when you want to calculate the variance,
            there’s a handy formula which I’m not proving but you will see it in recitation. It’s just two lines of
            algebra.
        </p>
        <p>它允许我们以一种稍微简单的方式计算它。我们需要计算随机变量的期望值和随机变量平方的期望值，这两个值将给出方差。总结一下我们在这里所做的，方差，根据定义，由这个公式给出。它是平方偏差的期望值。</p><p>And it allows us to calculate it in a somewhat simpler way. We need to calculate the expected value of the
            random
            variable and the expected value of the squares of the random variable, and these two are going to give us
            the
            variance. So to summarize what we did up here, the variance, by definition, is given by this formula. It’s
            the
            expected value of the squared deviation.</p>
        <p>但是我们有等效公式，它来自预期值规则的应用，对于 X 的函数 g，等于 x 减去（X 的预期值）平方。好的。这就是定义。它来自预期值规则。方差的一些属性是什么？当然，方差总是非负的。为什么它总是非负的？好吧，你看看定义，你只是把非负的东西加起来。我们正在添加平方偏差。</p><p>But we have the equivalent formula, which comes from application of the expected value rule, to the function
            g of
            X, equals to x minus the (expected value of X) squared. OK. So this is the definition. This comes from the
            expected value rule. What are some properties of the variance? Of course variances are always non negative.
            Why
            is it always non negative? Well, you look at the definition and your just adding up non negative things.
            We’re
            adding squared deviations.</p>
        <p>因此，当你添加非负数时，你会得到非负数。下一个问题是，如果你取一个随机变量的线性函数，事情会如何扩展？让我们考虑一下 beta 的影响。如果我取一个随机变量并将常数添加到它，这会如何影响我们的传播量？它没有影响。
        </p><p>So when you add non negative things, you get something non negative. The next question is, how do things
            scale if
            you take a linear function of a random variable? Let’s think about the effects of beta. If I take a random
            variable and add the constant to it, how does this affect the amount of spread that we have? It doesn’t
            affect.
        </p>
        <p>无论这个东西的传播程度如何，如果我添加常数 beta，它只会将这个图表移到这里，但传播程度不会增加或减少。问题是，当我将常数添加到随机变量时，所有将要出现的 x 都会向右移动，但预期值也会向右移动。而且由于我们只对与平均值的距离感兴趣，因此这些距离不会受到影响。</p><p>Whatever the spread of this thing is, if I add the constant beta, it just moves this diagram here, but the
            spread
            doesn’t grow or get reduced. The thing is that when I’m adding a constant to a random variable, all the x’s
            that
            are going to appear are further to the right, but the expected value also moves to the right. And since
            we’re
            only interested in distances from the mean, these distances do not get affected.</p>
        <p>X 增加了一些。平均值也增加了一些。差值保持不变。因此，向随机变量添加一个常数不会对其方差产生任何影响。但如果我将随机变量乘以常数 alpha，这会对其方差产生什么影响？</p><p>X gets increased by something. The mean gets increased by that same something. The difference stays the same.
            So
            adding a constant to a random variable doesn’t do anything to it’s variance. But if I multiply a random
            variable
            by a constant alpha, what is that going to do to its variance?</p>
        <p>因为这里有一个平方，当我将随机变量乘以一个常数时，这个 x 会乘以一个常数，平均值会乘以一个常数，平方会乘以该常数的平方。由于这个原因，我们得到了这里显示的 alpha 平方。这就是方差在线性变换下的变换方式。将随机变量乘以常数，方差会增加该常数的平方。好的。今天就到这里。周三见。</p><p>Because we have a square here, when I multiply my random variable by a constant, this x gets multiplied by a
            constant, the mean gets multiplied by a constant, the square gets multiplied by the square of that constant.
            And
            because of that reason, we get this square of alpha showing up here. So that’s how variances transform under
            linear transformations. You multiply your random variable by constant, the variance goes up by the square of
            that same constant. OK. That’s it for today. See you on Wednesday.</p>
        <h1 id="discrete-random-variables-ii">6.离散随机变量（二）</h1><h1>6. Discrete Random Variables II</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 50:53 <a href="https://youtube.com/watch?v=-qCEoqpwjf4">https://youtube.com/watch?v=-qCEoqpwjf4</a></p><p> 12 years ago (Nov 10, 2012) — 50:53 <a href="https://youtube.com/watch?v=-qCEoqpwjf4">https://youtube.com/watch?v=-qCEoqpwjf4</a></p>
        <h2 id="unknown-98">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：好的，我们开始吧。今天，我们将继续上次的主题。主题是随机变量。正如我们所讨论的，随机变量基本上将数值与实验结果相关联。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: OK so let’s start. So
            today,
            we’re going to continue the subject from last time. And the subject is random variables. As we discussed,
            random
            variables basically associate numerical values with the outcomes of an experiment.</p>
        <p>我们想要学习如何操纵它们。现在，在很大程度上，本章将要发生的事情是，我们将重新审视第一章中看到的相同概念。但我们将引入许多新符号，但实际上处理的是相同类型的东西。</p><p>And we want to learn how to manipulate them. Now to a large extent, what’s going to happen, what’s happening
            during this chapter, is that we are revisiting the same concepts we have seen in chapter one. But we’re
            going to
            introduce a lot of new notation, but really dealing with the same kind of stuff.</p>
        <p>除了新符号之外，本章中新概念的唯一区别是期望或期望值的概念。我们将学习如何操纵期望。因此，让我们先快速回顾一下上次讨论的内容。我们讨论了随机变量。粗略地说，随机变量是实验产生的随机量。</p><p>The only difference where we go beyond the new notation, the new concept in this chapter is the concept of
            the
            expectation or expected values. And we’re going to learn how to manipulate expectations. So let us start
            with a
            quick review of what we discussed last time. We talked about random variables. Loosely speaking, random
            variables are random quantities that result from an experiment.</p>
        <h2 id="unknown-99">未知</h2><h2>Unknown</h2>
        <p>更准确地说，从数学上讲，随机变量是从样本空间到实数的函数。也就是说，你给我一个结果，基于这个结果，我可以告诉你随机变量的值。所以随机变量的值是我们得到的结果的函数。现在给定一个随机变量，一些数值结果比其他结果更有可能出现。</p><p>More precisely speaking, mathematically speaking, a random variable is a function from the sample space to
            the
            real numbers. That is, you give me an outcome, and based on that outcome, I can tell you the value of the
            random
            variable. So the value of the random variable is a function of the outcome that we have. Now given a random
            variable, some of the numerical outcomes are more likely than others.</p>
        <p>我们想说哪些更有可能，以及它们的可能性有多大。我们这样做的方法是写下不同可能的数字结果的概率。注意这里的符号。我们用大写字母表示随机变量。我们用小写字母表示实数。所以你读这个的方式，这是随机变量大写 X 恰好取数值小写 x 的概率。</p><p>And we want to say which ones are more likely and how likely they are. And the way we do that is by writing
            down
            the probabilities of the different possible numerical outcomes. Notice here, the notation. We use uppercase
            to
            denote the random variable. We use lowercase to denote real numbers. So the way you read this, this is the
            probability that the random variable, capital X, happens to take the numerical value, little x.</p>
        <p>这是从第一章开始就熟悉的概念。这只是我们将要使用的这个概念的新符号。它是随机变量（大写 X）的概率质量函数。因此，下标仅表示我们正在讨论哪个随机变量。它是分配给特定结果的概率。我们希望为所有可能的数值分配这样的概率。因此，您可以将其视为小 x 的函数。</p><p>This is a concept that’s familiar from chapter one. And this is just the new notation we will be using for
            that
            concept. It’s the Probability Mass Function of the random variable, capital X. So the subscript just
            indicates
            which random variable we’re talking about. And it’s the probability assigned to a particular outcome. And we
            want to assign such probabilities for all possibly numerical values. So you can think of this as being a
            function of little x.</p>
        <h2 id="unknown-100">未知</h2><h2>Unknown</h2>
        <p>它会告诉你每个小 x 出现的可能性有多大。我们上次介绍的新概念是随机变量的期望值，它是这样定义的。你看看所有可能的结果。然后你对随机变量大写 X 的所有可能数值形成某种平均值。你考虑所有可能的数值，然后形成一个平均值。</p><p>And it tells you how likely every little x is going to be. Now the new concept we introduced last time is the
            concept of the expected value for random variable, which is defined this way. You look at all the possible
            outcomes. And you form some kind of average of all the possible numerical values over the random variable
            capital X. You consider all the possible numerical values, and you form an average.</p>
        <p>事实上，这是一个加权平均值，对于每个小 x，你分配一个权重，该权重等于特定小 x 实现的概率。现在，正如我们上次讨论的那样，如果你有一个随机变量，你可以取一个随机变量的函数。这将是一个新的随机变量。因此，如果大写 X 是一个随机变量，而 g 是一个函数，那么 X 的 g 就是一个新的随机变量。</p><p>In fact, it’s a weighted average where, to every little x, you assign a weight equal to the probability that
            particular little x is going to be realized. Now, as we discussed last time, if you have a random variable,
            you
            can take a function of a random variable. And that’s going to be a new random variable. So if capital X is a
            random variable and g is a function, g of X is a new random variable.</p>
        <p>你做了实验。你得到了一个结果。这决定了 X 的值。这决定了 X 的 g 值。因此，X 的 g 的数值由实验中发生的任何情况决定。它是随机的。这使它成为一个随机变量。由于它是一个随机变量，它有自己的期望。那么我们如何计算 X 的 g 的期望呢？</p><p>You do the experiment. You get an outcome. This determines the value of X. And that determines the value of g
            of
            X. So the numerical value of g of X is determined by whatever happens in the experiment. It’s random. And
            that
            makes it a random variable. Since it’s a random variable, it has an expectation of its own. So how would we
            calculate the expectation of g of X?</p>
        <h2 id="unknown-101">未知</h2><h2>Unknown</h2>
        <p>您可以仅使用定义来进行，这将要求您找到 X 的随机变量 g 的 PMF。因此，找到 X 的 g 的 PMF，然后应用已知 PMF 的随机变量的期望值公式。</p><p>You could proceed by just using the definition, which would require you to find the PMF of the random
            variable g
            of X. So find the PMF of g of X, and then apply the formula for the expected value of a random variable with
            known PMF.</p>
        <p>但也有捷径，只是计数和计算的另一种方式，我们不需要找到 X 的 g 的 PMF。我们只需使用原始随机变量的 PMF。这意味着 X 的 g 的平均值按以下方式获得。你看看所有可能的结果，X，看看它们有多大可能。</p><p>But there is also a shortcut, which is just a different way of doing the counting and the calculations, in
            which
            we do not need to find the PMF of g of X. We just work with the PMF of the original random variable. And
            what
            this is saying is that the average value of g of X is obtained as follows. You look at all the possible
            results,
            the X’s, how likely they are.</p>
        <p>当特定的 X 发生时，这就是你得到的结果。这样，你把这些加起来。你就会得到你将得到的平均数，即 X 的 g 的平均值，其中你对不同 X 的可能性取平均值。现在预期值有一些总是正确的属性，也有一些有时不正确的属性。</p><p>And when that particular X happens, this is how much you get. And so this way, you add these things up. And
            you
            get the average amount that you’re going to get, the average value of g of X, where you average over the
            likelihoods of the different X’s. Now expected values have some properties that are always true and some
            properties that sometimes are not true.</p>
        <h2 id="unknown-102">未知</h2><h2>Unknown</h2>
        <p>因此，并非总是正确的属性是，这将与 X 的期望值的 g 相同。因此，一般来说，这不是正确的。您不能互换函数和期望，这意味着您通常不能根据平均值进行推理。但也有一些例外。当 g 是线性函数时，线性函数的期望值与期望的相同线性函数相同。</p><p>So the property that is not always true is that this would be the same as g of the expected value of X. So in
            general, this is not true. You cannot interchange function and expectation, which means you cannot reason on
            the
            average, in general. But there are some exceptions. When g is a linear function, then the expected value for
            a
            linear function is the same as that same linear function of the expectation.</p>
        <p>因此，对于线性函数，对于随机变量，期望表现良好。所以这基本上告诉你，如果 X 是摄氏度，alpha X 加 b 是华氏度，你可以先转换成华氏度，然后取平均值。或者你可以找到摄氏平均温度，然后转换成华氏度。两种方法都有效。</p><p>So for linear functions, so for random variable, the expectation behaves nicely. So this is basically telling
            you
            that, if X is degrees in Celsius, alpha X plus b is degrees in Fahrenheit, you can first do the conversion
            to
            Fahrenheit and take the average. Or you can find the average temperature in Celsius, and then do the
            conversion
            to Fahrenheit. Either is valid.</p>
        <p>因此，当您将其绘制为条形图时，预期值会告诉我们分布中心的位置，更具体地说，PMF 的质量中心或重心。除了平均值之外，您可能还想知道您与平均值之间的距离通常有多远。让我们看看这个量，X 减去 X 的预期值。这是与平均值的距离。</p><p>So the expected value tells us something about where is the center of the distribution, more specifically,
            the
            center of mass or the center of gravity of the PMF, when you plot it as a bar graph. Besides the average
            value,
            you may be interested in knowing how far will you be from the average, typically. So let’s look at this
            quantity, X minus expected value of X. This is the distance from the average value.</p>
        <h2 id="unknown-103">未知</h2><h2>Unknown</h2>
        <p>因此，对于实验的随机结果，这里的这个量衡量了你与平均值的距离。括号内的这个量是一个随机变量。为什么？因为大写 X 是随机的。我们这里的是大写 X，它是随机的，减去一个数字。记住，期望值是数字。现在，一个随机变量减去一个数字就是一个新的随机变量。它有自己的期望。</p><p>So for a random outcome of the experiment, this quantity in here measures how far away from the mean you
            happen
            to be. This quantity inside the brackets is a random variable. Why? Because capital X is random. And what we
            have here is capital X, which is random, minus a number. Remember, expected values are numbers. Now a random
            variable minus a number is a new random variable. It has an expectation of its own.</p>
        <p>我们可以使用线性规则，某事物的期望值减去另一事物的期望值就是它们的期望值的差。所以它将是 X 的期望值减去该事物的期望值。现在这个事物是一个数字。数字的期望值就是数字本身。所以我们从这里得到这是期望值减去期望值。我们得到零。这告诉我们什么？</p><p>We can use the linearity rule, expected value of something minus something else is just the difference of
            their
            expected value. So it’s going to be expected value of X minus the expected value over this thing. Now this
            thing
            is a number. And the expected value of a number is just the number itself. So we get from here that this is
            expected value minus expected value. And we get zero. What is this telling us?</p>
        <p>平均而言，与平均值的指定差值等于零。也就是说，平均值就在这里。有时 X 会落在右边。有时 X 会落在左边。平均而言，与平均值的平均距离将为零，因为有时实际距离为正，有时为负。正数和负数相互抵消。</p><p>That, on the average, the assigned difference from the mean is equal to zero. That is, the mean is here.
            Sometimes X will fall to the right. Sometimes X will fall to the left. On the average, the average distance
            from
            the mean is going to be zero, because sometimes the realized distance will be positive, sometimes it will be
            negative. Positives and negatives cancel out.</p>
        <h2 id="unknown-104">未知</h2><h2>Unknown</h2>
        <p>因此，如果我们想了解我们与平均值之间的差距有多大，仅仅查看与平均值之间的指定距离不会给我们提供任何有用的信息。因此，如果我们想说明我们之间的差距有多大，通常我们应该采取不同的做法。一种可能性可能是取差值的绝对值。而这正是人们有时感兴趣的数量。</p><p>So if we want to capture the idea of how far are we from the mean, just looking at the assigned distance from
            the
            mean is not going to give us any useful information. So if we want to say something about how far we are,
            typically, we should do something different. One possibility might be to take the absolute values of the
            differences. And that’s a quantity that sometimes people are interested in.</p>
        <p>但事实证明，更有用的量恰好是随机变量的方差，它实际上衡量的是与均值的平均平方距离。因此，你会得到随机结果、随机结果、随机变量的随机数值。它与均值有一定的距离。这个距离是随机的。我们取它的平方。这就是与均值的平方距离，它又是随机的。</p><p>But it turns out that a more useful quantity happens to be the variance of a random variable, which actually
            measures the average squared distance from the mean. So you have a random outcome, random results, random
            numerical value of the random variable. It is a certain distance away from the mean. That certain distance
            is
            random. We take the square of that. This is the squared distance from the mean, which is again random.</p>
        <p>因为它是随机的，所以它有自己的预期值。这个预期值，我们称之为 X 的方差。所以我们有这个特定的定义。使用我们这里关于如何计算随机变量函数期望的规则，为什么适用呢？好吧，这里括号内的内容是随机变量（大写 X）的函数。</p><p>Since it’s random, it has an expected value of its own. And that expected value, we call it the variance of
            X.
            And so we have this particular definition. Using the rule that we have up here for how to calculate
            expectations
            of functions of a random variable, why does that apply? Well, what we have inside the brackets here is a
            function of the random variable, capital X.</p>
        <h2 id="unknown-105">未知</h2><h2>Unknown</h2>
        <p>因此，我们可以应用此规则，其中 g 是这个特定函数。我们可以使用它来计算方差，从随机变量 X 的 PMF 开始。然后我们有一个有用的公式，有时，如果你想进行计算，这是一个很好的捷径。现在，方差的一个稍微错误是单位不正确，如果你想讨论分布的传播 a。</p><p>So we can apply this rule where g is this particular function. And we can use that to calculate the variance,
            starting with the PMF of the random variable X. And then we have a useful formula that’s a nice shortcut,
            sometimes, if you want to do the calculation. Now one thing that’s slightly wrong with the variance is that
            the
            units are not right, if you want to talk about the spread a of a distribution.</p>
        <p>假设 X 是一个以米为单位的随机变量。方差的单位是米的平方。所以这有点不同。如果你想用与 X 相同的单位来讨论分布的扩展，那么取方差的平方根会很方便。这是我们定义的。
        </p><p>Suppose that X is a random variable measured in meters. The variance will have the units of meters squared.
            So
            it’s a kind of a different thing. If you want to talk about the spread of the distribution using the same
            units
            as you have for X, it’s convenient to take the square root of the variance. And that’s something that we
            define.
        </p>
        <p>我们称其为 X 的标准差，或 X 分布的标准差。因此，它告诉您分布的散布量。它与您正在处理的随机变量本身的单位相同。我们只需用一个尽可能简单的例子来说明这些数量。请考虑以下实验。</p><p>And we call it to the standard deviation of X, or the standard deviation of the distribution of X. So it
            tells
            you the amount of spread in your distribution. And it is in the same units as the random variable itself
            that
            you are dealing with. And we can just illustrate those quantities with an example that’s about as simple as
            it
            can be. So consider the following experiment.</p>
        <h2 id="unknown-106">未知</h2><h2>Unknown</h2>
        <p>假设您要从这里前往纽约，距离为 200 英里。您有两个选择。要么乘坐私人飞机，以每小时 200 英里的速度飞行，在旅途中保持恒定速度；要么决定以非常非常慢的速度步行，以每小时一英里的悠闲速度行走。因此，您可以通过这个实验，即抛硬币来随机选择速度。</p><p>You’re going to go from here to New York, let’s say, 200 miles. And you have two alternatives. Either you’ll
            get
            your private plane and go at a speed of 200 miles per hour, constant speed during your trip, or otherwise,
            you’ll decide to walk really, really slowly, at the leisurely pace of one mile per hour. So you pick the
            speed
            at random by doing this experiment, by flipping a coin.</p>
        <p>有一半的概率，你会做一件事。有一半的概率，你会做另一件事。所以你的 V 是一个随机变量。如果你想知道到达目的地需要多长时间，那么时间等于距离除以速度。这就是公式。时间本身是一个随机变量，因为它是 V 的函数，而 V 是随机的。</p><p>And with probability one half, you do one thing. With probably one half, you do the other thing. So your V is
            a
            random variable. In case you’re interested in how much time it’s going to take you to get there, well, time
            is
            equal to distance divided by speed. So that’s the formula. The time itself is a random variable, because
            it’s a
            function of V, which is random.</p>
        <p>您需要花费多少时间取决于您在开始时抛硬币以决定您将拥有的速度。好的，作为热身，进行一些简单的计算。要找到 V 的期望值，您可以进行以下推理。有一半的概率，V 将为 1。</p><p>How much time it’s going to take you depends on the coin flip that you do in the beginning to decide what
            speed
            you are going to have. OK, just as a warm up, the trivial calculations. To find the expected value of V, you
            argue as follows. With probability one half, V is going to be one.</p>
        <h2 id="unknown-107">未知</h2><h2>Unknown</h2>
        <p>有一半的概率，V 会是 200。所以你的速度的预期值为 100.5。如果你想计算 V 的方差，那么你可以这样论证。有一半的概率，我会以 1 的速度行驶，而平均值是 100.5。所以，如果我决定以 1 的速度行驶，这就是与平均值的距离。我们从平均值的平方中取这个距离。</p><p>And with probability one half, V is going to be 200. And so the expected value of your speed is 100.5. If you
            wish to calculate the variance of V, then you argue as follows. With probability one half, I’m going to
            travel
            at the speed of one, whereas, the mean is 100.5. So this is the distance from the mean, if I decide to
            travel at
            the speed of one. We take that distance from the mean squared.</p>
        <p>这是方差的一个贡献。有一半的概率，你将以 200 的速度行驶，这与平均值相差这么多。你取它的平方。好的，那么这个数字大约有多大？嗯，大约是 100 的平方。这也是 100 的平方。所以大约，这个随机变量的方差是 100 的平方。</p><p>That’s one contribution to the variance. And with probability one half, you’re going to travel at the speed
            of
            200, which is this much away from the mean. You take the square of that. OK, so approximately how big is
            this
            number? Well, this is roughly 100 squared. That’s also 100 squared. So approximately, the variance of this
            random variable is 100 squared.</p>
        <p>现在，如果我告诉你这个分布的方差是 10,000，它实际上并不会帮助你将它与这个图表联系起来。而标准差，即你取平方根的地方，则更有趣。它是 100 平方的平方根，也就是 100。而标准差确实让我们了解了这个分布与平均值的分散程度。</p><p>Now if I tell you that the variance of this distribution is 10,000, it doesn’t really help you to relate it
            to
            this diagram. Whereas, the standard deviation, where you take the square root, is more interesting. It’s the
            square root of 100 squared, which is a 100. And the standard deviation, indeed, gives us a sense of how
            spread
            out this distribution is from the mean.</p>
        <h2 id="unknown-108">未知</h2><h2>Unknown</h2>
        <p>因此，标准偏差基本上为我们提供了有关此处间距的一些指示。它告诉我们分布的分散程度。好的，现在让我们看看时间会发生什么。V 是一个随机变量。T 是一个随机变量。现在让我们看看时间的预期值和所有​​这些。好的，时间是随机变量的函数。</p><p>So the standard deviation basically gives us some indication about this spacing that we have here. It tells
            us
            the amount of spread in our distribution. OK, now let’s look at what happens to time. V is a random
            variable. T
            is a random variable. So now let’s look at the expected values and all of that for the time. OK, so the time
            is
            a function of a random variable.</p>
        <p>我们可以通过查看实验的所有可能结果（V）来找到预期时间，根据它们的概率对它们进行加权，并针对每个特定的 V 跟踪我们花费的时间。因此，如果 V 为 1，发生概率为一半，则所需的时间将是 200。如果我们以 1 的速度行进，则需要 200 个时间单位。</p><p>We can find the expected time by looking at all possible outcomes of the experiment, the V’s, weigh them
            according to their probabilities, and for each particular V, keep track of how much time it took us. So if V
            is
            one, which happens with probability one half, the time it takes is going to be 200. If we travel at speed of
            one, it takes us 200 time units.</p>
        <p>否则，如果我们的速度等于 200，时间就是 1。因此 T 的预期值再次与之前相同。它是 100.5。因此预期速度是 100.5。预期时间也是 100.5。因此这些预期的乘积大约是 10,000。T 和 V 乘积的预期值如何？</p><p>And otherwise, if our speed is equal to 200, the time is one. So the expected value of T is once more the
            same as
            before. It’s 100.5. So the expected speed is 100.5. The expected time is also 100.5. So the product of these
            expectations is something like 10,000. How about the expected value of the product of T and V?</p>
        <h2 id="unknown-109">未知</h2><h2>Unknown</h2>
        <p>嗯，T 乘以 V 等于 200。无论实验结果如何，在特定结果中，T 乘以 V 就是总行驶距离，正好是 200。因此，在这个简单示例中，我们得到的结果是，这两个随机变量乘积的期望值与它们的期望值的乘积不同。这是我们无法根据平均值进行推理的又一个例子。</p><p>Well, T times V is 200. No matter what outcome you have in the experiment, in that particular outcome, T
            times V
            is total distance traveled, which is exactly 200. And so what do we get in this simple example is that the
            expected value of the product of these two random variables is different than the product of their expected
            values. This is one more instance of where we cannot reason on the average.</p>
        <p>因此，平均而言，在大量行程中，您的平均时间将是 100。平均而言，在大量行程中，您的平均速度将是 100。但您的平均行驶距离不是 100 乘以 100。这是其他东西。因此，当您处理非线性事物时，您无法根据平均值进行推理。</p><p>So on the average, over a large number of trips, your average time would be 100. On the average, over a large
            number of trips, your average speed would be 100. But your average distance traveled is not 100 times 100.
            It’s
            something else. So you cannot reason on the average, whenever you’re dealing with non linear things.</p>
        <p>这里的非线性是指你有一个函数，它是各种事物的乘积，而不是事物的线性和。另一种观察这里发生的事情的方法是时间的预期值。根据定义，时间是速度的 200 倍。</p><p>And the non linear thing here is that you have a function which is a product of stuff, as opposed to just
            linear
            sums of stuff. Another way to look at what’s happening here is the expected value of the time. Time, by
            definition, is 200 over the speed.</p>
        <h2 id="unknown-110">未知</h2><h2>Unknown</h2>
        <p>我们发现，当时的期望值约为 100。因此，200 除以 V 的期望值约为 100。但这与这里的这个量不同，这里的量大致等于 2，也就是 200。V 的期望值约为 100。因此，这个量大约等于 2。而这里的这个量约为 100。那么，我们这里得到了什么？我们得到了 V 的非线性函数。</p><p>Expected value of the time, we found it to be about a 100. And so expected value of 200 over V is about a
            100.
            But it’s different from this quantity here, which is roughly equal to 2, and so 200. Expected value of V is
            about 100. So this quantity is about equal to two. Whereas, this quantity up here is about 100. So what do
            we
            have here? We have a non linear function of V.</p>
        <p>我们发现，这个函数的期望值与期望值的函数不是一回事。所以，这也是一个不能互换期望值和函数的例子。这是因为事物是非线性的。好的，现在让我们介绍一个新概念。或者它可能不是一个新概念。所以我们在第一章中讨论了概率。我们也有条件概率。它们之间有什么区别？</p><p>And we find that the expected value of this function is not the same thing as the function of the expected
            value.
            So again, that’s an instance where you cannot interchange expected values and functions. And that’s because
            things are non linear. OK, so now let us introduce a new concept. Or maybe it’s not quite a new concept. So
            we
            discussed, in chapter one, that we have probabilities. We also have conditional probabilities. What’s the
            difference between them?</p>
        <p>本质上没有。概率只是在给定特定模型的情况下分配概率值以产生不同的结果。有人来给你新的信息。所以你想出了一个新模型。然后你就有了新的概率。我们称这些为条件概率，但它们的性质和行为与普通概率完全相同。既然我们可以有条件概率，那么为什么不也有条件 PMF，因为 PMF 无论如何都要处理概率。</p><p>Essentially, none. Probabilities are just an assignment of probability values to give different outcomes,
            given a
            particular model. Somebody comes and gives you new information. So you come up with a new model. And you
            have a
            new probabilities. We call these conditional probabilities, but they taste and behave exactly the same as
            ordinary probabilities. So since we can have conditional probabilities, why not have conditional PMFs as
            well,
            since PMFs deal with probabilities anyway.</p>
        <h2 id="unknown-111">未知</h2><h2>Unknown</h2>
        <p>因此，我们有一个随机变量，大写 X。它有自己的 PMF。例如，它可能是这张图片中的 PMF，即取可能的不同值的均匀 PMF。我们还有一个事件。有人来告诉我们这个事件已经发生。PMF 会告诉您大写 X 等于某个小写 x 的概率。</p><p>So we have a random variable, capital X. It has a PMF of its own. For example, it could be the PMF in this
            picture, which is a uniform PMF that takes for possible different values. And we also have an event. And
            somebody comes and tells us that this event has occurred. The PMF tells you the probability that capital X
            equals to some little x.</p>
        <p>有人告诉你发生了某个事件，这将使你改变分配给不同值的概率。你将使用条件概率。因此，这一部分的含义从第一章开始就很清楚了。这一部分只是我们在本章中用来讨论条件概率的新符号。所以这只是一个定义。所以条件 PMF 是一个普通的 PMF。</p><p>Somebody tells you that a certain event has occurred that’s going to make you change the probabilities that
            you
            assign to the different values. You are going to use conditional probabilities. So this part, it’s clear
            what it
            means from chapter one. And this part is just the new notation we’re using in this chapter to talk about
            conditional probabilities. So this is just a definition. So the conditional PMF is an ordinary PMF.</p>
        <p>但是，PMF 适用于一种新模型，在该模型中，我们已获得一些有关实验结果的信息。因此，为了使其具体化，请考虑以下事件。以大写 X 大于或等于 2 的事件为例。在图中，事件 A 是什么？事件 A 由这三个结果组成。好吧，假设我们被告知事件 A 已经发生，那么条件 PMF 是什么？</p><p>But it’s the PMF that applies to a new model in which we have been given some information about the outcome
            of
            the experiment. So to make it concrete, consider this event here. Take the event that capital X is bigger
            than
            or equal to two. In the picture, what is the event A? The event A consists of these three outcomes. OK, what
            is
            the conditional PMF, given that we are told that event A has occurred?</p>
        <h2 id="unknown-112">未知</h2><h2>Unknown</h2>
        <p>假设事件 A 已经发生，这基本上告诉我们这个结果没有发生。现在只有三种可能的结果。在新的宇宙中，在我们以 A 为条件的新模型中，只有三种可能的结果。这三种可能的结果在我们开始时是同样可能的。所以在条件宇宙中，它们将保持同样的可能性。记住，无论何时你设定条件，相对可能性都保持不变。它们保持相同的比例。</p><p>Given that the event A has occurred, it basically tells us that this outcome has not occurred. There’s only
            three
            possible outcomes now. In the new universe, in the new model where we condition on A, there’s only three
            possible outcomes. Those three possible outcomes were equally likely when we started. So in the conditional
            universe, they will remain equally likely. Remember, whenever you condition, the relative likelihoods remain
            the
            same. They keep the same proportions.</p>
        <p>它们只需要重新缩放，使它们加起来等于 1。因此，它们中的每一个都具有相同的概率。现在在新世界中，概率需要加起来等于 1。因此，它们中的每一个在条件宇宙中的概率都是 1/3。这就是我们的条件模型。因此，当 X 等于 2、3 和 4 时，我们的 PMF 等于 1/3。好的。</p><p>They just need to be re scaled, so that they add up to one. So each one of these will have the same
            probability.
            Now in the new world, probabilities need to add up to 1. So each one of them is going to get a probability
            of
            1/3 in the conditional universe. So this is our conditional model. So our PMF is equal to 1/3 for X equals
            to
            2,3 and 4. All right.</p>
        <p>现在，只要您有一个涉及随机变量的概率模型，并且您有该随机变量的 PMF，您就可以讨论该随机变量的预期值。我们几分钟前定义了预期值。在这里，我们处理的是条件模型和条件概率。因此，我们还可以讨论这个新宇宙中随机变量 X 的预期值，以及我们正在处理的这个新条件模型中的预期值。</p><p>Now whenever you have a probabilistic model involving a random variable and you have a PMF for that random
            variable, you can talk about the expected value of that random variable. We defined expected values just a
            few
            minutes ago. Here, we’re dealing with a conditional model and conditional probabilities. And so we can also
            talk
            about the expected value of the random variable X in this new universe, in this new conditional model that
            we’re
            dealing with.</p>
        <h2 id="unknown-113">未知</h2><h2>Unknown</h2>
        <p>这引出了条件期望概念的定义。条件期望只不过是一种普通期望，只不过你不使用原始 PMF。你使用条件 PMF。你使用条件概率。这只是一种普通期望，但应用于我们拥有的条件宇宙的新模型，在该模型中，我们被告知某个事件已经发生。</p><p>And this leads us to the definition of the notion of a conditional expectation. The conditional expectation
            is
            nothing but an ordinary expectation, except that you don’t use the original PMF. You use the conditional
            PMF.
            You use the conditional probabilities. It’s just an ordinary expectation, but applied to the new model that
            we
            have to the conditional universe where we are told that the certain event has occurred.</p>
        <p>因此我们现在可以计算条件期望，在这个特定示例中，该期望为 1/3。这是 2 的概率，加上 1/3，也就是 3 的概率，再加上 1/3，也就是 4 的概率。然后您可以使用计算器来找到答案，或者您也可以通过对称性进行论证。</p><p>So we can now calculate the condition expectation, which, in this particular example, would be 1/3. That’s
            the
            probability of a 2, plus 1/3 which is the probability of a 3 plus 1/3, the probability of a 4. And then you
            can
            use your calculator to find the answer, or you can just argue by symmetry.</p>
        <p>期望值必须是我们正在处理的 PMF 的重心，等于 3。因此，条件期望与普通期望没有什么不同。它们只是应用于新类型情况或新类型模型的普通期望。我们对期望的任何了解对于条件期望仍然有效。</p><p>The expected value has to be the center of gravity of the PMF we’re working with, which is equal to 3. So
            conditional expectations are no different from ordinary expectations. They’re just ordinary expectations
            applied
            to a new type of situation or a new type of model. Anything we might know about expectations will remain
            valid
            about conditional expectations.</p>
        <h2 id="unknown-114">未知</h2><h2>Unknown</h2>
        <p>例如，随机变量的线性函数的条件期望将是条件期望的线性函数。或者你可以采用任何你可能知道的公式，例如 X 的期望值等于...抱歉。X 的 g 的期望值是 X 的 g 的所有 X 的总和乘以 X 的 PMF。</p><p>So for example, the conditional expectation of a linear function of a random variable is going to be the
            linear
            function of the conditional expectations. Or you can take any formula that you might know, such as the
            formula
            that expected value of X is equal to the. sorry. expected value of g of X is the sum over all X’s of g of X
            times the PMF of X.</p>
        <p>所以这就是我们已经知道的计算随机变量函数期望的公式。如果我们转向条件宇宙，会发生什么变化？在条件宇宙中，我们讨论的是条件期望，假设事件 A 已经发生。我们使用条件概率，假设 A 已经发生。所以任何公式都有一个条件对应项。在条件对应项中，期望被条件期望所取代。</p><p>So this is the formula that we already know about how to calculate expectations of a function of a random
            variable. If we move to the conditional universe, what changes? In the conditional universe, we’re talking
            about
            the conditional expectation, given that event A has occurred. And we use the conditional probabilities,
            given
            that A has occurred. So any formula has a conditional counterpart. In the conditional counterparts,
            expectations
            get replaced by conditional expectations.</p>
        <p>概率被条件概率取代。所以一旦你知道了第一个公式，并且知道了总体思路，你就完全没有必要去记住这样的公式了。你甚至不需要把它写在考试的备忘单上，好吗？</p><p>And probabilities get replaced by conditional probabilities. So once you know the first formula and you know
            the
            general idea, there’s absolutely no reason for you to memorize a formula like this one. You shouldn’t even
            have
            to write it on your cheat sheet for the exam, OK?</p>
        <h2 id="unknown-115">未知</h2><h2>Unknown</h2>
        <p>好的，现在让我们看一个我们之前见过的随机变量的例子，几何随机变量，这次用它做一些更有趣的事情。你还记得上次几何随机变量是什么吗？我们抛硬币。每次掷出正面的概率为 P。我们感兴趣的是，在第一次观察到正面之前，我们需要抛硬币的次数。</p><p>OK, all right, so now let’s look at an example of a random variable that we’ve seen before, the geometric
            random
            variable, and this time do something a little more interesting with it. Do you remember from last time what
            the
            geometric random variable is? We do coin flips. Each time there’s a probability of P of obtaining heads. And
            we’re interested in the number of tosses we’re going to need until we observe heads for the first time.</p>
        <p>随机变量取 K 值的概率，这是第 K 次投掷中出现第一个 K 的概率。所以这是 K 减 1 次连续反面然后出现正面的概率。所以这是必须对 K 次投掷进行加权的概率。当我们绘制这个 PMF 时，它具有这种形状，即几何级数的形状。</p><p>The probability that the random variable takes the value K, this is the probability that the first K appeared
            at
            the K th toss. So this is the probability of K minus 1 consecutive tails followed by a head. So this is the
            probability of having to weight K tosses. And when we plot this PMF, it has this kind of shape, which is the
            shape of a geometric progression.</p>
        <p>它从 1 开始，一直到无穷大。所以这是一个离散随机变量，其值取自一个无限集，即正整数集。所以它是一个随机变量，因此它有一个期望值。根据定义，期望值是，我们会考虑随机变量的所有可能值。我们根据它们的概率对它们进行加权，这导致了这个表达式。</p><p>It starts at 1, and it goes all the way to infinity. So this is a discrete random variable that takes values
            over
            an infinite set, the set of the positive integers. So it’s a random variable, therefore, it has an
            expectation.
            And the expected value is, by definition, we’ll consider all possible values of the random variable. And we
            weigh them according to their probabilities, which leads us to this expression.</p>
        <h2 id="unknown-116">未知</h2><h2>Unknown</h2>
        <p>你可能在前世的某个时候求过这个表达式。有一些技巧可以求这个表达式并得到一个封闭形式的答案。但这是一种代数技巧。你可能不记得了。我们如何进行这个求和？好吧，我们将使用一个概率技巧，并设法求出 X 的期望，本质上，不需要做任何代数运算。</p><p>You may have evaluated that expression some time in your previous life. And there are tricks for how to
            evaluate
            this and get a closed form answer. But it’s sort of an algebraic trick. You might not remember it. How do we
            go
            about doing this summation? Well, we’re going to use a probabilistic trick and manage to evaluate the
            expectation of X, essentially, without doing any algebra.</p>
        <p>在此过程中，我们将对抛硬币和几何随机变量中发生的情况有一些直观的了解。因此，我们让两个人做同样的实验，抛硬币直到第一次出现正面。其中一个人将使用字母 Y 来计算出现了多少次正面。所以这个人现在就开始抛硬币。这是当前时间。</p><p>And in the process of doing so, we’re going to get some intuition about what happens in coin tosses and with
            geometric random variables. So we have two people who are going to do the same experiment, flip a coin until
            they obtain heads for the first time. One of these people is going to use the letter Y to count how many
            heads
            it took. So that person starts flipping right now. This is the current time.</p>
        <p>他们将会得到反面、反面、反面，直到最终得到正面。这个随机变量 Y 当然是几何的，所以它有这种形式的 PMF。好的，现在有第二个人正在做同样的实验。第二个人将再次取一个随机数 X，直到他们第一次得到正面。当然，X 将具有与 Y 相同的 PMF。</p><p>And they are going to obtain tails, tails, tails, until eventually they obtain heads. And this random
            variable Y
            is, of course, geometric, so it has a PMF of this form. OK, now there is a second person who is doing that
            same
            experiment. That second person is going to take, again, a random number, X, until they obtain heads for the
            first time. And of course, X is going to have the same PMF as Y.</p>
        <h2 id="unknown-117">未知</h2><h2>Unknown</h2>
        <p>但那个人很不耐烦。他其实更早开始抛硬币，比 Y 人更早。他抛了两次硬币。他运气不好，两次都掷到了反面。所以他必须继续。看看现在的情况，这两个人相比如何？你认为谁会先掷到正面？一个比另一个更有可能吗？</p><p>But that person was impatient. And they actually started flipping earlier, before the Y person started
            flipping.
            They flipped the coin twice. And they were unlucky, and they obtained tails both times. And so they have to
            continue. Looking at the situation at this time, how do these two people compare? Who do you think is going
            to
            obtain heads first? Is one more likely than the other?</p>
        <p>所以如果你经常去赌场玩，你会说，哦，连续两次都是反面，所以正面应该很快就会出现。但这是一个错误的论点，因为至少在我们的模型中，抛硬币是独立的。这两次恰好都是反面的事实并不会改变我们对这里将要发生的事情的信念。</p><p>So if you play at the casino a lot, you’ll say, oh, there were two tails in a row, so a head should be coming
            up
            sometime soon. But this is a wrong argument, because coin flips, at least in our model, are independent. The
            fact that these two happened to be tails doesn’t change anything about our beliefs about what’s going to be
            happening here.</p>
        <p>那么，这个人将要发生的事情是，他们将抛出独立的硬币。这个人也将抛出独立的硬币。他们俩都等到第一次出现正面。从此时开始，他们面临着相同的情况。好的，现在这个人面临的概率模型是什么？这个人第一次获得正面的时间是 X。</p><p>So what’s going to be happening to that person is they will be flipping independent coin flips. That person
            will
            also be flipping independent coin flips. And both of them wait until the first head occurs. They’re facing
            an
            identical situation, starting from this time. OK, now what’s the probabilistic model of what this person is
            facing? The time until that person obtains heads for the first time is X.</p>
        <h2 id="unknown-118">未知</h2><h2>Unknown</h2>
        <p>因此，直到他们第一次掷出正面为止的抛掷次数将是 X 减 2。因此，X 是第一次掷出正面为止的总次数。X 减 2 是从这里开始的抛掷次数。现在，我们掌握了有关此人的哪些信息？我们掌握的信息是，他们前两次抛掷都是反面。</p><p>So this number of flips until they obtain heads for the first time is going to be X minus 2. So X is the
            total
            number until the first head. X minus 2 is the number or flips, starting from here. Now what information do
            we
            have about that person? We have the information that their first two flips were tails.</p>
        <p>因此，我们得到的信息是 X 大于 2。因此，描述实验这一部分的概率模型是，在第一次出现正面之前，需要进行随机次数的抛硬币。从现在开始到下一次出现正面为止，抛硬币的次数是 X 减 2。但我们得到的信息是，这个人已经浪费了 2 次抛硬币机会。</p><p>So we’re given the information that X was bigger than 2. So the probabilistic model that describes this piece
            of
            the experiment is that it’s going to take a random number of flips until the first head. That number of
            flips,
            starting from here until the next head, is that number X minus 2. But we’re given the information that this
            person has already wasted 2 coin flips.</p>
        <p>现在我们论证说，从概率上讲，这个人，这里的实验的这一部分与实验的那部分是相同的。因此，基于此信息的随机变量的 PMF（即 X 减 2）应该与我们下面的 PMF 相同。</p><p>Now we argued that probabilistically, this person, this part of the experiment here is identical with that
            part
            of the experiment. So the PMF of this random variable, which is X minus 2, conditioned on this information,
            should be the same as that PMF that we have down there.</p>
        <h2 id="unknown-119">未知</h2><h2>Unknown</h2>
        <p>因此，我做出的正式陈述是，如果 X 大于 2，则这里的 X 减 2 的 PMF 与 X 本身的 PMF 相同。这句话是什么意思？假设我告诉你，你已经做了几次抛硬币，但都失败了，那么直到第一次正面出现，剩下的抛硬币次数与从头开始的几何分布相同。</p><p>So the formal statement that I’m making is that this PMF here of X minus 2, given that X is bigger than 2, is
            the
            same as the PMF of X itself. What is this saying? Given that I tell you that you already did a few flips and
            they were failures, the remaining number of flips until the first head has the same geometric distribution
            as if
            you were starting from scratch.</p>
        <p>无论过去发生了什么，它都已经发生了，但与未来会发生什么无关。剩下的硬币会一直翻转，直到正面的分布相同，无论你是现在开始，还是过去做过其他事情。所以这是我们称之为几何分布的无记忆性属性。本质上，它表示未来发生的一切都与过去发生的一切都无关。</p><p>Whatever happened in the past, it happened, but has no bearing what’s going to happen in the future.
            Remaining
            coin flips until a head has the same distribution, whether you’re starting right now, or whether you had
            done
            some other stuff in the past. So this is a property that we call the memorylessness property of the
            geometric
            distribution. Essentially, it says that whatever happens in the future is independent from whatever happened
            in
            the past.</p>
        <p>从定义上看，这几乎是正确的，因为我们假设硬币翻转是独立的。实际上，独立性意味着实验的一部分信息与实验其他部分将要发生的事情无关。我试图使用硬币翻转的直觉给出的论证，你可以通过正式操纵 PMF 使其正式化。所以这是 X 的原始 PMF。</p><p>And that’s true almost by definition, because we’re assuming independent coin flips. Really, independence
            means
            that information about one part of the experiment has no bearing about what’s going to happen in the other
            parts
            of the experiment. The argument that I tried to give using the intuition of coin flips, you can make it
            formal
            by just manipulating PMFs formally. So this is the original PMF of X.</p>
        <h2 id="unknown-120">未知</h2><h2>Unknown</h2>
        <p>假设你以 X 大于 3 为条件。这个条件信息的作用是告诉你这件事没有发生。你只是以这个事件为条件。当你以那个事件为条件时，剩下的就是条件 PMF，它的形状与这个相同，只是需要重新规范化，这样概率加起来才等于一。</p><p>Suppose that you condition on the event that X is bigger than 3. This conditioning information, what it does
            is
            it tells you that this piece did not happen. You’re conditioning just on this event. When you condition on
            that
            event, what’s left is the conditional PMF, which has the same shape as this one, except that it needs to be
            re
            normalized up, so that the probabilities add up to one.</p>
        <p>所以你拍了那张照片，但你需要改变它的高度，这样这些项加起来等于 1。这是 X 的条件 PMF，假设 X 大于 2。但我们这里讨论的不是 X。我们讨论的是剩余的头的数量。剩余的头的数量是 X 减 2。如果我们有 X 的 PMF，我们能找到 X 减 2 的 PMF 吗？</p><p>So you take that picture, but you need to change the height of it, so that these terms add up to 1. And this
            is
            the conditional PMF of X, given that X is bigger than 2. But we’re talking here not about X. We’re talking
            about
            the remaining number of heads. Remaining number of heads is X minus 2. If we have the PMF of X, can we find
            the
            PMF of X minus 2?</p>
        <p>嗯，如果 X 等于 3，则相当于 X 减 2 等于 1。所以这里的概率应该等于那个概率。X 等于 4 的概率应该与 X 减 2 等于 2 的概率相同。所以基本上，X 减 2 的 PMF 与 X 的 PMF 相同，只是它偏移了这 2 个单位。</p><p>Well, if X is equal to 3, that corresponds to X minus 2 being equal to 1. So this probability here should be
            equal to that probability. The probability that X is equal to 4 should be the same as the probability that X
            minus 2 is equal to 2. So basically, the PMF of X minus 2 is the same as the PMF of X, except that it gets
            shifted by these 2 units.</p>
        <h2 id="unknown-121">未知</h2><h2>Unknown</h2>
        <p>这样，我们正式推导出了在前两次抛硬币都是反面的情况下，剩余抛硬币次数的条件 PMF。我们发现它与我们开始时的 PMF 完全相同。所以这是这个陈述的正式证明。</p><p>So this way, we have formally derived the conditional PMF of the remaining number of coin tosses, given that
            the
            first two flips were tails. And we see that it’s exactly the same as the PMF that we started with. And so
            this
            is the formal proof of this statement here.</p>
        <p>因此，消化并理解这两个正式陈述以及理解这里涉及的符号是很有用的，但也要真正理解这实际上在说什么的直观论证。好的，现在我们要使用这个观察，这个无记忆性，最终计算几何随机变量的期望值。</p><p>So it’s useful here to digest both these formal statements and understand it and understand the notation that
            is
            involved here, but also to really appreciate the intuitive argument what this is really saying. OK, all
            right,
            so now we want to use this observation, this memorylessness, to eventually calculate the expected value for
            a
            geometric random variable.</p>
        <p>我们要做的方法是使用分而治之的工具，它类似于我们之前见过的方法。还记得我们的故事吗？世界上有许多可能的情况？在任何一种可能的情况下，都可能发生某个事件 B。我们有全概率论。</p><p>And the way we’re going to do it is by using a divide and conquer tool, which is an analog of what we have
            already seen sometime before. Remember our story that there’s a number of possible scenarios about the
            world?
            And there’s a certain event, B, that can happen under any of these possible scenarios. And we have the total
            probability theory.</p>
        <h2 id="unknown-122">未知</h2><h2>Unknown</h2>
        <p>这告诉我们，要找到事件 B 的概率，您需要考虑每种情况下 B 的概率。然后根据我们拥有的不同场景的概率来权衡这些概率。所以这是我们已经知道并已经使用过的公式。下一步是什么？这有什么深奥之处吗？不，这只是以不同的符号进行的翻译。这是完全相同的公式，但带有 PMF。</p><p>And that tells us that, to find the probability of this event, B, you consider the probabilities of B under
            each
            scenario. And you weigh those probabilities according to the probabilities of the different scenarios that
            we
            have. So that’s a formula that we already know and have worked with. What’s the next step? Is it something
            deep?
            No, it’s just translation in different notation. This is the exactly same formula, but with PMFs.</p>
        <p>大写 X 等于小写 x 的事件可以以多种不同的方式发生。它可以在任一情况下发生。在每种情况下，您都需要使用该事件的条件概率，前提是该情况已经发生。因此，这个公式与那个公式相同，只是我们使用的是条件 PMF，而不是条件概率。但条件 PMF 当然只是条件概率。所以到目前为止没有什么新东西。</p><p>The event that capital X is equal to little x can happen in many different ways. It can happen under either
            scenario. And within each scenario, you need to use the conditional probabilities of that event, given that
            this
            scenario has occurred. So this formula is identical to that one, except that we’re using conditional PMFs,
            instead of conditional probabilities. But conditional PMFs, of course, are nothing but conditional
            probabilities
            anyway. So nothing new so far.</p>
        <p>然后我要做的是取这个公式，两边乘以 X，然后对所有 X 求和。我们在这一边得到什么？我们得到了 X 的期望值。我们在那边得到什么？A1 的概率。然后在这里，对所有 X 乘以 P 的 X 求和。</p><p>Then what I do is to take this formula here and multiply both sides by X and take the sum over all X’s. What
            do
            we get on this side? We get the expected value of X. What do we get on that side? Probability of A1. And
            then
            here, sum over all X’s of X times P.</p>
        <h2 id="unknown-123">未知</h2><h2>Unknown</h2>
        <p>这与我们处理期望时的计算方法相同，只不过，由于我们在这里处理的是条件概率，所以我们将得到条件期望。这就是总期望定理。这是一种使用分治法计算期望的非常有用的方法。我们计算出每种可能情况下 X 的平均值。</p><p>That’s, again, the same calculation we have when we deal with expectations, except that, since here, we’re
            dealing with conditional probabilities, we’re going to get the conditional expectation. And this is the
            total
            expectation theorem. It’s a very useful way for calculating expectations using a divide and conquer method.
            We
            figure out the average value of X under each one of the possible scenarios.</p>
        <p>X 的总体平均值是不同情况下 X 的期望值的加权线性组合，其中权重根据不同的概率选择。好的，现在我们将把它应用于几何随机变量的情况。我们将分而治之，分别考虑第一次抛掷正面的两种情况和第一次抛掷反面的另一种情况。
        </p><p>The overall average value of X is a weighted linear combination of the expected values of X in the different
            scenarios where the weights are chosen according to the different probabilities. OK, and now we’re going to
            apply this to the case of a geometric random variable. And we’re going to divide and conquer by considering
            separately the two cases where the first toss was heads, and the other case where the first toss was tails.
        </p>
        <p>因此，X 的期望值是第一次掷出正面的概率，因此 X 等于 1，如果发生这种情况，则期望值是多少。假设 X 等于 1，X 的期望值是多少？如果已知 X 等于 1，则 X 只是一个数字。数字的期望值就是数字本身。</p><p>So the expected value of X is the probability that the first toss was heads, so that X is equal to 1, and the
            expected value if that happened. What is the expected value of X, given that X is equal to 1? If X is known
            to
            be equal to 1, then X becomes just a number. And the expected value of a number is the number itself.</p>
        <h2 id="unknown-124">未知</h2><h2>Unknown</h2>
        <p>因此，第一行是第一次抛掷中正面朝上的概率乘以数字 1。因此，X 大于 1 的概率是 1 减去 P。然后我们需要对这个条件期望做点什么。它是什么？我可以用一种更建议的形式来写，即预期 X 减 1 的值，假设 X 减 1 大于 1。啊。</p><p>So this first line here is the probability of heads in the first toss times the number 1. So the probability
            that
            X is bigger than 1 is 1 minus P. And then we need to do something about this conditional expectation. What
            is
            it? I can write it in, perhaps, a more suggested form, as expected the value of X minus 1, given that X
            minus 1
            is bigger than 1. Ah.</p>
        <p>好的，这样，X 大于 1 就等于 X 减 1 为正数。X 减 1 等于正数加 1。我在这里做了什么？我加了 1 又减了 1。现在这是什么？这是剩余抛硬币的期望值，直到我得到正面，假设第一次抛的是反面。这和我们在下面经历的故事是一样的。</p><p>OK, X bigger than 1 is the same as X minus 1 being positive, this way. X minus 1 is positive plus 1. What did
            I
            do here? I added and subtracted 1. Now what is this? This is the expected value of the remaining coin flips,
            until I obtain heads, given that the first one was tails. It’s the same story that we were going through
            down
            there.</p>
        <p>假设第一次抛硬币是反面，这并不能告诉我关于未来、关于剩余抛硬币的任何信息。所以这个期望应该与刚刚开始的人所面临的期望相同。所以这应该等于 X 本身的期望值。然后我们得到从那里得到的加 1，好吗？
        </p><p>Given that the first coin flip was tails doesn’t tell me anything about the future, about the remaining coin
            flips. So this expectation should be the same as the expectation faced by a person who was starting just
            now. So
            this should be equal to the expected value of X itself. And then we have the plus 1 that’s come from there,
            OK?
        </p>
        <h2 id="unknown-125">未知</h2><h2>Unknown</h2>
        <p>假设我昨天掷的是反面，那么抛硬币直到正面的剩余次数与对于一个刚刚开始并且昨天什么都没做的人来说抛硬币直到正面的预期次数相同。因此，我昨天抛了一枚硬币这一事实并没有改变我对第一次掷出正面需要多长时间的信念。因此，一旦我们相信了这种关系，我们就可以在这里插入它。</p><p>Remaining coin flips until a head, given that I had a tail yesterday, is the same as expected number of flips
            until heads for a person just starting now and wasn’t doing anything yesterday. So the fact that they I had
            a
            coin flip yesterday doesn’t change my beliefs about how long it’s going to take me until the first head. So
            once
            we believe that relation, than we plug this here.</p>
        <p>这个红色项变成了 X 的期望值加 1。所以现在我们并没有得到我们想要的答案，但我们得到了一个涉及 X 期望值的方程。它是该方程中唯一的未知数。X 的期望值等于 P 加（1 减 P）乘以这个表达式。你解这个方程得到 X 的期望值，你得到 1/P 的值。最终答案确实符合直觉。</p><p>And this red term becomes expected value of X plus 1. So now we didn’t exactly get the answer we wanted, but
            we
            got an equation that involves the expected value of X. And it’s the only unknown in that equation. Expected
            value of X equals to P plus (1 minus P) times this expression. You solve this equation for expected value of
            X,
            and you get the value of 1/P. The final answer does make intuitive sense.</p>
        <p>如果 P 很小，则很难得到正面。所以你预计要花很长时间才能第一次看到正面。所以这肯定是一个合理的答案。现在我们在这里使用的技巧，分而治之的技巧，是一个非常好的技巧。它为我们提供了解决这个问题的非常好的捷径。</p><p>If P is small, heads are difficult to obtain. So you expect that it’s going to take you a long time until you
            see
            heads for the first time. So it is definitely a reasonable answer. Now the trick that we used here, the
            divide
            and conquer trick, is a really nice one. It gives us a very good shortcut in this problem.</p>
        <h2 id="unknown-126">未知</h2><h2>Unknown</h2>
        <p>但你一定要花点时间确保自己理解为什么这里的表达式与那里的表达式相同。本质上，它的意思是，如果我告诉你 X 大于 1，第一次抛硬币是反面，我告诉你的只是那个人浪费了一次抛硬币的机会，他们要重新开始。所以他们浪费了 1 次抛硬币的机会。他们要重新开始。</p><p>But you must definitely spend some time making sure you understand why this expression here is the same as
            that
            expression there. Essentially, what it’s saying is that, if I tell you that X is bigger than 1, that the
            first
            coin flip was tails, all I’m telling you is that person has wasted a coin flip, and they are starting all
            over
            again. So they’ve wasted 1 coin flip. And they’re starting all over again.</p>
        <p>如果我告诉你第一次抛硬币是反面，那基本上就是我给你的唯一信息，一次浪费的抛硬币，然后一切重新开始。好了，现在剩下的几分钟里，我们将快速介绍一些新概念，我们将在接下来的十天左右玩这些概念。你将有很多机会来操纵它们。所以这就是想法。</p><p>If I tell you that the first flip was tails, that’s the only information that I’m basically giving you, a
            wasted
            flip, and then starts all over again. All right, so in the few remaining minutes now, we’re going to quickly
            introduce a few new concepts that we will be playing with in the next ten days or so. And you will get
            plenty of
            opportunities to manipulate them. So here’s the idea.</p>
        <p>一个典型的实验可能有几个与该实验相关的随机变量。因此，一个典型的学生有身高和体重。如果我给你身高的 PMF，这会告诉我一些有关班级身高分布的信息。如果我给你体重的 PMF，它会告诉我一些有关这个班级不同体重的信息。</p><p>A typical experiment may have several random variables associated with that experiment. So a typical student
            has
            height and weight. If I give you the PMF of height, that tells me something about distribution of heights in
            the
            class. I give you the PMF of weight, it tells me something about the different weights in this class.</p>
        <h2 id="unknown-127">未知</h2><h2>Unknown</h2>
        <p>但如果我想问一个问题，身高和体重之间是否有关联，那么我需要多了解一下身高和体重之间的关系。而身高个体的 PMF 和体重的 PMF 本身并不能告诉我有关这些关系的任何信息。</p><p>But if I want to ask a question, is there an association between height and weight, then I need to know a
            little
            more how height and weight relate to each other. And the PMF of height individuality and PMF of weight just
            by
            itself do not tell me anything about those relations.</p>
        <p>为了能够描述这些关系，我需要了解联合概率，即某些 X 与某些 Y 结合在一起的可能性有多大。因此，这些概率本质上捕捉了这两个随机变量之间的关联。这是我进行任何试图将这两个随机变量相互关联的统计研究所需的信息。这些都是普通概率。这是一个事件。</p><p>To be able to say something about those relations, I need to know something about joint probabilities, how
            likely
            is it that certain X’s go together with certain Y’s. So these probabilities, essentially, capture
            associations
            between these two random variables. And it’s the information I would need to have to do any kind of
            statistical
            study that tries to relate the two random variables with each other. These are ordinary probabilities. This
            is
            an event.</p>
        <p>这是这件事发生和那件事发生的事件。这只是我们将要使用的符号。它被称为联合 PMF。它是两个随机变量 X 和 Y 的联合概率质量函数。它给出了任何特定数值结果对发生的概率。因此，在有限情况下，您可以用表格等表示联合 PMF。</p><p>It’s the event that this thing happens and that thing happens. This is just the notation that we will be
            using.
            It’s called the joint PMF. It’s the joint Probability Mass Function of the two random variables X and Y
            looked
            at together, jointly. And it gives me the probability that any particular numerical outcome pair does
            happen. So
            in the finite case, you can represent joint PMFs, for example, by a table.</p>
        <h2 id="unknown-128">未知</h2><h2>Unknown</h2>
        <p>此处的特定表格将为您提供信息，例如，让我们看看在 2,3 处评估的联合 PMF。这是 X 等于 3 且 Y 同时等于 3 的概率。所以它就是这里的数字。它是 4/20。好的，PMF 的基本属性是什么？首先，这些是概率，因此所有条目都必须是非负的。</p><p>This particular table here would give you information such as, let’s see, the joint PMF evaluated at 2,3.
            This is
            the probability that X is equal to 3 and, simultaneously, Y is equal to 3. So it would be that number here.
            It’s
            4/20. OK, what is a basic property of PMFs? First, these are probabilities, so all of the entries have to be
            non
            negative.</p>
        <p>如果你采用所有可能的数字对的概率，那么总概率当然必须等于 1。所以这是我们想要的另一件事。现在假设有人给我这个模型，但我不关心 Y。我只关心 X 的分布。所以我要找出 X 取特定值的概率。我可以从表格中找到它吗？当然可以。</p><p>If you adopt the probabilities over all possible numerical pairs that you could get, of course, the total
            probability must be equal to 1. So that’s another thing that we want. Now suppose somebody gives me this
            model,
            but I don’t care about Y’s. All I care is the distribution of the X’s. So I’m going to find the probability
            that
            X takes on a particular value. Can I find it from the table? Of course, I can.</p>
        <p>如果你问我 X 等于 3 的概率是多少，我会把这三个概率加在一起。这些概率加在一起就是 X 等于 3 的概率。这些都是 X 等于 3 事件发生的所有可能方式。所以我们把它们加起来，得到 6/20。我刚才做的，我们可以把它转化为公式吗？</p><p>If you ask me what’s the probability that X is equal to 3, what I’m going to do is to add up those three
            probabilities together. And those probabilities, taken all together, give me the probability that X is equal
            to
            3. These are all the possible ways that the event X equals to 3 can happen. So we add these, and we get the
            6/20. What I just did, can we translate it to a formula?</p>
        <h2 id="unknown-129">未知</h2><h2>Unknown</h2>
        <p>我做了什么？我固定了 X 的特定值。然后我将联合 PMF 的值与 Y 的所有可能值相加。这就是你要做的事情。你取联合。你取联合的一片，保持 X 不变，然后将 Y 的不同值相加。</p><p>What did I do? I fixed the particular value of X. And I added up the values of the joint PMF over all the
            possible values of Y. So that’s how you do it. You take the joint. You take one slice of the joint, keeping
            X
            fixed, and adding up over the different values of Y.</p>
        <p>这个例子的寓意是，如果你知道联合 PMF，那么你就可以找到每个随机变量的单独 PMF。我们给它们起了个名字。我们把它们称为边际 PMF。我们有联合 PMF 来讨论两者，还有边际 PMF 来分别讨论它们。最后，因为我们喜欢条件概率，所以我们肯定会定义一个称为条件 PMF 的对象。</p><p>The moral of this example is that, if you know the joint PMFs, then you can find the individual PMFs of every
            individual random variable. And we have a name for these. We call them the marginal PMFs. We have the joint
            that
            talks about both together, and the marginal that talks about them one at the time. And finally, since we
            love
            conditional probabilities, we will certainly want to define an object called the conditional PMF.</p>
        <p>所以，这里的这个量很熟悉。它只是一个条件概率。它是在 Y 取某个特定值的情况下，X 取某个特定值的概率。在我们的例子中，我们假设 y 等于 2，这意味着我们适应于生活在这个宇宙中。这里的红色宇宙是 y 等于 2 的宇宙。这些是该宇宙中不同 X 的条件概率。</p><p>So this quantity here is a familiar one. It’s just a conditional probability. It’s the probability that X
            takes
            on a particular value, given that Y takes a certain value. For our example, let’s take little y to be equal
            to
            2, which means that we’re conditioning to live inside this universe. This red universe here is the y equal
            to 2
            universe. And these are the conditional probabilities of the different X’s inside that universe.</p>
        <h2 id="unknown-130">未知</h2><h2>Unknown</h2>
        <p>好的，再一次，这只是一个符号练习。这是我们在第一章中以这种方式表示的符号的第二章版本。理解它的方法是，它是一个与两个随机变量有关的条件 PMF，X 的 PMF 以有关 Y 的信息为条件。我们正在固定大写 Y 的一个特定值，这就是我们要以该值为条件。</p><p>OK, once more, just an exercise in notation. This is the chapter two version of the notation of what we were
            denoting this way in chapter one. The way to read this is that it’s a conditional PMF having to do with two
            random variables, the PMF of X conditioned on information about Y. We are fixing a particular value of
            capital
            Y, that’s the value on which we are conditioning.</p>
        <p>我们正在研究不同 X 的概率。所以它实际上是两个参数的函数，小 x 和小 y。但思考它的最佳方式是固定小 y，并将其视为 X 的函数。所以我在这里固定小 y，假设 y 等于 2。所以我只考虑这一点。现在，这个量成为小 x 的函数。</p><p>And we’re looking at the probabilities of the different X’s. So it’s really a function of two arguments,
            little x
            and little y. But the best way to think about it is to fix little y and think of it as a function of X. So
            I’m
            fixing little y here, let’s say, to y equal to 2. So I’m considering only this. And now, this quantity
            becomes a
            function of little x.</p>
        <p>对于不同的小 x，我们将有不同的条件概率。这些条件概率是什么？好的，条件概率与原始概率成比例。所以它将是那些数字，但按比例放大。</p><p>For the different little x’s, we’re going to have different conditional probabilities. What are those
            conditional
            probabilities? OK, conditional probabilities are proportional to original probabilities. So it’s going to be
            those numbers, but scaled up.</p>
        <h2 id="unknown-131">未知</h2><h2>Unknown</h2>
        <p>并且它们需要缩放，以便它们加起来等于 1。所以我们有 1、3 和 1。总共是 5。因此条件 PMF 的形状将是零、1/5、3/5 和 1/5。这是给定特定 Y 值的条件 PMF。它具有与这些数字相同的形状，其中形状是指尝试可视化条形图。</p><p>And they need to be scaled so that they add up to 1. So we have 1,3 and 1. That’s a total of 5. So the
            conditional PMF would have the shape zero, 1/5,3/5, and 1/5. This is the conditional PMF, given a particular
            value of Y. It has the same shape as those numbers, where by shape, I mean try to visualize a bar graph.</p>
        <p>与这些数字相关的条形图与与这些数字相关的条形图具有完全相同的形状。唯一改变的是缩放比例。大道理在于，让我换一种说法，给定一个特定的 Y 值，条件 PMF 只是联合 PMF 的一个片段，其中保持相同的形状，但重新缩放数字，使它们加起来等于 1。当然，现在从数学上讲，所有这些所做的就是获取原始联合 PDF，然后按某个因子重新缩放它。</p><p>The bar graph associated with those numbers has exactly the same shape as the bar graph associated with those
            numbers. The only thing that has changed is the scaling. Big moral, let me say in different words, the
            conditional PMF, given a particular value of Y, is just a slice of the joint PMF where you maintain the same
            shape, but you rescale the numbers so that they add up to 1. Now mathematically, of course, what all of this
            is
            doing is it’s taking the original joint PDF and it rescales it by a certain factor.</p>
        <p>这与 X 无关，因此形状（X 的函数）没有改变。我们保持与 X 函数相同的形状，但我们除以某个数字。这就是我们需要的数字，这样条件概率加起来就是 1。那么这个公式从何而来？嗯，这只是条件概率的定义。</p><p>This does not involve X, so the shape, is a function of X, has not changed. We’re keeping the same shape as a
            function of X, but we divide by a certain number. And that’s the number that we need, so that the
            conditional
            probabilities add up to 1. Now where does this formula come from? Well, this is just the definition of
            conditional probabilities.</p>
        <h2 id="unknown-132">未知</h2><h2>Unknown</h2>
        <p>某事以另一事为条件的概率是两件事发生的概率，即两件事的交集除以条件事件的概率。最后要说的是，正如我刚才所说，条件概率与普通概率没什么不同。因此，无论你以什么为条件，条件 PMF 的总和都必须为 1。好的，这是我们对新符号的快速介绍。但在接下来的几天里，你会得到大量的练习。</p><p>Probability of something conditioned on something else is the probability of both things happening, the
            intersection of the two divided by the probability of the conditioning event. And last remark is that, as I
            just
            said, conditional probabilities are nothing different than ordinary probabilities. So a conditional PMF must
            sum
            to 1, no matter what you are conditioning on. All right, so this was sort of quick introduction into our new
            notation. But you get a lot of practice in the next days to come.</p>
        <h1 id="discrete-random-variables-iii">7.离散随机变量 III</h1><h1>7. Discrete Random Variables III</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 50:42 <a href="https://youtube.com/watch?v=EObHWIEKGjA">https://youtube.com/watch?v=EObHWIEKGjA</a></p><p> 12 years ago (Nov 10, 2012) — 50:42 <a href="https://youtube.com/watch?v=EObHWIEKGjA">https://youtube.com/watch?v=EObHWIEKGjA</a></p>
        <h2 id="mit-opencourseware">麻省理工学院开放课程</h2><h2>MIT OpenCourseWare</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.</p>
        <h2 id="introduction">介绍</h2><h2>Introduction</h2>
        <p>教授：好的，早上好。今天，我们的讲座内容很丰富。我们将以第二章“离散随机变量”作为结束。我们将主要讨论多个随机变量。</p><p>PROFESSOR: OK, good morning. So today, we’re going to have a fairly packed lecture. We are going to conclude
            with
            chapter two, discrete random variables. And we will be talking mostly about multiple random variables.</p>
        <p>就测验一而言，这也是最后一堂课。所以它将涵盖今天为止的内容，当然还有下一次复习和教程。好的，我们将快速回顾一下上一堂课结束时介绍的内容，我们讨论了两个随机变量的联合 PMF。我们还将讨论两个以上随机变量的情况。</p><p>And this is also the last lecture as far as quiz one is concerned. So it’s going to cover the material until
            today, and of course the next recitation and tutorial as well. OK, so we’re going to review quickly what we
            introduced at the end of last lecture, where we talked about the joint PMF of two random variables. We’re
            going
            to talk about the case of more than two random variables as well.</p>
        <p>我们将讨论熟悉的条件和独立性概念，但应用于随机变量而不是事件。我们将再次查看期望，讨论它们的一些属性，然后解决几个问题并以巧妙的方式计算一些东西。</p><p>We’re going to talk about the familiar concepts of conditioning and independence, but applied to random
            variables
            instead of events. We’re going to look at the expectations once more, talk about a few properties that they
            have, and then solve a couple of problems and calculate a few things in somewhat clever ways.</p>
        <h2 id="exercising-notation">练习符号</h2><h2>Exercising Notation</h2>
        <p>因此，我想指出的第一点是，在很大程度上，我们关于离散随机变量的章节中的内容只是符号练习。有些内容和概念您已经很熟悉了。概率、两件事发生的概率、条件概率。在某种程度上，我们所做的就是用新的符号重写这些熟悉的概念。例如，这是两个随机变量的联合 PMF。</p><p>So the first point I want to make is that, to a large extent, whatever is happening in our chapter on
            discrete
            random variables is just an exercise in notation. There is stuff and concepts that you are already familiar
            with. probabilities, probabilities of two things happening, conditional probabilities. And all that we’re
            doing,
            to some extent, is rewriting those familiar concepts in new notation. So for example, this is the joint PMF
            of
            two random variable.</p>
        <p>对于这些随机变量的任何一对或可能值，它给出了这对值同时出现的概率。所以它是 x 同时取该值，y 同时取另一个值的概率。同样，我们有条件 PMF 的概念，它只是条件的列表</p><p>It gives us, for any pair or possible values of those random variables, the probability that pair occurs
            simultaneously. So it’s the probability that simultaneously x takes that value, and y takes that other
            value.
            And similarly, we have the notion of the conditional PMF, which is just a list of the condition of</p>
        <p>感兴趣的各种条件概率，一个随机变量取该值的条件下另一个随机变量取该值的条件概率。</p><p>The various conditional probabilities of interest, conditional probability that one random variable takes
            this
            value given that the other random variable takes that value.</p>
        <h2 id="conditional-probability">条件概率</h2><h2>Conditional Probability</h2>
        <p>现在，我们来谈谈条件概率。条件概率通常与普通概率类似。你以某个特定事物为条件。因此，我们在这里以某个特定的 y 为条件。因此，将小 y 视为一个固定量。然后将其视为 x 的函数。</p><p>Now, a remark about conditional probabilities. Conditional probabilities generally are like ordinary
            probabilities. You condition on something particular. So here we condition on a particular y. So think of
            little
            y as a fixed quantity. And then look at this as a function of x.</p>
        <p>因此，给定 y（我们以此为条件），给定我们的新宇宙，我们正在考虑 x 的各种可能性及其概率。现在，所有 x 的概率当然需要加到 1。所以我们应该有这种关系。所以它们就像宇宙中不同 x 的普通概率一样，我们被告知随机变量 y 的值。现在，它们是如何关联的？</p><p>So given that y, which we condition on, given our new universe, we’re considering the various possibilities
            for x
            and the probabilities that they have. Now, the probabilities over all x’s, of course, needs to add to 1. So
            we
            should have a relation of this kind. So they’re just like ordinary probabilities over the different x’s in a
            universe where we are told the value of the random variable y. Now, how are these related?</p>
        <p>所以我们称这些为边际，这些为联合，这些为条件。它们之间存在一些关系。例如，要从联合中找到边际，这非常简单。x 取特定值的概率是该特定值可能出现的所有不同方式的概率之和。不同的方式是什么？</p><p>So we call these the marginal, these the joint, these the conditional. And there are some relations between
            these. For example, to find the marginal from the joint, it’s pretty straightforward. The probability that x
            takes a particular value is the sum of the probabilities of all of the different ways that this particular
            value
            may occur. What are the different ways?</p>
        <p>嗯，它可能与某个 y 一起出现，或者与其他 y 一起出现，或者与其他 y 一起出现。因此，您要查看所有可能与此 x 一起出现的 y，并将所有得到此特定 x 值的对的概率相加。然后，这两个概率与条件概率之间存在一种关系。这就是这种关系。</p><p>Well, it may occur together with a certain y, or together with some other y, or together with some other y.
            So
            you look at all the possible y’s that can go together with this x, and add the probabilities of all of those
            pairs for which we get this particular value of x. And then there’s a relation between that connects these
            two
            probabilities with the conditional probability. And it’s this relation.</p>
        <h2 id="joint-probability">联合概率</h2><h2>Joint Probability</h2>
        <p>这并不是什么新鲜事。</p><p>It’s nothing new.</p>
        <p>这只是我们已知的新符号，即两件事发生的概率是第一件事发生的概率，假设第一件事发生，第二件事发生的概率。那么我们如何从一件事得到另一件事呢？将 A 视为 X 取值（小 x）的事件，将 B 视为 Y 取值（小 y）的事件。</p><p>It’s just new notation for writing what we already know, that the probability of two things happening is the
            probability that the first thing happens, and then given that the first thing happens, the probability that
            the
            second one happened. So how do we go from one to the other? Think of A as being the event that X takes the
            value, little x, and B being the event that Y takes the value, little y.</p>
        <p>因此，联合概率是这两件事同时发生的概率。它是 X 取此值的概率乘以 Y 取此值的条件概率（假设 X 取第一个值）。所以这是熟悉的乘法规则，只是用我们的新符号转录了。所以到目前为止没有什么新东西。好吧，我们为什么要进行这个练习和这个符号？</p><p>So the joint probability is the probability that these two things happen simultaneously. It’s the probability
            that X takes this value times the conditional probability that Y takes this value, given that X took that
            first
            value. So it’s the familiar multiplication rule, but just transcribed in our new notation. So nothing new so
            far. OK, why did we go through this exercise and this notation?</p>
        <p>这是因为在我们对现实世界感兴趣的实验中，通常会有很多不确定的量。会有多个随机变量。我们希望能够同时讨论它们。</p><p>It’s because in the experiments where we’re interested in the real world, typically there’s going to be lots
            of
            uncertain quantities. There’s going to be multiple random variables. And we want to be able to talk about
            them
            simultaneously.</p>
        <h2 id="three-random-variables">三个随机变量</h2><h2>Three Random Variables</h2>
        <p>好的。为什么是两个而不是两个以上？三个随机变量怎么样？好吧，如果你理解了这张幻灯片中的内容，你应该能够自动将其推广到多个随机变量的情况。</p><p>Okay. Why two and not more than two? How about three random variables? Well, if you understand what’s going
            on in
            this slide, you should be able to kind of automatically generalize this to the case of multiple random
            variables.</p>
        <p>例如，如果我们有三个随机变量，X、Y 和 Z，你看到这样的表达式，它的含义应该很清楚。它是 X 取这个值，同时 Y 取那个值，同时 Z 取那个值的概率。我想这里的 Z 是大写的，z 是小写的。</p><p>So for example, if we have three random variables, X, Y, and Z, and you see an expression like this, it
            should be
            clear what it means. It’s the probability that X takes this value and simultaneously Y takes that value and
            simultaneously Z takes that value. I guess that’s an uppercase Z here, that’s a lowercase z.</p>
        <p>如果我让你求出 X 的边际，如果我告诉你三个随机变量的联合 PMF，并要求你给出这个值，你会如何求出它？好吧，你会尝试在这里概括这个关系。x 发生的概率是所有使 X 取该特定值的事件的概率之和。那么所有事件是什么呢？</p><p>And if I ask you to find the marginal of X, if I tell you the joint PMF of the three random variables and I
            ask
            you for this value, how would you find it? Well, you will try to generalize this relation here. The
            probability
            that x occurs is the sum of the probabilities of all events that make X to take that particular value. So
            what
            are all the events?</p>
        <p>嗯，这个特定的 x 可以与某些 y 和某些 z 一起发生。我们不关心是哪个 y 和 z。任何 y 和 z 都可以。因此，当我们考虑所有可能性时，我们需要将 y 和 z 的所有可能值相加。因此，考虑所有三元组，x、y、z。固定 x 并考虑其余变量 y 和 z 的所有可能性，将它们相加，这样就可以得到 X 的边际 PMF。</p><p>Well, this particular x can happen together with some y and some z. We don’t care which y and z. Any y and z
            will
            do. So when we consider all possibilities, we need to add here over all possible values of y’s and z’s. So
            consider all triples, x, y, z. Fix x and consider all the possibilities for the remaining variables, y and
            z,
            add these up, and that gives you the marginal PMF of X.</p>
        <p>您还可以做其他事情。</p><p>And then there’s other things that you can do.</p>
        <h2 id="multiplication-rule-for-two-events">两个事件的乘法规则</h2><h2>Multiplication Rule for Two Events</h2>
        <p>这是两个事件的乘法规则。我们在第一章中看到，当你谈论两个以上的事件时，有一个乘法规则。你可以写出一系列条件概率。我们当然可以用我们的新符号做同样的事情。所以让我们看看这里的这个规则。</p><p>This is the multiplication rule for two events. We saw back in chapter one that there’s a multiplication rule
            when you talk about more than two events. And you can write a chain of conditional probabilities. We can
            certainly do the same in our new notation. So let’s look at this rule up here.</p>
        <h2 id="multiplication-rule-for-three-events">三个事件的乘法规则</h2><h2>Multiplication Rule for Three Events</h2>
        <p>三个随机变量的乘法规则，它说了什么？</p><p>Multiplication rule for three random variables, what does it say?</p>
        <p>三件事同时发生的概率，X、Y、Z 取特定值，小 x、小 y、小 z，该概率是第一件事情发生的概率，X 取该值。假设 X 取该值，我们将其与 Y 也取特定值的概率相乘。</p><p>The probability of three things happening simultaneously, X, Y, Z taking specific values, little x, little y,
            little z, that probability is the probability that the first thing happens, that X takes that value. Given
            that
            X takes that value, we multiply it with the probability that Y takes also a certain value.</p>
        <p>现在，假设 X 和 Y 都取了这些特定值，我们乘以条件概率，假设前两件事发生，第三件事发生。所以这只是三个事件的乘法规则，即 A 相交 B 相交 C 的概率相等。你知道公式的其余部分。你只需用 PMF 符号重写这个公式即可。</p><p>And now, given that X and Y have taken those particular values, we multiply with a conditional probability
            that
            the third thing happens, given that the first two things happen. So this is just the multiplication rule for
            three events, which would be probability of A intersection B intersection C equals. you know the rest of the
            formula. You just rewrite this formula in PMF notation.</p>
        <p>A 交点 B 交点 C 的概率是 A 的概率，对应于该术语，乘以在 A 给定的情况下 B 的概率，乘以在 A 和 B 给定的情况下 C 的概率。</p><p>Probability of A intersection B intersection C is the probability of A, which corresponds to this term, times
            the
            probability of B given A, times the probability of C given A and B.</p>
        <h2 id="independence-1">独立</h2><h2>Independence</h2>
        <p>那么，第一章中还有哪些内容我们可以或应该推广到随机变量呢？嗯，有独立性的概念。那么让我们来定义独立性的含义。</p><p>So what else is there that’s left from chapter one that we can or should generalize to random variables?
            Well,
            there’s the notion of independence. So let’s define what independence means.</p>
        <p>我们先不讨论两个随机变量，直接讨论多个随机变量的情况。当我们讨论事件时，事情有点复杂。我们对两个事件的独立性有一个简单的定义。如果两个事件的概率等于概率的乘积，则两个事件是独立的。但对于三个事件，情况就有点混乱了。我们需要写下很多条件。</p><p>Instead of talking about just two random variables, let’s go directly to the case of multiple random
            variables.
            When we talked about events, things were a little complicated. We had a simple definition for independence
            of
            two events. Two events are independent if the probability of both is equal to the product of the
            probabilities.
            But for three events, it was kind of messy. We needed to write down lots of conditions.</p>
        <p>对于随机变量，事情在某种意义上要简单一些。我们只需要写下一个公式，并将其作为独立性的定义。三个随机变量是独立的，当且仅当根据定义，它们的联合概率质量函数分解为单独的概率质量函数。因此，三件事情同时发生的概率是这三件事情分别发生的概率的乘积。</p><p>For random variables, things in some sense are a little simpler. We only need to write down one formula and
            take
            this as the definition of independence. Three random variables are independent if and only if, by
            definition,
            their joint probability mass function factors out into individual probability mass functions. So the
            probability
            that all three things happen is the product of the individual probabilities that each one of these three
            things
            is happening.</p>
        <p>因此，独立性在数学上意味着，你只需将概率相乘，就能得到几件事同时发生的概率。因此，对于三个事件，我们必须写出大量的方程，其中必须包含大量的等式。对于随机变量，我们怎么可能只用一个等式来处理呢？嗯，问题在于，这实际上不只是一个等式。我们要求这个等式对每个小 x、y 和 z 都成立。</p><p>So independence means mathematically that you can just multiply probabilities to get to the probability of
            several things happening simultaneously. So with three events, we have to write a huge number of equations,
            of
            equalities that have to hold. How can it be that with random variables we can only manage with one equality?
            Well, the catch is that this is not really just one equality. We require this to be true for every little x,
            y,
            and z.</p>
        <p>所以从某种意义上说，这是对联合 PMF 施加的一系列条件，是我们需要检查的一系列条件。</p><p>So in some sense, this is a bunch of conditions that are being put on the joint PMF, a bunch of conditions
            that
            we need to check.</p>
        <h2 id="intuitive-content">直观的内容</h2><h2>Intuitive Content</h2>
        <p>这就是数学定义。这个定义的直观内容是什么？直观内容与事件相同。</p><p>So this is the mathematical definition. What is the intuitive content of this definition? The intuitive
            content
            is the same as for events.</p>
        <p>如果了解一些随机变量的实际值不会改变我们对其余随机变量各种值可能性的信念，则随机变量是独立的。因此，独立性可以转化为这样的条件：给定 y，X 的条件 PMF 应该等于 X 的边际 PMF。这句话是什么意思？</p><p>Random variables are independent if knowing something about the realized values of some of these random
            variables
            does not change our beliefs about the likelihood of various values for the remaining random variables. So
            independence would translate, for example, to a condition such as the conditional PMF of X,given y, should
            be
            equal to the marginal PMF of X. What is this saying?</p>
        <p>你对 X 取这个值的可能性有一些原始信念。现在，有人来告诉你 Y 取了某个值。这在原则上会导致你修改你的信念。你的新信念将被条件 PMF 或条件概率捕获。独立性意味着你修改后的信念实际上将与你原来的信念相同。</p><p>That you have some original beliefs about how likely it is for X to take this value. Now, someone comes and
            tells
            you that Y took on a certain value. This causes you, in principle, to revise your beliefs. And your new
            beliefs
            will be captured by the conditional PMF, or the conditional probabilities. Independence means that your
            revised
            beliefs actually will be the same as your original beliefs.</p>
        <p>告诉你有关 Y 值的信息不会改变你对随机变量 X 的期望。为什么我们不对独立性使用这个定义？因为只有当这个条件定义明确时，这个定义才有意义。而且只有当 Y 在特定值上发生的事件具有正概率时，这个条件才定义明确。</p><p>Telling you information about the value of Y doesn’t change what you expect for the random variable X. Why
            didn’t
            we use this definition for independence? Well, because this definition only makes sense when this
            conditional is
            well defined. And this conditional is only well defined if the events that Y takes on that particular value
            has
            positive probability.</p>
        <p>我们不能对概率为零的事件进行条件化，因此条件概率仅针对可能发生且概率为正的 y 进行定义。现在，类似地，对于多个随机变量，如果它们是独立的，则您将得到诸如给定 y 和 z 时 X 的条件应该与 X 的边际相同的关系。这句话是什么意思？</p><p>We cannot condition on events that have zero probability, so conditional probabilities are only defined for
            y’s
            that are likely to occur, that have a positive probability. Now, similarly, with multiple random variables,
            if
            they’re independent, you would have relations such as the conditional of X, given y and z, should be the
            same as
            the marginal of X. What is this saying?</p>
        <p>再说一遍，如果我告诉你随机变量 Y 和 Z 的实际值，这不会改变你对 x 发生可能性的看法。无论你一开始相信什么，你之后都会相信同样的事情。所以记住这种直觉很重要，因为有时这样你就可以判断随机变量是否独立，而不必进行计算和检查这个公式。</p><p>Again, that if I tell you the values, the realized values of random variables Y and Z, this is not going to
            change your beliefs about how likely x is to occur. Whatever you believed in the beginning, you’re going to
            believe the same thing afterwards. So it’s important to keep that intuition in mind, because sometimes this
            way
            you can tell whether random variables are independent without having to do calculations and to check this
            formula.</p>
        <p>好的，让我们用一个简单的例子来检查一下我们的概念。让我们看两个离散的随机变量，每个变量取 1 和 1 之间的值。这是一个给出联合 PMF 的表格。所以它告诉我们 X 等于 2 且 Y 等于 1 同时发生的概率。这是一个概率为 1/20 的事件。这两个随机变量是独立的吗？您可以尝试检查这样的条件。</p><p>OK, so let’s check our concepts with a simple example. Let’s look at two random variables that are discrete,
            take
            values between one and for each. And this is a table that gives us the joint PMF. So it tells us the
            probability
            that X equals to 2 and Y equals to 1 happening simultaneously. It’s an event that has probability 1/20. Are
            these two random variables independent? You can try to check a condition like this.</p>
        <p>但是我们能直接从表中看出吗？如果我告诉你 Y 的值，这能给你关于 X 的有用信息吗？当然。如果我告诉你 Y 等于 1，这说明 X 一定等于 2。但如果我告诉你 Y 等于 3，这说明 X 仍然可以是任何值。</p><p>But can we tell directly from the table? If I tell you a value of Y, could that give you useful information
            about
            X? Certainly. If I tell you that Y is equal to 1, this tells you that X must be equal to 2. But if I tell
            you
            that Y was equal to 3, this tells you that, still, X could be anything.</p>
        <p>因此，告诉你 Y 的值会改变你对其他随机变量值的期望或你认为可能的值。因此，只要检查一下，我们就能知道随机变量不是独立的。
        </p><p>So telling you the value of Y kind of changes what you expect or what you consider possible for the values of
            the
            other random variable. So by just inspecting here, we can tell that the random variables are not
            independent.
        </p>
        <h2 id="conditional-independence">条件独立</h2><h2>Conditional Independence</h2>
        <p>好的。我们在第一章中介绍的另一个概念是什么？我们介绍了条件独立性的概念。条件独立性就像普通的独立性，但应用于我们得到一些信息的条件宇宙。</p><p>Okay. What’s the other concept we introduced in chapter one? We introduced the concept of conditional
            independence. And conditional independence is like ordinary independence but applied to a conditional
            universe
            where we’re given some information.</p>
        <p>假设有人告诉你实验的结果是 X 小于或等于 2，Y 大于或等于 3。所以我们得到的信息是我们现在生活在这个宇宙中。那么这个宇宙里会发生什么？在这个宇宙里，我们的随机变量将有一个新的联合 PMF，它取决于我们被告知发生的事件。</p><p>So suppose someone tells you that the outcome of the experiment is such that X is less than or equal to 2 and
            Y
            is larger than or equal to 3. So we are given the information that we now live inside this universe. So what
            happens inside this universe? Inside this universe, our random variables are going to have a new joint PMF
            which
            is conditioned on the event that we were told that it has occurred.</p>
        <p>因此，让 A 对应于此处的此类事件。现在我们要处理的是条件概率。这些条件概率是什么？我们可以将它们放在一个表中。因此，这是一个 2x2 表，因为我们只有两个可能的值。它们会是什么？这些概率以比率 1、2.2 和 4 的形式出现。这些比率必须保持不变。概率需要加起来等于一。</p><p>So let A correspond to this sort of event here. And now we’re dealing with conditional probabilities. What
            are
            those conditional probabilities? We can put them in a table. So it’s a two by two table, since we only have
            two
            possible values. What are they going to be? Well, these probabilities show up in the ratios 1,2.2, and 4.
            Those
            ratios have to stay the same. The probabilities need to add up to one.</p>
        <p>那么，既然这些数字加起来是九，那么分母应该是什么呢？这些是条件概率。所以这是本例中的条件 PMF。现在，在这个条件宇宙中，x 独立于 y 吗？如果我告诉你 y 取这个值，那么我们就生活在这个宇宙中，你对 x 了解多少？你对 x 的了解是这个值的两倍。</p><p>So what should the denominators be since these numbers add up to nine? These are the conditional
            probabilities.
            So this is the conditional PMF in this example. Now, in this conditional universe, is x independent from y?
            If I
            tell you that y takes this value, so we live in this universe, what do you know about x? What you know about
            x
            is at this value is twice as likely as that value.</p>
        <p>如果我以 y 为条件，取这个值，那么我们就住在这里，你对 x 了解多少？你对 x 了解的是，这个值的可能性是那个值的两倍。所以是一样的。无论我们住在这里还是住在那里，这个 x 的可能性都是那个 x 的两倍。</p><p>If I condition on y taking this value, so we’re living here, what do you know about x? What you know about x
            is
            that this value is twice as likely as that value. So it’s the same. Whether we live here or we live there,
            this
            x is twice as likely as that x.</p>
        <p>因此，这个新宇宙中的条件 PMF，即给定 y 的 X 的条件 PMF，在新宇宙中与 X 的边际 PMF 相同，当然是在新宇宙中。因此，无论 y 是什么，X 的条件 PMF 都是相同的。并且该条件 PMF 是 1/3 和 2/3。无论 y 发生什么，这都是新宇宙中 X 的条件 PMF。</p><p>So the conditional PMF in this new universe, the conditional PMF of X given y, in the new universe is the
            same as
            the marginal PMF of X, but of course in the new universe. So no matter what y is, the conditional PMF of X
            is
            the same. And that conditional PMF is 1/3 and 2/3. This is the conditional PMF of X in the new universe no
            matter what y occurs.</p>
        <p>因此 Y 不会给我们提供任何有关 X 的信息，也不会导致我们在这个小宇宙中改变我们的信念。因此这两个随机变量是独立的。现在，验证我们具有独立性的另一种方法是找到这两个随机变量的边际 PMF。X 的边际 PMF 可以通过将这两个项相加来找到。</p><p>So Y does not give us any information about X, doesn’t cause us to change our beliefs inside this little
            universe. And therefore the two random variables are independent. Now, the other way that you can verify
            that we
            have independence is to find the marginal PMFs of the two random variables. The marginal PMF of X, you find
            it
            by adding those two terms.</p>
        <p>你得到 1/3。将这两个项相加，得到 2/3。Y 的边际 PMF，你找到它，将这两个项相加，得到 1/3。这里 Y 的边际 PMF 将是 2/3。然后你会问，联合是边际的乘积吗？确实如此。</p><p>You get 1/3. Adding those two terms, you get 2/3. Marginal PMF of Y, you find it, you add these two terms,
            and
            you get 1/3. And the marginal PMF of Y here is going to be 2/3. And then you ask the question, is the joint
            the
            product of the marginals? And indeed it is.</p>
        <p>乘以这个得到 1/9。乘以这个得到 2/9。因此，表中联合 PMF 的值是该宇宙中 X 和 Y 的边际 PMF 的乘积，因此这两个随机变量在该宇宙中是独立的。所以我们说它们是条件独立的。好的。</p><p>This times this gives you 1/9. This times this gives you 2/9. So the values in the table with the joint PMFs
            is
            the product of the marginal PMFs of X and Y in this universe, so the two random variables are independent
            inside
            this universe. So we say that they’re conditionally independent. All right.</p>
        <h2 id="expectations">期望</h2><h2>Expectations</h2>
        <p>现在让我们转到新的主题，转到本章介绍的新概念，即期望的概念。</p><p>Now let’s move to the new topic, to the new concept that we introduce in this chapter, which is the concept
            of
            expectations.</p>
        <p>那么这里需要了解什么呢？首先是总体思路。考虑期望的方式是，如果你反复进行实验，并且将概率解释为频率，那么期望值就像是随机变量的平均值。因此，你会反复得到 x，频率为 P(x)，一个特定的值（小 x）就会实现。每次发生这种情况时，你都会得到 x 美元。</p><p>So what are the things to know here? One is the general idea. The way to think about expectations is that
            it’s
            something like the average value for random variable if you do an experiment over and over, and if you
            interpret
            probabilities as frequencies. So you get x’s over and over with a certain frequency P(x) a particular value,
            little x, gets realized. And each time that this happens, you get x dollars.</p>
        <p>平均下来你能得到多少美元？这个公式可以给出特定的平均值。所以我们要做的第一件事就是写下这种概念的定义。但接下来你需要知道的其他事情是，有时如何使用捷径来计算期望值，以及它们具有哪些属性。</p><p>How many dollars do you get on the average? Well, this formula gives you that particular average. So first
            thing
            we do is to write down a definition for this sort of concept. But then the other things you need to know is
            how
            to calculate expectations using shortcuts sometimes, and what properties they have.</p>
        <p>最重要的捷径是，如果你想计算一个随机变量的期望值，即平均值，你不需要找到该随机变量的 PMF。但你可以直接使用 x 和 y。所以你一遍又一遍地做实验。实验的结果是一对 (x,y)。每次发生某个 (x,y) 时，你都会得到很多美元。</p><p>The most important shortcut there is that, if you want to calculate the expected value, the average value for
            a
            random variable, you do not need to find the PMF of that random variable. But you can work directly with the
            x’s
            and the y’s. So you do the experiment over and over. The outcome of the experiment is a pair (x,y). And each
            time that a certain (x,y) happens, you get so many dollars.</p>
        <p>所以在这段时间内，某个 (x,y) 会发生。这段时间内，你会得到这么多美元，所以这是你得到的平均美元数。所以你最终得到的结果，既然是平均值，那就意味着它与预期值相对应。当然，这需要一点数学证明。但这只是一种不同的会计方式。</p><p>So this fraction of the time, a certain (x,y) happens. And that fraction of the time, you get so many
            dollars, so
            this is the average number of dollars that you get. So what you end up, since it is the average, then that
            means
            that it corresponds to the expected value. Now, this is something that, of course, needs a little bit of
            mathematical proof. But this is just a different way of accounting.</p>
        <p>事实证明我们给出了正确的答案。这是一个非常有用的捷径。现在，当我们谈论随机变量函数时，一般来说，我们不能只谈论平均值。也就是说，随机变量函数的期望值与期望值的函数不同。平均值函数与函数的平均值不同。所以一般来说，这是不正确的。</p><p>And it turns out we give you the right answer. And it’s a very useful shortcut. Now, when we’re talking about
            functions of random variables, in general, we cannot speak just about averages. That is, the expected value
            of a
            function of a random variable is not the same as the function of the expected values. A function of averages
            is
            not the same as the average of a function. So in general, this is not true.</p>
        <p>但重要的是要知道这条规则的例外情况。重要的例外主要有两种。一种是随机变量的线性函数的情况。我们上次讨论过这个问题。因此，摄氏温度的期望值是，你首先找到华氏温度的期望值，然后将其转换为摄氏温度。</p><p>But what it’s important to know is to know the exceptions to this rule. And the important exceptions are
            mainly
            two. One is the case of linear functions of a random variable. We discussed this last time. So the expected
            value of temperature in Celsius is, you first find the expected value of temperature in Fahrenheit, and then
            you
            do the conversion to Celsius.</p>
        <p>因此，无论您是否先求平均值，然后再转换为新单位，获得结果的时间都无关紧要。当您谈论多个随机变量时，另一个被证明是正确的属性是期望仍然呈线性变化。因此，让 X、Y 和 Z 成为随机学生在 SAT 三个部分中每个部分的分数。</p><p>So whether you first average and then do the conversion to the new units or not, it shouldn’t matter when you
            get
            the result. The other property that turns out to be true when you talk about multiple random variables is
            that
            expectation still behaves linearly. So let X, Y, and Z be the score of a random student at each one of the
            three
            sections of the SAT.</p>
        <p>因此，SAT 总分等于 X 加 Y 加 Z。这是平均分，即 SAT 总分的平均值。计算平均值的另一种方法是查看 SAT 第一部分，看看平均值是多少。查看第二部分，看看平均值是多少，然后查看第三部分，再将平均值相加。</p><p>So the overall SAT score is X plus Y plus Z. This is the average score, the average total SAT score. Another
            way
            to calculate that average is to look at the first section of the SAT and see what was the average. Look at
            the
            second section, look at what was the average, and so the third, and add the averages.</p>
        <p>因此，您可以分别计算每个部分的平均分，然后将平均分相加，或者您可以计算每个学生的总分并取平均分。因此，如果您只谈论平均分数，我猜您可能认为这是正确的。由于期望只是平均值的变化，因此事实证明这在一般情况下也是正确的。并且基于期望值规则，推导起来非常简单。</p><p>So you can do the averages for each section separately, add the averages, or you can find total scores for
            each
            student and average them. So I guess you probably believe that this is correct if you talk just about
            averaging
            scores. Since expectations are just the variation of averages, it turns out that this is also true in
            general.
            And the derivation of this is very simple, based on the expected value rule.</p>
        <p>你可以在笔记中看到。这是一个例外，即线性。第二个重要的例外是独立随机变量的情况，即两个随机变量的乘积有一个期望，该期望是期望的乘积。一般来说，这不是真的。但对于独立性的情况，期望如下。</p><p>And you can look at it in the notes. So this is one exception, which is linearity. The second important
            exception
            is the case of independent random variables, that the product of two random variables has an expectation
            which
            is the product of the expectations. In general, this is not true. But for the case where we have
            independence,
            the expectation works out as follows.</p>
        <p>使用期望值规则，这就是计算随机变量函数期望值的方法。因此，将其视为 g(X, Y)，将其视为 g(小 x, y)。因此，这通常是正确的。现在，如果我们具有独立性，那么 PMF 就会被分解，然后您可以通过将 x 项放在一起，将它们放在 y 求和之外来分离这个总和。</p><p>Using the expected value rule, this is how you calculate the expected value of a function of a random
            variable.
            So think of this as being your g(X, Y) and this being your g(little x, y). So this is something that’s
            generally
            true. Now, if we have independence, then the PMFs factor out, and then you can separate this sum by bringing
            together the x terms, bring them outside the y summation.</p>
        <p>您会发现，这与 X 的期望值乘以 Y 的期望值相同。因此，在此步骤中使用了独立性。好的，现在如果 X 和 Y 是独立的，但我们不是取 X 乘以 Y 的期望，而是取 X 和 Y 两个函数乘积的期望，会怎样？</p><p>And you find that this is the same as expected value of X times the expected value of Y. So independence is
            used
            in this step here. OK, now what if X and Y are independent, but instead of taking the expectation of X times
            Y,
            we take the expectation of the product of two functions of X and Y?</p>
        <p>我声称乘积的期望值仍将是期望值的乘积。我们如何证明这一点？我们可以通过重新进行推导来证明这一点。我们将得到 g(X) 和 h(Y)，而不是 X 和 Y，这样代数就可以完成。但有一种更好的思考方式，它更概念化。这就是想法。如果 X 和 Y 是独立的，这意味着什么？</p><p>I claim that the expected value of the product is still going to be the product of the expected values. How
            do we
            show that? We could show it by just redoing this derivation here. Instead of X and Y, we would have g(X) and
            h(Y), so the algebra goes through. But there’s a better way to think about it which is more conceptual. And
            here’s the idea. If X and Y are independent, what does it mean?</p>
        <p>X 不传达任何有关 Y 的信息。如果 X 不传达任何有关 Y 的信息，那么 X 会传达有关 h(Y) 的信息吗？不会。如果 X 没有告诉我有关 Y 的任何信息，没有任何新信息，那么它就不应该告诉我有关 h(Y) 的任何信息。现在，如果 X 没有告诉我有关 h(Y) 的 h 的任何信息，那么 g(X) 能告诉我有关 h(Y) 的信息吗？不会。&nbsp;</p><p>X does not convey any information about Y. If X conveys no information about Y, does X convey information
            about
            h(Y)? No.&nbsp;If X tells me nothing about Y, nothing new, it shouldn’t tell me anything about h(Y). Now, if X
            tells
            me nothing about h of h(Y), could g(X) tell me something about h(Y)? No.&nbsp;</p>
        <p>因此，这个想法是，如果 X 与 Y 无关，没有任何有用的信息，那么 g(X) 就不可能对 h(Y) 有任何有用的信息。因此，如果 X 和 Y 是独立的，那么 g(X) 和 h(Y) 也是独立的。因此，人们可以尝试用数学来证明这一点，但更重要的是从概念上理解为什么会这样。这是在传递信息方面。</p><p>So the idea is that, if X is unrelated to Y, doesn’t have any useful information, then g(X) could not have
            any
            useful information for h(Y). So if X and Y are independent, then g(X) and h(Y) are also independent. So this
            is
            something that one can try to prove mathematically, but it’s more important to understand conceptually why
            this
            is so. It’s in terms of conveying information.</p>
        <p>因此，如果 X 无法告诉我有关 Y 的任何信息，X 也无法告诉我有关 Y 立方的任何信息，或者 X 也无法告诉我有关 Y 平方的任何信息，等等。这就是想法。一旦我们确信 g(X) 和 h(Y) 是独立的，那么我们就可以应用之前的规则，即对于独立的随机变量，期望会以正确的方式相乘。应用之前的规则，但现在将其应用于这两个独立的随机变量。</p><p>So if X tells me nothing about Y, X cannot tell me anything about Y cubed, or X cannot tell me anything by Y
            squared, and so on. That’s the idea. And once we are convinced that g(X) and h(Y) are independent, then we
            can
            apply our previous rule, that for independent random variables, expectations multiply the right way. Apply
            the
            previous rule, but apply it now to these two independent random variables.</p>
        <p>我们得到了我们想要的结论。</p><p>And we get the conclusion that we wanted.</p>
        <h2 id="variance-1">方差</h2><h2>Variance</h2>
        <p>现在，除了期望之外，我们还引入了方差的概念。如果你还记得方差的定义，让我写下 aX 方差的公式。它是我们观察的随机变量的期望值减去我们观察的随机变量的期望值。所以这是随机变量与其均值的差。</p><p>Now, besides expectations, we also introduced the concept of the variance. And if you remember the definition
            of
            the variance, let me write down the formula for the variance of aX. It’s the expected value of the random
            variable that we’re looking at minus the expected value of the random variable that we’re looking at. So
            this is
            the difference of the random variable from its mean.</p>
        <p>我们取这个差值并求平方，所以它是与均值的平方距离，然后取整个值的期望值。所以当你看这个表达式时，你会发现可以从这些表达式中抽出 a。因为有一个平方，当你抽出 a 时，它会变成一个 a 的平方。
        </p><p>And we take that difference and square it, so it’s the squared distance from the mean, and then take
            expectations
            of the whole thing. So when you look at that expression, you realize that a can be pulled out of those
            expressions. And because there is a squared, when you pull out the a, it’s going to come out as an a
            squared.
        </p>
        <p>因此，这为我们提供了查找随机变量的尺度或乘积的方差的规则。方差反映了某个分布的宽度和分布范围。方差越大，分布范围越广。现在，如果你取一个随机变量和它的常数，它会对其分布产生什么影响？它只会移动它，而不会改变它的宽度。</p><p>So that gives us the rule for finding the variance of a scale or product of a random variable. The variance
            captures the idea of how wide, how spread out a certain distribution is. Bigger variance means it’s more
            spread
            out. Now, if you take a random variable and the constants to it, what does it do to its distribution? It
            just
            shifts it, but it doesn’t change its width.</p>
        <p>因此直观地看，这意味着方差不应该改变。你可以用数学方法检查这一点，但它也应该直观地有意义。因此，当你添加常数时，方差不会改变。现在，你能像添加期望值一样添加方差吗？方差是否呈线性变化？事实证明并非总是如此。这里，我们需要一个条件。只有在特殊情况下，例如，当两个随机变量是独立的时，你才能添加方差。</p><p>So intuitively it means that the variance should not change. You can check that mathematically, but it should
            also make sense intuitively. So the variance, when you add the constant, does not change. Now, can you add
            variances is the way we added expectations? Does variance behave linearly? It turns out that not always.
            Here,
            we need a condition. It’s only in special cases. for example, when the two random variables are independent.
            that you can add variances.</p>
        <p>如果 X 和 Y 独立，则和的方差就是方差之和。这个推导过程同样非常简短和简单。我们将跳过它，但这是一个需要记住的重要事实。现在，为了理解为什么这个等式并不总是成立，我们可以想出一些极端的例子。假设 X 与 Y 相同。X 加 Y 的方差是多少？</p><p>The variance of the sum is the sum of the variances if X and Y are independent. The derivation of this is,
            again,
            very short and simple. We’ll skip it, but it’s an important fact to remember. Now, to appreciate why this
            equality is not true always, we can think of some extreme examples. Suppose that X is the same as Y. What’s
            going to be the variance of X plus Y?</p>
        <p>嗯，在这种情况下，X 加 Y 等于 2X，所以我们会得到 4 倍的 X 方差，这与 X 的方差加上 X 的方差不同。所以那个表达式会给我们两倍的 X 方差。但实际上现在是 4 倍的 X 方差。另一个极端是 X 等于 Y。</p><p>Well, X plus Y, in this case, is the same as 2X, so we’re going to get 4 times the variance of X, which is
            different than the variance of X plus the variance of X. So that expression would give us twice the variance
            of
            X. But actually now it’s 4 times the variance of X. The other extreme would be if X is equal to Y.</p>
        <p>那么方差就是随机变量的方差，它总是等于 0。现在，一个总是等于 0 的随机变量没有不确定性。它总是等于它的平均值，所以在这种情况下，方差就是 0。所以在这两种情况下，我们当然都有高度依赖的随机变量。为什么它们是依赖的？</p><p>Then the variance is the variance of the random variable, which is always equal to 0. Now, a random variable
            which is always equal to 0 has no uncertainty. It is always equal to its mean value, so the variance, in
            this
            case, turns out to be 0. So in both of these cases, of course we have random variables that are extremely
            dependent. Why are they dependent?</p>
        <p>因为如果我告诉你一些关于 Y 的信息，它就会告诉你很多关于 X 的值的信息。如果我告诉你 Y，那么就会有很多关于 X 的信息，无论是在这种情况下还是在那种情况下。最后，一个简短的练习。如果我告诉你随机变量是独立的，而你想计算这种线性组合的方差，那么你该如何论证？</p><p>Because if I tell you something about Y, it tells you an awful lot about the value of X. There’s a lot of
            information about X if I tell you Y, in this case or in that case. And finally, a short drill. If I tell you
            that the random variables are independent and you want to calculate the variance of a linear combination of
            this
            kind, then how do you argue?</p>
        <p>您认为，由于 X 和 Y 是独立的，这意味着 X 和 3Y 也是独立的。X 没有关于 Y 的信息，因此 X 没有关于 Y 的信息。X 没有关于 Y 的信息，因此 X 不应该有任何关于 3Y 的信息。因此 X 和 3Y 是独立的。</p><p>You argue that, since X and Y are independent, this means that X and 3Y are also independent. X has no
            information about Y, so X has no information about Y. X has no information about Y, so X should not have any
            information about 3Y. So X and 3Y are independent.</p>
        <p>因此，Z 的方差应为 X 的方差加上 3Y 的方差，即 X 的方差加上 9 倍的 Y 方差。这里要注意的重要一点是，无论发生什么情况，最终都会得到一个正数，而不是负数。所以这是在这种计算中需要记住的重要事项。</p><p>So the variance of Z should be the variance of X plus the variance of 3Y, which is the variance of X plus 9
            times
            the variance of Y. The important thing to note here is that no matter what happens, you end up getting a
            plus
            here, not a minus. So that’s the sort of important thing to remember in this type of calculation.</p>
        <h2 id="binomial-distribution">二项分布</h2><h2>Binomial Distribution</h2>
        <p>这就是所有的概念、评论、新概念等等。</p><p>So this has been all concepts, reviews, new concepts and all that.</p>
        <p>这是常见的消防水带。现在让我们用它们来做一些有用的事情。让我们重新回顾一下我们的老例子，二项分布，它计算硬币在独立试验中的成功次数。这是一枚有偏硬币，每次试验正面的概率或成功概率等于 p。最后，我们可以进行计算这个随机变量的预期值的练习。</p><p>It’s the usual fire hose. Now let’s use them to do something useful finally. So let’s revisit our old
            example,
            the binomial distribution, which counts the number of successes in independent trials of a coin. It’s a
            biased
            coin that has a probability of heads, or probability of success, equal to p at each trial. Finally, we can
            go
            through the exercise of calculating the expected value of this random variable.</p>
        <p>有一种计算期望值的方法，是那些喜欢代数的人最喜欢的，即写下期望值的定义。我们将随机变量的所有可能值、所有可能的 k 相加，并根据这个特定 k 发生的概率对它们进行加权。X 取特定值 k 的概率当然是二项式 PMF，这是熟悉的公式。</p><p>And there’s the way of calculating that expectation that would be the favorite of those people who enjoy
            algebra,
            which is to write down the definition of the expected value. We add over all possible values of the random
            variable, over all the possible k’s, and weigh them according to the probabilities that this particular k
            occurs. The probability that X takes on a particular value k is, of course, the binomial PMF, which is this
            familiar formula.</p>
        <p>显然，这将是一个混乱而具有挑战性的计算。我们能找到捷径吗？有一个非常聪明的技巧。概率中有很多问题可以通过将感兴趣的随机变量分解为更简单、更易于管理的随机变量之和来很好地解决。如果你能把它变成只有 0 或 1 的随机变量之和，那就更好了。生活会更轻松。</p><p>Clearly, that would be a messy and challenging calculation. Can we find a shortcut? There’s a very clever
            trick.
            There’s lots of problems in probability that you can approach really nicely by breaking up the random
            variable
            of interest into a sum of simpler and more manageable random variables. And if you can make it to be a sum
            of
            random variables that are just 0’s or 1’s, so much the better. Life is easier.</p>
        <p>取值为 0 或 1 的随机变量，我们称之为指示变量。它们指示事件是否发生。在这种情况下，我们一次查看一次抛硬币。对于第 i 次抛硬币，如果结果是正面或成功，我们将其记录为 1。如果不是，我们将其记录为 0。然后我们查看随机变量。如果我们对 Xi 求和，结果会是多少？</p><p>Random variables that take values 0 or 1, we call them indicator variables. They indicate whether an event
            has
            occurred or not. In this case, we look at each coin flip one at a time. For the i th flip, if it resulted in
            heads or a success, we record it 1. If not, we record it 0. And then we look at the random variable. If we
            take
            the sum of the Xi’s, what is it going to be?</p>
        <p>每次成功时，我们都会加一，所以总和就是成功的总数。所以我们将感兴趣的随机变量分解为非常简单的随机变量的总和。现在我们可以使用期望的线性。我们将通过找到 Xi 的期望然后将期望相加来找到 X 的期望。Xi 的期望值是多少？</p><p>We add one each time that we get a success, so the sum is going to be the total number of successes. So we
            break
            up the random variable of interest as a sum of really nice and simple random variables. And now we can use
            the
            linearity of expectations. We’re going to find the expectation of X by finding the expectation of the Xi’s
            and
            then adding the expectations. What’s the expected value of Xi?</p>
        <p>嗯，Xi 以概率 p 取值 1，以概率 1 p 取值 0。所以 Xi 的期望值就是 p。所以 X 的期望值就是 p 的 n 倍。因为 X 是 n 项之和，每项的期望值为 p，所以和的期望值就是期望值之和。</p><p>Well, Xi takes the value 1 with probability p, and takes the value 0 with probability 1 p.&nbsp;So the expected
            value
            of Xi is just p.&nbsp;So the expected value of X is going to be just n times p.&nbsp;Because X is the sum of n terms,
            each
            one of which has expectation p, the expected value of the sum is the sum of the expected values.</p>
        <p>所以我想这是完成上面这个可怕计算的一个非常好的捷径。所以如果你没意识到，这就是我们刚刚建立的，没有进行任何代数运算。好。X 的方差如何？Xi 的方差有两种计算方法。一种是直接使用方差公式，也就是让我们看看它会是什么。</p><p>So I guess that’s a pretty good shortcut for doing this horrendous calculation up there. So in case you
            didn’t
            realize it, that’s what we just established without doing any algebra. Good. How about the variance of X, of
            Xi?
            Two ways to calculate it. One is by using directly the formula for the variance, which would be let’s see
            what
            it would be.</p>
        <p>概率为 p，你会得到 1。在这种情况下，你离平均值很远。这就是你与平均值的平方距离。概率为 1 p，你会得到 0，离平均值很远。然后你可以简化该公式并得到答案。有没有更简单的方法。在这里，我不会做代数运算，而是让我指出更简单的方法。</p><p>With probability p, you get a 1. And in this case, you are so far from the mean. That’s your squared distance
            from the mean. With probability 1 p, you get a 0, which is so far away from the mean. And then you can
            simplify
            that formula and get an answer. How about a slightly easier way of doing it. Instead of doing the algebra
            here,
            let me indicate the slightly easier way.</p>
        <p>我们有一个方差公式，它告诉我们，我们可以通过这种方式找到方差。这个公式通常适用于方差。为什么这更容易？ Xi 平方的期望值是多少？回溯。 Xi 平方到底是什么？它和 Xi 是一样的。</p><p>We have a formula for the variance that tells us that we can find the variance by proceeding this way. That’s
            a
            formula that’s generally true for variances. Why is this easier? What’s the expected value of Xi squared?
            Backtrack. What is Xi squared, after all? It’s the same thing as Xi.</p>
        <p>由于 Xi 的取值为 0 和 1，因此 Xi 平方也取相同的值，即 0 和 1。因此 Xi 平方的期望值与 Xi 的期望值相同，等于 p。而 Xi 平方的期望值是 p 平方，因此我们得到最终答案，即 p 乘以 (1 p)。</p><p>Since Xi takes value 0 and 1, Xi squared also takes the same values, 0 and 1. So the expected value of Xi
            squared
            is the same as the expected value of Xi, which is equal to p.&nbsp;And the expected value of Xi squared is p
            squared,
            so we get the final answer, p times (1 p).</p>
        <p>如果您要仔细研究并取消此处混乱的表达式，那么在一行之后您也会得到相同的公式。但这说明，使用此公式计算方差，有时结果会更快一些。最后，我们开始了吗？我们也可以计算随机变量 X 的方差吗？</p><p>If you were to work through and do the cancellations in this messy expression here, after one line you would
            also
            get to the same formula. But this sort of illustrates that working with this formula for the variance,
            sometimes
            things work out a little faster. Finally, are we in business? Can we calculate the variance of the random
            variable X as well?</p>
        <p>好吧，我们有一条规则，对于独立随机变量，总和的方差就是方差的总和。因此，要找到 X 的方差，我们只需将 Xi 的方差相加。我们有 n 个 Xi，每个 Xi 的方差为 p_n 倍（1 p）。这样就完成了。这样，我们就计算出了二项式随机变量的均值和方差。</p><p>Well, we have the rule that for independent random variables, the variance of the sum is the sum of the
            variances. So to find the variance of X, we just need to add the variances of the Xi’s. We have n Xi’s, and
            each
            one of them has variance p_n times (1 p). And we are done. So this way, we have calculated both the mean and
            the
            variance of the binomial random variable.</p>
        <p>看看这个特定的公式并看看它告诉我们什么很有趣。如果你要绘制 X 的方差作为 p 的函数，它具有这种形状。最大值在这里是 1/2。当 p 等于 0 时，p 乘以 (1 p) 为 0。当 p 等于 1 时，它是一个二次函数，所以它必须具有这种特定的形状。那么它告诉我们什么呢？
        </p><p>It’s interesting to look at this particular formula and see what it tells us. If you are to plot the variance
            of
            X as a function of p, it has this shape. And the maximum is here at 1/2. p times (1 p) is 0 when p is equal
            to
            0. And when p equals to 1, it’s a quadratic, so it must have this particular shape. So what does it tell us?
        </p>
        <p>如果你将方差视为不确定性的度量，它会告诉你，当硬币是公平的时候，抛硬币的不确定性最大。当 p 等于 1/2 时，随机性最大。这有点直观。另一方面，如果我告诉你硬币的偏向性极强，p 非常接近 1，这意味着它几乎总是会抛出正面，那么这就是方差低的情况。</p><p>If you think about variance as a measure of uncertainty, it tells you that coin flips are most uncertain when
            your coin is fair. When p is equal to 1/2, that’s when you have the most randomness. And this is kind of
            intuitive. if on the other hand I tell you that the coin is extremely biased, p very close to 1, which means
            it
            almost always gives you heads, then that would be a case of low variance.</p>
        <p>结果的变异性很小。对于将会发生什么几乎没有不确定性。大多数情况下正面朝上，偶尔也会出现反面。所以 p 等于 1/2。公平硬币，从某种意义上说，是所有硬币中最不确定的硬币。它对应于最大的方差。它对应于分布最广的 X。</p><p>There’s low variability in the results. There’s little uncertainty about what’s going to happen. It’s going
            to be
            mostly heads with some occasional tails. So p equals 1/2. Fair coin, that’s the coin which is the most
            uncertain
            of all coins, in some sense. And it corresponds to the biggest variance. It corresponds to an X that has the
            widest distribution.</p>
        <p>现在我们运气不错，可以用简单的方法计算出如此复杂的总和，让我们试试运气，做一道这种口味的题目，但要难一点。所以你去参加那些老式的鸡尾酒会。所有男性至少都会戴上那些看起来一模一样的标准大帽子。他们走进来时会检查一下。</p><p>Now that we’re on a roll and we can calculate such hugely complicated sums in simple ways, let us try to push
            our
            luck and do a problem with this flavor, but a little harder than that. So you go to one of those old
            fashioned
            cocktail parties. All males at least will have those standard big hats which look identical. They check them
            in
            when they walk in.</p>
        <p>当他们走出去时，由于他们看起来非常相似，他们就随便挑一顶帽子回家。所以 n 个人完全随机地挑选帽子，然后离开。问题是，有多少人最终偶然或幸运地拿回了自己的帽子，和他们入住时一模一样。好吧，首先我们所说的完全随机是什么意思？</p><p>And when they walk out, since they look pretty identical, they just pick a random hat and go home. So n
            people,
            they pick their hats completely at random, quote, unquote, and then leave. And the question is, to say
            something
            about the number of people who end up, by accident or by luck, to get back their own hat, the exact same hat
            that they checked in. OK, first what do we mean completely at random?</p>
        <p>完全随机，我们基本上是指帽子的任何排列都是同样可能的。任何将这 n 顶帽子分配给 n 个人的方法，任何特定方法都与其他方法一样可能。因此，帽子和人之间是完全对称的。</p><p>Completely at random, we basically mean that any permutation of the hats is equally likely. Any way of
            distributing those n hats to the n people, any particular way is as likely as any other way. So there’s
            complete
            symmetry between hats and people.</p>
        <h2 id="expected-value">预期价值</h2><h2>Expected Value</h2>
        <p>因此，我们要做的是计算这个随机变量 X 的期望值和方差。让我们从期望值开始。让我们重用二项式案例中的技巧。</p><p>So what we want to do is to calculate the expected value and the variance of this random variable X. Let’s
            start
            with the expected value. Let’s reuse the trick from the binomial case.</p>
        <p>因此，挑选的帽子总数，我们将把挑选的帽子总数视为 (0,1) 个随机变量的总和。X1 告诉我们第 1 个人是否拿回了自己的帽子。如果他们拿回了，我们记录为 1。X2，同样的事情。将所有 X 相加就是我们得到多少个 1，这表示有多少人选择了自己的帽子。</p><p>So total number of hats picked, we’re going to think of total number of hats picked as a sum of (0,1) random
            variables. X1 tells us whether person 1 got their own hat back. If they did, we record a 1. X2, the same
            thing.
            By adding all X’s is how many 1’s did we get, which counts how many people selected their own hats.</p>
        <p>因此，我们将感兴趣的随机变量（即拿回自己帽子的人数）分解为随机变量的总和。这些随机变量同样易于处理，因为它们是二进制的。它们只取两个值。Xi 等于 1 的概率是多少，第 i 个人拿回自己帽子的概率是多少？根据对称性，一共有 n 顶帽子。</p><p>So we broke down the random variable of interest, the number of people who get their own hats back, as a sum
            of
            random variables. And these random variables, again, are easy to handle, because they’re binary. The only
            take
            two values. What’s the probability that Xi is equal to 1, the i th person has a probability that they get
            their
            own hat? There’s n hats by symmetry.</p>
        <p>他们最终得到自己的帽子的概率是 1/n，而不是其他 n 1 顶帽子中的任何一顶。那么 Xi 的期望值是多少？它是 1 乘以 1/n。以 1/n 的概率，你会得到自己的帽子，或者以 1 1/n 的概率得到 0 的值，也就是 1/n。好的，所以我们得到了 Xi 的期望值。
        </p><p>The chance is that they end up getting their own hat, as opposed to any one of the other n 1 hats, is going
            to be
            1/n.&nbsp;So what’s the expected value of Xi? It’s one times 1/n.&nbsp;With probability 1/n, you get your own hat, or
            you
            get a value of 0 with probability 1 1/n, which is 1/n.&nbsp;All right, so we got the expected value of the Xi’s.
        </p>
        <p>请记住，我们想要做的是使用这种分解来计算 X 的期望值？随机变量 Xi 是否彼此独立？您可以尝试通过写下 X 的联合 PMF 来回答这个问题，但我确信您不会成功。但你能直观地思考吗？如果我告诉您有关部分 Xi 的信息，它会为您提供有关其余 Xi 的信息吗？是的。</p><p>And remember, we want to do is to calculate the expected value of X by using this decomposition? Are the
            random
            variables Xi independent of each other? You can try to answer that question by writing down a joint PMF for
            the
            X’s, but I’m sure that you will not succeed. But can you think intuitively? If I tell you information about
            some
            of the Xi’s, does it give you information about the remaining ones? Yeah.</p>
        <p>如果我告诉你，10 个人中有 9 个人拿回了自己的帽子，这能告诉你关于第 10 个人的一些信息吗？是的。如果 9 个人拿回了自己的帽子，那么第 10 个人也一定拿回了自己的帽子。因此，前 9 个随机变量会告诉你关于第 10 个人的一些信息。传递此类信息就是依赖性的情况。好吧，所以随机变量不是独立的。</p><p>If I tell you that out of 10 people, 9 of them got their own hat back, does that tell you something about the
            10th person? Yes. If 9 got their own hat, then the 10th must also have gotten their own hat back. So the
            first 9
            random variables tell you something about the 10th one. And conveying information of this sort, that’s the
            case
            of dependence. All right, so the random variables are not independent.</p>
        <p>我们陷入困境了吗？我们还能计算出 X 的期望值吗？是的，我们可以。我们可以这样做的原因是期望是线性的。随机变量总和的期望就是期望的总和。这始终是正确的。没有使用独立性假设来应用该规则。因此，我们可以得出 X 的期望值是 Xi 的期望值的总和。</p><p>Are we stuck? Can we still calculate the expected value of X? Yes, we can. And the reason we can is that
            expectations are linear. Expectation of a sum of random variables is the sum of the expectations. And that’s
            always true. There’s no independence assumption that’s being used to apply that rule. So we have that the
            expected value of X is the sum of the expected value of the Xi’s.</p>
        <p>这是一个始终正确的属性。你不需要独立性。你不在乎。所以我们添加了 n 个项，每个项的期望值为 1/n。最终答案是 1。因此，在随机选择帽子的 100 人中，平均而言，你预计其中只有一个人最终会拿回自己的帽子。非常好。</p><p>And this is a property that’s always true. You don’t need independence. You don’t care. So we’re adding n
            terms,
            each one of which has expected value 1/n.&nbsp;And the final answer is 1. So out of the 100 people who selected
            hats
            at random, on the average, you expect only one of them to end up getting their own hat back. Very good.</p>
        <p>既然到目前为止我们都很成功，那么让我们尝试看看我们是否也能成功计算方差。当然，我们会的。但这会稍微复杂一些。它会稍微复杂一些的原因是 Xi 不是独立的，所以总和的方差与方差的总和不同。所以仅仅找到 Xi 的方差是不够的。</p><p>So since we are succeeding so far, let’s try to see if we can succeed in calculating the variance as well.
            And of
            course, we will. But it’s going to be a little more complicated. The reason it’s going to be a little more
            complicated is because the Xi’s are not independent, so the variance of the sum is not the same as the sum
            of
            the variances. So it’s not enough to find the variances of the Xi’s.</p>
        <p>我们还得做更多的工作。下面是涉及的内容。让我们从方差的一般公式开始，正如我之前提到的，这通常是计算方差的更简单的方法。所以我们需要计算 X 平方的期望值，并从中减去期望平方。好吧，我们已经找到了 X 的期望值。</p><p>We’ll have to do more work. And here’s what’s involved. Let’s start with the general formula for the
            variance,
            which, as I mentioned before, it’s usually the simpler way to go about calculating variances. So we need to
            calculate the expected value for X squared, and subtract from it the expectation squared. Well, we already
            found
            the expected value of X.</p>
        <p>它等于 1。所以 1 的平方等于 1。所以我们剩下的任务是计算 X 平方的期望值，即随机变量 X 平方。让我们尝试遵循相同的想法。将这个混乱的随机变量 X 平方写成希望更简单的随机变量的总和。所以 X 是 Xi 的总和，所以你对它的两边求平方。然后展开右边。</p><p>It’s equal to 1. So 1 squared gives us just 1. So we’re left with the task of calculating the expected value
            of X
            squared, the random variable X squared. Let’s try to follow the same idea. Write this messy random variable,
            X
            squared, as a sum of hopefully simpler random variables. So X is the sum of the Xi’s, so you square both
            sides
            of this. And then you expand the right hand side.</p>
        <p>当你展开右边时，你会得到这里出现的项的平方。然后你会得到所有的交叉项。对于每一对不同的 (i,j)，i 不同于 j，在和中都会有一个交叉项。那么现在，为了计算 X 平方的期望值，我们的任务简化为什么？它简化为计算这个项的期望值和计算那个项的期望值。</p><p>When you expand the right hand side, you get the squares of the terms that appear here. And then you get all
            the
            cross terms. For every pair of (i,j) that are different, I different than j, you’re going to have a cross
            term
            in the sum. So now, in order to calculate the expected value of X squared, what does our task reduce to? It
            reduces to calculating the expected value of this term and calculating the expected value of that term.</p>
        <p>让我们一次解决一个问题。Xi 平方的期望值是多少？和前面一样。Xi 取值为 0 或 1，因此 Xi 平方取的值也一样，0 或 1。所以这是最简单的。它与 Xi 的期望值相同，我们已经知道它是 1/n。所以这给了我们第一个贡献。</p><p>So let’s do them one at a time. Expected value of Xi squared, what is it going to be? Same trick as before.
            Xi
            takes value 0 or 1, so Xi squared takes just the same values, 0 or 1. So that’s the easy one. That’s the
            same as
            expected value of Xi, which we already know to be 1/n.&nbsp;So this gives us a first contribution down here.</p>
        <p>这一项的期望值是多少？求和中有 n 项。每一项的期望值为 1/n。所以我们完成了拼图的一部分。现在让我们处理拼图的第二部分。让我们求出 Xi 乘以 Xj 的期望值。</p><p>The expected value of this term is going to be what? We have n terms in the summation. And each one of these
            terms has an expectation of 1/n.&nbsp;So we did a piece of the puzzle. So now let’s deal with the second piece of
            the
            puzzle. Let’s find the expected value of Xi times Xj.</p>
        <p>现在根据对称性，无论 I 和 j 是什么，Xi 乘以 Xj 的期望值都将相同。所以让我们只考虑 X1 和 X2，并尝试找到 X1 和 X2 的期望值。X1 乘以 X2 是一个随机变量。它取什么值？只有 0 或 1？</p><p>Now by symmetry, the expected value of Xi times Xj is going to be the same no matter what I and j you see. So
            let’s just think about X1 and X2 and try to find the expected value of X1 and X2. X1 times X2 is a random
            variable. What values does it take? Only 0 or 1?</p>
        <p>由于 X1 和 X2 为 0 或 1，因此它们的乘积只能取值 0 或 1。因此，要找到这个随机变量的概率分布，只需找到它取值 1 的概率就足够了。那么，X1 乘以 X2 等于 1 意味着什么呢？</p><p>Since X1 and X2 are 0 or 1, their product can only take the values of 0 or 1. So to find the probability
            distribution of this random variable, it’s just sufficient to find the probability that it takes the value
            of 1.
            Now, what does X1 times X2 equal to 1 mean?</p>
        <p>这意味着 X1 为 1，X2 为 1。只有当它们两个都为 1 时，才能得到 1 的乘积。因此，这相当于说，人 1 和人 2 都挑选了自己的帽子。</p><p>It means that X1 was 1 and X2 was 1. The only way that you can get a product of 1 is if both of them turned
            out
            to be 1’s. So that’s the same as saying, persons 1 and 2 both picked their own hats.</p>
        <p>第一个人和第二个人都挑选自己的帽子的概率是两件事发生的概率，即第一件事发生乘以第二件事发生的条件概率（假设第一件事发生）。换句话说，这是第一个人挑选自己的帽子的概率乘以第二个人挑选自己的帽子的概率（假设第一个人已经挑选了自己的帽子）。
        </p><p>The probability that person 1 and person 2 both pick their own hats is the probability of two things
            happening,
            which is the product of the first thing happening times the conditional probability of the second, given
            that
            the first happened. And in words, this is the probability that the first person picked their own hat times
            the
            probability that the second person picks their own hat, given that the first person already picked their
            own.
        </p>
        <p>那么第一个人挑选自己帽子的概率是多少？我们知道是 1/n。那么第二个人呢？如果我告诉你一个人有自己的帽子，然后那个人拿着帽子走开了，从第二个人的角度来看，还有 n 1 个人在看 n 1 顶帽子。而且他们随机挑选帽子。我挑选自己的帽子的概率是多少？</p><p>So what’s the probability that the first person picks their own hat? We know that it’s 1/n.&nbsp;Now, how about
            the
            second person? If I tell you that one person has their own hat, and that person takes their hat and goes
            away,
            from the point of view of the second person, there’s n 1 people left looking at n 1 hats. And they’re
            getting
            just hats at random. What’s the chance that I will get my own?</p>
        <p>是 1/n 1。所以想象一下，第 1 个人去随机挑选一顶帽子，碰巧是他自己的，然后就离开了。剩下 n 1 个人，一共有 n 1 顶帽子。第 2 个人去随机挑选一顶帽子，概率是 1/n 1，他会挑选自己的帽子。所以现在这个随机变量的期望值还是那个数字，因为这是一个 0,1 随机变量。</p><p>It’s 1/n 1. So think of them as person 1 goes, picks a hat at random, it happens to be their own, and it
            leaves.
            You’re left with n 1 people, and there are n 1 hats out there. Person 2 goes and picks a hat at random, with
            probability 1/n 1, is going to pick his own hat. So the expected value now of this random variable is,
            again,
            that same number, because this is a 0,1 random variable.</p>
        <p>因此，当 I 不同于 j 时，这与 Xi 乘以 Xj 的期望值相同。因此，这里剩下要做的就是添加这些项的期望值。这些项中的每一个都有一个期望值，即 1/n 乘以 (1/n 1)。我们有多少项？我们要加多少项？它是 n 平方 n。当你展开二次项时，总共有 n 个平方项。有些是自项，其中有 n 个。</p><p>So this is the same as expected value of Xi times Xj when I different than j. So here, all that’s left to do
            is
            to add the expectations of these terms. Each one of these terms has an expected value that’s 1/n times (1/n
            1).
            And how many terms do we have? How many of these are we adding up? It’s n squared n.&nbsp;When you expand the
            quadratic, there’s a total of n squared terms. Some are self terms, n of them.</p>
        <p>剩下的项数是 n 平方 n。所以这里我们得到了 n 平方 n 项。所以我们需要在这里乘以 n 平方 n。当你意识到这个数字是 1，并且你意识到它与分母相同时，你会得到答案，即 X 平方的期望值等于 2。然后，最后回到上面的公式，我们得到 X 平方的期望值，即 2 1，方差正好等于 1。所以这个随机变量的方差，即拿回自己帽子的人数，也等于 1，等于平均值​​。</p><p>And the remaining number of terms is n squared n.&nbsp;So here we got n squared n terms. And so we need to
            multiply
            here with n squared n.&nbsp;And after you realize that this number here is 1, and you realize that this is the
            same
            as the denominator, you get the answer that the expected value of X squared equals 2. And then, finally
            going up
            to the top formula, we get the expected value of X squared, which is 2 1, and the variance is just equal to
            1.
            So the variance of this random variable, number of people who get their own hats back, is also equal to 1,
            equal
            to the mean.</p>
        <p>看起来很神奇。为什么会这样？好吧，这两个数字为什么会相同，这背后还有更深层次的解释。但这可能要等上几章才能真正解释清楚。所以我就到此为止了。</p><p>Looks like magic. Why is this the case? Well, there’s a deeper explanation why these two numbers should come
            out
            to be the same. But this is something that would probably have to wait a couple of chapters before we could
            actually explain it. And so I’ll stop here.</p>
        <h1 id="continuous-random-variables">8.连续随机变量</h1><h1>8. Continuous Random Variables</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 50:29 <a href="https://youtube.com/watch?v=mHfn_7ym6to">https://youtube.com/watch?v=mHfn_7ym6to</a></p><p> 12 years ago (Nov 10, 2012) — 50:29 <a href="https://youtube.com/watch?v=mHfn_7ym6to">https://youtube.com/watch?v=mHfn_7ym6to</a></p>
        <h2 id="unknown-133">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。约翰·齐西克利斯：好的。我们可以开始了。早上好。我们现在要开始一个新单元。在接下来的几节课中，我们将讨论连续随机变量。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. JOHN TSITSIKLIS: OK. We can start.
            Good
            morning. So we’re going to start now a new unit. For the next couple of lectures, we will be talking about
            continuous random variables.</p>
        <p>所以这是新内容，不会出现在测验中。下周你将有一个长假，没有任何讲座，只有测验、背诵和辅导。那么这个新单元会发生什么呢？
        </p><p>So this is new material which is not going to be in the quiz. You are going to have a long break next week
            without any lecture, just a quiz and recitation and tutorial. So what’s going to happen in this new unit?
        </p>
        <p>基本上，我们想做我们为离散随机变量所做的一切，重新引入同类概念，但要看看它们如何应用，以及需要如何修改它们才能讨论取连续值的随机变量。在某种程度上，它们都是一样的。在某种程度上，这要困难得多，因为当事物是连续的时候，微积分就派上用场了。</p><p>Basically, we want to do everything that we did for discrete random variables, reintroduce the same sort of
            concepts but see how they apply and how they need to be modified in order to talk about random variables
            that
            take continuous values. At some level, it’s all the same. At some level, it’s quite a bit harder because
            when
            things are continuous, calculus comes in.</p>
        <h2 id="unknown-134">未知</h2><h2>Unknown</h2>
        <p>因此，有时你需要在课外进行一些计算，这需要你多花点心思。至于新概念，今天不会讲太多，我们只是讲一些我们做过的事情的类似内容。我们将介绍累积分布函数的概念，它使我们能够一次性处理离散和连续随机变量。</p><p>So the calculations that you have to do on the side sometimes need a little bit more thinking. In terms of
            new
            concepts, there’s not going to be a whole lot today, some analogs of things we have done. We’re going to
            introduce the concept of cumulative distribution functions, which allows us to deal with discrete and
            continuous
            random variables, all of them in one shot.</p>
        <p>最后，介绍一种著名的连续随机变量，即正态随机变量。好吧，故事是怎样的呢？连续随机变量是在连续体上取值的随机变量。因此，随机变量的数值可以是任何实数。它们不只在离散集合中取值。所以我们有样本空间。实验开始了。我们得到一些欧米茄，即样本空间中的样本点。</p><p>And finally, introduce a famous kind of continuous random variable, the normal random variable. OK, so what’s
            the
            story? Continuous random variables are random variables that take values over the continuum. So the
            numerical
            value of the random variable can be any real number. They don’t take values just in a discrete set. So we
            have
            our sample space. The experiment happens. We get some omega, a sample point in the sample space.</p>
        <p>一旦确定了该点，它就确定了随机变量的数值。请记住，随机变量是样本空间上的函数。你选择一个样本点。这决定了随机变量的数值。因此，该数值将是该线上的某个实数。现在我们想谈谈随机变量的分布。</p><p>And once that point is determined, it determines the numerical value of the random variable. Remember, random
            variables are functions on the sample space. You pick a sample point. This determines the numerical value of
            the
            random variable. So that numerical value is going to be some real number on that line. Now we want to say
            something about the distribution of the random variable.</p>
        <h2 id="unknown-135">未知</h2><h2>Unknown</h2>
        <p>我们想说明在某种意义上哪些值比其他值更有可能发生。例如，您可能对某个特定事件感兴趣，即随机变量在 a 到 b 的区间内取值的事件。我们想说明该事件发生的概率。原则上，这是如何做到的？</p><p>We want to say which values are more likely than others to occur in a certain sense. For example, you may be
            interested in a particular event, the event that the random variable takes values in the interval from a to
            b.
            And we want to say something about the probability of that event. In principle, how is this done?</p>
        <p>你回到样本空间，找到所有那些结果，这些结果的随机变量值恰好位于该区间。随机变量落在此处的概率与所有使随机变量落在此处的结果的概率相同。因此，原则上，你可以在原始样本空间上工作，找到此事件的概率，这样就大功告成了。</p><p>You go back to the sample space, and you find all those outcomes for which the value of the random variable
            happens to be in that interval. The probability that the random variable falls here is the same as the
            probability of all outcomes that make the random variable to fall in there. So in principle, you can work on
            the
            original sample space, find the probability of this event, and you would be done.</p>
        <p>但与第 2 章中的情况类似，我们希望将样本空间推到后台，直接在实轴上工作并讨论概率。因此，我们现在需要一种方法来指定概率，以及它们如何沿着实轴聚集或排列。那么我们对离散随机变量做了什么？我们引入了 PMF，即概率质量函数。</p><p>But similar to what happened in chapter 2, we want to kind of push the sample space in the background and
            just
            work directly on the real axis and talk about probabilities up here. So we want now a way to specify
            probabilities, how they are bunched together, or arranged, along the real line. So what did we do for
            discrete
            random variables? We introduced PMFs, probability mass functions.</p>
        <h2 id="unknown-136">未知</h2><h2>Unknown</h2>
        <p>我们描述随机变量的方式是说这个点上面有这么多的质量，那个点上面有这么多的质量，等等。所以我们分配了总量为 1 个单位的概率。我们将其分配给不同的质量，并将其放在实轴上的不同点上。</p><p>And the way that we described the random variable was by saying this point has so much mass on top of it,
            that
            point has so much mass on top of it, and so on. And so we assigned a total amount of 1 unit of probability.
            We
            assigned it to different masses, which we put at different points on the real axis.</p>
        <p>如果有人给你一磅离散物质，一磅质量的小块，你就会这样做。然后你把这些小块放在几个点上。现在，在连续的情况下，这个概率质量的总单位不只是位于离散点上，而是遍布整个实轴。所以现在我们将有一个遍布实轴顶部的质量单位。</p><p>So that’s what you do if somebody gives you a pound of discrete stuff, a pound of mass in little chunks. And
            you
            place those chunks at a few points. Now, in the continuous case, this total unit of probability mass does
            not
            sit just on discrete points but is spread all over the real axis. So now we’re going to have a unit of mass
            that
            spreads on top of the real axis.</p>
        <p>我们如何描述连续扩散的质量？我们描述它们的方式是通过指定密度。也就是说，这里的质量有多厚？那里的质量有多稠密？这正是我们要做的。我们将引入概率密度函数的概念，它告诉我们概率如何在实轴的不同部分累积。</p><p>How do we describe masses that are continuously spread? The way we describe them is by specifying densities.
            That
            is, how thick is the mass that’s sitting here? How dense is the mass that’s sitting there? So that’s exactly
            what we’re going to do. We’re going to introduce the concept of a probability density function that tells us
            how
            probabilities accumulate at different parts of the real axis.</p>
        <h2 id="unknown-137">未知</h2><h2>Unknown</h2>
        <p>这是可能的概率密度函数的一个例子或图片。这个密度函数直观地传达了什么？嗯，这些 x 相对不太可能发生。那些 x 发生的可能性更大，因为密度更高。现在，对于更正式的定义，我们将说，如果一个随机变量 X 可以用密度函数以以下方式描述，那么它就是连续的。
        </p><p>So here’s an example or a picture of a possible probability density function. What does that density function
            kind of convey intuitively? Well, that these x’s are relatively less likely to occur. Those x’s are somewhat
            more likely to occur because the density is higher. Now, for a more formal definition, we’re going to say
            that a
            random variable X is said to be continuous if it can be described by a density function in the following
            sense.
        </p>
        <p>我们有一个密度函数。我们通过找到位于该区间顶部的曲线下面积来计算落入区间的概率。所以这有点像连续随机变量的定义关系。这是一个隐式定义。它告诉我们，如果我们能用这种方式计算概率，那么随机变量就是连续的。所以落入这个区间的概率就是这个曲线下的面积。</p><p>We have a density function. And we calculate probabilities of falling inside an interval by finding the area
            under the curve that sits on top of that interval. So that’s sort of the defining relation for continuous
            random
            variables. It’s an implicit definition. And it tells us a random variable is continuous if we can calculate
            probabilities this way. So the probability of falling in this interval is the area under this curve.</p>
        <p>从数学上讲，它是密度在这个特定区间的积分。如果密度恰好在这个区间内保持不变，那么曲线下的面积就是区间长度乘以密度高度，这在某种程度上是有道理的。现在，因为密度不是恒定的，而是会四处移动，所以你需要写下一个积分。</p><p>Mathematically, it’s the integral of the density over this particular interval. If the density happens to be
            constant over that interval, the area under the curve would be the length of the interval times the height
            of
            the density, which sort of makes sense. Now, because the density is not constant but it kind of moves
            around,
            what you need is to write down an integral.</p>
        <h2 id="unknown-138">未知</h2><h2>Unknown</h2>
        <p>现在，这个公式与离散随机变量的公式非常相似。对于离散随机变量，如何计算这个概率？查看此间隔内的所有 x。然后将该范围内的概率质量函数相加。因此，仅供比较，这将是离散情况的公式。从 a 到 b 的间隔内的所有 x 除以概率质量函数之和。</p><p>Now, this formula is very much analogous to what you would do for discrete random variables. For a discrete
            random variable, how do you calculate this probability? You look at all x’s in this interval. And you add
            the
            probability mass function over that range. So just for comparison, this would be the formula for the
            discrete
            case. the sum over all x’s in the interval from a to b over the probability mass function.</p>
        <p>这里有一个句法类比，当我们处理连续随机变量时，它将是一个持久的主题。求和被积分取代。在离散情况下，你加法。在连续情况下，你积分。质量函数被密度函数取代。因此，你可以从离散情况下取出几乎任何公式，并将其转换为该公式的连续类比，正如我们将要看到的。好的。
        </p><p>And there is a syntactic analogy that’s happening here and which will be a persistent theme when we deal with
            continuous random variables. Sums get replaced by integrals. In the discrete case, you add. In the
            continuous
            case, you integrate. Mass functions get replaced by density functions. So you can take pretty much any
            formula
            from the discrete case and translate it to a continuous analog of that formula, as we’re going to see. OK.
        </p>
        <p>现在让我们以此作为我们的模型。如果我们有一个连续随机变量，那么随机变量取特定值的概率是多少？嗯，情况就是这样。这是一个平凡区间的情况，其中两个端点重合。所以它将是从 a 到自身的积分。所以你只是在一个点上积分。</p><p>So let’s take this now as our model. What is the probability that the random variable takes a specific value
            if
            we have a continuous random variable? Well, this would be the case. It’s a case of a trivial interval, where
            the
            two end points coincide. So it would be the integral from a to itself. So you’re integrating just over a
            single
            point.</p>
        <h2 id="unknown-139">未知</h2><h2>Unknown</h2>
        <p>现在，当你对一个点进行积分时，积分就是 0。如果你只看一个点，曲线下的面积就是 0。连续随机变量的一大特性是任何单个点的概率都是 0。特别是，当你看密度值时，密度并不能告诉你该点的概率。该点本身的概率为 0。</p><p>Now, when you integrate over a single point, the integral is just 0. The area under the curve, if you’re only
            looking at a single point, it’s 0. So big property of continuous random variables is that any individual
            point
            has 0 probability. In particular, when you look at the value of the density, the density does not tell you
            the
            probability of that point. The point itself has 0 probability.</p>
        <p>因此，密度会告诉你一些不同的东西。我们很快就会看到那是什么。在此之前，密度可以是任意函数吗？几乎可以，但不完全可以。我们想要两件事。首先，由于密度用于计算概率，并且概率必须为非负数，因此密度也应该是非负数。否则，你会得到负概率，这不是一件好事。</p><p>So the density tells you something a little different. We are going to see shortly what that is. Before we
            get
            there, can the density be an arbitrary function? Almost, but not quite. There are two things that we want.
            First, since densities are used to calculate probabilities, and since probabilities must be non negative,
            the
            density should also be non negative. Otherwise you would be getting negative probabilities, which is not a
            good
            thing.</p>
        <p>所以这是任何密度函数都应遵循的基本属性。我们需要的第二个属性是整个实线的总体概率应该等于 1。所以如果你问我，x 落在负无穷和正无穷之间的概率是多少，嗯，我们肯定 x 会落在那个范围内。
        </p><p>So that’s a basic property that any density function should obey. The second property that we need is that
            the
            overall probability of the entire real line should be equal to 1. So if you ask me, what is the probability
            that
            x falls between minus infinity and plus infinity, well, we are sure that x is going to fall in that range.
        </p>
        <h2 id="unknown-140">未知</h2><h2>Unknown</h2>
        <p>所以该事件的概率应该是 1。所以介于负无穷和正无穷之间的概率应该是 1，这意味着从负无穷到正无穷的积分应该是 1。所以这告诉我们，总概率的 1 个单位分布在我们的空间中。现在，直观地思考密度函数的作用的最佳方式是什么？</p><p>So the probability of that event should be 1. So the probability of being between minus infinity and plus
            infinity should be 1, which means that the integral from minus infinity to plus infinity should be 1. So
            that
            just tells us that there’s 1 unit of total probability that’s being spread over our space. Now, what’s the
            best
            way to think intuitively about what the density function does?</p>
        <p>我发现最自然、最容易传达密度含义的解释是查看小间隔的概率。因此，让我们在这里取一个 x，然后 x 加上紧挨着它的 delta。因此 delta 是一个小数。让我们看看我们得到该范围内的值的事件的概率。
        </p><p>The interpretation that I find most natural and easy to convey the meaning of a density is to look at
            probabilities of small intervals. So let us take an x somewhere here and then x plus delta just next to it.
            So
            delta is a small number. And let’s look at the probability of the event that we get a value in that range.
        </p>
        <p>对于连续随机变量，我们找到落在该范围内的概率的方法是对该范围内的密度进行积分。所以我们画了这张图。我们想取这条曲线下的面积。现在，如果 delta 是一个相当小的数字会发生什么？如果 delta 非常小，我们的密度在该范围内不会发生太大变化。所以你可以假装密度大致恒定。</p><p>For continuous random variables, the way we find the probability of falling in that range is by integrating
            the
            density over that range. So we’re drawing this picture. And we want to take the area under this curve. Now,
            what
            happens if delta is a fairly small number? If delta is pretty small, our density is not going to change much
            over that range. So you can pretend that the density is approximately constant.</p>
        <h2 id="unknown-141">未知</h2><h2>Unknown</h2>
        <p>因此，要找到曲线下的面积，只需将底乘以高即可。而在该间隔中将高取到什么位置并不重要，因为密度在该间隔内变化不大。因此积分变为底乘以高。因此，对于小间隔，小间隔的概率大约为密度乘以 delta。因此，密度本质上为我们提供了小间隔的概率。</p><p>And so to find the area under the curve, you just take the base times the height. And it doesn’t matter where
            exactly you take the height in that interval, because the density doesn’t change very much over that
            interval.
            And so the integral becomes just base times the height. So for small intervals, the probability of a small
            interval is approximately the density times delta. So densities essentially give us probabilities of small
            intervals.</p>
        <p>如果你想换个角度思考，你可以把这里的增量发送到那里的分母。这告诉你，密度是小间隔的单位长度概率。所以密度的单位是单位长度概率。密度不是概率。它们是概率累积的速率，单位长度概率。</p><p>And if you want to think about it a little differently, you can take that delta from here and send it to the
            denominator there. And what this tells you is that the density is probability per unit length for intervals
            of
            small length. So the units of density are probability per unit length. Densities are not probabilities. They
            are
            rates at which probabilities accumulate, probabilities per unit length.</p>
        <p>而且由于密度不是概率，它们不必小于 1。普通概率总是必须小于 1。但密度是另一种东西。在某些地方，密度可能会变得很大。在某些地方，密度甚至可能会爆炸。只要曲线下的总面积为 1，除此之外，曲线可以做任何它想做的事情。</p><p>And since densities are not probabilities, they don’t have to be less than 1. Ordinary probabilities always
            must
            be less than 1. But density is a different kind of thing. It can get pretty big in some places. It can even
            sort
            of blow up in some places. As long as the total area under the curve is 1, other than that, the curve can do
            anything that it wants.</p>
        <h2 id="unknown-142">未知</h2><h2>Unknown</h2>
        <p>现在，密度为我们规定了区间的概率。有时我们可能想找到更一般集合的概率。我们该怎么做呢？好吧，对于好集合，你只需将密度积分到该好集合上。我并没有完全定义“好”是什么意思。这是概率论中一个相当技术性的话题。但就我们的目的而言，通常我们会将 b 视为区间的并集。</p><p>Now, the density prescribes for us the probability of intervals. Sometimes we may want to find the
            probability of
            more general sets. How would we do that? Well, for nice sets, you will just integrate the density over that
            nice
            set. I’m not quite defining what “nice” means. That’s a pretty technical topic in the theory of probability.
            But
            for our purposes, usually we will take b to be something like a union of intervals.</p>
        <p>那么如何找到落在两个区间并集的概率呢？嗯，你找到落在该区间的概率加上落在该区间的概率。所以它是该区间的积分加上该区间的积分。你可以认为这只是对两个区间的并集进行积分。</p><p>So how do you find the probability of falling in the union of two intervals? Well, you find the probability
            of
            falling in that interval plus the probability of falling in that interval. So it’s the integral over this
            interval plus the integral over that interval. And you think of this as just integrating over the union of
            the
            two intervals.</p>
        <p>因此，一旦你能计算出区间概率，通常你就可以开始做生意了，你可以计算出你想要的任何其他东西。因此，概率密度函数是我们可能感兴趣的连续随机变量的任何统计信息的完整描述。好的。现在我们可以开始研究离散随机变量的概念和定义，并将它们转化为连续的情况。</p><p>So once you can calculate probabilities of intervals, then usually you are in business, and you can calculate
            anything else you might want. So the probability density function is a complete description of any
            statistical
            information we might be interested in for a continuous random variable. OK. So now we can start walking
            through
            the concepts and the definitions that we have for discrete random variables and translate them to the
            continuous
            case.</p>
        <h2 id="unknown-143">未知</h2><h2>Unknown</h2>
        <p>第一个重要概念是期望的概念。我们可以从数学定义开始。在这里，我们只需翻译符号即可给出定义。在离散情况下，只要有和，我们现在就写成积分。在有概率质量函数的地方，我们现在就加上概率密度函数。这个公式。你可能在大一物理课上见过。</p><p>The first big concept is the concept of the expectation. One can start with a mathematical definition. And
            here
            we put down a definition by just translating notation. Wherever we have a sum in the discrete case, we now
            write
            an integral. And wherever we had the probability mass function, we now throw in the probability density
            function. This formula. you may have seen it in freshman physics.</p>
        <p>基本上，它再次给出了密度时图像的重心。它是位于概率密度函数下方的物体的重心。因此解释仍然适用。同样，我们对期望含义的概念解释在这种情况下也是有效的。</p><p>Basically, it again gives you the center of gravity of the picture that you have when you have the density.
            It’s
            the center of gravity of the object sitting underneath the probability density function. So that the
            interpretation still applies. It’s also true that our conceptual interpretation of what an expectation means
            is
            also valid in this case.</p>
        <p>也就是说，如果你重复一个实验无数次，每次都抽取随机变量 x 的独立样本，从长远来看，你将得到的平均值应该是期望值。人们可以用手势推理，有点直观，就像我们在离散随机变量的情况下所做的那样。但这也是某种定理。</p><p>That is, if you repeat an experiment a zillion times, each time drawing an independent sample of your random
            variable x, in the long run, the average that you are going to get should be the expectation. One can reason
            in
            a hand waving way, sort of intuitively, the way we did it for the case of discrete random variables. But
            this is
            also a theorem of some sort.</p>
        <h2 id="unknown-144">未知</h2><h2>Unknown</h2>
        <p>这是一条极限定理，我们稍后会在本课中讨论。定义期望并声明期望的解释与之前相同后，我们就可以开始采用您之前见过的任何公式并将其翻译出来。例如，要找到连续随机变量函数的期望值，您不必找到 g(X) 的 PDF 或 PMF。</p><p>It’s a limit theorem that we’re going to visit later on in this class. Having defined the expectation and
            having
            claimed that the interpretation of the expectation is that same as before, then we can start taking just any
            formula you’ve seen before and just translate it. So for example, to find the expected value of a function
            of a
            continuous random variable, you do not have to find the PDF or PMF of g(X).</p>
        <p>您可以直接使用随机变量大写 X 的原始分布。此公式与离散情况相同。总和被积分取代。PMF 被 PDF 取代。具体而言，随机变量的方差再次以相同的方式定义。方差是期望值，即 X 与均值的距离的平均值，然后取平方。</p><p>You can just work directly with the original distribution of the random variable capital X. And this formula
            is
            the same as for the discrete case. Sums get replaced by integrals. And PMFs get replaced by PDFs. And in
            particular, the variance of a random variable is defined again the same way. The variance is the expected
            value,
            the average of the distance of X from the mean and then squared.</p>
        <p>因此，它是取这些数值的随机变量的期望值。与之前的公式相同，积分和 F 而不是求和，以及 P。我们推导的公式或您看到的离散情况的公式都适用于连续情况。例如，方差的有用关系，也就是这个，仍然成立。好的。现在来看一个例子。</p><p>So it’s the expected value for a random variable that takes these numerical values. And same formula as
            before,
            integral and F instead of summation, and the P. And the formulas that we have derived or formulas that you
            have
            seen for the discrete case, they all go through the continuous case. So for example, the useful relation for
            variances, which is this one, remains true. All right. So time for an example.</p>
        <h2 id="unknown-145">未知</h2><h2>Unknown</h2>
        <p>最简单的连续随机变量的例子就是所谓的均匀随机变量。因此，均匀随机变量的密度除了在某个区间之外都是 0。而在该区间内，它是恒定的。它想表达什么？它试图表达这样一个观点：这个范围内的所有 x 都具有相同的可能性。好吧，这并没有说明什么。任何单个 x 的概率都是 0。</p><p>The most simple example of a continuous random variable that there is, is the so called uniform random
            variable.
            So the uniform random variable is described by a density which is 0 except over an interval. And over that
            interval, it is constant. What is it meant to convey? It’s trying to convey the idea that all x’s in this
            range
            are equally likely. Well, that doesn’t say very much. Any individual x has 0 probability.</p>
        <p>所以它传达的信息比这要多一点。它说的是，如果我取一个给定长度 delta 的区间，并取另一个相同长度 delta 的区间，在均匀分布下，这两个区间将具有相同的概率。因此均匀分布意味着相同长度的区间具有相同的概率。因此，没有一个区间比其他区间更有可能发生。</p><p>So it’s conveying a little more than that. What it is saying is that if I take an interval of a given length
            delta, and I take another interval of the same length, delta, under the uniform distribution, these two
            intervals are going to have the same probability. So being uniform means that intervals of same length have
            the
            same probability. So no interval is more likely than any other to occur.</p>
        <p>从这个意义上讲，它传达了某种完全随机性的概念。我们范围内的任何小区间都与任何其他小区间的概率相同。好吧。那么这个密度的公式是什么？我只告诉你范围。高度是多少？嗯，密度下的面积必须等于 1。总概率等于 1。因此，高度不可避免地将是 1 除以 (b 减 a)。</p><p>And in that sense, it conveys the idea of sort of complete randomness. Any little interval in our range is
            equally likely as any other little interval. All right. So what’s the formula for this density? I only told
            you
            the range. What’s the height? Well, the area under the density must be equal to 1. Total probability is
            equal to
            1. And so the height, inescapably, is going to be 1 over (b minus a).</p>
        <h2 id="unknown-146">未知</h2><h2>Unknown</h2>
        <p>这就是使密度积分为 1 的高度。这就是公式。如果你不想在考试中丢一分，你就必须说它也是 0，否则。好的。好吧？这算是完整的答案。这个随机变量的期望值呢？好的。你可以用两种不同的方法找到期望值。一种是从定义开始。</p><p>That’s the height that makes the density integrate to 1. So that’s the formula. And if you don’t want to lose
            one
            point in your exam, you have to say that it’s also 0, otherwise. OK. All right? That’s sort of the complete
            answer. How about the expected value of this random variable? OK. You can find the expected value in two
            different ways. One is to start with the definition.</p>
        <p>因此，您要对感兴趣的范围乘以密度进行积分。然后计算出积分结果。或者您可以更聪明一点。由于重心解释仍然正确，因此它一定是这幅图的重心。重心当然是中点。只要有对称性，平均值就始终是提供 PDF 的图表的中点。好的。</p><p>And so you integrate over the range of interest times the density. And you figure out what that integral is
            going
            to be. Or you can be a little more clever. Since the center of gravity interpretation is still true, it must
            be
            the center of gravity of this picture. And the center of gravity is, of course, the midpoint. Whenever you
            have
            symmetry, the mean is always the midpoint of the diagram that gives you the PDF. OK.</p>
        <p>这就是 X 的期望值。最后，关于方差，嗯，你需要做一些微积分。我们可以写下定义。所以它是一个积分而不是和。随机变量的典型值减去期望值的平方，乘以密度。然后我们积分。</p><p>So that’s the expected value of X. Finally, regarding the variance, well, there you will have to do a little
            bit
            of calculus. We can write down the definition. So it’s an integral instead of a sum. A typical value of the
            random variable minus the expected value, squared, times the density. And we integrate.</p>
        <h2 id="unknown-147">未知</h2><h2>Unknown</h2>
        <p>你进行积分，你会发现它是 (b 减去 a) 除以该数字的平方，这个数字恰好是 12。也许更有趣的是标准差本身。你会发现标准差与该间隔的宽度成正比。这符合我们的直觉，即标准差是为了捕捉我们的分布有多分散的感觉。标准差的单位与随机变量本身相同。</p><p>You do this integral, and you find it’s (b minus a) squared over that number, which happens to be 12. Maybe
            more
            interesting is the standard deviation itself. And you see that the standard deviation is proportional to the
            width of that interval. This agrees with our intuition, that the standard deviation is meant to capture a
            sense
            of how spread out our distribution is. And the standard deviation has the same units as the random variable
            itself.</p>
        <p>所以这很好。你可以根据那幅图合理地解释它。好的，是的。现在，让我们再上一层，思考以下问题。我们有离散情况的公式，连续情况的公式。所以你可以把它们并排写出来。一个有总和，另一个有积分。假设你想提出一个论点，说有些东西对每个随机变量都是正确的。</p><p>So it’s sort of good to. you can interpret it in a reasonable way based on that picture. OK, yes. Now, let’s
            go
            up one level and think about the following. So we have formulas for the discrete case, formulas for the
            continuous case. So you can write them side by side. One has sums, the other has integrals. Suppose you want
            to
            make an argument and say that something is true for every random variable.</p>
        <p>本质上，你需要对离散变量和连续变量分别进行证明。有没有办法用某种统一的符号一次性处理一个随机变量？有没有统一的概念？幸运的是，有一个。这就是随机变量的累积分布函数的概念。这个概念同样适用于离散和连续随机变量。</p><p>You would essentially need to do two separate proofs, for discrete and for continuous. Is there some way of
            dealing with random variables just one at a time, in one shot, using a sort of uniform notation? Is there a
            unifying concept? Luckily, there is one. It’s the notion of the cumulative distribution function of a random
            variable. And it’s a concept that applies equally well to discrete and continuous random variables.</p>
        <h2 id="unknown-148">未知</h2><h2>Unknown</h2>
        <p>所以它是一个我们可以在两种情况下用来描述分布的对象，只使用一种符号。那么定义是什么呢？它是随机变量取值小于某个小数 x 的概率。所以你看图表，你会看到我落在左边的概率是多少。然后你为所有 x 指定这些概率。</p><p>So it’s an object that we can use to describe distributions in both cases, using just one piece of notation.
            So
            what’s the definition? It’s the probability that the random variable takes values less than a certain number
            little x. So you go to the diagram, and you see what’s the probability that I’m falling to the left of this.
            And
            you specify those probabilities for all x’s.</p>
        <p>在连续情况下，您可以使用积分公式计算这些概率。因此，您要从这里积分到 x。在离散情况下，要找到某个点左侧的概率，您需要转到这里，然后再次从左侧添加概率。因此，在连续和离散情况下，累积分布函数的计算方式略有不同。在一种情况下，您需要积分。在另一种情况下，您需要求和。</p><p>In the continuous case, you calculate those probabilities using the integral formula. So you integrate from
            here
            up to x. In the discrete case, to find the probability to the left of some point, you go here, and you add
            probabilities again from the left. So the way that the cumulative distribution function is calculated is a
            little different in the continuous and discrete case. In one case you integrate. In the other, you sum.</p>
        <p>但撇开如何计算，概念是什么，在两种情况下，概念是一样的。让我们看看在这两种情况下累积分布函数的形状。所以这里我们想要的是记录每个小 x 落在 x 左边的概率。让我们从这里开始。</p><p>But leaving aside how it’s being calculated, what the concept is, it’s the same concept in both cases. So
            let’s
            see what the shape of the cumulative distribution function would be in the two cases. So here what we want
            is to
            record for every little x the probability of falling to the left of x. So let’s start here.</p>
        <h2 id="unknown-149">未知</h2><h2>Unknown</h2>
        <p>落到这里左边的概率是 0。0,0.0。一旦我们到达这里并开始向右移动，落到这里左边的概率就是这个小矩形的面积。随着我继续移动，这个小矩形的面积会线性增加。因此，CDF 会线性增加，直到我到达那个点。在那个点，我的 CDF 值是多少？</p><p>Probability of falling to the left of here is 0. 0,0.0. Once we get here and we start moving to the right,
            the
            probability of falling to the left of here is the area of this little rectangle. And the area of that little
            rectangle increases linearly as I keep moving. So accordingly, the CDF increases linearly until I get to
            that
            point. At that point, what’s the value of my CDF?</p>
        <ol type="1">
            <li>我已经积累了所有存在的概率。我已经对它们进行了积分。这个总面积必须等于 1。因此，它达到 1，然后就没有更多的概率可以积累了。它只是停留在 1。所以这里的值等于 1。好的。如果有人给你 CDF，你会如何找到密度？CDF 是密度的积分。因此，密度是 CDF 的导数。</li><li>I have accumulated all the probability there is. I have integrated it. This total area has to be equal
                to 1.
                So it reaches 1, and then there’s no more probability to be accumulated. It just stays at 1. So the
                value
                here is equal to 1. OK. How would you find the density if somebody gave you the CDF? The CDF is the
                integral
                of the density. Therefore, the density is the derivative of the CDF.</li>
        </ol>
        <p>所以你看这幅图并取导数。导数在这里是 0，在这里是 0。它在那里是一个常数，与那个常数相对应。所以更一般地，一个重要的事情是要知道，CDF 的导数等于密度。几乎，但有一点例外。例外是什么？在那些 CDF 没有导数的地方。这里它有一个角。</p><p>So you look at this picture and take the derivative. Derivative is 0 here, 0 here. And it’s a constant up
            there,
            which corresponds to that constant. So more generally, and an important thing to know, is that the
            derivative of
            the CDF is equal to the density. almost, with a little bit of an exception. What’s the exception? At those
            places where the CDF does not have a derivative. here where it has a corner.</p>
        <h2 id="unknown-150">未知</h2><h2>Unknown</h2>
        <p>导数未定义。从某种意义上说，该点的密度也是不明确的。我在端点的密度是 0 还是 1？这并不重要。如果你只改变一个点的密度，它不会影响你计算的任何积分的值。因此，你可以让端点的密度值保持模糊，也可以指定它。</p><p>The derivative is undefined. And in some sense, the density is also ambiguous at that point. Is my density at
            the
            endpoint, is it 0 or is it 1? It doesn’t really matter. If you change the density at just a single point,
            it’s
            not going to affect the value of any integral you ever calculate. So the value of the density at the
            endpoint,
            you can leave it as being ambiguous, or you can specify it.</p>
        <p>这没关系。所以在 CDF 有导数的所有地方，这都是正确的。在那些有角的地方，有时会出现，嗯，你真的不在乎。离散情况怎么样？在离散情况下，CDF 具有更奇特的形状。让我们来计算一下。我们想找到 b 在此处左侧的概率。</p><p>It doesn’t matter. So at all places where the CDF has a derivative, this will be true. At those places where
            you
            have corners, which do show up sometimes, well, you don’t really care. How about the discrete case? In the
            discrete case, the CDF has a more peculiar shape. So let’s do the calculation. We want to find the
            probability
            of b to the left of here.</p>
        <p>那个概率是 0,0.0。一旦我们越过那个点，位于这里左边的概率就是 1/6。所以一旦我们越过点 1，我们就得到 1/6 的概率，这意味着我们这里的跳跃大小是 1/6。现在，问题是。在这个点 1，哪个是 CDF 的正确值？是 0 还是 1/6？是 1/6，因为。</p><p>That probability is 0,0.0. Once we cross that point, the probability of being to the left of here is 1/6. So
            as
            soon as we cross the point 1, we get the probability of 1/6, which means that the size of the jump that we
            have
            here is 1/6. Now, question. At this point 1, which is the correct value of the CDF? Is it 0, or is it 1/6?
            It’s
            1/6 because.</p>
        <h2 id="unknown-151">未知</h2><h2>Unknown</h2>
        <p>您需要仔细查看定义，x 小于或等于小 x 的概率。如果我将小 x 取为 1，则大写 X 小于或等于 1 的概率。因此，它包括 x 等于 1 的事件。因此，它包括此处的这个概率。因此，在跳跃点处，CDF 的正确值将是这个值。</p><p>You need to look carefully at the definitions, the probability of x being less than or equal to little x. If
            I
            take little x to be 1, it’s the probability that capital X is less than or equal to 1. So it includes the
            event
            that x is equal to 1. So it includes this probability here. So at jump points, the correct value of the CDF
            is
            going to be this one.</p>
        <p>现在，当我追踪时，x 向右移动。一旦我越过这个点，我就又增加了 3/6 的概率。因此 3/6 会导致 CDF 跳跃。这决定了新值。最后，一旦我越过最后一个点，我又会得到 2/6 的跳跃。从这两个例子和这些图片中可以得出一个普遍的教训。在这两种情况下，CDF 都定义得很好。</p><p>And now as I trace, x is going to the right. As soon as I cross this point, I have added another 3/6
            probability.
            So that 3/6 causes a jump to the CDF. And that determines the new value. And finally, once I cross the last
            point, I get another jump of 2/6. A general moral from these two examples and these pictures. CDFs are well
            defined in both cases.</p>
        <p>对于连续随机变量的情况，CDF 将是一个连续函数。它从 0 开始。最终达到 1 并平稳变化。好吧，从较小的值连续到较大的值。它只能上升。它不能下降，因为我们在向右移动时积累了越来越多的概率。在离散情况下，它再次从 0 开始，然后达到 1。但它是以阶梯式的方式进行的。</p><p>For the case of continuous random variables, the CDF will be a continuous function. It starts from 0. It
            eventually goes to 1 and goes smoothly. well, continuously from smaller to higher values. It can only go up.
            It
            cannot go down since we’re accumulating more and more probability as we are going to the right. In the
            discrete
            case, again it starts from 0, and it goes to 1. But it does it in a staircase manner.</p>
        <h2 id="unknown-152">未知</h2><h2>Unknown</h2>
        <p>并且，在 PMF 分配正质量的每个位置，您都会得到一个跳跃。因此，CDF 中的跳跃与我们分布中的点质量相关。在连续情况下，我们没有任何点质量，因此我们也没有任何跳跃。现在，除了节省符号之外，我们不必处理两次离散和连续。CDF 实际上为我们提供了更多的灵活性。并非所有随机变量都是连续的或离散的。</p><p>And you get a jump at each place where the PMF assigns a positive mass. So jumps in the CDF are associated
            with
            point masses in our distribution. In the continuous case, we don’t have any point masses, so we do not have
            any
            jumps either. Now, besides saving us notation. we don’t have to deal with discrete and continuous twice.
            CDFs
            give us actually a little more flexibility. Not all random variables are continuous or discrete.</p>
        <p>你可以编造出既不是也不是，或者两者兼而有之的随机变量。举个例子，假设你在玩游戏。在一定的概率下，你会得到一定数量的美元。所以你抛硬币。在概率为 1/2 的情况下，你会得到 1/2 美元的奖励。在概率为 1/2 的情况下，你会被带到一个黑暗的房间，在那里旋转命运之轮。</p><p>You can cook up random variables that are kind of neither or a mixture of the two. An example would be, let’s
            say
            you play a game. And with a certain probability, you get a certain number of dollars in your hands. So you
            flip
            a coin. And with probability 1/2, you get a reward of 1/2 dollars. And with probability 1/2, you are led to
            a
            dark room where you spin a wheel of fortune.</p>
        <p>命运之轮会给你一个 0 到 1 之间的随机奖励。所以这些结果都是可能的。假设你得到的数额是均匀的。所以你抛硬币。根据硬币的结果，你要么得到一个特定的值，要么得到一个连续区间内的值。那么它是什么样的随机变量？它是连续的吗？</p><p>And that wheel of fortune gives you a random reward between 0 and 1. So any of these outcomes is possible.
            And
            the amount that you’re going to get, let’s say, is uniform. So you flip a coin. And depending on the outcome
            of
            the coin, either you get a certain value or you get a value that ranges over a continuous interval. So what
            kind
            of random variable is it? Is it continuous?</p>
        <h2 id="unknown-153">未知</h2><h2>Unknown</h2>
        <p>好吧，连续随机变量将 0 概率分配给各个点。这里的情况是这样吗？不是，因为你获得 1/2 美元的概率是正的。所以我们的随机变量不是连续的。它是离散的吗？它不是离散的，因为我们的随机变量也可以在连续范围内取值。所以我们把这样的随机变量称为混合随机变量。</p><p>Well, continuous random variables assign 0 probability to individual points. Is it the case here? No, because
            you
            have positive probability of obtaining 1/2 dollar. So our random variable is not continuous. Is it discrete?
            It’s not discrete, because our random variable can take values also over a continuous range. So we call such
            a
            random variable a mixed random variable.</p>
        <p>如果你要非常松散地画出它的分布，你可能需要画出像这样的图，这样可以传达出正在发生的事情。所以，就把它想象成一幅放在桌子上的物体的图。我们放置一个重半磅的物体，但它不占用任何空间。所以半磅就放在那个点的上面。</p><p>If you were to draw its distribution very loosely, probably you would want to draw a picture like this one,
            which
            kind of conveys the idea of what’s going on. So just think of this as a drawing of masses that are sitting
            over
            a table. We place an object that weighs half a pound, but it’s an object that takes zero space. So half a
            pound
            is just sitting on top of that point.</p>
        <p>我们再取半磅概率，并将其均匀分布在该间隔内。所以这就像来自质量函数的一部分。这部分看起来更像密度函数。我们只是将它们放在一起。我并不是想将任何形式意义与这幅图联系起来。它只是概率分布的示意图，帮助我们直观地了解正在发生的事情。</p><p>And we take another half pound of probability and spread it uniformly over that interval. So this is like a
            piece
            that comes from mass functions. And that’s a piece that looks more like a density function. And we just
            throw
            them together in the picture. I’m not trying to associate any formal meaning with this picture. It’s just a
            schematic of how probabilities are distributed, help us visualize what’s going on.</p>
        <h2 id="unknown-154">未知</h2><h2>Unknown</h2>
        <p>现在，如果你已经上过系统等课程，你可能已经了解了脉冲函数的概念。你可能会开始说，哦，我应该从数学上把它当作所谓的脉冲函数。但是我们这堂课不需要这个。只要把它想象成一张漂亮的图片，传达出这个特定案例中发生的事情就行了。那么现在，在这种情况下，CDF 会是什么样子？</p><p>Now, if you have taken classes on systems and all of that, you may have seen the concept of an impulse
            function.
            And you my start saying that, oh, I should treat this mathematically as a so called impulse function. But we
            do
            not need this for our purposes in this class. Just think of this as a nice picture that conveys what’s going
            on
            in this particular case. So now, what would the CDF look like in this case?</p>
        <p>不管您拥有哪种随机变量，CDF 总是定义明确的。因此，只要我们能够计算这种概率，它不是连续的、不是离散的这一事实就不应该成为问题。因此，这里向左落的概率为 0。一旦我开始穿过那里，向左落的概率就会随着我走的距离而线性增加。所以我们得到了这个线性增加。</p><p>The CDF is always well defined, no matter what kind of random variable you have. So the fact that it’s not
            continuous, it’s not discrete shouldn’t be a problem as long as we can calculate probabilities of this kind.
            So
            the probability of falling to the left here is 0. Once I start crossing there, the probability of falling to
            the
            left of a point increases linearly with how far I have gone. So we get this linear increase.</p>
        <p>但一旦我越过该点，我就会立即积累另一个 1/2 单位的概率。一旦我积累了 1/2 单位，就意味着我的 CDF 将跳跃 1/2。然后，我仍然以固定的速率继续积累概率，这个速率就是密度。</p><p>But as soon as I cross that point, I accumulate another 1/2 unit of probability instantly. And once I
            accumulate
            that 1/2 unit, it means that my CDF is going to have a jump of 1/2. And then afterwards, I still keep
            accumulating probability at a fixed rate, the rate being the density.</p>
        <h2 id="unknown-155">未知</h2><h2>Unknown</h2>
        <p>我继续以线性速率累积，直到稳定在 1。所以这是一个 CDF，它有某些部分会持续增加。这对应于我们的随机化变量的连续部分。它也有一些地方有离散跳跃。这些区域跳跃对应于我们放置正质量的地方。顺便说一下。好的，是的。所以这个小 0 不应该在那里。</p><p>And I keep accumulating, again, at a linear rate until I settle to 1. So this is a CDF that has certain
            pieces
            where it increases continuously. And that corresponds to the continuous part of our randomize variable. And
            it
            also has some places where it has discrete jumps. And those district jumps correspond to places in which we
            have
            placed a positive mass. And by the. OK, yeah. So this little 0 shouldn’t be there.</p>
        <p>所以让我们把它划掉。好的。最后，我们将利用剩下的时间介绍我们的新朋友。它将是高斯分布或正态分布。所以它是整个概率论中最重要的分布。它起着非常核心的作用。它无处不在。我们将在课程的后面更详细地了解它出现的原因。但快速预览如下。</p><p>So let’s cross it out. All right. So finally, we’re going to take the remaining time and introduce our new
            friend. It’s going to be the Gaussian or normal distribution. So it’s the most important distribution there
            is
            in all of probability theory. It’s plays a very central role. It shows up all over the place. We’ll see
            later in
            the class in more detail why it shows up. But the quick preview is the following.</p>
        <p>如果你有一个现象，你测量了其中的某个量，但是这个量是由大量随机贡献组成的。那么你的随机变量实际上是大量独立的小随机变量的总和。那么不变性，无论小随机变量具有什么样的分布，它们的总和都会近似于正态分布。</p><p>If you have a phenomenon in which you measure a certain quantity, but that quantity is made up of lots and
            lots
            of random contributions. so your random variable is actually the sum of lots and lots of independent little
            random variables. then invariability, no matter what kind of distribution the little random variables have,
            their sum will turn out to have approximately a normal distribution.</p>
        <h2 id="unknown-156">未知</h2><h2>Unknown</h2>
        <p>因此，这使得正态分布在很多情况下非常自然地出现。每当有由许多不同的独立噪音组成的噪音时，最终结果将是一个正常的随机变量。所以我们稍后会回到这个话题。但这是预览评论，基本上是为了论证它很重要。好的。还有一个特殊情况。</p><p>So this makes the normal distribution to arise very naturally in lots and lots of contexts. Whenever you have
            noise that’s comprised of lots of different independent pieces of noise, then the end result will be a
            random
            variable that’s normal. So we are going to come back to that topic later. But that’s the preview comment,
            basically to argue that it’s an important one. OK. And there’s a special case.</p>
        <p>如果你处理的是二项分布，即大量伯努利随机变量的总和，那么如果你有很多、很多的点场，你会再次期望二项分布开始看起来像正态分布。好吧。那么这里涉及的数学是什么？让我们解析正态密度的公式。</p><p>If you are dealing with a binomial distribution, which is the sum of lots of Bernoulli random variables,
            again
            you would expect that the binomial would start looking like a normal if you have many, many. a large number
            of
            point fields. All right. So what’s the math involved here? Let’s parse the formula for the density of the
            normal.</p>
        <p>我们首先从函数 X 平方/2 开始。如果你要绘制 X 平方/2，它就是一条抛物线，形状为 X 平方/2。然后我们该怎么做？我们取它的负指数。因此，当 X 平方/2 为 0 时，负指数为 1。当 X 平方/2 增加时，它的负指数会下降，而且下降得非常快。</p><p>What we start with is the function X squared over 2. And if you are to plot X squared over 2, it’s a
            parabola,
            and it has this shape X squared over 2. Then what do we do? We take the negative exponential of this. So
            when X
            squared over 2 is 0, then negative exponential is 1. When X squared over 2 increases, the negative
            exponential
            of that falls off, and it falls off pretty fast.</p>
        <h2 id="unknown-157">未知</h2><h2>Unknown</h2>
        <p>因此，随着这个数字的上升，密度公式会下降。而且由于指数的下降速度非常快，这意味着该分布的尾部实际上下降得非常快。好的。这解释了正常 PDF 的形状。这个因子 1 除以平方根 2 pi 怎么样？这是从哪里来的？</p><p>So as this goes up, the formula for the density goes down. And because exponentials are pretty strong in how
            quickly they fall off, this means that the tails of this distribution actually do go down pretty fast. OK.
            So
            that explains the shape of the normal PDF. How about this factor 1 over square root 2 pi? Where does this
            come
            from?</p>
        <p>那么，积分必须等于 1。所以你必须去做微积分练习，找到这个减去 X 平方除以 2 的函数的积分，然后算出，我需要在前面放什么常数才能使积分等于 1？你如何评估这个积分？你可以去 Mathematica 或 Wolfram's Alpha 或其他什么软件，它会告诉你它是什么。</p><p>Well, the integral has to be equal to 1. So you have to go and do your calculus exercise and find the
            integral of
            this the minus X squared over 2 function and then figure out, what constant do I need to put in front so
            that
            the integral is equal to 1? How do you evaluate that integral? Either you go to Mathematica or Wolfram’s
            Alpha
            or whatever, and it tells you what it is.</p>
        <p>或者，这是一道非常漂亮的微积分题，你可能在某个时候见过。你再加一个这种指数，再引入极坐标，答案就会非常漂亮。但无论如何，这是你需要的常数，使它积分为 1，并成为合法的密度。我们称之为标准正态分布。对于标准正态分布，预期值是多少？</p><p>Or it’s a very beautiful calculus exercise that you may have seen at some point. You throw in another
            exponential
            of this kind, you bring in polar coordinates, and somehow the answer comes beautifully out there. But in any
            case, this is the constant that you need to make it integrate to 1 and to be a legitimate density. We call
            this
            the standard normal. And for the standard normal, what is the expected value?</p>
        <h2 id="unknown-158">未知</h2><h2>Unknown</h2>
        <p>嗯，对称性，所以它等于 0。方差是多少？嗯，这里没有捷径。你必须再做一次微积分练习。你会发现方差等于 1。好的。所以这是以 0 为中心的法线。那么以不同位置为中心的其他类型的法线呢？所以我们可以做同样的事情。</p><p>Well, the symmetry, so it’s equal to 0. What is the variance? Well, here there’s no shortcut. You have to do
            another calculus exercise. And you find that the variance is equal to 1. OK. So this is a normal that’s
            centered
            around 0. How about other types of normals that are centered at different places? So we can do the same kind
            of
            thing.</p>
        <p>我们可以不以 0 为中心，而是取一个我们想要的中心位置，写下一个二次函数，例如 (X 减去 mu) 的平方，然后取其负指数。这样我们就得到了以 mu 为中心的正常密度。现在，我可能希望控制密度的宽度。为了控制密度的宽度，我可以等效地控制抛物线的宽度。</p><p>Instead of centering it at 0, we can take some place where we want to center it, write down a quadratic such
            as
            (X minus mu) squared, and then take the negative exponential of that. And that gives us a normal density
            that’s
            centered at mu. Now, I may wish to control the width of my density. To control the width of my density,
            equivalently I can control the width of my parabola.</p>
        <p>如果我的抛物线更窄，如果我的抛物线看起来像这样，密度会发生什么变化？它会下降得更快。好的。我如何使我的抛物线变窄或变宽？我通过在这里输入一个常数来实现。因此，通过在这里输入一个 sigma，这会将我的抛物线拉伸或加宽一个 sigma 因子。让我们看看。它会朝哪个方向发展？如果 sigma 非常小，这是一个很大的数字。</p><p>If my parabola is narrower, if my parabola looks like this, what’s going to happen to the density? It’s going
            to
            fall off much faster. OK. How do I make my parabola narrower or wider? I do it by putting in a constant down
            here. So by putting a sigma here, this stretches or widens my parabola by a factor of sigma. Let’s see.
            Which
            way does it go? If sigma is very small, this is a big number.</p>
        <h2 id="unknown-159">未知</h2><h2>Unknown</h2>
        <p>我的抛物线上升得很快，这意味着我的法线下降得很快。所以小的 sigma 对应着较窄的密度。因此，标准差与 sigma 成正比应该是直观的。因为这就是你缩放图片的量。事实上，标准差就是 sigma。所以方差就是 sigma 的平方。</p><p>My parabola goes up quickly, which means my normal falls off very fast. So small sigma corresponds to a
            narrower
            density. And so it, therefore, should be intuitive that the standard deviation is proportional to sigma.
            Because
            that’s the amount by which you are scaling the picture. And indeed, the standard deviation is sigma. And so
            the
            variance is sigma squared.</p>
        <p>因此，我们在这里所做的就是创建一个具有给定均值和方差的一般正态分布，即拍摄这张图片，在空间中移动它，使均值位于 mu 而不是 0，然后按 sigma 因子缩放它。这给了我们一个具有给定均值和给定方差的正态分布。它的公式是这样的。好的。现在，正态随机变量有一些奇妙的性质。</p><p>So all that we have done here to create a general normal with a given mean and variance is to take this
            picture,
            shift it in space so that the mean sits at mu instead of 0, and then scale it by a factor of sigma. This
            gives
            us a normal with a given mean and a given variance. And the formula for it is this one. All right. Now,
            normal
            random variables have some wonderful properties.</p>
        <p>其中之一就是，当你对它们取线性函数时，它们表现良好。所以让我们固定一些常数 a 和 b，假设 X 是正态的，看看这个线性函数 Y。Y 的期望值是多少？这里我们不需要任何特殊的东西。我们知道线性函数的期望值是期望的线性函数。所以期望值是这样的。方差怎么样？</p><p>And one of them is that they behave nicely when you take linear functions of them. So let’s fix some
            constants a
            and b, suppose that X is normal, and look at this linear function Y. What is the expected value of Y? Here
            we
            don’t need anything special. We know that the expected value of a linear function is the linear function of
            the
            expectation. So the expected value is this. How about the variance?</p>
        <h2 id="unknown-160">未知</h2><h2>Unknown</h2>
        <p>我们知道线性函数的方差与常数项无关。但方差要乘以平方。所以我们得到这些方差，其中 sigma 平方是原始正态分布的方差。那么到目前为止，我们是否使用了 X 是正态分布的属性？没有。当你取随机变量的线性函数时，这里的计算通常是正确的。</p><p>We know that the variance of a linear function doesn’t care about the constant term. But the variance gets
            multiplied by a squared. So we get these variance, where sigma squared is the variance of the original
            normal.
            So have we used so far the property that X is normal? No, we haven’t. This calculation here is true in
            general
            when you take a linear function of a random variable.</p>
        <p>但是如果 X 是正态的，我们就会得到另一个额外的事实，即 Y 也将是正态的。这就是我在这里声称的事实中不平凡的部分。因此，正态随机变量的线性函数本身就是正态的。我们如何说服自己相信这一点？好的。这是我们今天大约两三节课后会做的事情。所以我们要证明这一点。</p><p>But if X is normal, we get the other additional fact that Y is also going to be normal. So that’s the
            nontrivial
            part of the fact that I’m claiming here. So linear functions of normal random variables are themselves
            normal.
            How do we convince ourselves about it? OK. It’s something that we will do formerly in about two or three
            lectures from today. So we’re going to prove it.</p>
        <p>但如果你直观地思考，正常意味着这个特定的钟形曲线。而这个钟形曲线可以位于任何地方，可以以任何方式缩放。所以你从一条钟形曲线开始。如果你取钟形的 X，并将其乘以一个常数，结果会怎样？乘以一个常数就像缩放轴或更改测量它的单位一样。</p><p>But if you think about it intuitively, normal means this particular bell shaped curve. And that bell shaped
            curve
            could be sitting anywhere and could be scaled in any way. So you start with a bell shaped curve. If you take
            X,
            which is bell shaped, and you multiply it by a constant, what does that do? Multiplying by a constant is
            just
            like scaling the axis or changing the units with which you’re measuring it.</p>
        <h2 id="unknown-161">未知</h2><h2>Unknown</h2>
        <p>因此，它会呈现钟形，然后将其展开或缩小。但它仍然是钟形。然后，当您添加常数时，您只需将该钟形移到其他地方。因此，在线性变换下，钟形将保持钟形，只是位于不同位置，宽度也不同。这就是为什么法线在这种变换下保持法线的直觉。那么为什么这很有用呢？</p><p>So it will take a bell shape and spread it or narrow it. But it will still be a bell shape. And then when you
            add
            the constant, you just take that bell and move it elsewhere. So under linear transformations, bell shapes
            will
            remain bell shapes, just sitting at a different place and with a different width. And that sort of the
            intuition
            of why normals remain normals under this kind of transformation. So why is this useful?</p>
        <p>好吧，我们有一个密度公式。但通常我们想计算概率。你如何计算概率？如果我问你，正态分布小于 3 的概率是多少，你怎么算？</p><p>Well, OK. We have a formula for the density. But usually we want to calculate probabilities. How will you
            calculate probabilities? If I ask you, what’s the probability that the normal is less than 3, how do you
            find
            it?</p>
        <p>您需要对从负无穷到 3 的密度进行积分。不幸的是，您必须计算显示的表达式的积分，这种从负无穷到某个数字的积分，是未知的闭式。所以，如果您正在寻找此的闭式公式。X bar。</p><p>You need to integrate the density from minus infinity up to 3. Unfortunately, the integral of the expression
            that
            shows up that you would have to calculate, an integral of this kind from, let’s say, minus infinity to some
            number, is something that’s not known in closed form. So if you’re looking for a closed form formula for
            this. X
            bar.</p>
        <h2 id="unknown-162">未知</h2><h2>Unknown</h2>
        <p>如果您正在寻找一个闭式公式，该公式可给出该积分作为 X bar 函数的值，那么您将找不到它。那么我们能做什么呢？好吧，既然它是一个有用的积分，我们可以将其制成表格。一次性计算所有 X bar 值，直到达到一定精度，然后得到该表格并使用它。这就是我们要做的。好的，但现在有一个问题。</p><p>If you’re looking for a closed form formula that gives you the value of this integral as a function of X bar,
            you’re not going to find it. So what can we do? Well, since it’s a useful integral, we can just tabulate it.
            Calculate it once and for all, for all values of X bar up to some precision, and have that table, and use
            it.
            That’s what one does. OK, but now there is a catch.</p>
        <p>我们要为每种可以想到的正态分布类型（即每个可能的均值和方差）写一个表格吗？我想那会是一张很长的表格。你不会想这么做的。幸运的是，只要有一个包含标准正态分布数值的表格就足够了。一旦你有了这些数值，你就可以巧妙地使用它们来计算更一般情况的概率。</p><p>Are we going to write down a table for every conceivable type of normal distribution. that is, for every
            possible
            mean and every variance? I guess that would be a pretty long table. You don’t want to do that. Fortunately,
            it’s
            enough to have a table with the numerical values only for the standard normal. And once you have those, you
            can
            use them in a clever way to calculate probabilities for the more general case.</p>
        <p>让我们看看这是如何做到的。我们的出发点是，有人慷慨地为我们计算了 CDF（累积分布函数）的值，即标准正态分布在不同位置低于某个点的概率。我们如何解读这个表格？X 小于 0.63 的概率就是这个数字。</p><p>So let’s see how this is done. So our starting point is that someone has graciously calculated for us the
            values
            of the CDF, the cumulative distribution function, that is the probability of falling below a certain point
            for
            the standard normal and at various places. How do we read this table? The probability that X is less than,
            let’s
            say, 0.63 is this number.</p>
        <h2 id="unknown-163">未知</h2><h2>Unknown</h2>
        <p>这个数字 0.7357 是标准正态分布低于 0.63 的概率。所以表格指的是标准正态分布。但是，假设有人给了我们一些其他数字，并告诉我们我们正在处理具有特定平均值和特定方差的正态分布。我们想计算该随机变量的值小于或等于 3 的概率。我们该怎么做呢？</p><p>This number, 0.7357, is the probability that the standard normal is below 0.63. So the table refers to the
            standard normal. But someone, let’s say, gives us some other numbers and tells us we’re dealing with a
            normal
            with a certain mean and a certain variance. And we want to calculate the probability that the value of that
            random variable is less than or equal to 3. How are we going to do it?</p>
        <p>嗯，有一个标准技巧，即所谓的标准化随机变量。标准化随机变量代表以下内容。查看随机变量，然后减去平均值。这使它成为一个平均值为 0 的随机变量。然后，如果我除以标准差，这个随机变量的方差会发生什么？除以一个数字就是将方差除以 sigma 平方。X 的原始方差是 sigma 平方。</p><p>Well, there’s a standard trick, which is so called standardizing a random variable. Standardizing a random
            variable stands for the following. You look at the random variable, and you subtract the mean. This makes it
            a
            random variable with 0 mean. And then if I divide by the standard deviation, what happens to the variance of
            this random variable? Dividing by a number divides the variance by sigma squared. The original variance of X
            was
            sigma squared.</p>
        <p>因此，当我除以 sigma 时，我得到的是单位方差。因此，在进行此转换后，我得到一个具有 0 均值和单位方差的随机变量。它也是正常的。为什么它是正常的？因为这个表达式是我开始的 X 的线性函数。它是正态随机变量的线性函数。因此，它是正常的。而且它是标准正态的。</p><p>So when I divide by sigma, I end up with unit variance. So after I do this transformation, I get a random
            variable that has 0 mean and unit variance. It is also normal. Why is its normal? Because this expression is
            a
            linear function of the X that I started with. It’s a linear function of a normal random variable. Therefore,
            it
            is normal. And it is a standard normal.</p>
        <h2 id="unknown-164">未知</h2><h2>Unknown</h2>
        <p>因此，通过取一个一般的正态随机变量并进行标准化，你最终会得到一个标准正态分布，然后你可以将表格应用到该分布上。有时人们称之为标准化分数。如果你正在考虑测试结果，你会如何解释这个数字？它告诉你你离平均值有多少标准差。这就是你离平均值的距离。</p><p>So by taking a general normal random variable and doing this standardization, you end up with a standard
            normal
            to which you can then apply the table. Sometimes one calls this the normalized score. If you’re thinking
            about
            test results, how would you interpret this number? It tells you how many standard deviations are you away
            from
            the mean. This is how much you are away from the mean.</p>
        <p>然后你用标准差来计算。所以这个数字等于 3 说明 X 恰好比平均值高出 3 个标准差。我想如果你在看你的测验分数，你经常会想到这个数字。所以这是一个有用的数量。但它对于我们现在要做的计算也很有用。</p><p>And you count it in terms of how many standard deviations it is. So this number being equal to 3 tells you
            that X
            happens to be 3 standard deviations above the mean. And I guess if you’re looking at your quiz scores, very
            often that’s the kind of number that you think about. So it’s a useful quantity. But it’s also useful for
            doing
            the calculation we’re now going to do.</p>
        <p>假设 X 的均值为 2，方差为 16，标准差为 4。我们将计算此事件的概率。此事件是根据均值和方差都很丑陋的 X 来描述的。但我们可以将此事件重写为等效事件。</p><p>So suppose that X has a mean of 2 and a variance of 16, so a standard deviation of 4. And we’re going to
            calculate the probability of this event. This event is described in terms of this X that has ugly means and
            variances. But we can take this event and rewrite it as an equivalent event.</p>
        <h2 id="unknown-165">未知</h2><h2>Unknown</h2>
        <p>X 小于 3 等于 X 减 2 小于 3 减 2，等于这个比率小于那个比率。所以我从不等式的两边减去平均值，然后除以标准差。这个事件和那个事件是一样的。为什么我们更喜欢这个而不是那个？我们喜欢它是因为它是 X 的标准化或规范化版本。</p><p>X less than 3 is this same as X minus 2 being less than 3 minus 2, which is the same as this ratio being less
            than that ratio. So I’m subtracting from both sides of the inequality the mean and then dividing by the
            standard
            deviation. This event is the same as that event. Why do we like this better than that? We like it because
            this
            is the standardized, or normalized, version of X.</p>
        <p>我们知道这是标准正态分布。所以我们要问的是，标准正态分布小于这个数字（即 1/4）的概率是多少？关键属性是正态分布 (0,1)。因此，我们现在可以查看表格，并求出标准正态分布随机变量小于 0.25 的概率。这个概率在哪里？0.2,0.25，就在这里。</p><p>We know that this is standard normal. And so we’re asking the question, what’s the probability that the
            standard
            normal is less than this number, which is 1/4? So that’s the key property, that this is normal (0,1). And so
            we
            can look up now with the table and ask for the probability that the standard normal random variable is less
            than
            0.25. Where is that going to be? 0.2,0.25, it’s here.</p>
        <p>所以答案是 0.987。所以我想这只是一个你可以在高中学到的练习。你不必来这里学习它。但当我们一直计算正态概率时，这是一个非常有用的练习。所以确保你知道如何使用表格以及如何将一般正态随机变量转换为标准正态随机变量。好的。</p><p>So the answer is 0.987. So I guess this is just a drill that you could learn in high school. You didn’t have
            to
            come here to learn about it. But it’s a drill that’s very useful when we will be calculating normal
            probabilities all the time. So make sure you know how to use the table and how to massage a general normal
            random variable into a standard normal random variable. OK.</p>
        <h2 id="unknown-166">未知</h2><h2>Unknown</h2>
        <p>所以，再花一分钟时间回顾一下大局，盘点一下我们迄今为止所做的工作以及未来的发展方向。第 2 章就是这部分内容，我们处理的是离散随机变量。而今天，我们开始讨论连续随机变量。我们引入了密度函数，它是概率质量函数的模拟。我们有期望、方差和 CDF 的概念。</p><p>So just one more minute to look at the big picture and take stock of what we have done so far and where we’re
            going. Chapter 2 was this part of the picture, where we dealt with discrete random variables. And this time,
            today, we started talking about continuous random variables. And we introduced the density function, which
            is
            the analog of the probability mass function. We have the concepts of expectation and variance and CDF.</p>
        <p>这种符号既适用于离散情况，也适用于连续情况。两种情况下的计算方式相同，只是在连续情况下，使用总和。在离散情况下，使用积分。因此，在那一边，你有积分。在这种情况下，你有总和。在这种情况下，你的公式中总是有 F。在这种情况下，你的公式中总是有 P。</p><p>And this kind of notation applies to both discrete and continuous cases. They are calculated the same way in
            both
            cases except that in the continuous case, you use sums. In the discrete case, you use integrals. So on that
            side, you have integrals. In this case, you have sums. In this case, you always have Fs in your formulas. In
            this case, you always have Ps in your formulas.</p>
        <p>因此，我们剩下要做的就是研究这两个概念，即联合概率质量函数和条件质量函数，并找出连续方面的等效概念。因此，当我们处理多个随机变量时，我们需要一些联合密度的概念。对于连续随机变量的情况，我们还需要条件密度的概念。</p><p>So what’s there that’s left for us to do is to look at these two concepts, joint probability mass functions
            and
            conditional mass functions, and figure out what would be the equivalent concepts on the continuous side. So
            we
            will need some notion of a joint density when we’re dealing with multiple random variables. And we will also
            need the concept of conditional density, again for the case of continuous random variables.</p>
        <h2 id="unknown-167">未知</h2><h2>Unknown</h2>
        <p>这些对象的直觉和含义将与此处完全相同，只是稍微微妙一些，因为密度不是概率。它们是概率累积的速率。因此，这在这里增加了一点潜在的混淆，希望我们能在接下来的几节中完全解决。好的。谢谢。</p><p>The intuition and the meaning of these objects is going to be exactly the same as here, only a little subtler
            because densities are not probabilities. They’re rates at which probabilities accumulate. So that adds a
            little
            bit of potential confusion here, which, hopefully, we will fully resolve in the next couple of sections. All
            right. Thank you.</p>
        <h1 id="multiple-continuous-random-variables">9. 多个连续随机变量</h1><h1>9. Multiple Continuous Random Variables</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 50:51 <a href="https://youtube.com/watch?v=CadZXGNauY0">https://youtube.com/watch?v=CadZXGNauY0</a></p><p> 12 years ago (Nov 10, 2012) — 50:51 <a href="https://youtube.com/watch?v=CadZXGNauY0">https://youtube.com/watch?v=CadZXGNauY0</a></p>
        <h2 id="unknown-168">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。约翰·齐西克利斯：好的，我们开始吧。我们已经进行了测验。我想这里面有好消息也有坏消息。昨天，如您所知，是坏消息。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. JOHN TSITSIKLIS: OK let’s start. So
            we’ve
            had the quiz. And I guess there’s both good and bad news in it. Yesterday, as you know, the bad news.</p>
        <p>平均成绩比我们想要的要低一点。另一方面，好消息是分数分布很均匀。这个测验的主要目的基本上就是让你校准并大致了解你的成绩。另一个好消息是，如你所知，这个测验对你的最终成绩影响不大。</p><p>The average was a little lower than what we would have wanted. On the other hand, the good news is that the
            distribution was nicely spread. And that’s the main purpose of this quiz is basically for you to calibrate
            and
            see roughly where you are standing. The other piece of the good news is that, as you know, this quiz doesn’t
            count for very much in your final grade.</p>
        <p>所以这实际上就是一个校准问题，让你的心态适当地为第二次测验做准备，第二次测验更重要。而且它更加重要。我们会确保第二次测验的平均分更高。好的。那么让我们开始讨论我们的材料。我们这些天讨论的是连续随机变量。我会提醒你们我们上次讨论的内容。</p><p>So it’s really a matter of calibration and to get your mind set appropriately to prepare for the second quiz,
            which counts a lot more. And it’s more substantial. And we’ll make sure that the second quiz will have a
            higher
            average. All right. So let’s go to our material. We’re talking now these days about continuous random
            variables.
            And I’ll remind you what we discussed last time.</p>
        <h2 id="unknown-169">未知</h2><h2>Unknown</h2>
        <p>我会提醒大家单个随机变量的概率密度函数的概念。然后，我们将快速回顾离散随机变量的情况中涉及的所有概念，并讨论它们在连续情况下的类似概念。并讨论诸如条件独立性等概念。因此，大局已然清晰。我们拥有针对离散随机变量的情况开发的所有概念。
        </p><p>I’ll remind you of the concept of the probability density function of a single random variable. And then
            we’re
            going to rush through all the concepts that we covered for the case of discrete random variables and discuss
            their analogs for the continuous case. And talk about notions such as conditioning independence and so on.
            So
            the big picture is here. We have all those concepts that we developed for the case of discrete random
            variables.
        </p>
        <p>现在我们只讨论连续情况下的类似函数。我们上周已经讨论过这种类似函数，即单个随机变量的密度。然后，有一些概念在离散和连续情况下都会出现。因此，我们有累积分布函数，它是随机变量概率分布的描述，无论你有一个离散随机变量还是连续随机变量，它都适用。
        </p><p>And now we will just talk about their analogs in the continuous case. We already discussed this analog last
            week,
            the density of a single random variable. Then there are certain concepts that show up both in the discrete
            and
            the continuous case. So we have the cumulative distribution function, which is a description of the
            probability
            distribution of a random variable and which applies whether you have a discrete or continuous random
            variable.
        </p>
        <p>然后是期望值的概念。在这两种情况下，期望值的计算方式略有不同，但差别不大。一种情况下是求和，另一种情况下是积分。这就是我们将要得到的一般模式。离散情况下的公式转换为连续情况下的相应公式或表达式。我们通常用积分代替求和，用密度函数代替所有函数。</p><p>Then there’s the notion of the expected value. And in the two cases, the expected value is calculated in a
            slightly different way, but not very different. We have sums in one case, integrals in the other. And this
            is
            the general pattern that we’re going to have. Formulas for the discrete case translate to corresponding
            formulas
            or expressions in the continuous case. We generically replace sums by integrals, and we replace must
            functions
            with density functions.</p>
        <h2 id="unknown-170">未知</h2><h2>Unknown</h2>
        <p>今天的新内容主要是联合密度函数的概念，这是我们通常描述两个有某种关联的随机变量的概率分布的方式，然后是条件密度函数的概念，当你知道另一个随机变量 Y 的值时，它告诉我们一个随机变量 X 的分布。</p><p>Then the new pieces for today are going to be mostly the notion of a joint density function, which is how we
            describe the probability distribution of two random variables that are somehow related, in general, and then
            the
            notion of a conditional density function that tells us the distribution of one random variable X when you’re
            told the value of another random variable Y.</p>
        <p>还有另一个概念，即在特定事件发生的情况下的条件 PDF。这个概念在某些方面更简单。在上周的复习和教程中，您已经了解了一些。这个想法是，我们有一个随机变量。它由密度描述。然后你被告知特定事件已经发生。你的模型改变了你正在处理的宇宙。</p><p>There’s another concept, which is the conditional PDF given that the certain event has happened. This is a
            concept that’s in some ways simpler. You’ve already seen a little bit of that in last week’s recitation and
            tutorial. The idea is that we have a single random variable. It’s described by a density. Then you’re told
            that
            the certain event has occurred. Your model changes the universe that you are dealing with.</p>
        <p>在新的宇宙中，你要面对一个新的密度函数，这个密度函数适用于我们已知的特定事件已经发生的情况。好吧。那么我们到底是如何描述连续随机变量的呢？首先是定义，如果我们给定一个特定的对象，我们称之为概率密度函数，并且我们可以根据这个密度函数计算区间概率，那么这个随机变量就被称为连续的。</p><p>In the new universe, you are dealing with a new density function, the one that applies given the knowledge
            that
            we have that the certain event has occurred. All right. So what exactly did we say about continuous random
            variables? The first thing is the definition, that a random variable is said to be continuous if we are
            given a
            certain object that we call the probability density function and we can calculate interval probabilities
            given
            this density function.</p>
        <h2 id="unknown-171">未知</h2><h2>Unknown</h2>
        <p>因此，定义是，如果您可以根据该公式计算与该随机变量相关的概率，则该随机变量是连续的。因此，该公式告诉您，随机变量落入该区间的概率是密度曲线下的面积。好的。密度函数必须满足几个属性。由于我们讨论的是概率，并且概率是非负的，因此我们认为密度函数始终是非负函数。</p><p>So the definition is that the random variable is continuous if you can calculate probabilities associated
            with
            that random variable given that formula. So this formula tells you that the probability that your random
            variable falls inside this interval is the area under the density curve. OK. There’s a few properties that a
            density function must satisfy. Since we’re talking about probabilities, and probabilities are non negative,
            we
            have that the density function is always a non negative function.</p>
        <p>整个实线上的总概率必须等于 1。因此，对整个实线进行积分时，积分必须等于 1。这是第二个属性。你得到的另一个属性是，如果你让 a 等于 b，这个积分就变成 0。这告诉你，连续情况下单个点的概率始终等于 0。所以这些是正式属性。</p><p>The total probability over the entire real line must be equal to 1. So the integral when you integrate over
            the
            entire real line has to be equal to 1. That’s the second property. Another property that you get is that if
            you
            let a equal to b, this integral becomes 0. And that tells you that the probability of a single point in the
            continuous case is always equal to 0. So these are formal properties.</p>
        <p>当你想直观地思考时，思考密度函数的最好方法是从小区间的角度来思考，我的随机变量落在小区间内的概率。好吧，在那个小区间内，这里的密度函数大致是恒定的。因此，积分变成密度值乘以你积分的区间长度，也就是 delta。</p><p>When you want to think intuitively, the best way to think about what the density function is to think in
            terms of
            little intervals, the probability that my random variable falls inside the little interval. Well, inside
            that
            little interval, the density function here is roughly constant. So that integral becomes the value of the
            density times the length of the interval over which you are integrating, which is delta.</p>
        <h2 id="unknown-172">未知</h2><h2>Unknown</h2>
        <p>因此，密度函数基本上给出了小事件、小事件的概率。密度应解释为图中某个位置每单位长度的概率。因此，在图中那个位置，该邻域周围每单位长度的概率将是该点的密度函数高度。还有什么？我们有一个计算随机变量函数期望值的公式。
        </p><p>And so the density function basically gives us probabilities of little events, of small events. And the
            density
            is to be interpreted as probability per unit length at a certain place in the diagram. So in that place in
            the
            diagram, the probability per unit length around this neighborhood would be the height of the density
            function at
            that point. What else? We have a formula for calculating expected values of functions of random variables.
        </p>
        <p>在离散情况下，我们有一个公式，其中我们得到了总和，并且得到了 PMF 而不是密度。同样的公式在连续情况下也有效。推导起来并不难，但我们不会这么做。但让我们想想这个公式的直觉。你试图算出平均 g(X) 是多少。</p><p>In the discrete case, we had the formula where here we had the sum, and instead of the density, we had the
            PMF.
            The same formula is also valid in the continuous case. And it’s not too hard to derive, but we will not do
            it.
            But let’s think of the intuition of what this formula says. You’re trying to figure out on the average how
            much
            g(X) is going to be.</p>
        <p>然后你推理说，嗯，X 可能最终取一个特定值或一个很小的值区间。这就是 X 落在这个小区间内的概率。当这种情况发生时，g(X) 取该值。因此，在这段时间内，你会落在 x 的小邻域中，你会得到很多。然后你对所有可能发生的 x 取平均值。</p><p>And then you reason, and you say, well, X may turn out to take a particular value or a small interval of
            values.
            This is the probability that X falls inside the small interval. And when that happens, g(X) takes that
            value. So
            this fraction of the time, you fall in the little neighborhood of x, and you get so much. Then you average
            over
            all the possible x’s that can happen.</p>
        <h2 id="unknown-173">未知</h2><h2>Unknown</h2>
        <p>这给出了函数 g(X) 的平均值。好的。这是简单的东西。现在让我们开始新内容。我们想同时讨论多个随机变量。因此，我们现在想讨论两个连续的随机变量，从某种意义上说，它们是联合连续的。让我们看看这意味着什么。</p><p>And that gives you the average value of the function g(X). OK. So this is the easy stuff. Now let’s get to
            the
            new material. We want to talk about multiple random variables simultaneously. So we want to talk now about
            two
            random variables that are continuous, and in some sense that they are jointly continuous. And let’s see what
            this means.</p>
        <p>这个定义类似于我们对单个随机变量的定义，我在这里将此公式作为连续随机变量的定义。如果我们可以通过对感兴趣的集合积分某个我们称之为联合密度函数的函数来计算概率，则称两个随机变量是联合连续的。所以我们有二维平面。这是 xy 平面。有一个我们感兴趣的特定事件 S。</p><p>The definition is similar to the definition we had for a single random variable, where I take this formula
            here
            as the definition of continuous random variables. Two random variables are said to be jointly continuous if
            we
            can calculate probabilities by integrating a certain function that we call the joint density function over
            the
            set of interest. So we have our two dimensional plane. This is the x y plane. There’s a certain event S that
            we’re interested in.</p>
        <p>我们想计算概率。我们该怎么做呢？我们给出这个函数 f_(X,Y)，即联合密度。它是两个参数 x 和 y 的函数。因此，可以把这个函数想象成位于二维平面顶部的某种表面。落入集合 S 内的概率，我们通过查看表面下的体积（位于 S 顶部的体积）来计算。</p><p>We want to calculate the probability. How do we do that? We are given this function f_(X,Y), the joint
            density.
            It’s a function of the two arguments x and y. So think of that function as being some kind of surface that
            sits
            on top of the two dimensional plane. The probability of falling inside the set S, we calculate it by looking
            at
            the volume under the surface, that volume that sits on top of S.</p>
        <h2 id="unknown-174">未知</h2><h2>Unknown</h2>
        <p>因此，其下方的表面具有一定的总体积。总体积应该是多少？好吧，我们将这些体积视为概率。因此总概率应该等于 1。该表面下的总体积应该等于 1。因此，这是我们希望密度函数具有的一个属性。因此，当您对整个空间进行积分时，这就是表面下的体积。</p><p>So the surface underneath it has a certain total volume. What should that total volume be? Well, we think of
            these volumes as probabilities. So the total probability should be equal to 1. The total volume under this
            surface, should be equal to 1. So that’s one property that we want our density function to have. So when you
            integrate over the entire space, this is of the volume under your surface.</p>
        <p>这应该等于 1。当然，既然我们讨论的是概率，联合密度应该是一个非负函数。因此，想象一下这种情况：有一磅概率分布在整个空间中。这个联合密度函数的高度基本上告诉你，相对于空间的其他部分，在空间的某些区域积累的概率有多少。</p><p>That should be equal to 1. Of course, since we’re talking about probabilities, the joint density should be a
            non
            negative function. So think of the situation as having one pound of probability that’s spread all over your
            space. And the height of this joint density function basically tells you how much probability tends to be
            accumulated in certain regions of space as opposed to other parts of the space.</p>
        <p>因此，密度大的地方，就意味着二维平面上这个区域更有可能出现。密度小的地方，就意味着那些 x y 出现的可能性较小。您已经看到了一个连续密度的例子。这是我们在课程一开始就讲过的单位正方形上均匀分布的例子。这是密度函数为常数的一个特殊情况。
        </p><p>So wherever the density is big, that means that this is an area of the two dimensional plane that’s more
            likely
            to occur. Where the density is small, that means that those x y’s are less likely to occur. You have already
            seen one example of continuous densities. That was the example we had in the very beginning of the class
            with a
            uniform distribution on the unit square. That was a special case of a density function that was constant.
        </p>
        <h2 id="unknown-175">未知</h2><h2>Unknown</h2>
        <p>因此，单位正方形内所有位置的概率与其他地方的概率大致相同。但在其他模型中，空间的某些部分可能比其他部分更有可能。我们使用这个密度函数来描述这些相对可能性。所以如果有人给我们密度函数，这就为我们确定了二维平面所有子集的概率。现在，为了直观的解释，最好考虑一下小事件。
        </p><p>So all places in the unit square were roughly equally likely as any other places. But in other models, some
            parts
            of the space may be more likely than others. And we describe those relative likelihoods using this density
            function. So if somebody gives us the density function, this determines for us probabilities of all the
            subsets
            of the two dimensional plane. Now for an intuitive interpretation, it’s good to think about small events.
        </p>
        <p>因此，我们在这里取一个特定的 x，然后取 x 加上 delta。因此这是一个小间隔。在这里取另一个从 y 到 y 加上 delta 的小间隔。让我们看看 x 落在这里而 y 落在那里的事件。这个事件是什么？好吧，这个事件将落入这个小矩形内。使用这个计算概率的规则，这个矩形的概率是多少？</p><p>So let’s take a particular x here and then x plus delta. So this is a small interval. Take another small
            interval
            here that goes from y to y plus delta. And let’s look at the event that x falls here and y falls right
            there.
            What is this event? Well, this is the event that will fall inside this little rectangle. Using this rule for
            calculating probabilities, what is the probability of that rectangle going to be?</p>
        <p>嗯，它应该是这个矩形的密度积分。或者它是位于矩形顶部的表面下的体积。现在，如果矩形非常小，那么关节密度在附近不会发生很大变化。所以我们可以把它当作一个常数。所以体积将是高度乘以底面积。</p><p>Well, it should be the integral of the density over this rectangle. Or it’s the volume under the surface that
            sits on top of that rectangle. Now, if the rectangle is very small, the joint density is not going to change
            very much in that neighborhood. So we can treat it as a constant. So the volume is going to be the height
            times
            the area of the base.</p>
        <h2 id="unknown-176">未知</h2><h2>Unknown</h2>
        <p>该点的高度就是函数在该点周围的值。底面面积是 delta 平方。因此，这是直观地了解联合密度函数真正告诉您的内容的方法。它为您指定小正方形、小矩形的概率。它允许您将联合密度函数视为单位面积的概率。</p><p>The height at that point is whatever the function happens to be around that point. And the area of the base
            is
            delta squared. So this is the intuitive way to understand what a joint density function really tells you. It
            specifies for you probabilities of little squares, of little rectangles. And it allows you to think of the
            joint
            density function as probability per unit area.</p>
        <p>这些就是密度的单位，即某个点附近每单位面积的概率。那么，一旦我们掌握了这个密度函数，我们该怎么做呢？我们可以用它来计算期望值。假设你有一个由联合密度描述的两个随机变量的函数。你也许可以找到这个随机变量的分布，然后使用期望的基本定义。</p><p>So these are the units of the density, its probability per unit area in the neighborhood of a certain point.
            So
            what do we do with this density function once we have it in our hands? Well, we can use it to calculate
            expected
            values. Suppose that you have a function of two random variables described by a joint density. You can find,
            perhaps, the distribution of this random variable and then use the basic definition of the expectation.</p>
        <p>或者，您可以使用原始随机变量的分布直接计算期望。这个公式与我们离散情况下的公式相同。在离散情况下，我们在这里有一个双重和，并且我们有 PMF。因此，这个公式背后的直觉与离散情况下的直觉相同。只是机制不同。</p><p>Or you can calculate expectations directly, using the distribution of the original random variables. This is
            a
            formula that’s again identical to the formula that we had for the discrete case. In the discrete case, we
            had a
            double sum here, and we had PMFs. So the intuition behind this formula is the same that one had for the
            discrete
            case. It’s just that the mechanics are different.</p>
        <h2 id="unknown-177">未知</h2><h2>Unknown</h2>
        <p>然后我们在离散情况下所做的就是找到一种方法，将两个随机变量的联合密度转化为其中一个随机变量的密度。所以我们有一个离散情况的公式。让我们看看在连续情况下事情会如何发展。所以在连续情况下，我们有两个随机变量。我们有它们的密度。</p><p>Then something that we did in the discrete case was to find a way to go from the joint density of the two
            random
            variables taken together to the density of just one of the random variables. So we had a formula for the
            discrete case. Let’s see how things are going to work out in the continuous case. So in the continuous case,
            we
            have here our two random variables. And we have a density for them.</p>
        <p>假设我们想计算 x 落在这个区间内的概率。所以我们要看的是随机变量 X 落在从小 x 到 x 加 delta 的区间内的概率。现在，根据我们已经掌握的解释单个随机变量密度函数的属性，小区间的概率大约等于该单个随机变量的密度乘以 delta。
        </p><p>And let’s say that we want to calculate the probability that x falls inside this interval. So we’re looking
            at
            the probability that our random variable X falls in the interval from little x to x plus delta. Now, by the
            properties that we already have for interpreting the density function of a single random variable, the
            probability of a little interval is approximately the density of that single random variable times delta.
        </p>
        <p>现在我们要根据联合密度找到这个边际密度的公式。好的。所以这是 x 落在这个区间内的概率。就二维平面而言，这是 (x,y) 落在这个带内的概率。</p><p>And now we want to find a formula for this marginal density in terms of the joint density. OK. So this is the
            probability that x falls inside this interval. In terms of the two dimensional plane, this is the
            probability
            that (x,y) falls inside this strip.</p>
        <h2 id="unknown-178">未知</h2><h2>Unknown</h2>
        <p>因此，为了找到该概率，我们需要计算 (x,y) 落入此处的概率，这将是联合密度在此条带上的区间的二重积分。我们要积分什么？y 从负无穷到正无穷。虚拟变量 x 从小 x 到 x 加 delta。</p><p>So to find that probability, we need to calculate the probability that (x,y) falls in here, which is going to
            be
            the double integral over the interval over this strip, of the joint density. And what are we integrating
            over? y
            goes from minus infinity to plus infinity. And the dummy variable x goes from little x to x plus delta.</p>
        <p>因此，要对这个带进行积分，我们要做的就是对任何给定的 y 进行积分。这是 x 积分。然后我们对 y 维度进行积分。现在这个内部积分是什么？因为 x 变化很小，所以它在这个范围内近似为常数。因此，关于 x 的积分只是 delta 乘以 f(x,y)。然后我们就得到了 dy。这就是内部积分将要计算的结果。</p><p>So to integrate over this strip, what we do is for any given y, we integrate in this dimension. This is the x
            integral. And then we integrate over the y dimension. Now what is this inner integral? Because x only varies
            very little, this is approximately constant in that range. So the integral with respect to x just becomes
            delta
            times f(x,y). And then we’ve got our dy. So this is what the inner integral will evaluate to.</p>
        <p>我们对小区间进行积分。因此我们保持 y 不变。在这里积分时，我们将密度值乘以我们要积分的数。然后我们得到这个公式。好的。现在，这个表达式必须等于那个表达式。因此，如果我们取消增量，我们会看到边际密度必须等于联合密度的积分，其中我们已经积分了 y 的值。</p><p>We are integrating over the little interval. So we’re keeping y fixed. Integrating over here, we take the
            value
            of the density times how much we’re integrating over. And we get this formula. OK. Now, this expression must
            be
            equal to that expression. So if we cancel the deltas, we see that the marginal density must be equal to the
            integral of the joint density, where we have integrated out the value of y.</p>
        <h2 id="unknown-179">未知</h2><h2>Unknown</h2>
        <p>所以这个公式现在应该不足为奇了。它与我们用于离散随机变量的公式完全相同。但现在我们用积分代替了和。我们使用联合 PDF 而不是联合 PMF。然后，继续列出我们为离散随机变量所做的事情，我们现在可以引入两个随机变量独立性概念的定义。</p><p>So this formula should come as no surprise at this point. It’s exactly the same as the formula that we had
            for
            discrete random variables. But now we are replacing the sum with an integral. And instead of using the joint
            PMF, we are using the joint PDF. Then, continuing going down the list of things we did for discrete random
            variables, we can now introduce a definition of the notion of independence of two random variables.</p>
        <p>与离散情况类似，我们将独立性定义为以下条件。两个随机变量是独立的，当且仅当它们的联合密度函数分解为边际密度的乘积。并且此属性对于所有 x 和 y 都成立。所以这是正式定义。从操作和直观上讲，它意味着什么？嗯，直观上它的含义与离散情况相同。</p><p>And by analogy with the discrete case, we define independence to be the following condition. Two random
            variables
            are independent if and only if their joint density function factors out as a product of their marginal
            densities. And this property needs to be true for all x and y. So this is the formal definition.
            Operationally
            and intuitively, what does it mean? Well, intuitively it means the same thing as in the discrete case.</p>
        <p>知道任何关于 X 的信息都不应该告诉你任何关于 Y 的信息。也就是说，关于 X 的信息不会改变你对 Y 的信念。我们稍后会回到这个陈述。它允许你做的另一件事。我不会推导它。它允许你通过将单个概率相乘来计算概率。</p><p>Knowing anything about X shouldn’t tell you anything about Y. That is, information about X is not going to
            change
            your beliefs about Y. We are going to come back to this statement in a second. The other thing that it
            allows
            you to do. I’m not going to derive this. is it allows you to calculate probabilities by multiplying
            individual
            probabilities.</p>
        <h2 id="unknown-180">未知</h2><h2>Unknown</h2>
        <p>因此，如果您要计算 x 属于某个集合 A 且 y 属于某个集合 B 的概率，那么您可以通过将各个概率相乘来计算该概率。这只需要两行推导，我不会这样做。但它又回到了事件独立性的通常概念。基本上，操作上的独立性意味着您可以将概率相乘。现在让我们看一个例子。</p><p>So if you ask for the probability that x falls in a certain set A and y falls in a certain set B, then you
            can
            calculate that probability by multiplying individual probabilities. This takes just two lines of derivation,
            which I’m not going to do. But it comes back to the usual notion of independence of events. Basically,
            operationally independence means that you can multiply probabilities. So now let’s look at an example.</p>
        <p>有一种非常著名和经典的数学公式。它可以追溯到 100 多年前。这就是著名的布丰针。布丰是一位法国博物学家，出于某种原因，他也决定玩概率游戏。看看下面的问题。所以你有二维平面。我们在平面上画出一组平行线。这些平行线之间相隔一段距离。</p><p>There’s a sort of pretty famous and classical one. It goes back a lot more than a 100 years. And it’s the
            famous
            Needle of Buffon. Buffon was a French naturalist who, for some reason, also decided to play with
            probability.
            And look at the following problem. So you have the two dimensional plane. And on the plane we draw a bunch
            of
            parallel lines. And those parallel lines are separated by a length.</p>
        <p>线与线之间的距离为 d。我们随机地、完全随机地抛出一根针。我们必须给“完全随机”赋予一个含义。当我们抛出一根针时，有两种可能性。要么针会以不与任何一条线相交的方式落下，要么针会以与其中一条线相交的方式落下。</p><p>And the lines are apart at distance d.&nbsp;And we throw a needle at random, completely at random. And we’ll have
            to
            give a meaning to what “completely at random” means. And when we throw a needle, there’s two possibilities.
            Either the needle is going to fall in a way that does not intersect any of the lines, or it’s going to fall
            in a
            way that it intersects one of the lines.</p>
        <h2 id="unknown-181">未知</h2><h2>Unknown</h2>
        <p>我们假设针比这个距离短，所以针不能同时与两条线相交。它要么与 0 相交，要么与其中一条线相交。问题是找出针与一条线相交的概率。这个概率是多少？好的。我们将使用标准的四步程序来解决这个问题。</p><p>We’re taking the needle to be shorter than this distance, so the needle cannot intersect two lines
            simultaneously. It either intersects 0, or it intersects one of the lines. The question is to find the
            probability that the needle is going to intersect a line. What’s the probability of this? OK. We are going
            to
            approach this problem by using our standard four step procedure.</p>
        <p>设置样本空间，描述该样本空间的概率定律，确定感兴趣的事件，然后计算。这四个步骤基本上对应于这三个要点，然后是下面的最后一个方程。所以第一件事是设置样本空间。我们需要一些变量来描述实验中发生的事情。所以实验中发生的事情是针头落在某个地方。</p><p>Set up your sample space, describe a probability law on that sample space, identify the event of interest,
            and
            then calculate. These four steps basically correspond to these three bullets and then the last equation down
            here. So first thing is to set up a sample space. We need some variables to describe what happened in the
            experiment. So what happens in the experiment is that the needle lands somewhere.</p>
        <p>至于它落在哪里，我们可以通过指定针头中心的位置来描述。那么中心位置是什么意思呢？我们可以将针头中心到最近线的距离作为变量。因此，它告诉我们针头中心到最近线的垂直距离。另一个重要的事情是针头的方向。</p><p>And where it lands, we can describe this by specifying the location of the center of the needle. And what do
            we
            mean by the location of the center? Well, we can take as our variable to be the distance from the center of
            the
            needle to the nearest line. So it tells us the vertical distance of the center of the needle from the
            nearest
            line. The other thing that matters is the orientation of the needle.</p>
        <h2 id="unknown-182">未知</h2><h2>Unknown</h2>
        <p>因此，我们需要一个变量，我们将其视为针与线形成的角度。我们可以将角度放在这里，也可以放在那里。是的，它仍然是相同的角度。因此，我们有这两个变量来描述实验中发生的事情。我们可以将样本空间视为所有可能的 x 和 theta 的集合。可能的 x 是什么？</p><p>So we need one more variable, which we take to be the angle that the needle is forming with the lines. We can
            put
            the angle here, or you can put in there. Yes, it’s still the same angle. So we have these two variables that
            described what happened in the experiment. And we can take our sample space to be the set of all possible
            x’s
            and theta’s. What are the possible x’s?</p>
        <p>线条相距 d，因此最近的线条距离将在 0 到 d/2 之间。这样我们就可以知道可能的 x 是多少。至于 theta，这实际上取决于你如何定义角度。我们将把 theta 定义为针和线条之间形成的锐角（如果你要延长它的话）。</p><p>The lines are d apart, so the nearest line is going to be anywhere between 0 and d/2 away. So that tells us
            what
            the possible x’s will be. As for theta, it really depends how you define your angle. We are going to define
            our
            theta to be the acute angle that’s formed between the needle and a line, if you were to extend it.</p>
        <p>因此，theta 将在 0 和 pi/2 之间。所以我猜这些红色部分实际上对应于设置样本空间的部分。好的。这是第一部分。第二部分是我们需要一个模型。好的。让我们假设我们的模型是，我们基本上不知道针是如何落下的。它可以以任何可能的方式落下，并且所有可能的方式都是同样可能的。</p><p>So theta is going to be something between 0 and pi/2. So I guess these red pieces really correspond to the
            part
            of setting up the sample space. OK. So that’s part one. Second part is we need a model. OK. Let’s take our
            model
            to be that we basically know nothing about how the needle falls. It can fall in any possible way, and all
            possible ways are equally likely.</p>
        <h2 id="unknown-183">未知</h2><h2>Unknown</h2>
        <p>现在，如果你有这些平行线，你完全闭上眼睛，完全随机地抛出一根针，任何 x 都应该是同样可能出现的。所以我们这样描述这种情况：X 应该具有均匀分布。也就是说，它应该在感兴趣的范围内具有恒定的密度。同样，如果你完全随机地旋转针，任何角度都应该与任何其他角度一样可能出现。</p><p>Now, if you have those parallel lines, and you close your eyes completely and throw a needle completely at
            random, any x should be equally likely. So we describe that situation by saying that X should have a uniform
            distribution. That is, it should have a constant density over the range of interest. Similarly, if you kind
            of
            spin your needle completely at random, any angle should be as likely as any other angle.</p>
        <p>我们决定通过假设 theta 在感兴趣的范围内也具有均匀分布来对这种情况进行建模。最后，我们将其放置的位置应该与我们旋转它的程度无关。我们通过假设 X 将独立于 theta 来从数学上捕捉这一点。现在，这将是我们的模型。我不会从任何东西中推导出模型。</p><p>And we decide to model this situation by saying that theta also has a uniform distribution over the range of
            interest. And finally, where we put it should have nothing to do with how much we rotate it. And we capture
            this
            mathematically by saying that X is going to be independent of theta. Now, this is going to be our model. I’m
            not
            deriving the model from anything.</p>
        <p>我只是说，这听起来像是一个不假设任何知识或偏好的模型，即 x 的某些值而不是 theta 的其他值。在没有任何其他特定信息的情况下，这是最合理的模型。所以你用这种方式建模问题。那么联合密度的公式是什么？它将是 X 和 Theta 密度的乘积。</p><p>I’m only saying that this sounds like a model that does not assume any knowledge or preference for certain
            values
            of x rather than other values of theta. In the absence of any other particular information you might have in
            your hands, that’s the most reasonable model to come up with. So you model the problem that way. So what’s
            the
            formula for the joint density? It’s going to be the product of the densities of X and Theta.</p>
        <h2 id="unknown-184">未知</h2><h2>Unknown</h2>
        <p>为什么是乘积？这是因为我们假设了独立性。X 的密度是均匀的，而且需要积分为 1，所以密度需要为 2/d。这就是 X 的密度。Theta 的密度需要为 2/pi。这就是 Theta 密度的值，因此该间隔内的总概率最终为 1。所以现在我们确实掌握了联合密度。</p><p>Why is it the product? This is because we assumed independence. And the density of X, since it’s uniform, and
            since it needs to integrate to 1, that density needs to be 2/d.&nbsp;That’s the density of X. And the density of
            Theta needs to be 2/pi. That’s the value for the density of Theta so that the overall probability over this
            interval ends up being 1. So now we do have our joint density in our hands.</p>
        <p>接下来要做的是识别感兴趣的事件。最好用图片来表示。可能有两种情况。要么指针朝这个方向落下，要么指针朝那个方向落下。那么我们如何判断会发生这两种情况呢？这取决于这里的间隔是小于还是大于。</p><p>The next thing to do is to identify the event of interest. And this is best done in a picture. And there’s
            two
            possible situations that one could have. Either the needle falls this way, or it falls this way. So how can
            we
            tell if one or the other is going to happen? It has to do with whether this interval here is smaller than
            that
            or bigger than that.</p>
        <p>因此，我们将这个间隔的高度与那个间隔的高度进行比较。这里的间隔是大写 X。这里的间隔是什么？这是针长度的一半，即 l/2。要找到这个高度，我们取 l/2 并将其乘以我们得到的角度的正弦。所以这里这个间隔的长度是 l/2 乘以正弦 theta。</p><p>So we are comparing the height of this interval to that interval. This interval here is capital X. This
            interval
            here, what is it? This is half of the length of the needle, which is l/2. To find this height, we take l/2
            and
            multiply it with the sine of the angle that we have. So the length of this interval up here is l/2 times
            sine
            theta.</p>
        <h2 id="unknown-185">未知</h2><h2>Unknown</h2>
        <p>如果小于 x，则针不会与线相交。如果大于 x，则针会与线相交。因此，感兴趣的事件（针与线相交）可以用 x 和 theta 来描述。</p><p>If this is smaller than x, the needle does not intersect the line. If this is bigger than x, then the needle
            intersects the line. So the event of interest, that the needle intersects the line, is described this way in
            terms of x and theta.</p>
        <p>现在我们已经用数学的方式描述了感兴趣的事件，我们需要做的就是找到这个事件的概率，我们将联合密度积分到 (x, theta) 空间中不等式成立的部分。所以它是所有 x 和 theta 集合的双重积分，其中不等式成立。</p><p>And now that we have the event of interest described mathematically, all that we need to do is to find the
            probability of this event, we integrate the joint density over the part of (x, theta) space in which this
            inequality is true. So it’s a double integral over the set of all x’s and theta’s where this is true.</p>
        <p>计算这个积分的方法是固定 theta，然后对从 0 到该数字的 x 进行积分。theta 可以是 0 到 pi/2 之间的任何值。因此，这个集合的积分基本上就是这里的二重积分。我们已经有了联合密度的公式。它是 4 除以 pi d，所以我们把它放在这里。现在，幸运的是，这是一个相当容易计算的积分。</p><p>The way to do this integral is we fix theta, and we integrate for x’s that go from 0 up to that number. And
            theta
            can be anything between 0 and pi/2. So the integral over this set is basically this double integral here. We
            already have a formula for the joint density. It’s 4 over pi d, so we put it here. And now, fortunately,
            this is
            a pretty easy integral to evaluate.</p>
        <h2 id="unknown-186">未知</h2><h2>Unknown</h2>
        <p>关于 x 的积分这里什么都没有。所以积分只是我们积分的区间长度。它是 l/2 正弦θ。然后我们需要对θ进行积分。我们知道正弦的积分是负余弦。你在两个端点代入负余弦的值。我相信你能做这个积分</p><p>The integral with respect to x there’s nothing in here. So the integral is just the length of the interval
            over
            which we’re integrating. It’s l/2 sine theta. And then we need to integrate this with respect to theta. We
            know
            that the integral of a sine is a negative cosine. You plug in the values for the negative cosine at the two
            end
            points. I’m sure you can do this integral</p>
        <p>我们最终得到了答案，对于一个看起来相当复杂的问题来说，答案出奇的简单。答案是 2l 除以 pi d。很久很久以前，有些人在看过这个答案后，说也许这为我们提供了一种有趣的方法，例如，可以通过实验来估计 pi ​​的值。怎么做呢？确定 l 和 d，即问题的维度。在纸上扔一百万根针。</p><p>And we finally obtain the answer, which is amazingly simple for such a pretty complicated looking problem.
            It’s
            2l over pi d.&nbsp;So some people a long, long time ago, after they looked at this answer, they said that maybe
            that
            gives us an interesting way where one could estimate the value by pi, for example, experimentally. How do
            you do
            that? Fix l and d, the dimensions of the problem. Throw a million needles on your piece of paper.</p>
        <p>看看你的针与线相交的频率。这样你就得到了这个数量的数字。你知道 l 和 d，所以你可以用它来推断 pi。还有一个关于美国内战后医院里一名受伤士兵的杜撰故事，他实际上听说过这件事，并在医院里把针扔在纸上。我不知道这是不是真的。</p><p>See how often your needless do intersect the line. That gives you a number for this quantity. You know l and
            d,
            so you can use that to infer pi. And there’s an apocryphal story about a wounded soldier in a hospital after
            the
            American Civil War who actually had heard about this and was spending his time in the hospital throwing
            needles
            on pieces of paper. I don’t know if it’s true or not.</p>
        <h2 id="unknown-187">未知</h2><h2>Unknown</h2>
        <p>但是我们在这里做类似的事情。让我们看看这个图表。我们固定尺寸。这应该是我们的小 d。那应该是我们的小 l。我们从上一张幻灯片中得到公式，p 等于 2l 除以 pi d。在这个例子中，我们选择 d 为 l 的两倍。所以这个数字是 1/pi。所以针碰到线的概率是 1/pi。</p><p>But let’s do something similar here. So let’s look at this diagram. We fix the dimensions. This is supposed
            to be
            our little d.&nbsp;That’s supposed to be our little l. We have the formula from the previous slide that p is 2l
            over
            pi d.&nbsp;In this instance, we choose d to be twice l. So this number is 1/pi. So the probability that the
            needle
            hits the line is 1/pi.</p>
        <p>所以我需要长度为 3.1 厘米的针。我找不到这样的针。但我能找到长度为 3.1 厘米的回形针。所以让我们开始随机抛出回形针，看看其中有多少会与线相交。很好。好的。所以在八个回形针中，我们恰好有四个与线相交。
        </p><p>So I need needles that are 3.1 centimeters long. I couldn’t find such needles. But I could find paper clips
            that
            are 3.1 centimeters long. So let’s start throwing paper clips at random and see how many of them will end up
            intersecting the lines. Good. OK. So out of eight paper clips, we have exactly four that intersected the
            line.
        </p>
        <p>因此，我们对直线相交概率的估计是 1/2，这给了我们一个 π 值的估计，即 2。好吧，我的意思是，在工程近似中，我们的估计是正确的，对吧？因此，这看起来可能是一种愚蠢的估计 π 的方法。可能确实如此。另一方面，这种方法尤其被物理学家和统计学家使用。它被广泛使用。</p><p>So our estimate for the probability of intersecting the line is 1/2, which gives us an estimate for the value
            of
            pi, which is two. Well, I mean, within an engineering approximation, we’re in the right ballpark, right? So
            this
            might look like a silly way of trying to estimate pi. And it probably is. On the other hand, this kind of
            methodology is being used especially by physicists and also by statisticians. It’s used a lot.</p>
        <h2 id="unknown-188">未知</h2><h2>Unknown</h2>
        <p>什么时候用它？如果你要计算一个积分，比如这个积分，但你运气不好，你的函数不是那么简单，你不能手工计算，也许维度更大。你不是有两个随机变量，而是有 100 个随机变量，所以它是一个 100 倍积分。那么在计算机上就无法做到这一点。</p><p>When is it used? If you have an integral to calculate, such as this integral, but you’re not lucky, and your
            functions are not so simple where you can do your calculations by hand, and maybe the dimensions are larger.
            instead of two random variables you have 100 random variables, so it’s a 100 fold integral. then there’s no
            way
            to do that in the computer.</p>
        <p>但实际上，你可以这样做，即生成随机变量的随机样本，反复进行多次模拟。也就是说，通过将积分解释为概率，你可以使用模拟来估计该概率。这为你提供了一种计算积分的方法。物理学家、统计学家、计算机科学家等确实经常使用这种方法。这就是所谓的蒙特卡罗方法来评估积分。</p><p>But the way that you can actually do it is by generating random samples of your random variables, doing that
            simulation over and over many times. That is, by interpreting an integral as a probability, you can use
            simulation to estimate that probability. And that gives you a way of calculating integrals. And physicists
            do
            actually use that a lot, as well as statisticians, computer scientists, and so on. It’s a so called Monte
            Carlo
            method for evaluating integrals.</p>
        <p>如今，它是科学工具箱中的一个基本组成部分。最后，当今较难的概念是条件的概念。当你处理连续随机变量时，事情就变得有点微妙了。好的。首先，再次回忆一下我们对密度的基本解释。密度给出了小区间的概率。那么我们应该如何定义条件密度呢？</p><p>And it’s a basic piece of the toolbox in science these days. Finally, the harder concept of the day is the
            idea
            of conditioning. And here things become a little subtle when you deal with continuous random variables. OK.
            First, remember again our basic interpretation of what a density is. A density gives us probabilities of
            little
            intervals. So how should we define conditional densities?</p>
        <h2 id="unknown-189">未知</h2><h2>Unknown</h2>
        <p>条件密度应该再次为我们提供小区间的概率，但在一个条件世界中，我们已经被告知了有关其他随机变量的一些信息。因此，我们希望以下内容是真实的。我们想定义一个给定另一个随机变量 Y 的值的随机变量 X 的条件密度概念。它应该表现为以下方式，即条件密度为我们提供小区间的概率。</p><p>Conditional densities should again give us probabilities of little intervals, but inside a conditional world
            where we have been told something about the other random variable. So what we would like to be true is the
            following. We would like to define a concept of a conditional density of a random variable X given the value
            of
            another random variable Y. And it should behave the following way, that the conditional density gives us the
            probability of little intervals.</p>
        <p>和这里一样。假设我们被告知 y 的值。这就是微妙之处。要注意的主要一点是，这里我没有写“相等”，而是写了“近似相等”。我们为什么需要它？事实上，当你以概率为 0 的事件为条件时，条件概率是没有定义的。所以我们需要这里的条件事件来提出这个概率。</p><p>Same as here. given that we are told the value of y. And here’s where the subtleties come. The main thing to
            notice is that here I didn’t write “equal,” I wrote “approximately equal.” Why do we need that? Well, the
            thing
            is that conditional probabilities are not defined when you condition on an event that has 0 probability. So
            we
            need the conditioning event here to have posed this probability.</p>
        <p>因此，我们不想说 Y 恰好等于小 y，而是想说我们处在一个新的宇宙中，其中大写 Y 非常接近小 y。然后这个“非常接近”的概念就把极限看作是无限接近。所以这就是解释条件概率的方式。这就是它们应该的意思。现在，在实践中，当你真正使用概率时，你会忘记这种微妙之处。</p><p>So instead of saying that Y is exactly equal to little y, we want to instead say we’re in a new universe
            where
            capital Y is very close to little y. And then this notion of “very close” kind of takes the limit and takes
            it
            to be infinitesimally close. So this is the way to interpret conditional probabilities. That’s what they
            should
            mean. Now, in practice, when you actually use probability, you forget about that subtlety.</p>
        <h2 id="unknown-190">未知</h2><h2>Unknown</h2>
        <p>你说，好吧，有人告诉我 Y 等于 1.3。请告诉我 X 的条件分布。但正式或严格地说，你应该说有人告诉我 Y 无限接近 1.3。请告诉我 X 的分布。现在，如果这就是我们想要的，那么这个数量应该是多少？这是一个条件概率，所以它应该是两件事发生的概率。</p><p>And you say, well, I’ve been told that Y is equal to 1.3. Give me the conditional distribution of X. But
            formally
            or rigorously, you should say I’m being told that Y is infinitesimally close to 1.3. Tell me the
            distribution of
            X. Now, if this is what we want, what should this quantity be? It’s a conditional probability, so it should
            be
            the probability of two things happening.</p>
        <p>X 接近小 x，Y 接近小 y。这基本上是由联合密度除以条件事件的概率得出的，这与 Y 本身的密度有关。如果你仔细研究，就会发现满足这种关系的唯一方法是通过这个特定公式定义条件密度。好的。</p><p>X being close to little x, Y being close to little y. And that’s basically given to us by the joint density
            divided by the probability of the conditioning event, which has something to do with the density of Y
            itself.
            And if you do things carefully, you see that the only way to satisfy this relation is to define the
            conditional
            density by this particular formula. OK.</p>
        <p>最终的大讨论归结于您现在可能已经猜到的内容。我们只需从离散情况中取出任何公式和表达式，并用 PDF 替换 PMF。因此，条件 PDF 由这个公式定义，其中我们有联合 PDF 和边际 PDF，而不是离散情况，其中我们有联合 PMF 和边际 PMF。所以从某种意义上说，这只是一个句法变化。</p><p>Big discussion to come down in the end to what you should have probably guessed by now. We just take any
            formulas
            and expressions from the discrete case and replace PMFs by PDFs. So the conditional PDF is defined by this
            formula where here we have joint PDF and marginal PDF, as opposed to the discrete case where we had the
            joint
            PMF and the marginal PMF. So in some sense, it’s just a syntactic change.</p>
        <h2 id="unknown-191">未知</h2><h2>Unknown</h2>
        <p>从另一个意义上讲，它更微妙地取决于你如何实际解释它。说到解释，思考联合密度的一些方法是什么？嗯，最好的思考方式是有人为你固定了一点 y。所以这里固定了一点 y。我们将这个密度视为 X 的函数。我已经告诉你 Y 是什么。告诉我你对 X 了解多少。</p><p>In another sense, it’s a little subtler on how you actually interpret it. Speaking about interpretation, what
            are
            some ways of thinking about the joint density? Well, the best way to think about it is that somebody has
            fixed
            little y for you. So little y is being fixed here. And we look at this density as a function of X. I’ve told
            you
            what Y is. Tell me what you know about X.</p>
        <p>你告诉我 X 具有某种分布。那个分布是什么样子的？它的形状与联合密度完全相同。记住，我们固定了 Y。所以这是一个常数。所以唯一变化的是 X。因此，当你固定 y 时，我们得到了一个函数，它的行为类似于联合密度，这实际上是你取联合密度，然后取其中的一部分。</p><p>And you tell me that X has a certain distribution. What does that distribution look like? It has exactly the
            same
            shape as the joint density. Remember, we fixed Y. So this is a constant. So the only thing that varies is X.
            So
            we get the function that behaves like the joint density when you fix y, which is really you take the joint
            density, and you take a slice of it.</p>
        <p>你固定 ay，你会看到它如何随 x 变化。所以从这个意义上讲，条件 PDF 只是联合 PDF 的一部分。但我们需要除以某个数字，这只会缩放它并改变其形状。我们马上回到图片上。但在看图片之前，让我们先回到对独立性的解释。
        </p><p>You fix a y, and you see how it varies with x. So in that sense, the conditional PDF is just a slice of the
            joint
            PDF. But we need to divide by a certain number, which just scales it and changes its shape. We’re coming
            back to
            a picture in a second. But before going to the picture, lets go back to the interpretation of independence.
        </p>
        <h2 id="unknown-192">未知</h2><h2>Unknown</h2>
        <p>如果两个随机变量是独立的，根据我们在上一张幻灯片中的定义，联合密度将作为边际密度的乘积。分子中 Y 的密度抵消了分母中的密度。我们只剩下 X 的密度。因此，在独立的情况下，我们得到的是条件密度与边际密度相同。</p><p>If the two random the variables are independent, according to our definition in the previous slide, the joint
            density is going to factor as the product of the marginal densities. The density of Y in the numerator
            cancels
            the density in the denominator. And we’re just left with the density of X. So in the case of independence,
            what
            we get is that the conditional is the same as the marginal.</p>
        <p>这巩固了我们的直觉，即在独立的情况下，被告知 Y 的值不会改变我们对 X 分布方式的看法。因此，即使我们被告知 Y 的值，我们对 X 的任何预期仍将保持不变。让我们看一些图片。联合 PDF 可能如下所示。这里有 x 轴和 y 轴。</p><p>And that solidifies our intuition that in the case of independence, being told something about the value of Y
            does not change our beliefs about how X is distributed. So whatever we expected about X is going to remain
            true
            even after we are told something about Y. So let’s look at some pictures. Here is what the joint PDF might
            look
            like. Here we’ve got our x and y axis.</p>
        <p>如果你想计算某个事件的概率，你要做的就是观察该事件，看看有多少质量位于该事件之上。现在让我们开始切片。让我们固定 x 的值，然后沿着该切片查看，我们得到这个函数。现在这个切片有什么作用？</p><p>And if you want to calculate the probability of a certain event, what you do is you look at that event and
            you
            see how much of that mass is sitting on top of that event. Now let’s start slicing. Let’s fix a value of x
            and
            look along that slice where we obtain this function. Now what does that slice do?</p>
        <h2 id="unknown-193">未知</h2><h2>Unknown</h2>
        <p>该切片告诉我们，对于特定的 x，y 的可能值是什么，以及这些值的概率有多大。如果我们对所有 y 求积分，会得到什么？对所有 y 求积分只会得到 X 的边际密度。这就是我们在这里进行的计算。通过对所有 y 求积分，我们会找到 X 的边际密度。因此，该切片下的总面积会得到 X 的边际密度。</p><p>That slice tells us for that particular x what the possible values of y are going to be and how likely they
            are.
            If we integrate over all y’s, what do we get? Integrating over all y’s just gives us the marginal density of
            X.
            It’s the calculation that we did here. By integrating over all y’s, we find the marginal density of X. So
            the
            total area under that slice gives us the marginal density of X.</p>
        <p>通过查看不同的切片，我们可以发现 x 的不同值出现的可能性有多大。条件呢？如果我们对给定 X 的 Y 条件感兴趣，你会怎么想？这指的是一个宇宙，我们被告知大写 X 会取一个特定的值。所以我们把自己放在这条线发生的宇宙中。仍然有可能发生 y 的值。</p><p>And by looking at the different slices, we find how likely the different values of x are going to be. How
            about
            the conditional? If we’re interested in the conditional of Y given X, how would you think about it? This
            refers
            to a universe where we are told that capital X takes on a specific value. So we put ourselves in the
            universe
            where this line has happened. There’s still possible values of y that can happen.</p>
        <p>这种形状告诉我们不同 y 的相对可能性。这确实是在 X 发生的情况下 Y 的条件分布的形状。</p><p>And this shape kind of tells us the relative likelihoods of the different y’s. And this is indeed going to be
            the
            shape of the conditional distribution of Y given that X has occurred.</p>
        <h2 id="unknown-194">未知</h2><h2>Unknown</h2>
        <p>另一方面，条件分布必须加起来为 1。因此，这个宇宙中所有不同 y 的总概率，即总概率应该等于 1。这里它不等于 1。总面积是边际密度。</p><p>On the other hand, the conditional distribution must add up to 1. So the total probability over all of the
            different y’s in this universe, that total probability should be equal to 1. Here it’s not equal to 1. The
            total
            area is the marginal density.</p>
        <p>为了使其等于 1，我们需要除以边际密度，这基本上就是重新规范化这个形状，以便该切片下、该形状下的总面积等于 1。所以我们从关节开始。我们取切片。然后我们调整切片，以便每个切片下的面积都等于 1。这给了我们条件。例如，在这里。</p><p>To make it equal to 1, we need to divide by the marginal density, which is basically to renormalize this
            shape so
            that the total area under that slice, under that shape, is equal to 1. So we start with the joint. We take
            the
            slices. And then we adjust the slices so that every slice has an area underneath equal to 1. And this gives
            us
            the conditional. So for example, down here.</p>
        <p>你甚至无法在这个图中看到它。但在你重新规范化它，使它的总面积等于 1 之后，你会看到这种向上的窄尖峰。所以这是你针对不同 x 值得到的条件分布图。给定一个特定的 x 值，你将得到这个特定的条件分布。</p><p>You can not even see it in this diagram. but after you renormalize it so that its total area is equal to 1,
            you
            get this sort of narrow spike that goes up. And so this is a plot of the conditional distributions that you
            get
            for the different values of x. Given a particular value of x, you’re going to get this certain conditional
            distribution.</p>
        <h2 id="unknown-195">未知</h2><h2>Unknown</h2>
        <p>所以这幅图的价值与本章中的其他内容一样重要。确保你完全理解这幅图的所有部分。最后，在剩下的时间里，让我们通过一个例子来介绍到目前为止介绍过的所有概念和符号。例子如下。我们从一根有一定长度的棍子开始。</p><p>So this picture is worth about as much as anything else in this particular chapter. Make sure you kind of
            understand exactly all these pieces of the picture. And finally, let’s go, in the remaining time, through an
            example where we’re going to throw in the bucket all the concepts and notations that we have introduced so
            far.
            So the example is as follows. We start with a stick that has a certain length.</p>
        <p>我们把它分解成一个完全随机的位置。是的，这个 1 应该是 l。好的。所以它的长度是 l。我们要把它分解成一个随机的位置。我们称这个分解的随机位置为 X。X 可以是任何地方，均匀分布。这意味着 X 的密度从 0 到 l。</p><p>And we break it a completely random location. And. yes, this 1 should be l. OK. So it has length l. And we’re
            going to break it at the random place. And we call that random place where we break it, we call it X. X can
            be
            anywhere, uniform distribution. So this means that X has a density that goes from 0 to l.</p>
        <p>我猜这个大写字母 L 应该与小写字母 l 相同。所以这就是 X 的密度。由于密度需要积分为 1，所以该密度的高度必须是 1/l。</p><p>I guess this capital L is supposed to be the same as the lower case l. So that’s the density of X. And since
            the
            density needs to integrate to 1, the height of that density has to be 1/l.</p>
        <h2 id="unknown-196">未知</h2><h2>Unknown</h2>
        <p>现在，既然木棍已经折断，而且只剩下这一段木棍，我现在要在一个完全随机的地方再次折断它，也就是说，我要选择一个点，在木棍的长度上均匀地折断它。这是什么意思？我们把 Y 称为我折断它的位置。所以 Y 的范围是 0 到 x。x 就是我剩下的那段木棍。</p><p>Now, having broken the stick and given that we are left with this piece of the stick, I’m now going to break
            it
            again at a completely random place, meaning I’m going to choose a point where I break it uniformly over the
            length of the stick. What does this mean? And let’s call Y the location where I break it. So Y is going to
            range
            between 0 and x. x is the stick that I’m left with.</p>
        <p>所以我要把它分解成两个数。所以我在 0 和 x 之间取一个 ay。当然，x 小于 l。我要把它分解成两个数。所以 y 在 0 和 x 之间是均匀的。这是什么意思呢？假设你已经告诉我 x，那么 y 的密度范围从 0 到小 x？</p><p>So I’m going to break it somewhere in between. So I pick a y between 0 and x. And of course, x is less than
            l.
            And I’m going to break it there. So y is uniform between 0 and x. What does that mean, that the density of
            y,
            given that you have already told me x, ranges from 0 to little x?</p>
        <p>如果我告诉你第一次中断发生在特定的 x 处，那么 y 只能在此间隔内变化。我假设该间隔内呈均匀分布。所以我们有这种形状。这为我们确定了条件密度的高度。那么这两个随机变量的联合密度是多少？根据条件密度的定义，条件密度被定义为此除以彼的比率。</p><p>If I told you that the first break happened at a particular x, then y can only range over this interval. And
            I’m
            assuming a uniform distribution over that interval. So we have this kind of shape. And that fixes for us the
            height of the conditional density. So what’s the joint density of those two random variables? By the
            definition
            of conditional densities, the conditional was defined as the ratio of this divided by that.</p>
        <h2 id="unknown-197">未知</h2><h2>Unknown</h2>
        <p>因此，我们可以通过取边际密度然后乘以条件密度来找到联合密度。这与离散情况下的公式相同。这是我们非常熟悉的乘法规则，但针对连续随机变量的情况进行了调整。因此 Ps 变成了 Fs。好的。所以我们确实有一个公式。它是什么？它是 1/l。这是 X 的密度乘以 1/x，即 Y 的条件密度。</p><p>So we can find the joint density by taking the marginal and then multiplying by the conditional. This is the
            same
            formula as in the discrete case. This is our very familiar multiplication rule, but adjusted to the case of
            continuous random variables. So Ps become Fs. OK. So we do have a formula for this. What is it? It’s 1/l.
            that’s
            the density of X times 1/x, which is the conditional density of Y.</p>
        <p>这是联合密度的公式。但我们必须小心。这个公式并非在任何地方都有效。它只对可能的 x 和 y 有效。可能的 x 和 y 由这些不等式给出。因此 x 的范围可以从 0 到 l，y 只能小于 x。所以这是我们空间这部分的密度公式。</p><p>This is the formula for the joint density. But we must be careful. This is a formula that’s not valid
            anywhere.
            It’s only valid for the x’s and y’s that are possible. And the x’s and y’s that are possible are given by
            these
            inequalities. So x can range from 0 to l, and y can only be smaller than x. So this is the formula for the
            density on this part of our space.</p>
        <p>其他地方的密度都是 0。那么它是什么样子的呢？它基本上是一个 1/x 函数。所以它在这个维度上是恒定的。但是当 x 趋于 0 时，密度就会上升，甚至会爆炸。它看起来有点像一面升起的、有点弯曲的帆，上面有一个指向无穷远的点。所以这就是联合密度。</p><p>The density is 0 anywhere else. So what does it look like? It’s basically a 1/x function. So it’s sort of
            constant along that dimension. But as x goes to 0, your density goes up and can even blow up. It sort of
            looks
            like a sail that’s raised and somewhat curved and has a point up there going to infinity. So this is the
            joint
            density.</p>
        <h2 id="unknown-198">未知</h2><h2>Unknown</h2>
        <p>现在，一旦你掌握了联合密度，那么原则上你就可以回答任何问题。这只是一个代入并进行计算的问题。给定 x 的值，计算 Y 的条件期望之类的东西怎么样？好的。这是我们迄今为止尚未定义的概念。但我们应该如何定义它？意思是合理的事情。</p><p>Now once you have in your hands a joint density, then you can answer in principle any problem. It’s just a
            matter
            of plugging in and doing computations. How about calculating something like a conditional expectation of Y
            given
            a value of x? OK. That’s a concept we have not defined so far. But how should we define it? Means the
            reasonable
            thing.</p>
        <p>我们将以与普通期望相同的方式定义它，只不过由于我们得到了一些条件信息，我们应该使用适用于该特定情况的概率分布。因此，在我们知道 x 的值的情况下，适用的分布是 Y 的条件分布。因此，它将是给定 x 值的 Y 的条件密度。现在，我们知道这是什么。它由 1/x 给出。</p><p>We’ll define it the same way as ordinary expectations except that since we’re given some conditioning
            information, we should use the probability distribution that applies to that particular situation. So in a
            situation where we are told the value of x, the distribution that applies is the conditional distribution of
            Y.
            So it’s going to be the conditional density of Y given the value of x. Now, we know what this is. It’s given
            by
            1/x.</p>
        <p>因此我们需要对 y 乘以 1/x dy 进行积分。那么我们应该对什么进行积分呢？给定 x 的值，y 的范围只能是 0 到 x。因此这就是我们得到的结果。然后你进行积分，得到 x/2。这很令人惊讶吗？不应该。这只是在 X 已经实现并且 Y 由该分布给出的宇宙中 Y 的预期值。</p><p>So we need to integrate y times 1/x dy. And what should we integrate over? Well, given the value of x, y can
            only
            range from 0 to x. So this is what we get. And you do your integral, and you get that this is x/2. Is it a
            surprise? It shouldn’t be. This is just the expected value of Y in a universe where X has been realized and
            Y is
            given by this distribution.</p>
        <h2 id="unknown-199">未知</h2><h2>Unknown</h2>
        <p>Y 在 0 和 x 之间是均匀分布的。Y 的预期值应该是这个区间的中点，即 x/2。现在让我们做一些更复杂的事情。既然我们有联合分布，我们应该能够计算边际。Y 的分布是什么？把棍子折断两次后，剩下的小块有多大？我们如何找到它？</p><p>Y is uniform between 0 and x. The expected value of Y should be the midpoint of this interval, which is x/2.
            Now
            let’s do fancier stuff. Since we have the joint distribution, we should be able to calculate the marginal.
            What
            is the distribution of Y? After breaking the stick twice, how big is the little piece that I’m left with?
            How do
            we find this?</p>
        <p>为了找到边际，我们只需取联合并积分出我们不想要的变量。一个特定的 y 可以以多种方式发生。它可以与任何 x 一起发生。因此，我们考虑所有可能与此 y 一起出现的 x，并对所有这些 x 取平均值。因此，我们代入上一张幻灯片中的联合密度公式。我们知道它是 1/lx。那么 x 的范围是多少？</p><p>To find the marginal, we just take the joint and integrate out the variable that we don’t want. A particular
            y
            can happen in many ways. It can happen together with any x. So we consider all the possible x’s that can go
            together with this y and average over all those x’s. So we plug in the formula for the joint density from
            the
            previous slide. We know that it’s 1/lx. And what’s the range of the x’s?</p>
        <p>因此，为了找到此处特定 y 的 Y 密度，我将对 x 求积分。密度在这里和那里为 0。只有这一部分密度不为零。因此，我需要对从这里到那里的 x 求积分。那么“这里”是什么？这条线以 1 的斜率向上。因此，这是 x 等于 y 的线。</p><p>So to find the density of Y for a particular y up here, I’m going to integrate over x’s. The density is 0
            here
            and there. The density is nonzero only in this part. So I need to integrate over x’s going from here to
            there.
            So what’s the “here”? This line goes up at the slope of 1. So this is the line x equals y.</p>
        <h2 id="unknown-200">未知</h2><h2>Unknown</h2>
        <p>所以如果我固定 y，这意味着我的积分从 x 的值开始，该值也等于 y。所以积分从 x 等于 y 开始。它一直到我们棍子的长度末端，也就是 l。所以我们需要从小 y 积分到 l。所以这几乎总是会出现。</p><p>So if I fix y, it means that my integral starts from a value of x that is also equal to y. So where the
            integral
            starts from is at x equals y. And it goes all the way until the end of the length of our stick, which is l.
            So
            we need to integrate from little y up to l. So that’s something that almost always comes up.</p>
        <p>仅有这个公式来积分联合密度是不够的。你需要跟踪不同的区域。如果联合密度在某些区域为 0，那么你就将这些区域排除在积分范围之外。因此，积分范围仅限于特定公式有效的那些值，即联合密度非零的地方。好的。1/x dx 的积分会给出对数。
        </p><p>It’s not enough to have just this formula for integrating the joint density. You need to keep track of
            different
            regions. And if the joint density is 0 in some regions, then you exclude those regions from the range of
            integration. So the range of integration is only over those values where the particular formula is valid,
            the
            places where the joint density is nonzero. All right. The integral of 1/x dx, that gives you a logarithm.
        </p>
        <p>因此，我们计算这个积分，得到这种表达式。因此，Y 的密度具有某种出乎意料的形状。所以它是一个对数函数。它是这样的。它是 y 一直到 l。当 y 等于 l 时，1 的对数等于 0。但是当 y 接近 0 时，某个大数的对数会爆炸，我们得到这种形式的形状。好的。</p><p>So we evaluate this integral, and we get an expression of this kind. So the density of Y has a somewhat
            unexpected shape. So it’s a logarithmic function. And it goes this way. It’s for y going all the way to l.
            When
            y is equal to l, the logarithm of 1 is equal to 0. But when y approaches 0, logarithm of something big blows
            up,
            and we get a shape of this form. OK.</p>
        <h2 id="unknown-201">未知</h2><h2>Unknown</h2>
        <p>最后，我们可以计算 Y 的期望值。我们可以使用期望的定义来做到这一点。所以 y 乘以 y 密度的积分。我们已经找到了密度，所以我们可以在这里代入它。我们对可能的 y 的范围（从 0 到 l）进行积分。</p><p>Finally, we can calculate the expected value of Y. And we can do this by using the definition of the
            expectation.
            So integral of y times the density of y. We already found what that density is, so we can plug it in here.
            And
            we’re integrating over the range of possible y’s, from 0 to l.</p>
        <p>现在这涉及到 y log y 的积分，我相信你在微积分课上遇到过这个，但可能不记得怎么做了。无论如何，你都可以在一些积分表中查找它，或者分部分进行计算。你得到的最终答案是 l/4。这时，你会说，这是一个非常简单的答案。我难道不应该期望它是 l/4 吗？我想，是的。</p><p>Now this involves the integral for y log y, which I’m sure you have encountered in your calculus classes but
            maybe do not remember how to do it. In any case, you look it up in some integral tables or do it by parts.
            And
            you get the final answer of l/4. And at this point, you say, that’s a really simple answer. Shouldn’t I have
            expected it to be l/4? I guess, yes.</p>
        <p>我的意思是，当你折断一次后，预期剩余部分的长度应该是你折断部分的一半。当你下次折断时，预期剩余部分的长度应该是你现在折断部分的一半。</p><p>I mean, when you break it once, the expected value of what you are left with is going to be 1/2 of what you
            started with. When you break it the next time, the expected length of what you’re left with should be 1/2 of
            the
            piece that you are now breaking.</p>
        <h2 id="unknown-202">未知</h2><h2>Unknown</h2>
        <p>因此，每次你随机分解它时，你都希望它变小 1/2 倍。因此，如果你分解两次，你得到的结果是预期的 1/4。这是基于平均值的推理，在这种情况下，它恰好会给你正确的答案。但同样，需要注意的是，基于平均值的推理并不总是能给你正确的答案。所以在进行这种类型的论证时要小心。很好。周三见。</p><p>So each time that you break it at random, you expected it to become smaller by a factor of 1/2. So if you
            break
            it twice, you are left something that’s expected to be 1/4. This is reasoning on the average, which happens
            to
            give you the right answer in this case. But again, there’s the warning that reasoning on the average doesn’t
            always give you the right answer. So be careful about doing arguments of this type. Very good. See you on
            Wednesday.</p>
        <h1 id="continuous-bayes-rule-derived-distributions">10.连续贝叶斯规则；派生分布</h1><h1>10. Continuous Bayes’ Rule; Derived Distributions</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 48:53 <a href="https://youtube.com/watch?v=H_k1w3cfny8">https://youtube.com/watch?v=H_k1w3cfny8</a></p><p> 12 years ago (Nov 10, 2012) — 48:53 <a href="https://youtube.com/watch?v=H_k1w3cfny8">https://youtube.com/watch?v=H_k1w3cfny8</a></p>
        <h2 id="unknown-203">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：今天的议程是再谈一些有关连续随机变量的内容。我们主要会谈一下推理。
        </p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So today’s agenda is to
            say a
            few more things about continuous random variables. Mainly we’re going to talk a little bit about inference.
        </p>
        <p>这是我们学期末要重新讨论的一个主题。但是现在我们已经可以说一些事情了。今天的新主题是派生分布。基本上，如果你知道一个随机变量的分布，并且你有该随机变量的函数，那么如何找到该函数的分布。</p><p>This is a topic that we’re going to revisit at the end of the semester. But there’s a few things that we can
            already say at this point. And then the new topic for today is the subject of derived distributions.
            Basically
            if you know the distribution of one random variable, and you have a function of that random variable, how to
            find a distribution for that function.</p>
        <p>这是一项相当机械的技能，但很重要，所以我们将进行讲解。让我们看看我们的立场。这是总体情况。到目前为止，我们所做的就是这些。我们讨论了离散随机变量，我们用概率质量函数来描述它们。因此，如果我们有多个随机变量，我们用联合质量函数来描述它们。</p><p>And it’s a fairly mechanical skill, but that’s an important one, so we’re going to go through it. So let’s
            see
            where we stand. Here is the big picture. That’s all we have done so far. We have talked about discrete
            random
            variables, which we described by probability mass function. So if we have multiple random variables, we
            describe
            them with the a joint mass function.</p>
        <h2 id="unknown-204">未知</h2><h2>Unknown</h2>
        <p>然后我们定义条件概率，或条件 PMF，这三者根据这个公式相互关联，也就是说，你可以把它看作条件概率的定义。或者看作乘法规则，两件事发生的概率是第一件事发生的概率和第二件事发生的概率的乘积，前提是第一件事已经发生。</p><p>And then we define conditional probabilities, or conditional PMFs, and the three are related according to
            this
            formula, which is, you can think of it either as the definition of conditional probability. Or as the
            multiplication rule, the probability of two things happening is the product of the probabilities of the
            first
            thing happening, and then the second happening, given that the first has happened.</p>
        <p>这两者之间还有另一种关系，即 x 发生的概率是 x 可能出现的不同方式的不同概率之和，这与 y 的不同值有关。在连续世界中，所有这些都有类似之处，你所做的就是用 f 代替 p，用积分代替和。所以公式看起来都一样。</p><p>There’s another relation between this, which is the probability of x occurring, is the sum of the different
            probabilities of the different ways that x may occur, which is in conjunction with different values of y.
            And
            there’s an analog of all that in the continuous world, where all you do is to replace p’s by f’s, and
            replace
            sums by integrals. So the formulas all look the same.</p>
        <p>解释起来稍微微妙一些，所以 f 不是概率，而是概率密度。所以它们是单位长度的概率，或者在联合 PDf 的情况下，它们是单位面积的概率。所以它们是某种密度。这可能是理解条件密度真正含义的更微妙的概念。从某种意义上说，它很简单。</p><p>The interpretations are a little more subtle, so the f’s are not probabilities, they’re probability
            densities. So
            they’re probabilities per unit length, or in the case of joint PDf’s, these are probabilities per unit area.
            So
            they’re densities of some sort. Probably the more subtle concept to understand what it really is the
            conditional
            density. In some sense, it’s simple.</p>
        <h2 id="unknown-205">未知</h2><h2>Unknown</h2>
        <p>它只是在已知随机变量 Y 值的世界中 X 的密度。它是一个具有两个参数的函数，但最好的思考方式是说我们固定了 y。我们已知随机变量 Y 的值，并将其视为 x 的函数。</p><p>It’s just the density of X in a world where you have been told the value of the random variable Y. It’s a
            function that has two arguments, but the best way to think about it is to say that we fixed y. We’re told
            the
            value of the random variable Y, and we look at it as a function of x.</p>
        <p>因此，作为 x 的函数，分母是一个常数，当我们保持 y 不变时，它看起来就像联合密度。因此，它实际上是一个参数的函数，即参数 x。当您取其切片时，它具有与联合密度相同的形状。因此，条件 PDF 只是联合 PDF 的切片。</p><p>So as a function of x, the denominator is a constant, and it just looks like the joint density. when we keep
            y
            fixed. So it’s really a function of one argument, just the argument x. And it has the same shape as the
            joint’s
            density when you take that slice of it. So conditional PDFs are just slices of joint PDFs.</p>
        <p>有很多概念、期望、方差、累积分布函数同样适用于离散或连续随机变量的宇宙。那么为什么概率有用呢？概率之所以有用，是因为我们用它来理解我们周围的世界。我们用它来推断我们无法直接看到的事物。这是使用基本规则以非常简单的方式完成的。</p><p>There’s a bunch of concepts, expectations, variances, cumulative distribution functions that apply equally
            well
            for to both universes of discrete or continuous random variables. So why is probability useful? Probability
            is
            useful because, among other things, we use it to make sense of the world around us. We use it to make
            inferences
            about things that we do not see directly. And this is done in a very simple manner using the base rule.</p>
        <h2 id="unknown-206">未知</h2><h2>Unknown</h2>
        <p>我们已经看到了一些，现在我们将通过一系列不同的变化来重新审视它。变化的出现是因为我们的随机变量有时是离散的，有时是连续的，或者我们可以将两者结合起来。所以总体情况是，存在一些未知的随机变量，我们知道随机变量的分布。在离散情况下，它将由 PMF 给出。</p><p>We’ve already seen some of that, and now we’re going to revisit it with a bunch of different variations. And
            the
            variations come because sometimes our random variable are discrete, sometimes they’re continuous, or we can
            have
            a combination of the two. So the big picture is that there’s some unknown random variable out of there, and
            we
            know the distribution that’s random variable. And in the discrete case, it’s going to be given by PMF.</p>
        <p>在连续情况下，它会被赋予一个 PDF。然后我们有一些现象、一些噪声现象或一些测量设备，并且该测量设备会产生可观察的随机变量 Y。我们不知道 x 是什么，但我们对 X 的分布有一些信念。我们观察随机变量 Y。我们需要这个盒子的模型。而这个盒子的模型将是随机变量 Y 的 PMF。</p><p>In the continuous case, it’s given a PDF. Then we have some phenomenon, some noisy phenomenon or some
            measuring
            device, and that measuring device produces observable random variables Y. We don’t know what x is, but we
            have
            some beliefs about how X is distributed. We observe the random variable Y. We need a model of this box. And
            the
            model of that box is going to be either a PMF, for the random variable Y.</p>
        <p>该模型告诉我们，如果世界的真实状态是 X，我们预期 Y 会如何分布？这是针对 Y 是离散的情况。如果 Y 是连续的，那么 Y 可能会有一个密度，或者类似形式的值。因此，无论哪种情况，这都应该是我们已知的函数。这是我们的测量设备模型。</p><p>And that model tells us, if the true state of the world is X, how do we expect to Y to be distributed? That’s
            for
            the case where Y is this discrete. If Y is a continuous, you might instead have a density for Y, or
            something of
            that form. So in either case, this should be a function that’s known to us. This is our model of the
            measuring
            device.</p>
        <h2 id="unknown-207">未知</h2><h2>Unknown</h2>
        <p>现在，在观察到 y 之后，我们想要对 x 做出推断。做出推断意味着什么？好吧，推断问题中最完整的答案是告诉我未知量的概率分布。但是当我说概率分布时，我不是指这个。我指的是考虑到你得到的测量值的概率分布。</p><p>And now having observed y, we want to make inferences about x. What does it mean to make inferences? Well the
            most complete answer in the inference problem is to tell me the probability distribution of the unknown
            quantity. But when I say the probability distribution, I don’t mean this one. I mean the probability
            distribution that takes into account the measurements that you got.</p>
        <p>因此，推理问题的输出是根据我们已经观察到的情况得出 X（未知量）的分布。在离散情况下，它将是这样的物体。如果 X 是连续的，它将是这种类型的物体。好的，所以我们给出了这种类型的条件概率，我们想要得到相反类型的条件分布，其中条件的顺序是相反的。</p><p>So the output of an inference problem is to come up with the distribution of X, the unknown quantity, given
            what
            we have already observed. And in the discrete case, it would be an object like that. If X is continuous, it
            would be an object of this kind. OK, so we’re given conditional probabilities of this type, and we want to
            get
            conditional distributions of the opposite type where the order of the conditioning is being reversed.</p>
        <p>因此，起点始终是这样的公式。x 发生的概率，然后在 x 发生的情况下 y 也发生的概率。这是特定 x 和 y 同时发生的概率。但这也等于 y 发生的概率，然后在 y 发生的情况下 x 也发生的概率。</p><p>So the starting point is always a formula such as this one. The probability of x happening, and then y
            happening
            given that x happens. This is the probability that a particular x and y happen simultaneously. But this is
            also
            equal to the probability that y happens, and then that x happens, given that y has happened.</p>
        <h2 id="unknown-208">未知</h2><h2>Unknown</h2>
        <p>然后你取这个表达式并将一个项发送给另一边的分母，这就为我们提供了离散情况的基本规则。这个规则你已经看过了，并且已经尝试过了。所以这就是离散情况下的公式。两个随机变量都是离散的典型例子是我们前段时间讨论过的。</p><p>And you take this expression and send one term to the denominator of the other side, and this gives us the
            base
            rule for the discrete case. Which is this one that you have already seen, and you have played with it. So
            this
            is what the formula looks like in the discrete case. And the typical example where both random variables are
            discrete is the one we discussed some time ago.</p>
        <p>假设 X 是一个二元变量，即飞机是否存在于那里。Y 是一个离散测量，例如，我们的雷达是否发出哔哔声。根据我们所做的测量，我们可以进行推断，计算飞机在那里的概率，或者飞机不在那里的概率。当然，X 和 Y 不必只是二进制的。</p><p>X is, let’s say, a binary variable, or whether an airplane is present up there or not. Y is a discrete
            measurement, for example, whether our radar beeped or it didn’t beep. And we make inferences and calculate
            the
            probability that the plane is there, or the probability that the plane is not there, given the measurement
            that
            we have made. And of course X and Y do not need to be just binary.</p>
        <p>它们可能是更一般的离散随机变量。那么在连续情况下情况会如何变化呢？首先，连续情况可能的应用是什么？好吧，将 X 视为某个在连续范围内取值的信号。假设 X 是通过电阻器的电流。然后你有一些测量电流的测量设备，但该设备很嘈杂，它会受到高斯噪声的影响。</p><p>They could be more general discrete random variables. So how does the story change in the continuous case?
            First,
            what’s a possible application of the continuous case? Well, think of X as being some signal that takes
            values
            over a continuous range. Let’s say X is the current through a resistor. And then you have some measuring
            device
            that measures currents, but that device is noisy, it gets hit, let’s say for example, by Gaussian noise.</p>
        <h2 id="unknown-209">未知</h2><h2>Unknown</h2>
        <p>你观察到的 Y 是 X 的噪声版本。但你的仪器是模拟的，所以你测量的是连续的。在这种情况下你会怎么做？推理问题，推理问题的输出，将是 X 的条件分布。你认为你的电流是基于你观察到的 Y 的特定值吗？</p><p>And the Y that you observe is a noisy version of X. But your instruments are analog, so you measure things on
            a
            continuous scale. What are you going to do in that case? Well the inference problem, the output of the
            inference
            problem, is going to be the conditional distribution of X. What do you think your current is based on a
            particular value of Y that you have observed?</p>
        <p>因此，我们的推理问题的输出是，给定 Y 的特定值，计算整个函数作为 x 的函数，然后绘制它。我们如何计算它？您需要进行与离散情况相同的计算，只是所有 x 都被 p 替换。在连续情况下，关节密度是边际密度与条件密度的乘积，这同样正确。</p><p>So the output of our inference problem is, given the specific value of Y, to calculate this entire function
            as a
            function of x, and then go and plot it. How do we calculate it? You go through the same calculation as in
            the
            discrete case, except that all of the x’s gets replaced by p’s. In the continuous case, it’s equally true
            that
            the joint’s density is the product of the marginal density with the conditional density.</p>
        <p>因此，只要符号稍作改变，公式仍然有效。因此，我们最终得到相同的公式，只是用 p 替换了 x。因此，所有这些函数都是已知的。我们有它们的公式。我们固定 y 的特定值，将其代入，这样我们就得到了 x 的函数。这给了我们后验分布。</p><p>So the formula is still valid with just a little change of notation. So we end up with the same formula here,
            except that we replace x’s with p’s. So all of these functions are known to us. We have formulas for them.
            We
            fix a specific value of y, we plug it in, so we’re left with a function of x. And that gives us the
            posterior
            distribution.</p>
        <h2 id="unknown-210">未知</h2><h2>Unknown</h2>
        <p>实际上，还有一个分母项，它不一定是已知的，但如果我们有 X 的边际，并且有测量设备的模型，我们总是可以计算它。然后我们总能找到 Y 的边际分布。所以这个量，那个数字，一般也是已知的，不会给我们带来任何问题。</p><p>Actually there’s also a denominator term that’s not necessarily given to us, but we can always calculate it
            if we
            have the marginal of X, and we have the model for measuring device. Then we can always find the marginal
            distribution of Y. So this quantity, that number, is in general a known one, as well, and doesn’t give us
            any
            problems.</p>
        <p>为了让事情稍微复杂一点，我们还可以研究两个随机变量属于不同类型的情况。例如，一个随机变量可能是离散的，另一个可能是连续的。有两种情况。一种情况是 X 是离散的，但 Y 是连续的。这方面的例子是什么？
        </p><p>So to complicate things a little bit, we can also look into situations where our two random variables are of
            different kinds. For example, one random variable could be discrete, and the other it might be continuous.
            And
            there’s two versions. Here one version is when X is discrete, but Y is continuous. What’s an example of
            this?
        </p>
        <p>假设我发送了一个比特的信息，所以我的 X 是 0 或 1。我测量的是 Y，也就是 X 加上高斯噪声。这是任何通信或信号处理教科书中都会出现的标准示例。你发送了一个比特，但你观察到的是该比特的噪声版本。你从 x 的模型开始。这些将是你的先验概率。</p><p>Well suppose that I send a single bit of information so my X is 0 or 1. And what I measure is Y, which is X
            plus,
            let’s say, Gaussian noise. This is the standard example that shows up in any textbook on communication, or
            signal processing. You send a single bit, but what you observe is a noisy version of that bit. You start
            with a
            model of your x’s. These would be your prior probabilities.</p>
        <h2 id="unknown-211">未知</h2><h2>Unknown</h2>
        <p>例如，您可能认为 0 或 1 的可能性相同，在这种情况下，您的 PMF 会为两个可能值赋予相同的权重。然后我们需要测量设备的模型。这是一个特定的模型。一般模型的形状如下。Y 具有分布，即其密度。但是，该密度取决于 X 的值。</p><p>For example, you might be believe that either 0 or 1 are equally likely, in which case your PMF gives equal
            weight to two possible values. And then we need a model of our measuring device. This is one specific model.
            The
            general model would have a shape such as follows. Y has a distribution, its density. And that density,
            however,
            depends on the value of X.</p>
        <p>因此，当 x 为 0 时，我们可能会得到这种密度。而当 x 为 1 时，我们可能会得到另一种密度。因此，这些是 y 在由特定 x 值指定的宇宙中的条件密度。然后我们继续进行推断。好的，进行这种推断的正确公式是什么？</p><p>So when x is 0, we might get a density of this kind. And when x is 1, we might get the density of a different
            kind. So these are the conditional densities of y in a universe that’s specified by a particular value of x.
            And
            then we go ahead and do our inference. OK, what’s the right formula for doing this inference?</p>
        <p>我们需要一个与这个公式类似的公式，但适用于我们有两个不同类型的随机变量的情况。所以让我在这里重新计算一下。不过我不会给出取特定值的概率。它必须有点不同。所以事情是这样的。
        </p><p>We need a formula that’s sort of an analog of this one, but applies to the case where we have two random
            variables of different kinds. So let me just redo this calculation here. Except that I’m not going to have a
            probability of taking specific values. It will have to be something a little different. So here’s how it
            goes.
        </p>
        <h2 id="unknown-212">未知</h2><h2>Unknown</h2>
        <p>让我们看看 X 取特定值的概率，这在离散情况下是有意义的，但对于连续随机变量，让我们看看它在某个小区间取值的概率。现在，我将把这两件事发生的概率写成乘积。我将用两种不同的方式把它写成乘积。</p><p>Let’s look at the probability that X takes a specific value that makes sense in the discrete case, but for
            the
            continuous random variable, let’s look at the probability that it takes values in some little interval. And
            now
            this probability of two things happening, I’m going to write it as a product. And I’m going to write this as
            a
            product in two different ways.</p>
        <p>因此，一种方法是说这是 X 取该值的概率，然后假设 X 取该值，则 Y 落在该区间内的概率。因此，这是我们乘以概率的常用乘法规则，但我也可以以不同的方式使用乘法规则。这是 Y 落在感兴趣范围内的概率。</p><p>So one way is to say that this is the probability that X takes that value and then given that X takes that
            value,
            the probability that Y falls inside that interval. So this is our usual multiplication rule for multiplying
            probabilities, but I can use the multiplication rule also in a different way. It’s the probability that Y
            falls
            in the range of interest.</p>
        <p>然后，在 Y 满足第一个条件的情况下，X 取感兴趣的值的概率。所以这肯定是真的。我们只是使用乘法规则。现在让我们把它翻译成 PMF 即 PDF 符号。所以上面的条目是在 x 处求值的 X 的 PMF。第二个条目是什么？嗯，密度给我们提供了小间隔的概率。</p><p>And then the probability that X takes the value of interest given that Y satisfies the first condition. So
            this
            is something that’s definitely true. We’re just using the multiplication rule. And now let’s translate it
            into
            PMF is PDF notation. So the entry up there is the PMF of X evaluated at x. The second entry, what is it?
            Well
            probabilities of little intervals are given to us by densities.</p>
        <h2 id="unknown-213">未知</h2><h2>Unknown</h2>
        <p>但是我们处在条件宇宙中，X 取特定值。因此，给定 X 值乘以 delta，Y 的密度就是该值。因此，小间隔的概率由密度乘以小间隔的长度给出，但是因为我们是在条件宇宙中工作，所以它必须是条件密度。现在让我们尝试第二个表达式。</p><p>But we are in the conditional universe where X takes on a particular value. So it’s going to be the density
            of Y
            given the value of X times delta. So probabilities of little intervals are given by the density times the
            length
            of the little interval, but because we’re working in the conditional universe, it has to be the conditional
            density. Now let’s try the second expression.</p>
        <p>这是 Y 落入小区间的概率。所以这是 Y 密度乘以 delta。然后这里我们有一个对象，即在 Y 值给定的宇宙中的条件概率 X。现在这个关系有点近似。对于极限中非常小的 delta 来说，这是正确的。</p><p>This is the probability that the Y falls into the little interval. So that’s the density of Y times delta.
            And
            then here we have an object which is the conditional probability X in a universe where the value of Y is
            given
            to us. Now this relation is sort of approximate. This is true for very small delta in the limit.</p>
        <p>但是我们可以从两边抵消增量，这样就剩下一个公式，将 PMF 和 PDF 联系在一起。现在这看起来可能非常令人困惑，因为其中涉及到 p 和 f。但逻辑应该很清楚。如果一个随机变量是离散的，它就由 PMF 描述。所以这里我们讨论的是 X 在某个特定宇宙中的 PMF。X 是离散的，所以它有一个 PMF。这里也是一样。</p><p>But we can cancel the deltas from both sides, and we’re left with a formula that links together PMFs and
            PDFs.
            Now this may look terribly confusing because there’s both p’s and f’s involved. But the logic should be
            clear.
            If a random variable is discrete, it’s described by PMF. So here we’re talking about the PMF of X in some
            particular universe. X is discrete, so it has a PMF. Similarly here.</p>
        <h2 id="unknown-214">未知</h2><h2>Unknown</h2>
        <p>Y 是连续的，所以可以用 PDF 来描述。即使在条件宇宙中，我告诉你 X 的值，Y 仍然是一个连续随机变量，所以可以用 PDF 来描述。所以这是将 PMF 和 PDF 联系在一起的基本关系。在这个混合的世界里。现在在这个不等式中，你可以取这个项并将其发送到另一边的新分母。</p><p>Y is continuous so it’s described by a PDF. And even in the conditional universe where I tell you the value
            of X,
            Y is still a continuous random variable, so it’s been described by a PDF. So this is the basic relation that
            links together PMF and PDFs. In this mixed the world. And now in this inequality, you can take this term and
            send it to the new denominator to the other side.</p>
        <p>最后得到的就是我们上面的公式。当我们得知连续随机变量 Y 的值时，我们可以用这个公式推断离散随机变量 X。X 取特定值的概率与先验有关。除此之外，它与这个量成正比，即给定 X 时 Y 的条件。</p><p>And what you end up with is the formula that we have up here. And this is a formula that we can use to make
            inferences about the discrete random variable X when we’re told the value of the continuous random variable
            Y.
            The probability that X takes on a particular value has something to do with the prior. And other than that,
            it’s
            proportional to this quantity, the conditional of Y given X.</p>
        <p>这些就是我们在这里绘制的数量。假设 x 在你的先验中是等概率的，所以我们并不真正关心这个术语。它告诉我们，在给定 x 的情况下，X 的后验与该特定密度成正比。所以在这幅图中，如果我在这里得到一个特定的 y，我会说 x 等于 1 的概率与这个数量成正比。</p><p>So these are the quantities that we plotted here. Suppose that the x’s are equally likely in your prior, so
            we
            don’t really care about that term. It tells us that the posterior of X is proportional to that particular
            density under the given x’s. So in this picture, if I were to get a particular y here, I would say that x
            equals
            1 has a probability that’s proportional to this quantity.</p>
        <h2 id="unknown-215">未知</h2><h2>Unknown</h2>
        <p>X 等于 0 的概率与该数量成正比。因此，这两个数量的比率给出了我们观察到的 y 对应的不同 x 的相对概率。因此，我们将在课程结束时回到这个主题并重做大量此类示例，届时我们将花一些时间专门研究推理问题。</p><p>X equals 0 has a probability that’s proportional to this quantity. So the ratio of these two quantities gives
            us
            the relative odds of the different x’s given the y that we have observed. So we’re going to come back to
            this
            topic and redo plenty of examples of these kinds towards the end of the class, when we spend some time
            dedicated
            to inference problems.</p>
        <p>但在这个阶段，我们已经掌握了处理这些问题的基本技能。此时将所有公式汇总在一起很有用。最后，让我们看看剩下的最后一个案例。这里我们有一个连续现象，我们试图测量它，但我们的测量是离散的。这种情况可能发生的例子是什么？</p><p>But already at this stage, we sort of have the basic skills to deal with a lot of that. And it’s useful at
            this
            point to pull all the formulas together. So finally let’s look at the last case that’s remaining. Here we
            have a
            continuous phenomenon that we’re trying to measure, but our measurements are discrete. What’s an example
            where
            this might happen?</p>
        <p>因此，你有一个发光的设备，你用具有一定强度的电流驱动它。你不知道电流是多少，它是一个连续的随机变量。但该设备通过发出单个光子来发光。你的测量是另一个设备，它计算你在一秒钟内获得了多少光子。</p><p>So you have some device that emits light, and you drive it with a current that has a certain intensity. You
            don’t
            know what that current is, and it’s a continuous random variable. But the device emits light by sending out
            individual photons. And your measurement is some other device that counts how many photons did you get in a
            single second.</p>
        <h2 id="unknown-216">未知</h2><h2>Unknown</h2>
        <p>因此，如果我们的设备发射的强度非常低，那么在观察单个光子时，您实际上可以开始对其进行计数。因此，我们有一个离散测量，即问题的数量，并且我们有一个连续的隐藏随机变量，我们正试图估计它。在这种情况下我们该怎么做？好吧，我们再次从这种公式开始，并将 p 项发送到分母。</p><p>So if we have devices that emit a very low intensity you can actually start counting individual photons as
            they’re being observed. So we have a discrete measurement, which is the number of problems, and we have a
            continuous hidden random variable that we’re trying to estimate. What do we do in this case? Well we start
            again
            with a formula of this kind, and send the p term to the denominator.</p>
        <p>这就是我们在那里使用的公式，只是 x 和 y 的角色互换了。所以因为这里的 Y 是离散的，我们应该改变所有的下标。它将是 p_Y f_X 给定 y f_X，和 P(Y 给定 X)。所以只要改变所有这些下标就行了。因为现在我们习惯的连续变成了离散的，反之亦然。</p><p>And that’s the formula that we use there, except that the roles of x’s and y’s are interchanged. So since
            here we
            have Y being discrete, we should change all the subscripts. It would be p_Y f_X given y f_X, and P(Y given
            X).
            So just change all those subscripts. Because now what we’re used to be continuous became discrete, and vice
            versa.</p>
        <p>取该公式，将其他项代入分母，我们就可以得到密度或 X 的公式，给定我们获得的 Y 的特定测量值。从某种意义上说，这就是贝叶斯推理的全部内容。它使用这些非常简单的一行公式。但为什么有人靠解决推理问题为生呢？好吧，魔鬼就在细节中。</p><p>Take that formula, send the other terms to the denominator, and we have a formula for the density, or X,
            given
            the particular measurements for Y that we have obtained. In some sense that’s all there is in Bayesian
            inference. It’s using these very simple one line formulas. But why are there people then who make their
            living
            solving inference problems? Well, the devil is in the details.</p>
        <h2 id="unknown-217">未知</h2><h2>Unknown</h2>
        <p>正如我们将要讨论的，有一些现实世界的问题，比如你如何设计你的 f，如何为你的系统建模，然后如何进行计算。这可能并不总是那么容易。例如，有些积分或和需要求值，这可能很难做到等等。所以这个对象比这些公式要丰富得多。</p><p>As we’re going to discuss, there are some real world issues of how exactly do you design your f’s, how do you
            model your system, then how do you do your calculations. This might not be always easy. For example, there’s
            certain integrals or sums that have to be evaluated, which may be hard to do and so on. So this object is a
            lot
            of richer than just these formulas.</p>
        <p>另一方面，在概念层面，这是贝叶斯推理的基础，这些是基本概念。好的，现在让我们换个话题，转到新的主题，即寻找随机变量函数的分布。我们称这些分布为派生分布，因为我们给定了 X 的分布。我们对 X 的函数感兴趣。</p><p>On the other hand, at the conceptual level, that’s the basis for Bayesian inference, that these are the basic
            concepts. All right, so now let’s change gear and move to the new subject, which is the topic of finding the
            distribution of a functional for a random variable. We call those distributions derived distributions,
            because
            we’re given the distribution of X. We’re interested in a function of X.</p>
        <p>我们想根据已知的分布推导出该函数的分布。因此，它可能是一个随机变量的函数。也可能是多个随机变量的函数。我们打算在某个时刻求解的一个例子是，假设你必须计算变量 X 和 Y。有人告诉你，它们的分布是平方均匀分布。</p><p>We want to derive the distribution of that function based on the distribution that we already know. So it
            could
            be a function of just one random variable. It could be a function of several random variables. So one
            example
            that we are going to solve at some point, let’s say you have to run the variables X and Y. Somebody tells
            you
            their distribution, for example, is a uniform of the square.</p>
        <h2 id="unknown-218">未知</h2><h2>Unknown</h2>
        <p>出于某种原因，你对这两个随机变量的比率很感兴趣，并且想要找到该比率的分布。你可以想到很多情况，其中你感兴趣的随机变量是通过取一些其他未知变量并取它们的函数而创建的。因此，关心该随机变量的分布是合理的。但是，有一个警告。</p><p>For some reason, you’re interested in the ratio of these two random variables, and you want to find the
            distribution of that ratio. You can think of lots of cases where your random variable of interest is created
            by
            taking some other unknown variables and taking a function of them. And so it’s legitimate to care about the
            distribution of that random variable. A caveat, however.</p>
        <p>有一种重要的情况，你不需要找到该随机变量的分布。这就是你想要计算期望的时候。如果你关心的只是这个随机变量函数的期望值，那么你可以直接使用原始随机变量的分布，而不必找到 g 的 PDF。</p><p>There’s an important case where you don’t need to find the distribution of that random variable. And this is
            when
            you want to calculate the expectations. If all you care about is the expected value of this function of the
            random variables, you can work directly with the distribution of the original random variables without ever
            having to find the PDF of g.</p>
        <p>因此，如果不需要，你就不会做不必要的工作，但如果需要，或者被要求做，那么你就去做。那么我们如何找到函数的分布呢？作为热身，让我们看看离散的情况。假设 X 是一个离散随机变量，并采用某些值。我们有一个函数 g，它将 x 映射到 y。我们想找到 Y 的概率质量函数。</p><p>So you don’t do unnecessary work if it’s not needed, but if it’s needed, or if you’re asked to do it, then
            you
            just do it. So how do we find the distribution of the function? As a warm up, let’s look at the discrete
            case.
            Suppose that X is a discrete random variable and takes certain values. We have a function g that maps x’s
            into
            y’s. And we want to find the probability mass function for Y.</p>
        <h2 id="unknown-219">未知</h2><h2>Unknown</h2>
        <p>例如，如果我有兴趣找到 Y 取这个特定值的概率，他们会如何找到它？我会问，这些特定的 y 值可能有哪些不同的发生方式？而它可能发生的不同方式是 x 取这个值，或者 X 取那个值。所以我们将 y 空间中的这个事件与 x 空间中的那个事件等同起来。这两个事件是相同的。</p><p>So for example, if I’m interested in finding the probability that Y takes on this particular value, how would
            they find it? Well I ask, what are the different ways that these particular y value can happen? And the
            different ways that it can happen is either if x takes this value, or if X takes that value. So we identify
            this
            event in the y space with that event in the x space. These two events are identical.</p>
        <p>当且仅当 Y 属于该集合时，X 才属于该集合。因此，Y 属于该集合的概率就是 X 属于该集合的概率。X 属于该集合的概率就是该集合中各个 x 的概率之和。</p><p>X falls in this set if and only if Y falls in that set. Therefore, the probability of Y falling in that set
            is
            the probability of X falling in that set. The probability of X falling in that set is just the sum of the
            individual probabilities of the x’s in this set.</p>
        <p>因此，我们只需将不同 x 的概率相加，然后对所有 x 求和，即可得到 y 的特定值。非常好。离散情况下就是这样。这非常简单。让我们将这些方法转移到连续情况下。假设我们处于连续情况下。假设 X 和 Y 现在可以取任意值。</p><p>So we just add the probabilities of the different x’s where the summation is taken over all x’s that leads to
            that particular value of y. Very good. So that’s all there is in the discrete case. It’s a very nice and
            simple.
            So let’s transfer these methods to the continuous case. Suppose we are in the continuous case. Suppose that
            X
            and Y now can take values anywhere.</p>
        <h2 id="unknown-220">未知</h2><h2>Unknown</h2>
        <p>我尝试使用相同的方法，并问 Y 取这个值的概率是多少？至少如果图表是这样的，你会说这与 X 取这个值的概率相同。所以我可以根据 X 是那个值的概率来找到 Y 是这个值的概率。这有用吗？在连续情况下，它没用。</p><p>And I try to use same methods and I ask, what is the probability that Y is going to take this value? At least
            if
            the diagram is this way, you would say this is the same as the probability that X takes this value. So I can
            find the probability of Y being this in terms of the probability of X being that. Is this useful? In the
            continuous case, it’s not.</p>
        <p>因为在连续情况下，任何单个值的概率都是 0。所以从这个论点中你得到的结论是，Y 取这个值的概率是 0，等于 X 取这个值的概率也是 0。这对我们没有帮助。我们想做更多的事情。我们可能想真正找到 Y 的密度，而不是单个 y 的概率。</p><p>Because in the continuous case, any single value has 0 probability. So what you’re going to get out of this
            argument is that the probability Y takes this value is 0, is equal to the probability that X takes that
            value
            which also 0. That doesn’t help us. We want to do something more. We want to actually find, perhaps, the
            density
            of Y, as opposed to the probabilities of individual y’s.</p>
        <p>因此，要找到 Y 的密度，您可以进行如下论证。我查看 y 的一个区间，并询问落在这个区间的概率是多少。然后您返回并找到导致这些 y 的对应 x 集，并将这两个概率相等。所有这些 y 的概率加起来应该等于所有映射到该区间的 x 的概率加起来。</p><p>So to find the density of Y, you might argue as follows. I’m looking at an interval for y, and I ask what’s
            the
            probability of falling in this interval. And you go back and find the corresponding set of x’s that leads to
            those y’s, and equate those two probabilities. The probability of all of those y’s collectively should be
            equal
            to the probability of all of the x’s that map into that interval collectively.</p>
        <h2 id="unknown-221">未知</h2><h2>Unknown</h2>
        <p>这样你就可以把两者联系起来。就机制而言，在许多情况下，不处理小间隔会更容易，而使用过去处理大间隔的累积分布函数会更容易。所以你可以画一幅不同的图。看看这组 y。这是一组小于某个值的 y。</p><p>And this way you can relate the two. As far as the mechanics go, in many cases it’s easier to not to work
            with
            little intervals, but instead to work with cumulative distribution functions that used to work with sort of
            big
            intervals. So you can instead do a different picture. Look at this set of y’s. This is the set of y’s that
            are
            smaller than a certain value.</p>
        <p>这个集合的概率由随机变量 Y 的累积分布给出。现在这个 y 集合由一些对应的 x 集合产生。也许这些是映射到该集合中的 y 的 x。然后我们进行如下论证。Y 落在这个区间的概率与 X 落在该区间的概率相同。</p><p>The probability of this set is given by the cumulative distribution of the random variable Y. Now this set of
            y’s
            gets produced by some corresponding set of x’s. Maybe these are the x’s that map into y’s in that set. And
            then
            we argue as follows. The probability that the Y falls in this interval is the same as the probability that X
            falls in that interval.</p>
        <p>因此，Y 落在这里的事件和 X 落在那里的事件是相同的，所以它们的概率必须相等。然后我在这里进行计算。最后我得到了 Y 的累积分布函数。一旦我有了累积函数，我只需微分就可以得到密度。所以这是我们将用来计算派生分布的一般方法。</p><p>So the event of Y falling here and the event of X falling there are the same, so their probabilities must be
            equal. And then I do the calculations here. And I end up getting the cumulative distribution function of Y.
            Once
            I have the cumulative, I can get the density by just differentiating. So this is the general cookbook
            procedure
            that we will be using to calculate it derived distributions.</p>
        <h2 id="unknown-222">未知</h2><h2>Unknown</h2>
        <p>我们感兴趣的是随机变量 Y，它是 x 的函数。我们的目标是获得 Y 的累积分布。以某种方式设法计算出此事件的概率。一旦我们得到它，我所说的得到它，并不意味着得到单个小 y 值的概率。你需要得到所有小 y 值的概率。所以你需要得到函数本身，即累积分布。</p><p>We’re interested in a random variable Y, which is a function of the x’s. We will aim at obtaining the
            cumulative
            distribution of Y. Somehow, manage to calculate the probability of this event. Once we get it, and what I
            mean
            by get it, I don’t mean getting it for a single value of little y. You need to get this for all little y’s.
            So
            you need to get the function itself, the cumulative distribution.</p>
        <p>一旦得到该形式，就可以计算任意特定点的导数。这将给出 Y 的密度。这是一个简单的两步过程。关键在于如何执行机制的细节。让我们先举一个例子。假设 X 是一个均匀随机变量，取值在 0 到 2 之间。我们感兴趣的是随机变量 Y，它是 X 的立方。</p><p>Once you get it in that form, then you can calculate the derivative at any particular point. And this is
            going to
            give you the density of Y. So a simple two step procedure. The devil is in the details of how you carry the
            mechanics. So let’s do one first example. Suppose that X is a uniform random variable, takes values between
            0
            and 2. We’re interested in the random variable Y, which is the cube of X.</p>
        <p>它会有什么样的分布？现在首先注意到 Y 取 0 到 8 之间的值。所以 X 是均匀分布的，所以所有 x 都有同等的可能性。然后你可能会说，好吧，在这种情况下，所有 y 都应该有同等的可能性。所以 Y 也可能有一个均匀分布。这是真的吗？我们会找出答案。所以让我们开始应用食谱程序。</p><p>What kind of distribution is it going to have? Now first notice that Y takes values between 0 and 8. So X is
            uniform, so all the x’s are equally likely. You might then say, well, in that case, all the y’s should be
            equally likely. So Y might also have a uniform distribution. Is this true? We’ll find out. So let’s start
            applying the cookbook procedure.</p>
        <h2 id="unknown-223">未知</h2><h2>Unknown</h2>
        <p>我们首先要找到随机变量 Y 的累积分布，根据定义，它是随机变量小于或等于某个数字的概率。这就是我们想要找到的。我们手中掌握的是 X 的分布。这就是我们需要处理的。</p><p>We want to find first the cumulative distribution of the random variable Y, which by definition is the
            probability that the random variable is less than or equal to a certain number. That’s what we want to find.
            What we have in our hands is the distribution of X. That’s what we need to work with.</p>
        <p>所以你需要做的第一步是查看这些事件并对其进行转换，然后将其写成你知道有相关信息的随机变量。所以 Y 是 X 的立方，所以这个事件与那个事件相同。所以现在我们可以忘记 y 了。这只是一个涉及具有已知分布的单个随机变量的练习，我们想要计算某个事件的概率。</p><p>So the first step that you need to do is to look at this events and translate it, and write it in terms of
            the
            random variable about which you know you have information. So Y is X cubed, so this event is the same as
            that
            event. So now we can forget about the y’s. It’s just an exercise involving a single random variable with a
            known
            distribution and we want to calculate the probability of some event.</p>
        <p>我们现在来看这个事件。X 的立方小于或等于 Y。我们对这个表达式进行调整，这样它就直接涉及到 X，所以让我们对这个不等式的两边取立方根。这个事件与 X 小于或等于 Y 的 1/3 的事件相同。现在，如果采用均匀分布，这个概率是多少？</p><p>So we’re looking at this event. X cubed being less than or equal to Y. We massage that expression so that’s
            it
            involves X directly, so let’s take cubic roots of both sides of this inequality. This event is the same as
            the
            event that X is less than or equal to Y to the 1/3. Now with a uniform distribution on what is that
            probability
            going to be?</p>
        <h2 id="unknown-224">未知</h2><h2>Unknown</h2>
        <p>这是从 0 到 y 的 1/3 的区间内的概率，所以它将位于均匀分布的面积中，直到该点。均匀分布的面积是多少？所以这是 x。这是 X 的分布。它上升到 2。X 的分布就是这个。我们希望上升到 y 的 1/3。所以这个事件发生的概率就是这个面积。</p><p>It’s the probability of being in the interval from 0 to y to the 1/3, so it’s going to be in the area under
            the
            uniform going up to that point. And what’s the area under that uniform? So here’s x. Here is the
            distribution of
            X. It goes up to 2. The distribution of X is this one. We want to go up to y to the 1/3. So the probability
            for
            this event happening is this area.</p>
        <p>面积等于底边，也就是 y 除以 1/3 的高度。高度是多少？因为密度必须积分为 1，所以曲线下的总面积必须是 1。所以这里的高度是 1/2，这就解释了为什么我们得到下面的 1/2 因子。这就是累积分布的公式。然后剩下的就很简单了。你只需求导数即可。</p><p>And the area is equal to the base, which is y to the 1/3 times the height. What is the height? Well since the
            density must integrate to 1, the total area under the curve has to be 1. So the height here is 1/2, and that
            explains why we get the 1/2 factor down there. So that’s the formula for the cumulative distribution. And
            then
            the rest is easy. You just take derivatives.</p>
        <p>你对这个表达式求 1/2 乘以 1/3 的 y 的导数，y 会下降一个幂。所以分母是 y 的 2/3。所以如果你想画出这个图形，它就是 1/y 的 2/3。所以当 y 趋近于 0 时，它就会爆炸，然后继续这样下去。我画的这个图正确吗？它有什么问题？教授：是的。
        </p><p>You differentiate this expression with respect to y 1/2 times 1/3, and y drops by one power. So you get y to
            2/3
            in the denominator. So if you wish to plot this, it’s 1/y to the 2/3. So when y goes to 0, it sort of blows
            up
            and it goes on this way. Is this picture correct the way I’ve drawn it? What’s wrong with it? PROFESSOR:
            Yes.
        </p>
        <h2 id="unknown-225">未知</h2><h2>Unknown</h2>
        <p>Y 只取 0 到 8 之间的值。我在这里写的这个公式只有在预览图适用时才是正确的。我将 y 取为 1/3，使其介于 0 和 2 之间。因此，这里的这个公式只对 0 到 8 之间的 y 是正确的。因此，导数公式也只对 0 到 8 之间的 ay 成立。任何其他值都是不可能的，所以它们得到零密度。</p><p>Y only takes values from 0 to 8. This formula that I wrote here is only correct when the preview picture
            applies.
            I took my y to the 1/3 to be between 0 and 2. So this formula here is only correct for y between 0 and 8.
            And
            for that reason, the formula for the derivative is also true only for a y between 0 and 8. And any other
            values
            of why are impossible, so they get zero density.</p>
        <p>因此，为了完成这里的图像，y 的 PDF 的截止值为 8，其他地方也为 0。我们看到的一件事是 Y 的分布并不均匀。某些 y 比其他 y 更有可能，即使我们从均匀随机变量 X 开始。好的。所以我们将继续做这种例子，一个逐渐变得更有趣或更复杂的序列。</p><p>So to complete the picture here, the PDF of y has a cut off of 8, and it’s also 0 everywhere else. And one
            thing
            that we see is that the distribution of Y is not uniform. Certain y’s are more likely than others, even
            though
            we started with a uniform random variable X. All right. So we will keep doing examples of this kind, a
            sequence
            of progressively more interesting or more complicated.</p>
        <p>下一节课会继续讲这个。你们会在复习课和教程等中看到很多例子。让我们来做一个和之前做的非常相似的例子，但是它会在机械原理上增加一点小变化。好的，当你开始开车时，你要设置巡航控制。然后你继续以恒定速度行驶。</p><p>So that’s going to continue in the next lecture. You’re going to see plenty of examples in your recitations
            and
            tutorials and so on. So let’s do one that’s pretty similar to the one that we did, but it’s going to add to
            just
            a small twist in how we do the mechanics. OK so you set your cruise control when you start driving. And you
            keep
            driving at the constants based at the constant speed.</p>
        <h2 id="unknown-226">未知</h2><h2>Unknown</h2>
        <p>您的巡航控制设置值介于 30 和 60 之间。您要行驶 200 英里。因此，您行程所需的时间比巡航控制设置值多 200 英里。因此，它是 200/V。</p><p>Where you set your cruise control is somewhere between 30 and 60. You’re going to drive a distance of 200.
            And so
            the time it’s going to take for your trip is 200 over the setting of your cruise control. So it’s 200/V.</p>
        <p>有人给你 V 的分布，他们告诉你它不仅在 30 到 60 之间，而且在 30 到 60 之间的任何值都有可能，所以我们在这个范围内有一个均匀分布。所以我们有 V 的分布。我们想找到随机变量 T 的分布，也就是你的旅程结束所需的时间。那么我们该怎么做呢？</p><p>Somebody gives you the distribution of V, and they tell you not only it’s between 30 and 60, it’s roughly
            equally
            likely to be anything between 30 and 60, so we have a uniform distribution over that range. So we have a
            distribution of V. We want to find the distribution of the random variable T, which is the time it takes
            till
            your trip ends. So how are we going to proceed?</p>
        <p>我们将使用完全相同的常规程序。我们将首先找到 T 的累积分布。这是什么？根据定义，累积分布是 T 小于某个数字的概率。好的。现在我们不知道 T 的分布，所以我们不能直接处理这些事件。但我们会获取该事件并将其转换为 T 空间。</p><p>We’ll use the exact same cookbook procedure. We’re going to start by finding the cumulative distribution of
            T.
            What is this? By definition, the cumulative distribution is the probability that T is less than a certain
            number. OK. Now we don’t know the distribution of T, so we cannot to work with these event directly. But we
            take
            that event and translate it into T space.</p>
        <h2 id="unknown-227">未知</h2><h2>Unknown</h2>
        <p>因此，我们用我们已知的 T 来代替 V 或 v 的值。好了，我们有了 V 的分布。现在让我们计算这个数量。好的。让我们处理这个事件，并将其重写为 V 大于或等于 200/T 的概率。那么这个概率是多少呢？假设 200/T 是落在该范围内的某个数字。</p><p>So we replace the t’s by what we know T to be in terms of V or the v’s All right. So we have the distribution
            of
            V. So now let’s calculate this quantity. OK. Let’s massage this event and rewrite it as the probability that
            V
            is larger or equal to 200/T. So what is this going to be? So let’s say that 200/T is some number that falls
            inside the range.</p>
        <p>因此，如果 200/T 大于 30 且小于 60，则结果为真。这意味着 t 小于 30/200。不，是 200/30。并且大于 200/60。因此，对于该范围内的 t，这个数字 200/t 就属于该范围内。根据我们设定的问题描述，这是可能的 t 范围。</p><p>So that’s going to be true if 200/T is bigger than 30, and less than 60. Which means that t is less than
            30/200.
            No, 200/30. And bigger than 200/60. So for t’s inside that range, this number 200/t falls inside that range.
            This is the range of t’s that are possible, given the description of the problem the we have set up.</p>
        <p>那么对于该范围内的 t，V 大于该数字的概率是多少？因此，V 大于该数字是该事件的概率，因此它将是该曲线下的面积。因此，该曲线下的面积是曲线的高度，即底边乘以 30 的 1/3。底边有多大？</p><p>So for t’s in that range, what is the probability that V is bigger than this number? So V being bigger than
            that
            number is the probability of this event, so it’s going to be the area under this curve. So the area under
            that
            curve is the height of the curve, which is 1/3 over 30 times the base. How big is the base?</p>
        <h2 id="unknown-228">未知</h2><h2>Unknown</h2>
        <p>嗯，它从那个点到 60，所以底边的长度是 60 减去 200/t。这个公式对那些符合该图的 t 有效。如果 200/T 恰好落在这个区间内，也就是 T 落在那个区间内，那么这个图是正确的，这些 t 是可能的。最后让我们找到 T 的密度，这就是我们要找的。</p><p>Well it’s from that point to 60, so the base has a length of 60 minus 200/t. And this is a formula which is
            valid
            for those t’s for which this picture is correct. And this picture is correct if 200/T happens to fall in
            this
            interval, which is the same as T falling in that interval, which are the t’s that are possible. So finally
            let’s
            find the density of T, which is what we’re looking for.</p>
        <p>我们通过对该表达式求 t 的导数来得到这个值。我们只得到一个项。它将是 200/30,1 除以 t 平方。这是允许范围内 t 的密度公式。好的，这也是这个特定问题的解决方案的结束。我说与上一个相比，这里有一点曲折。是什么曲折？</p><p>We find this by taking the derivative in this expression with respect to t. We only get one term from here.
            And
            this is going to be 200/30,1 over t squared. And this is the formula for the density for t’s in the allowed
            to
            range. OK, so that’s the end of the solution to this particular problem as well. I said that there was a
            little
            twist compared to the previous one. What was the twist?</p>
        <p>好吧，问题的关键在于，在上一个问题中，我们处理的是 X 立方函数，它是单调递增的。这里我们处理的是单调递减的函数。因此，当我们必须找到 T 小于某个值的概率时，这转化为 V 大于某个值的事件。当且仅当你的速度大于某个值时，你的时间才会小于某个值。</p><p>Well the twist was that in the previous problem we dealt with the X cubed function, which was monotonically
            increasing. Here we dealt with the function that was monotonically decreasing. So when we had to find the
            probability that T is less than something, that translated into an event that V was bigger than something.
            Your
            time is less than something if and only if your velocity is bigger than something.</p>
        <h2 id="unknown-229">未知</h2><h2>Unknown</h2>
        <p>因此，当你处理单调递减函数时，某些不等式在某些时候必须被逆转。最后，让我们看一个非常有用的例子。在这种情况下，我们取一个随机变量的线性函数。因此，X 是一个具有给定分布的随机变量，我们可以看到有一个线性函数。
        </p><p>So for when you’re dealing with the monotonically decreasing function, at some point some inequalities will
            have
            to get reversed. Finally let’s look at a very useful one. Which is the case where we take a linear function
            of a
            random variable. So X is a random variable with given distribution, and we can see there is a linear
            function.
        </p>
        <p>因此，在这个特定实例中，我们取 a 等于 2，b 等于 5。我们先通过图片来讨论。因此，X 是一个具有给定分布的随机变量。假设它是这个奇怪的形状。x 的范围从 1 到 +2。我们一步一步来。我们先找到 2X 的分布。你认为你为什么了解 2X？</p><p>So in this particular instance, we take a to be equal to 2 and b equal to 5. And let us first argue just by
            picture. So X is a random variable that has a given distribution. Let’s say it’s this weird shape here. And
            x
            ranges from 1 to +2. Let’s do things one step at the time. Let’s first find the distribution of 2X. Why do
            you
            think you know about 2X?</p>
        <p>如果 x 的范围是 1 到 2，那么随机变量 X 的范围就是 2 到 +4。这就是范围。现在处理随机变量 2X，而不是随机变量 X，在某种意义上，它只是改变了我们测量该随机变量的单位。它只是改变了我们绘制和标绘事物的比例。</p><p>Well if x ranges from 1 to 2, then the random variable X is going to range from 2 to +4. So that’s what the
            range
            is going to be. Now dealing with the random variable 2X, as opposed to the random variable X, in some sense
            it’s
            just changing the units in which we measure that random variable. It’s just changing the scale on which we
            draw
            and plot things.</p>
        <h2 id="unknown-230">未知</h2><h2>Unknown</h2>
        <p>因此，如果只是比例变化，那么直觉会告诉你，随机变量 X 应该具有相同形状的 PDF，只是它被放大了 2 倍，因为我们的随机变量 2X 现在的范围是原来的两倍。因此，我们采用相同的 PDF，并通过将 x 轴拉伸 2 倍来放大它。那么，从公式上看，缩放对应什么呢？</p><p>So if it’s just a scale change, then intuition should tell you that the random variable X should have a PDF
            of
            the same shape, except that it’s scaled out by a factor of 2, because our random variable of 2X now has a
            range
            that’s twice as large. So we take the same PDF and scale it up by stretching the x axis by a factor of 2. So
            what does scaling correspond to in terms of a formula?</p>
        <p>因此，2X 的分布作为函数，假设为通用参数 z，将是 X 的分布，但会按 2 的倍数缩放。因此，取一个函数并将其参数替换为 2 的参数，它所做的就是将其拉伸 2 倍。从中学开始，您可能就一直在苦苦思索何时需要拉伸函数，是需要放置 2z 还是 z/2。而实际进行拉伸的是将 z/2 放在该位置。</p><p>So the distribution of 2X as a function, let’s say, a generic argument z, is going to be the distribution of
            X,
            but scaled by a factor of 2. So taking a function and replacing its arguments by the argument over 2, what
            it
            does is it stretches it by a factor of 2. You have probably been tortured ever since middle school to figure
            out
            when need to stretch a function, whether you need to put 2z or z/2. And the one that actually does the
            stretching is to put the z/2 in that place.</p>
        <p>这就是拉伸的作用。这能成为完整答案吗？不过，有一个问题。如果你将此函数拉伸 2 倍，函数下方的面积会发生什么变化？它会翻倍。</p><p>So that’s what the stretching does. Could that to be the full answer? Well there’s a catch. If you stretch
            this
            function by a factor of 2, what happens to the area under the function? It’s going to get doubled.</p>
        <h2 id="unknown-231">未知</h2><h2>Unknown</h2>
        <p>但是总概率必须加起来为 1，因此我们需要采取其他措施来确保曲线下的面积保持为 1。因此我们需要采用该函数并将其缩小 2 个倍数。因此，当您处理随机变量的倍数时，PDF 会根据倍数进行拉伸，然后按相同的数字缩小，以便保留该曲线下的面积。</p><p>But the total probability must add up to 1, so we need to do something else to make sure that the area under
            the
            curve stays to 1. So we need to take that function and scale it down by this factor of 2. So when you’re
            dealing
            with a multiple of a random variable, what happens to the PDF is you stretch it according to the multiple,
            and
            then scale it down by the same number so that you preserve the area under that curve.</p>
        <p>现在我们找到了 2X 的分布。那么 2X + 5 的分布呢？那么将 5 添加到随机变量中会产生什么影响呢？您将以相同的概率获得基本相同的值，只是这些值都会偏移 5。因此，您需要做的就是获取此处的 PDF，并将其偏移 5 个单位。</p><p>So now we found the distribution of 2X. How about the distribution of 2X + 5? Well what does adding 5 to
            random
            variable do? You’re going to get essentially the same values with the same probability, except that those
            values
            all get shifted by 5. So all that you need to do is to take this PDF here, and shift it by 5 units.</p>
        <p>所以范围以前是 2 到 4。新的范围将是从 3 到 9。这就是最终答案。这是 2X + 5 的分布，从 X 的这个特定分布开始。现在向右移动 b，这对函数有什么影响？向右移动一定量，从数学上讲，它相当于将 b 放在函数的参数中。</p><p>So the range used to be from 2 to 4. The new range is going to be from 3 to 9. And that’s the final answer.
            This
            is the distribution of 2X + 5, starting with this particular distribution of X. Now shifting to the right by
            b,
            what does it do to a function? Shifting to the right to by a certain amount, mathematically, it corresponds
            to
            putting b in the argument of the function.</p>
        <h2 id="unknown-232">未知</h2><h2>Unknown</h2>
        <p>所以我采用了这里的公式，即按 a 的倍数缩放。缩小以使总面积保持等于 1。然后我需要引入这个额外的项来进行平移。所以这是一个合理的论点。通过图片证明这应该是正确的答案。</p><p>So I’m taking the formula that I had here, which is the scaling by a factor of a. The scaling down to keep
            the
            total area equal to 1. And then I need to introduce this extra term to do the shifting. So this is a
            plausible
            argument. The proof by picture that this should be the right answer.</p>
        <p>但为了保持我们的技能得到调整和完善，让我们使用两步食谱程序以更正式的方式进行推导。我将在假设 a 为正的情况下进行推导，就像我们刚才的例子一样。那么两步程序是什么呢？我们想要找到 Y 的累积值，然后我们要进行区分。</p><p>But just in order to keep our skills tuned and refined, let us do this derivation in a more formal way using
            our
            two step cookbook procedure. And I’m going to do it under the assumption that a is positive, as in the
            example
            that’s we just did. So what’s the two step procedure? We want to find the cumulative of Y, and after that
            we’re
            going to differentiate.</p>
        <p>根据定义，累积是随机变量取值小于某个数字的概率。现在我们需要把这个事件转化成原始随机变量。所以 Y 的定义是 aX + b，所以我们正在研究这个事件。现在我们想用一种清晰的形式来表达这个事件，其中 X 以直接的方式出现。</p><p>By definition the cumulative is the probability that the random variable takes values less than a certain
            number.
            And now we need to take this event and translate it, and express it in terms of the original random
            variables.
            So Y is, by definition, aX + b, so we’re looking at this event. And now we want to express this event in a
            clean
            form where X shows up in a straight way.</p>
        <h2 id="unknown-233">未知</h2><h2>Unknown</h2>
        <p>假设我要处理这个事件，并把它写成这种形式。要使这个不等式成立，x 应该小于或等于 (y 减 b) 除以 a。好的，现在这是什么？这是在特定点评估的 X 的累积分布。所以我们得到了基于 X 的累积的 Y 的公式。下一步是什么？下一步是对两边求导。</p><p>Let’s say I’m going to massage this event and write it in this form. For this inequality to be true, x should
            be
            less than or equal to (y minus b) divided by a. OK, now what is this? This is the cumulative distribution of
            X
            evaluated at the particular point. So we got a formula for the cumulative Y based on the cumulative of X.
            What’s
            the next step? Next step is to take derivatives of both sides.</p>
        <p>因此 Y 的密度将是这个表达式关于 y 的导数。好的，现在我们需要使用链式法则。它将是 F 函数关于其自变量的导数。然后我们需要对自变量求导数。累积的导数是什么？累积的导数就是密度本身。</p><p>So the density of Y is going to be the derivative of this expression with respect to y. OK, so now here we
            need
            to use the chain rule. It’s going to be the derivative of the F function with respect to its argument. And
            then
            we need to take the derivative of the argument with respect to y. What is the derivative of the cumulative?
            The
            derivative of the cumulative is the density itself.</p>
        <p>我们在感兴趣的点上求导。然后链式法则告诉我们，我们需要对 y 求导，而对 y 求导就是 1/a。这给出了与我在这里写下的公式一致的公式，适用于 a 为正数的情况。如果 a 为负数呢？这个公式会成立吗？</p><p>And we evaluate it at the point of interest. And then the chain rule tells us that we need to take the
            derivative
            of this with respect to y, and the derivative of this with respect to y is 1/a. And this gives us the
            formula
            which is consistent with what I had written down here, for the case where a is a positive number. What if a
            was
            a negative number? Could this formula be true?</p>
        <h2 id="unknown-234">未知</h2><h2>Unknown</h2>
        <p>当然不是。密度不能为负数，对吧？所以那个公式不可能成立。需要改变一些东西。应该改变什么？当 a 为负数时，这个论点在哪里失效？所以当我以这种形式写出这个不等式时，我除以 a。但是当你除以负数时，不等式的方向就会改变。所以当 a 为负数时，这个不等式就会大于或等于。</p><p>Of course not. Densities cannot be negative, right? So that formula cannot be true. Something needs to
            change.
            What should change? Where does this argument break down when a is negative? So when I write this inequality
            in
            this form, I divide by a. But when you divide by a negative number, the direction of an inequality is going
            to
            change. So when a is negative, this inequality becomes larger than or equal to.</p>
        <p>在这种情况下，当这个大于这里时，上面的表达式会发生变化。我不会得到累积值，而是得到 1 减去 (y 减 b) 除以 a 的累积值。所以这是 X 大于这个特定数字的概率。现在，当你求导数时，会出现一个减号。这个减号最终会出现在这里。</p><p>And in that case, the expression that I have up there would change when this is larger than here. Instead of
            getting the cumulative, I would get 1 minus the cumulative of (y minus b) divided by a. So this is the
            probability that X is bigger than this particular number. And now when you take the derivatives, there’s
            going
            to be a minus sign that shows up. And that minus sign will end up being here.</p>
        <p>因此，我们取负数的负数，这基本上相当于取该数的绝对值。因此，当我们有负数 a 时，我们必须取比例因子的绝对值，而不是因子本身。好吧，所以这个通用公式对于处理随机变量的线性函数非常有用。</p><p>And so we’re taking the negative of a negative number, and that basically is equivalent to taking the
            absolute
            value of that number. So all that happens when we have a negative a is that we have to take the absolute
            value
            of the scaling factor instead of the factor itself. All right, so this general formula is quite useful for
            dealing with linear functions of random variables.</p>
        <h2 id="unknown-235">未知</h2><h2>Unknown</h2>
        <p>它的一个很好的应用是取正态随机变量的公式，考虑正态随机变量的线性函数，代入这个公式，你会发现 Y 也服从正态分布。所以使用这个公式，我们现在可以证明我在几节课前提出的一个说法，即正态随机变量的线性函数也是线性的。这就是你证明它的方法。我想今天就讲到这里。</p><p>And one nice application of it is to take the formula for a normal random variable, consider a linear
            function of
            a normal random variable, plug into this formula, and what you will find is that Y also has a normal
            distribution. So using this formula, now we can prove a statement that I had made a couple of lectures ago,
            that
            a linear function of a normal random variable is also linear. That’s how you would prove it. I think this is
            it
            for today so.</p>
        <h1 id="derived-distributions-ctd.-covariance">11. 派生分布（ctd.）；协方差</h1><h1>11. Derived Distributions (ctd.); Covariance</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:55 <a href="https://youtube.com/watch?v=l4NoMKEHQwM">https://youtube.com/watch?v=l4NoMKEHQwM</a></p><p> 12 years ago (Nov 10, 2012) — 51:55 <a href="https://youtube.com/watch?v=l4NoMKEHQwM">https://youtube.com/watch?v=l4NoMKEHQwM</a></p>
        <h2 id="unknown-236">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：早上好。今天我们将继续上次的主题。我们将进一步讨论派生分布，如何推导随机变量函数的分布。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality, educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Good morning. So today
            we’re
            going to continue the subject from last time. So we’re going to talk about derived distributions a little
            more,
            how to derive the distribution of a function of a random variable.</p>
        <p>上次我们讨论了几个例子，其中有一个单变量函数。如果我们知道 X 的分布，我们就能找到 Y 的分布。所以今天我们要举一个例子，处理两个随机变量的函数。</p><p>So last time we discussed a couple of examples in which we had a function of a single variable. And we found
            the
            distribution of Y, if we’re told the distribution of X. So today we’re going to do an example where we deal
            with
            the function of two random variables.</p>
        <p>然后我们将考虑这种类型中最有趣的例子，其中我们有一个形式为 W 的随机变量，它是两个独立随机变量的总和。这种情况经常出现。所以我们想看看在这个特定情况下究竟会发生什么。我只想说一句。</p><p>And then we’re going to consider the most interesting example of this kind, in which we have a random
            variable of
            the form W, which is the sum of two independent, random variables. That’s a case that shows up quite often.
            And
            so we want to see what exactly happens in this particular case. Just one comment that I should make.</p>
        <h2 id="unknown-237">未知</h2><h2>Unknown</h2>
        <p>我们现在要讲的内容，即第四章，在概念上比我们以前讲过的内容要难一些。所以我绝对鼓励你在开始之前先阅读课文，然后尝试解决你的问题集中的问题。好的，让我们从我们的例子开始，其中我们给出了两个随机变量。它们是联合连续的。它们的分布非常简单。它们在单位正方形上是均匀的。</p><p>The material that we’re covering now, chapter four, is sort of conceptually a little more difficult than one
            we
            have been doing before. So I would definitely encourage you to read the text before you jump and try to do
            the
            problems in your problem sets. OK, so let’s start with our example, in which we’re given two random
            variables.
            They’re jointly continuous. And their distribution is pretty simple. They’re uniform on the unit square.</p>
        <p>具体来说，每个随机变量在单位间隔上都是均匀分布的。两个随机变量是独立的。我们要找到的是两个随机变量比率的分布。我们该怎么做呢？，嗯，我们上次针对单个随机变量的情况使用的同样的常规程序。
        </p><p>In particular, each one of the random variables is uniform on the unit interval. And the two random variables
            are
            independent. What we’re going to find is the distribution of the ratio of the two random variables. How do
            we go
            about it?,Well, the same cookbook procedure that we used last time for the case of a single random variable.
        </p>
        <p>我们在本例中使用的菜谱程序也适用于有多个随机变量函数的情况。那么菜谱程序是什么呢？第一步是找到感兴趣的随机变量的累积分布函数，然后求导数以找到密度。所以让我们找到累积分布函数。</p><p>The cookbook procedure that we used for this case also applies to the case where you have a function of
            multiple
            random variables. So what was the cookbook procedure? The first step is to find the cumulative distribution
            function of the random variable of interest and then take the derivative in order to find the density. So
            let’s
            find the cumulative.</p>
        <h2 id="unknown-238">未知</h2><h2>Unknown</h2>
        <p>因此，根据定义，累积是随机变量小于或等于累积参数的概率。因此，如果我们用感兴趣的随机变量来表示这个事件，这就是我们的随机变量小于或等于 z 的概率。那么它是什么呢？</p><p>So, by definition, the cumulative is the probability that the random variable is less than or equal to the
            argument of the cumulative. So if we write this event in terms of the random variable of interest, this is
            the
            probability that our random variable is less than or equal to z. So what is that?</p>
        <p>好的，所以当且仅当 (x,y) 对恰好落在斜率为 z 的直线下方时，该比率才会小于或等于 z。好的，我们画一条斜率为 z 的直线。当且仅当我们得到落在这个三角形内的 x 和 y 对时，该比率才会小于这个数字。所以我们讨论的是这个特定事件的概率。</p><p>OK, so the ratio is going to be less than or equal to z, if and only if the pair, (x,y), happens to fall
            below
            the line that has a slope z. OK, so we draw a line that has a slope z. The ratio is less than this number,
            if
            and only if we get the pair of x and y that falls inside this triangle. So we’re talking about the
            probability
            of this particular event.</p>
        <p>由于这条线的斜率为 z，因此该点的高度等于 z。因此我们可以找到此事件的概率。它只是这个三角形的面积。因此面积为 1 乘以 z 乘以 1/2。我们得到答案 z/2。那么，这个答案总是正确的吗？</p><p>Since this line has a slope of z, the height at this point is equal to z. And so we can find the probability
            of
            this event. It’s just the area of this triangle. And so the area is 1 times z times 1/2. And we get the
            answer,
            z/2. Now, is this answer always correct?</p>
        <h2 id="unknown-239">未知</h2><h2>Unknown</h2>
        <p>现在，只有当斜率恰好能让我们得到这种图像时，这个答案才是正确的。那么我们什么时候才能得到这种图像呢？当斜率小于 1 时。如果我考虑一个不同的斜率，一个数字，小 z 恰好是这种斜率，那么图像就会改变。在这种情况下，我们得到这种图像，比方说。</p><p>Now, this answer is going to be correct only if the slope happens to be such that we get a picture of this
            kind.
            So when do we get a picture of this kind? When the slope is less than 1. If I consider a different slope, a
            number, little z that happens to be a slope of that kind then the picture changes. And in that case, we get
            a
            picture of this kind, let’s say.</p>
        <p>所以这是斜率 z 的线。这是我们的数字（小 z）大于 1 的第二种情况。那么我们该怎么做呢？再说一次，累计值是比率小于或等于该数字的概率。所以这是我们跌破红线的概率。所以我们在谈论这个事件，谈论这个事件。</p><p>So this is a line here of slope z, again. And this is the second case in which our number, little z, is
            bigger
            than 1. So how do we proceed? Once more, the cumulative is the probability that the ratio is less than or
            equal
            to that number. So it’s the probability that we fall below the red line. So we’re talking about the event,
            about
            this event.</p>
        <p>因此，要找到这个事件的概率，我们需要找到这个红色形状的面积。找到这个面积的一种方法是考虑整个面积并减去这个三角形的面积。所以我们这样做。它将是 1 减去三角形的面积。现在，三角形的面积是多少？它是这条边的 1/2 倍，也就是这条边的 1 倍。这条边有多大？</p><p>So to find the probability of this event, we need to find the area of this red shape. And one way of finding
            this
            area is to consider the whole area and subtract the area of this triangle. So let’s do it this way. It’s
            going
            to be 1 minus the area of the triangle. Now, what’s the area of the triangle? It’s 1/2 times this side,
            which is
            1 times this side. How big is that side?</p>
        <h2 id="unknown-240">未知</h2><h2>Unknown</h2>
        <p>好吧，如果 y 和斜率是 z，那么现在 z 就是 y 与 x 的比率。所以如果 y 与 x 的比率。此时我们有 y/x = z 和 y =1。这意味着 z 是 1/x。所以这个点的坐标是 1/x。这意味着我们要。1/z 所以这里我们得到了 1/z 的因子。我们基本上完成了。</p><p>Well, if y and the slope is z, now z is the ratio y over x. So if y over x. at this point we have y/x = z and
            y
            =1. This means that z is 1/x. So the coordinate of this point is 1/x. And this means that we’re going to.
            1/z So
            here we get the factor of 1/z. And we’re basically done.</p>
        <p>我想，如果你想得到一个完整的答案，你还应该给出 z 小于 0 时的公式。当 z 小于 0 时，累积值是多少，得到负比率的概率是多少？好吧，因为我们的随机变量是正的，所以你不可能得到负比率。所以下面的累积值等于 0。所以我们可以绘制累积值。</p><p>I guess if you want to have a complete answer, you should also give the formula for z less than 0. What is
            the
            cumulative when z is less than 0, the probability that you get the ratio that’s negative? Well, since our
            random
            variables are positive, there’s no way that you can get a negative ratio. So the cumulative down there is
            equal
            to 0. So we can plot the cumulative.</p>
        <p>我们可以求它的导数来求密度。因此，当 z 为负时，我们得到的累积值从 0 开始。然后它开始与 z 成比例增加，斜率为 1/2。因此，这将达到 1。然后，根据此函数，它开始向 1 增加。</p><p>And we can take its derivative in order to find the density. So the cumulative that we got starts at 0, when
            z’s
            are negative. Then it starts going up in proportion to z, at the slope of 1/2. So this takes us up to 1. And
            then it starts increasing towards 1, according to this function.</p>
        <h2 id="unknown-241">未知</h2><h2>Unknown</h2>
        <p>当 z 趋于无穷大时，累积值将趋于 1。它的形状大致是这样的。所以现在要得到密度，我们只需求导数。密度当然在这里是 0。在这里导数只是 1/2。超过这个点，我们需要对这个表达式求导数。导数将是 1/2 乘以 1 除以 z 平方。</p><p>When you let z go to infinity, the cumulative is going to go to 1. And it has a shape of, more or less, this
            kind. So now to get the density, we just take the derivative. And the density is, of course, 0 down here. Up
            here the derivative is just 1/2. And beyond that point we need to take the derivative of this expression.
            And
            the derivative is going to be 1/2 times 1 over z squared.</p>
        <p>所以它将是这种形状。我们就完成了。所以你会发现涉及多个随机变量函数的问题并不比处理单个随机变量函数的问题更难。一般程序也完全相同。你首先找到累积函数，然后进行微分。
        </p><p>So it’s going to be a shape of this kind. And we’re done. So you see that problems involving functions of
            multiple random variables are no harder than problems that deal with the functional of a single random
            variable.
            The general procedure is, again, exactly the same. You first find the cumulative, and then you
            differentiate.
        </p>
        <p>唯一的额外困难是，当你计算累积时，你需要找到涉及多个随机变量的事件的概率。有时这可能有点难做到。顺便说一句，既然我们已经处理了这个例子，那就只剩几个问题了。你认为随机变量 Z 的期望值是多少？</p><p>The only extra difficulty will be that when you calculate the cumulative, you need to find the probability of
            an
            event that involves multiple random variables. And sometimes this could be a little harder to do. By the
            way,
            since we dealt with this example, just a couple of questions. What do you think is going to be the expected
            value of the random variable Z?</p>
        <h2 id="unknown-242">未知</h2><h2>Unknown</h2>
        <p>让我们看看，随机变量 Z 的期望值将是 z 乘以密度的积分。当 z 从 0 到 1 时，密度等于 1/2。然后从 1 到无穷大还有另一个贡献。密度是 1/(2z 平方)。我们得到了 z，因为我们处理的是期望，dz。那么这个积分是什么？好吧，如果你看这里，你正在对 1/z 进行积分，一直到无穷大。</p><p>Let’s see, the expected value of the random variable Z is going to be the integral of z times the density.
            And
            the density is equal to 1/2 for z going from 0 to 1. And then there’s another contribution from 1 to
            infinity.
            There the density is 1/(2z squared). And we get the z, since we’re dealing with expectations, dz. So what is
            this integral? Well, if you look here, you’re integrating 1/z, all the way to infinity.</p>
        <p>1/z 有一个积分，它是 z 的对数。由于对数趋于无穷大，这意味着这个积分也是无穷大的。所以在这个例子中，随机变量 Z 的期望实际上是无穷大的。这没什么不对。许多随机变量都有无限的期望。如果密度尾部在参数趋于无穷大时下降得比较慢，那么你很可能会得到一个无穷大的积分。</p><p>1/z has an integral, which is the logarithm of z. And since the logarithm goes to infinity, this means that
            this
            integral is also infinite. So the expectation of the random variable Z is actually infinite in this example.
            There’s nothing wrong with this. Lots of random variables have infinite expectations. If the tail of the
            density
            falls kind of slowly, as the argument goes to infinity, then it may well turn out that you get an infinite
            integral.</p>
        <p>事情通常就是这样。没什么奇怪的。现在，既然我们还在这个例子中，让我再问一个问题。我们能否推断，平均而言，Z 的期望值是否正确？记得 Z 是 Y/X 的比率，Z 的期望值可能是这个数字吗？或者它是否等于这个数字？</p><p>So that’s just how things often are. Nothing strange about it. And now, since we are still in this example,
            let
            me ask another question. Would we reason, on the average, would it be true that the expected value of Z
            remember
            that Z is the ratio Y/X could it be that the expected value of Z is this number? Or could it be that it’s
            equal
            to this number?</p>
        <h2 id="unknown-243">未知</h2><h2>Unknown</h2>
        <p>或者可能以上都不是？好的，那么有多少人认为这是正确的？数量很少。有多少人认为这是正确的？数量稍微多一些，但仍然是个小数目。有多少人认为这是正确的？好的，那就是。这个赢得了投票。好的，让我们看看。这个不正确，只是因为没有理由它应该是正确的。所以，一般来说，你不能根据平均值来推理。</p><p>Or could it be that it’s none of the above? OK, so how many people think this is correct? Small number. How
            many
            people think this is correct? Slightly bigger, but still a small number. And how many people think this is
            correct? OK, that’s. this one wins the vote. OK, let’s see. This one is not correct, just because there’s no
            reason it should be correct. So, in general, you cannot reason on the average.</p>
        <p>函数的期望值与期望值的相同函数不同。这只有在处理随机变量的线性函数时才是正确的。所以这不是。事实证明这是不正确的。这个怎么样？嗯，根据假设，X 和 Y 是独立的。所以 1/X 和 Y 也是独立的。为什么会这样？独立意味着一个随机变量不会传达有关另一个变量的任何信息。</p><p>The expected value of a function is not the same as the same function of the expected values. This is only
            true
            if you’re dealing with linear functions of random variables. So this is not. this turns out to not be
            correct.
            How about this one? Well, X and Y are independent, by assumption. So 1/X and Y are also independent. Why is
            this? Independence means that one random variable does not convey any information about the other.</p>
        <p>因此 Y 不会提供任何有关 X 的信息。因此 Y 不会提供任何有关 1/X 的信息。或者换句话说，如果两个随机变量是独立的，则每个随机变量的函数也是独立的。如果 X 独立于 Y，则 g(X) 独立于 h(Y)。因此这适用于这种情况。这两个随机变量是独立的。</p><p>So Y doesn’t give you any information about X. So Y doesn’t give you any information about 1/X. Or to put it
            differently, if two random variables are independent, functions of each one of those random variables are
            also
            independent. If X is independent from Y, then g(X) is independent of h(Y). So this applies to this case.
            These
            two random variables are independent.</p>
        <h2 id="unknown-244">未知</h2><h2>Unknown</h2>
        <p>由于它们是独立的，这意味着它们的乘积的期望值等于期望值的乘积。所以这个关系实际上是正确的。因此，这不是正确的。好的。现在，让我们继续。我们有一个通过累积来找到派生分布的一般程序。在某些情况下，我们可以走捷径吗？</p><p>And since they are independent, this means that the expected value of their product is equal to the product
            of
            the expected values. So this relation actually is true. And therefore, this is not true. OK. Now, let’s move
            on.
            We have this general procedure of finding the derived distribution by going through the cumulative. Are
            there
            some cases where we can have a shortcut?</p>
        <p>事实证明，存在一种特殊情况或特殊结构，我们可以直接使用公式从密度获得密度。在这种情况下，我们不必经历累积。这种情况也很有趣，因为它让我们了解了一种密度如何转变为另一种密度，以及什么影响了这些密度的形状。</p><p>Turns out that there is a special case or a special structure in which we can get directly from densities to
            densities using directly just a formula. And in that case, we don’t have to go through the cumulative. And
            this
            case is also interesting, because it gives us some insight about how one density changes to a different
            density
            and what affects the shape of those densities.</p>
        <p>因此，事情比较简单，因为从一个随机变量到另一个随机变量的变换是严格单调的。因此，x 和 y 之间存在一对一的关系。在这里，我们可以从密度的角度直接推理，方法是考虑小区间的概率。因此，让我们看看 x 轴上的小区间，就像这个，X 的范围是。</p><p>So the case where things easy is when the transformation from one random variable to the other is a strictly
            monotonic one. So there’s a one to one relation between x’s and y’s. Here we can reason directly in terms of
            densities by thinking in terms of probabilities of small intervals. So let’s look at the small interval on
            the x
            axis, like this one, when X ranges from.</p>
        <h2 id="unknown-245">未知</h2><h2>Unknown</h2>
        <p>大写 X 的范围从小 x 到小 x 加 delta。所以这是一个长度为 delta 的小区间。每当 X 恰好落在这个区间内时，随机变量 Y 就会落在相应的区间内。所以在那里我们有一个相应的区间。这两个区间，红色和蓝色区间。这是蓝色区间。那是红色区间。</p><p>Where capital X ranges from a small x to a small x plus delta. So this is a small interval of length delta.
            Whenever X happens to fall in this interval, the random variable Y is going to fall in a corresponding
            interval
            up there. So up there we have a corresponding interval. And these two intervals, the red and the blue
            interval.
            this is the blue interval. And that’s the red interval.</p>
        <p>这两个区间应该具有相同的概率。它们是完全相同的事件。当 X 落在这里时，g(X) 恰好落在那里。所以我们可以说这个小区间的概率与那个小区间的概率相同。我们知道小区间的概率与密度有关。那么这个小区间的概率是多少？</p><p>These two intervals should have the same probability. They’re exactly the same event. When X falls here, g(X)
            happens to fall in there. So we can sort of say that the probability of this little interval is the same as
            the
            probability of that little interval. And we know that probabilities of little intervals have something to do
            with densities. So what is the probability of this little interval?</p>
        <p>此时，它是随机变量 X 的密度乘以间隔的长度。那么该间隔的概率呢？它将是随机变量 Y 的密度乘以该小间隔的长度。现在，这个间隔的长度为 delta。这是否意味着这个间隔的长度也为 delta？嗯，不一定。这个间隔的长度与函数 g 的斜率有关。
        </p><p>It’s the density of the random variable X, at this point, times the length of the interval. How about the
            probability of that interval? It’s going to be the density of the random variable Y times the length of that
            little interval. Now, this interval has length delta. Does that mean that this interval also has length
            delta?
            Well, not necessarily. The length of this interval has something to do with the slope of your function g.
        </p>
        <h2 id="unknown-246">未知</h2><h2>Unknown</h2>
        <p>所以斜率是 dy 乘以 dx。是多少。斜率告诉你，当你取一个特定长度的区间 x 时，y 区间有多大。所以斜率就是乘以这个区间的长度，得到这个区间的长度。所以这个区间的长度是 delta 乘以你的函数的斜率。所以区间的长度大约是 delta 乘以函数的斜率。</p><p>So slope is dy by dx. Is how much. the slope tells you how big is the y interval when you take an interval x
            of a
            certain length. So the slope is what multiplies the length of this interval to give you the length of that
            interval. So the length of this interval is delta times the slope of your function. So the length of the
            interval is delta times the slope of the function, approximately.</p>
        <p>因此，这个区间的概率将是 Y 的密度乘以我们正在考虑的区间长度。因此，这给出了此时评估的 X 密度与该点评估的 Y 密度之间的关系。这两个密度密切相关。如果这些 x 很可能发生，那么这个概率就很大，这意味着密度也会很大。</p><p>So the probability of this interval is going to be the density of Y times the length of the interval that we
            are
            considering. So this gives us a relation between the density of X, evaluated at this point, to the density
            of Y,
            evaluated at that point. The two densities are closely related. If these x’s are very likely to occur, then
            this
            is big, which means that density will also be big.</p>
        <p>如果这些 x 很可能发生，那么这些 y 也很可能发生。但还有另一个因素。那就是函数在这个特定点的斜率。所以我们得到了这两个密度之间的关系。现在，在解释这个方程时，你需要确定这两个变量之间的关系是什么。我有小 x 和小 y。</p><p>If these x’s are very likely to occur, then those y’s are also very likely to occur. But there’s also another
            factor that comes in. And that’s the slope of the function at this particular point. So we have this
            relation
            between the two densities. Now, in interpreting this equation, you need to make sure what’s the relation
            between
            the two variables. I have both little x’s and little y’s.</p>
        <h2 id="unknown-247">未知</h2><h2>Unknown</h2>
        <p>嗯，这个公式对于 (x,y) 对是正确的，它们根据这个特定函数相关联。所以如果我固定一个 x 并考虑相应的 y，那么这些 x 和相应 y 处的密度将通过该公式相关联。现在，最后，你想要得出一个公式，它只给出 Y 的密度作为 y 的函数。</p><p>Well, this formula is true for an (x,y) pair, that they’re related according to this particular function. So
            if I
            fix an x and consider the corresponding y, then the densities at those x’s and corresponding y’s will be
            related
            by that formula. Now, in the end, you want to come up with a formula that just gives you the density of Y as
            a
            function of y.</p>
        <p>这意味着你需要从图中消除 x。让我们看一个例子。假设我们处理的函数 y 等于 x 的立方，在这种情况下，我们的函数 g(x) 是函数 x 的立方。</p><p>And that means that you need to eliminate x from the picture. So let’s see how that would go in an example.
            So
            suppose that we’re dealing with the function y equal to x cubed, in which case our function, g(x), is the
            function x cubed.</p>
        <p>如果 x 的立方等于小 y，如果我们有一对以这种方式关联的 x 和 y，那么这意味着 x 将是 y 的立方根。所以这是让我们从 y 回到 x 的公式。这是从 x 直接构造 y 的函数。</p><p>And if x cubed is equal to a little y, If we have a pair of x’s and y’s that are related this way, then this
            means that x is going to be the cubic root of y. So this is the formula that takes us back from y’s to x’s.
            This
            is the direct function from x, how to construct y.</p>
        <h2 id="unknown-248">未知</h2><h2>Unknown</h2>
        <p>这本质上是反函数，它告诉我们，给定 y 对应的 x 是什么。现在，如果我们写下这个公式，它会告诉我们，特定 x 处的密度将是对应 y 处的密度乘以我们正在考虑的特定 x 处的函数斜率。函数的斜率为 3x 平方。</p><p>This is essentially the inverse function that tells us, from a given y what is the corresponding x. Now, if
            we
            write this formula, it tells us that the density at the particular x is going to be the density at the
            corresponding y times the slope of the function at the particular x that we are considering. The slope of
            the
            function is 3x squared.</p>
        <p>现在，我们想要得到 Y 密度的公式。所以我要取这个因子，将它发送到另一边。但由于我希望它是 y 的函数，所以我要消除 x。我将使用这里的对应关系来消除 x。</p><p>Now, we want to end up with a formula for the density of Y. So I’m going to take this factor, send it to the
            other side. But since I want it to be a function of y, I want to eliminate the x’s. And I’m going to
            eliminate
            the x’s using this correspondence here.</p>
        <p>所以我要得到在 y 的 1/3 次方处 X 的密度。然后这个分母的因数是 1/(3y 的 2/3 次方)。所以我们最终得到了随机变量 Y 的密度公式。如果你使用累积分布函数方法完成这个练习，你得到的答案和这个一样。你最终会得到相同的答案。</p><p>So I’m going to get the density of X evaluated at y to the 1/3. And then this factor in the denominator, it’s
            1/(3y to the power 2/3). So we end up finally with the formula for the density of the random variable Y. And
            this is the same answer that you would get if you go through this exercise using the cumulative distribution
            function method. You end up getting the same answer.</p>
        <h2 id="unknown-249">未知</h2><h2>Unknown</h2>
        <p>但在这里我们直接得到它。为了更深入地了解斜率出现的原因，假设我们有一个像这样的函数。所以函数有点平坦，然后快速移动，然后又变平坦。假设 X 具有某种合理的密度，某种平坦的密度，那么应该是什么。假设 X 是一个相当均匀的随机变量。</p><p>But here we sort of get it directly. Just to get a little more insight as to why the slope comes in. suppose
            that
            we have a function like this one. So the function is sort of flat, then moves quickly, and then becomes flat
            again. What should be and suppose that X has some kind of reasonable density, some kind of flat density.
            Suppose
            that X is a pretty uniform random variable.</p>
        <p>随机变量 Y 会发生什么？它应该具有什么样的分布？随机变量 Y 的典型值是什么？要么 x 落在这里，y 是一个非常小的数字，要么。让我们把这里的数字取为 2</p><p>What’s going to happen to the random variable Y? What kind of distribution should it have? What are the
            typical
            values of the random variable Y? Either x falls here, and y is a very small number, or. let’s take that
            number
            here to be let’s say 2</p>
        <p>或者 x 落在这个范围内，y 取接近 2 的值。x 有可能在中间某个位置，在这种情况下 y 取中间值。那么你期望 Y 的分布形状是什么？Y 取接近 0 的值的可能性很大。Y 取中间值的概率很小。</p><p>Or x falls in this range, and y takes a value close to 2. And there’s a small chance that x’s will be
            somewhere
            in the middle, in which case y takes intermediate values. So what kind of shape do you expect for the
            distribution of Y? There’s going to be a fair amount of probability that Y takes values close to 0. There’s
            a
            small probability that Y takes intermediate values.</p>
        <h2 id="unknown-250">未知</h2><h2>Unknown</h2>
        <p>这对应于 x 落入此处的情况。概率不大。因此 Y 取 0 到 2 之间的值的概率很小。但是有很多 x 会产生接近 2 的 y。因此 Y 取接近 2 的值的概率很大。因此，Y 的密度将具有这种形状。</p><p>That corresponds to the case where x falls in here. That’s not a lot of probability. So the probability that
            Y
            takes values between 0 and 2, that’s kind of small. But then there’s a lot of x’s that produces y’s that are
            close to 2. So there’s a significant probability that Y would take values that are close to 2. So you. the
            density of Y would have a shape of this kind.</p>
        <p>通过查看这幅图，您可以发现，最有可能的情况是 x 会落在这里或 x 会落在那里。因此，g(x) 最有可能接近 0 或接近 2。因此，由于 y 最有可能接近 0 或接近 y 的大部分概率都在这里。介于两者之间的概率很小。</p><p>By looking at this picture, you can tell that it’s most likely that either x will fall here or x will fall
            there.
            So the g(x) is most likely to be close to 0 or to be close to 2. So since y is most likely to be close to 0
            or
            close to most of the probability of y is here. And there’s a small probability of being in between.</p>
        <p>请注意，获得很大概率的 y 是与 g 函数平坦区域相关的 y。当 g 函数平坦时，Y 的密度会很大。因此，Y 的密度与函数的斜率成反比。这就是您从这里得到的。Y 的密度是。将该项发送到另一侧。</p><p>Notice that the y’s that get a lot of probability are those y’s associated with flats regions off your g
            function. When the g function is flat, that gives you big densities for Y. So the density of Y is inversely
            proportional to the slope of the function. And that’s what you get from here. The density of Y is. send that
            term to the other side.</p>
        <h2 id="unknown-251">未知</h2><h2>Unknown</h2>
        <p>与你正在处理的函数的斜率成反比。好的，这个公式对于函数为一一的情况非常有效。所以我们可以在 x 和 y 之间建立唯一的关联，并通过反函数从 y 到 x。它适用于单调递增的情况。它也适用于单调递减的情况。</p><p>Is inversely proportional to the slope of the function that you’re dealing with. OK, so this formula works
            nicely
            for the case where the function is one to one. So we can have a unique association between x’s and y’s and
            through an inverse function, from y’s to x’s. It works for the monotonically increasing case. It also works
            for
            the monotonically decreasing case.</p>
        <p>在单调递减的情况下，你唯一需要做的改变就是取斜率的绝对值，而不是斜率本身。好的，现在，这是另一个例子或一个特殊情况。让我们来谈谈最有趣的情况，它涉及两个随机变量的函数。在这种情况下，我们有两个独立的随机变量，我们想要找到这两个变量之和的分布。</p><p>In the monotonically decreasing case, the only change that you need to do is to take the absolute value of
            the
            slope, instead of the slope itself. OK, now, here’s another example or a special case. Let’s talk about the
            most
            interesting case that involves a function of two random variables. And this is the case where we have two
            independent, random variables, and we want to find the distribution of the sum of the two.</p>
        <p>我们真正感兴趣的是连续情况。但作为热身，首先看一下离散随机变量的离散情况很有用。假设我们想找出 X 和 Y 之和等于特定数字的概率。为了说明这一点，我们假设该数字等于 3。这两个随机变量之和等于 3 的概率是多少？</p><p>We’re really interested in the continuous case. But as a warm up, it’s useful to look at the discrete case
            first
            of discrete random variables. Let’s say we want to find the probability that the sum of X and Y is equal to
            a
            particular number. And to illustrate this, let’s take that number to be equal to 3. What’s the probability
            that
            the sum of the two random variables is equal to 3?</p>
        <h2 id="unknown-252">未知</h2><h2>Unknown</h2>
        <p>要找到和等于 3 的概率，你需要考虑所有可能得到和为 3 的方法。不同的方法就是这幅图中的点。它们对应于一条沿此方向延伸的线。因此，和等于某个数字的概率就是所有这些点的概率之和。</p><p>To find the probability that the sum is equal to 3, you consider all possible ways that you can get the sum
            of 3.
            And the different ways are the points in this picture. And they correspond to a line that goes this way. So
            the
            probability that the sum is equal to a certain number is the probability that is the sum of the
            probabilities of
            all of those points.</p>
        <p>这幅图中的典型点是什么？在典型点中，随机变量 X 取某个值。Y 取所需的值，以使和等于 W。任何 x 与 aw 减去 x 的组合，任何这样的组合都会给出和 w。因此，和为 w 的概率是所有可能 x 的总和。</p><p>What is a typical point in this picture? In a typical point, the random variable X takes a certain value. And
            Y
            takes the value that’s needed so that the sum is equal to W. Any combination of an x with a w minus x, any
            such
            combination gives you a sum of w. So the probability that the sum is w is the sum over all possible x’s.</p>
        <p>这就是我们得到某个 x 的概率的所有点。假设 x 等于随机变量 Y 取值 1 的相应概率的 2 倍。为什么我要在这里乘以概率？因为我们假设两个随机变量是独立的。因此，X 取某个值并且 Y 取互补值的概率，由于独立性，该概率是两个概率的乘积。</p><p>That’s over all these points of the probability that we get a certain x. Let’s say x equals 2 times the
            corresponding probability that random variable Y takes the value 1. And why am I multiplying probabilities
            here?
            That’s where we use the assumption that the two random variables are independent. So the probability that X
            takes a certain value and Y takes the complementary value, that probability is the product of two
            probabilities
            because of independence.</p>
        <h2 id="unknown-253">未知</h2><h2>Unknown</h2>
        <p>当我们将其写成我们通常的 PMF 符号时，它就是这种公式。所以这个公式被称为卷积公式。这是一个采用一个 PMF 和另一个 PMF 的运算。我们得到 X 和 Y 的 PMF，并产生一个新的 PMF。所以可以把这个公式想象成给你一个变换。你取两个 PMF，对它们进行一些操作，然后得到一个新的 PMF。</p><p>And when we write that into our usual PMF notation, it’s a formula of this kind. So this formula is called
            the
            convolution formula. It’s an operation that takes one PMF and another PMF. p we’re given the PMF’s of X and
            Y
            and produces a new PMF. So think of this formula as giving you a transformation. You take two PMF’s, you do
            something with them, and you obtain a new PMF.</p>
        <p>这个过程，这个公式的作用，在机械上得到了很好的说明。所以让我在这里给你看一张图片，说明一般的机械原理。所以你没有这些幻灯片，但让我们来推理一下。假设你得到了 X 的 PMF，它有这种形状。你得到了 Y 的 PMF。它有这种形状。我们会以某种方式进行这个计算。</p><p>This procedure, what this formula does is nicely illustrated sort of by mechanically. So let me show you a
            picture here and illustrate how the mechanics go, in general. So you don’t have these slides, but let’s just
            reason through it. So suppose that you are given the PMF of X, and it has this shape. You’re given the PMF
            of Y.
            It has this shape. And somehow we are going to do this calculation.</p>
        <p>现在，我们需要对 W 的每个值进行此计算，以便得到 W 的 PMF。让我们先对一种情况进行计算。假设 W 等于 0，在这种情况下，我们需要求出 Px(x) 和 Py(x) 之和。如何以图形方式进行此计算？它涉及 X 的 PMF。但它涉及 Y 的 PMF，参数相反。</p><p>Now, we need to do this calculation for every value of W, in order to get the PMF of W. Let’s start by doing
            the
            calculation just for one case. Suppose the W is equal to 0, in which case we need to find the sum of Px(x)
            and
            Py( x). How do you do this calculation graphically? It involves the PMF of X. But it involves the PMF of Y,
            with
            the argument reversed.</p>
        <h2 id="unknown-254">未知</h2><h2>Unknown</h2>
        <p>那么我们如何绘制这个图呢？好吧，为了反转这个参数，你需要的是取这个 PMF 并将其翻转。所以随身带一把剪刀很方便。所以你把它剪下来。现在你取随机变量 Y 的 PMF 并将其翻转。所以你在这里看到的是这个函数，其中的参数被反转了。然后我们该怎么做？</p><p>So how do we plot this? Well, in order to reverse the argument, what you need is to take this PMF and flip
            it. So
            that’s where it’s handy to have a pair of scissors with you. So you cut this down. And so now you take the
            PMF
            of the random variable Y and just flip it. So what you see here is this function where the argument is being
            reversed. And then what do we do?</p>
        <p>我们对两个图进行交叉乘法。这里的任何项都会与那里的相应项相乘。我们考虑所有这些乘积并将它们相加。在这个特定情况下，翻转的 PMF 与 X 的 PMF 没有任何重叠。所以我们会得到一个等于 0 的答案。所以对于 w 等于 0 的情况，在这个特定图中，Pw 将等于 0。</p><p>We cross multiply the two plots. Any entry here gets multiplied with the corresponding entry there. And we
            consider all those products and add them up. In this particular case, the flipped PMF doesn’t have any
            overlap
            with the PMF of X. So we’re going to get an answer that’s equal to 0. So for w’s equal to 0, the Pw is going
            to
            be equal to 0, in this particular plot.</p>
        <p>现在，如果我们有一个不同的 w 值。如果我们有一个不同的参数 w 值，那么我们这里就有 Y 的 PMF，它被翻转并偏移了 w 量。所以你所做的正确图像是取这个并将其移位一定量的 w。那么在这里，我将其移动了多少？</p><p>Now if we have a different value of w oops. If we have a different value of the argument w, then we have here
            the
            PMF of Y that’s flipped and shifted by an amount of w. So the correct picture of what you do is to take this
            and
            displace it by a certain amount of w. So here, how much did I shift it?</p>
        <h2 id="unknown-255">未知</h2><h2>Unknown</h2>
        <p>我将其移动，直到 1 刚好低于 4。因此，我总共移动了 5。因此，0 低于 5，而 0 最初低于 0。因此，我将其移动了 5 个单位。现在，我要进行交叉乘法和加法。这是否给出了正确的结果。它是否执行了正确的操作？</p><p>I shifted it until one falls just below 4. So I have shifted by a total amount of 5. So 0 falls under 5,
            whereas
            0 initially was under 0. So I’m shifting it by 5 units. And I’m now going to cross multiply and add. Does
            this
            give us the correct. does it do the correct thing?</p>
        <p>是的，因为典型项是该随机变量的概率是该随机变量的概率的 3 倍。这是获得 5 的总和的一种特殊方法。如果您看到这里，事物排列的方式，它会为您提供获得 5 的总和的所有不同方法。您可以通过 1 + 4、2 + 3、3 + 2 或 4 + 1 来获得 5 的总和。您需要将所有这些组合的概率相加。
        </p><p>Yes, because a typical term will be the probability that this random variable is 3 times the probability that
            this random variable is 2. That’s a particular way that you can get a sum of 5. If you see here, the way
            that
            things are aligned, it gives you all the different ways that you can get the sum of 5. You can get the sum
            of 5
            by having 1 + 4, or 2 + 3, or 3 + 2, or 4 + 1. You need to add the probabilities of all those combinations.
        </p>
        <p>因此，你取这个乘以那个。这是一个乘积项。然后这个乘以 0，这个乘以那个。所以 1. 你交叉。你找到所有相应项的乘积，然后将它们加在一起。所以这是一种方便的机械程序，可以进行这种计算，尤其是当 PMF 以图片的形式提供给你时。</p><p>So you take this times that. That’s one product term. Then this times 0, this times that. And so 1. you
            cross.
            you find all the products of the corresponding terms, and you add them together. So it’s a kind of handy
            mechanical procedure for doing this calculation, especially when the PMF’s are given to you in terms of a
            picture.</p>
        <h2 id="unknown-256">未知</h2><h2>Unknown</h2>
        <p>因此，这些机制的总结就是我们所做的，就是将 PMF 放在一起。取 Y 的 PMF。翻转它。对于您感兴趣的任何特定 w，取这个翻转的 PMF 并将其移动 w 量。给定特定 w 值的这个特定移动，您将项相乘，然后累加它们或将它们加在一起。</p><p>So the summary of these mechanics are just what we did, is that you put the PMF’s on top of each other. You
            take
            the PMF of Y. You flip it. And for any particular w that you’re interested in, you take this flipped PMF and
            shift it by an amount of w. Given this particular shift for a particular value of w, you cross multiply
            terms
            and then accumulate them or add them together.</p>
        <p>在连续情况下，你期望会发生什么？嗯，这个故事很熟悉。在连续情况下，事情几乎总是以相同的方式进行，只是我们用 PDF 代替 PMF。我们用积分代替总和。所以你得到这种公式应该不会感到惊讶。通过计算这个积分，可以从 X 的密度和 Y 的密度中获得 W 的密度。</p><p>What would you expect to happen in the continuous case? Well, the story is familiar. In the continuous case,
            pretty much, almost always things work out the same way, except that we replace PMF’s by PDF’s. And we
            replace
            sums by integrals. So there shouldn’t be any surprise here that you get a formula of this kind. The density
            of W
            can be obtained from the density of X and the density of Y by calculating this integral.</p>
        <p>本质上，这个积分的作用是拟合一个特定的 w。我们感兴趣的是随机变量大写 W 取等于小 w 或接近它的值的概率。所以这对应于事件，即二维空间上的这条特定线。所以我们需要找到这条线上的奇数概率。但由于设置是连续的，我们不会添加概率。我们将进行积分。</p><p>Essentially, what this integral does is it fits a particular w of interest. We’re interested in the
            probability
            that the random variable, capital W, takes a value equal to little w or values close to it. So this
            corresponds
            to the event, which is this particular line on the two dimensional space. So we need to find the sort of odd
            probabilities along that line. But since the setting is continuous, we will not add probabilities. We’re
            going
            to integrate.</p>
        <h2 id="unknown-257">未知</h2><h2>Unknown</h2>
        <p>对于此图中的任意典型点，在此邻域中获得结果的概率是。与特定 x 的密度以及与 x 互补的特定 y 的密度有关，以形成 w 的总和。因此，我们这里的积分实际上是对这条特定线的积分。好的，我将跳过此结果的正式推导。</p><p>And for any typical point in this picture, the probability of obtaining an outcome in this neighborhood is
            the.
            has something to do with the density of that particular x and the density of the particular y that would
            compliment x, in order to form a sum of w. So this integral that we have here is really an integral over
            this
            particular line. OK, so I’m going to skip the formal derivation of this result.</p>
        <p>文本中有几个推导。这里概述的推导是第三个推导。但理解这个公式最简单的方法是考虑离散情况下会发生什么。因此，在接下来的讲座中，我们将考虑一些额外的、更杂乱的主题、一些注释和一些定义。所以让我们换个角度。翻页并考虑下一个小主题。</p><p>There’s a couple of derivations in the text. And the one which is outlined here is yet a third derivation.
            But
            the easiest way to make sense of this formula is to consider what happens in the discrete case. So for the
            rest
            of the lecture we’re going to consider a few extra, more miscellaneous topics, a few remarks, and a few more
            definitions. So let’s change. flip a page and consider the next mini topic.</p>
        <p>这里不会讲太深奥的东西，但值得熟悉一些。如果你有两个独立的、具有特定参数的正态随机变量，那么问题是，合并后的 PDF 是什么样子的？因此，如果它们是独立的，那么根据定义，合并后的 PDF 就是各个 PDF 的乘积。每个 PDF 都涉及某个指数。两个指数的乘积是总和的指数。</p><p>There’s not going to be anything deep here, but just something that’s worth being familiar with. If you have
            two
            independent, normal random variables with certain parameters, the question is, what does the joined PDF look
            like? So if they’re independent, by definition the joint PDF is the product of the individual PDF’s. And the
            PDF’s each one of them involves an exponential of something. The product of two exponentials is the
            exponential
            of the sum.</p>
        <h2 id="unknown-258">未知</h2><h2>Unknown</h2>
        <p>所以你只需添加指数。所以这是联合 PDF 的公式。现在，你看看这个公式，然后问，它是什么样子的？好的，你可以通过思考这个函数的轮廓来理解它，这是一个两个变量的函数。看看函数取常数值的点。它在哪里？什么时候是常数？这个常数点集的形状是什么？</p><p>So you just add the exponents. So this is the formula for the joint PDF. Now, you look at that formula and
            you
            ask, what does it look like? OK, you can understand it, a function of two variables by thinking about the
            contours of this function. Look at the points at which the function takes a constant value. Where is it?
            When is
            it constant? What’s the shape of the set of points where this is a constant?</p>
        <p>因此，考虑所有 x 和 y，对于这些 x 和 y，这里的表达式都是常数，这里的表达式也是常数。这是什么形状？这是一个椭圆。它是一个以为中心的椭圆。它以 mu x、mu y 为中心。这是两个随机变量的平均值。如果这些 sigma 相等，那么这个椭圆实际上就是一个圆。你会得到这种轮廓。</p><p>So consider all x’s and y’s for which this expression here is a constant, that this expression here is a
            constant. What kind of shape is this? This is an ellipse. And it’s an ellipse that’s centered at. it’s
            centered
            at mu x, mu y. These are the means of the two random variables. If those sigmas were equal, that ellipse
            would
            be actually a circle. And you would get contours of this kind.</p>
        <p>但另一方面，如果 sigma 不同，您将得到具有这种轮廓的椭圆。如果我的轮廓是这种类型，那对应什么？ sigma x 大于 sigma y 或反之亦然。好的，这种轮廓基本上告诉您 X 比 Y 更可能分散。因此可能的 x 的范围更大。</p><p>But if, on the other hand, the sigmas are different, you’re going to get an ellipse that has contours of this
            kind. So if my contours are of this kind, that corresponds to what? Sigma x being bigger than sigma y or
            vice
            versa. OK, contours of this kind basically tell you that X is more likely to be spread out than Y. So the
            range
            of possible x’s is bigger.</p>
        <h2 id="unknown-259">未知</h2><h2>Unknown</h2>
        <p>这里的 X 和上面的 Y 一样有可能。所以大 X 的概率与某些较小的 y 的概率大致相同。所以在这种图片中，X 的方差将大于 Y 的方差。因此，取决于这些方差如何相互比较，这将决定椭圆的形状。如果 Y 的方差较大，那么你的椭圆将是相反的。</p><p>And X out here is as likely as a Y up there. So big X’s have roughly the same probability as certain smaller
            y’s.
            So in a picture of this kind, the variance of X is going to be bigger than the variance of Y. So depending
            on
            how these variances compare with each other, that’s going to determine the shape of the ellipse. If the
            variance
            of Y we’re bigger, then your ellipse would be the other way.</p>
        <p>它会在另一个维度上被拉长。再形象一点地想象一下。让我给你看一张具体的图片。这是一张特殊情况的图片。在这里，我认为方差是相等的。这就是你得到的形状。它看起来像一个二维的钟。所以记住，对于一个正态随机变量，对于一个单一的随机变量，你会得到一个钟形的 PDF。那只是一个钟形曲线。</p><p>It would be elongated in the other dimension. Just visualize it a little more. Let me throw at you a
            particular
            picture. This is one. this is a picture of one special case. Here, I think, the variances are equal. That’s
            the
            kind of shape that you get. It looks like a two dimensional bell. So remember, for a normal random
            variables,
            for a single random variable you get a PDF that’s bell shaped. That’s just a bell shaped curve.</p>
        <p>在二维情况下，我们得到了联合 PDF，它再次呈钟形。现在它看起来更像一个真正的钟，就像它在普通空间中布局的方式一样。如果你看看这个函数的轮廓，函数相等的地方，典型的轮廓会是这种形状。它将是一个椭圆形。在这种情况下，实际上，它更像一个圆形。</p><p>In the two dimensional case, we get the joint PDF, which is bell shaped again. And now it looks more like a
            real
            bell, the way it would be laid out in ordinary space. And if you look at the contours of this function, the
            places where the function is equal, the typcial contour would have this shape here. And it would be an
            ellipse.
            And in this case, actually, it will be more like a circle.</p>
        <h2 id="unknown-260">未知</h2><h2>Unknown</h2>
        <p>因此，这些将是不同的轮廓。轮廓是联合 PDF 为常数的地方。当您更改该常数的值时，您会得到不同的轮廓。当然，PDF 以两个随机变量的平均值为中心。因此，在这个特定情况下，由于钟形图以 (0,0) 向量为中心，因此这是具有 0 均值的双变量正态图。好的，就是这样。</p><p>So these would be the different contours for different. so the contours are places where the joint PDF is a
            constant. When you change the value of that constant, you get the different contours. And the PDF is, of
            course,
            centered around the mean of the two random variables. So in this particular case, since the bell is centered
            around the (0,0) vector, this is a plot of a bivariate normal with 0 means. OK, there’s.</p>
        <p>当你的钟在空间中方向不同时，二元法线也很有趣。我们谈到了这样的椭圆，这样的椭圆。你也可以想象你拿的钟，你以某种方式挤压它们，这样它们在一个维度上变窄，然后可能旋转它们。所以如果你有。</p><p>Bivariate normals are also interesting when your bell is oriented differently in space. We talked about
            ellipses
            that are this way, ellipses that are this way. You could imagine also bells that you take them, you squash
            them
            somehow, so that they become narrow in one dimension and then maybe rotate them. So if you had.</p>
        <p>我们不会深入讨论这个问题，但是如果你有一个轮廓线像这样的联合 pdf，它会对应什么？你的 x 和 y 是独立的吗？不会。这表明 x 和 y 之间存在关系。也就是说，当你有更大的 x 时，你会期望得到更大的 y。所以这将是一个依赖法线的情况。</p><p>We’re not going to go into this subject, but if you had a joint pdf whose contours were like this, what would
            that correspond to? Would your x’s and y’s be independent? No.&nbsp;This would indicate that there’s a relation
            between the x’s and the y’s. That is, when you have bigger x’s, you would expect to also get bigger y’s. So
            it
            would be a case of dependent normals.</p>
        <h2 id="unknown-261">未知</h2><h2>Unknown</h2>
        <p>我们马上就回到这一点。在讨论与随机变量之间的依赖关系之前，我们先再离题一下。如果我们有两条独立的法线，正如我们在这里讨论的那样，我们可以应用公式，也就是我们刚才讨论的卷积公式。假设你想找到这两个独立法线之和的分布。</p><p>And we’re coming back to this point in a second. Before we get to that point in a second that has to do with
            the
            dependencies between the random variables, let’s just do another digression. If we have our two normals that
            are
            independent, as we discussed here, we can go and apply the formula, the convolution formula that we were
            just
            discussing. Suppose you want to find the distribution of the sum of these two independent normals.</p>
        <p>怎么做呢？对于和的密度有一个闭式公式，就是这个。我们确实有 X 密度和 Y 密度的公式，因为它们都是正态随机变量。所以你需要计算这个特定的积分。它是关于 x 的积分。你必须对任何给定的 w 值计算这个积分。</p><p>How do you do this? There is a closed form formula for the density of the sum, which is this one. We do have
            formulas for the density of X and the density of Y, because both of them are normal, random variables. So
            you
            need to calculate this particular integral here. It’s an integral with respect to x. And you have to
            calculate
            this integral for any given value of w.</p>
        <p>这是一项积分练习，难度不大。结果发现，完成所有练习后，你会得到一个具有这种形式的答案。你看着它，突然意识到，哦，这是正常的。完成这项练习后，结论是两个独立的正态随机变量之和也是正态的。</p><p>So this is an exercise in integration, which is not very difficult. And it turns out that after you do
            everything, you end up with an answer that has this form. And you look at that, and you suddenly recognize,
            oh,
            this is normal. And conclusion from this exercise, once it’s done, is that the sum of two independent normal
            random variables is also normal.</p>
        <h2 id="unknown-262">未知</h2><h2>Unknown</h2>
        <p>现在，W 的平均值当然等于 X 和 Y 的平均值之和。在这种情况下，在这个公式中，我取平均值为 0。所以 W 的平均值也将是 0。在更一般的情况下，W 的平均值将只是两个平均值之和。</p><p>Now, the mean of W is, of course, going to be equal to the sum of the means of X and Y. In this case, in this
            formula I took the means to be 0. So the mean of W is also going to be 0. In the more general case, the mean
            of
            W is going to be just the sum of the two means.</p>
        <p>W 的方差始终是 X 和 Y 的方差之和，因为我们有独立的随机变量。所以这里没有什么奇怪的。这个计算中最令人惊讶的是这个事实，即独立正态随机变量之和是正态的。我在之前的讲座中提到过这个事实。这里我们要做的是基本上概述证明这一特定事实的论据。</p><p>The variance of W is always the sum of the variances of X and Y, since we have independent random variables.
            So
            there’s no surprise here. The main surprise in this calculation is this fact here, that the sum of
            independent
            normal random variables is normal. I had mentioned this fact in a previous lecture. Here what we’re doing is
            to
            basically outline the argument that justifies this particular fact.</p>
        <p>这是一项积分练习，你会发现，当你卷积两条正态曲线时，你也会得到一条正态曲线。现在，让我们回到我在这里的评论，如果你有一个等高线图，比如说，有这种形状，这表明你的两个随机变量之间存在某种依赖关系。所以，除了等高线图，我在这里放一个散点图。</p><p>It’s an exercise in integration, where you realize that when you convolve two normal curves, you also get
            back a
            normal one once more. So now, let’s return to the comment I was making here, that if you have a contour plot
            that has, let’s say, a shape of this kind, this indicates some kind of dependence between your two random
            variables. So instead of a contour plot, let me throw in here a scattered diagram.</p>
        <h2 id="unknown-263">未知</h2><h2>Unknown</h2>
        <p>这个散点图对应什么？假设你有一个离散分布，并且这个图中的每个点都有正概率。当你看到这个图时，你会说什么？我会说当 y 很大时，x 也趋向于更大。因此，从某种平均统计意义上讲，较大的 x 与较大的 y 有关。</p><p>What does this scattered diagram correspond to? Suppose you have a discrete distribution, and each one of the
            points in this diagram has positive probability. When you look at this diagram, what would you say? I would
            say
            that when y is big then x also tends to be larger. So bigger x’s are sort of associated with bigger y’s in
            some
            average, statistical sense.</p>
        <p>然而，如果你有这样的图片，它会告诉你，正 y 值在大多数情况下往往与负 x 值相关联。负 y 值往往与正 x 值相关联。因此，这里有一个关系，当一个变量很大时，另一个变量也预计会很大。这里有一个相反的关系。我们如何捕捉两个随机变量之间的这种关系？</p><p>Whereas, if you have a picture of this kind, it tells you in association that the positive y’s tend to be
            associated with negative x’s most of the time. Negative y’s tend to be associated mostly with positive x’s.
            So
            here there’s a relation that when one variable is large, the other one is also expected to be large. Here
            there’s a relation of the opposite kind. How can we capture this relation between two random variables?</p>
        <p>我们捕捉它的方式是通过定义协方差这个概念，它研究 X 是否比平常大？这就是问题所在，它是否是正数。这与 Y 是否比平常大这个问题的答案有什么关系？我们通过计算这个量来问，我们问的是，X 大和 Y 大之间是否存在系统关系？</p><p>The way we capture it is by defining this concept called the covariance, that looks at the relation of was X
            bigger than usual? That’s the question, whether this is positive. And how does this relate to the answer. to
            the
            question, was Y bigger than usual? We’re asking. by calculating this quantity, we’re sort of asking the
            question, is there a systematic relation between having a big X with having a big Y?</p>
        <h2 id="unknown-264">未知</h2><h2>Unknown</h2>
        <p>好的，为了更准确地理解它的作用，我们假设随机变量的均值为 0，这样我们就可以摆脱它。摆脱一些混乱。因此，协方差的定义就是这个乘积。这有什么作用？如果正 x 倾向于与正 y 相伴，而负 x 倾向于与负 y 相伴，则这个乘积将始终为正。协方差最终将为正。</p><p>OK,to understand more precisely what this does, let’s suppose that the random variable has 0 means, So that
            we
            get rid of this. get rid of some clutter. So the covariance is defined just as this product. What does this
            do?
            If positive x’s tends to go together with positive y’s, and negative x’s tend to go together with negative
            y’s,
            this product will always be positive. And the covariance will end up being positive.</p>
        <p>具体来说，如果你坐下来，拿着散点图进行计算，你会发现图中 X 和 Y 的协方差为正，因为大多数情况下，X 乘以 Y 为正。虽然会有一些负项，但比正项少。所以这是正协方差的情况。它表示两个随机变量之间存在正相关关系。</p><p>In particular, if you sit down with a scattered diagram and you do the calculations, you’ll find that the
            covariance of X and Y in this diagram would be positive, because here, most of the time, X times Y is
            positive.
            There’s going to be a few negative terms, but there are fewer than the positive ones. So this is a case of a
            positive covariance. It indicates a positive relation between the two random variables.</p>
        <p>当一个变量很大时，另一个变量也趋于很大。这是相反的情况。这里，当一个变量时，大部分动作发生在这个象限和那个象限，这意味着 X 乘以 Y 在大多数情况下是负数。你会得到一些正贡献，但很少。当你把所有东西加起来时，负项占主导地位。在这种情况下，我们有 X 和 Y 的协方差为负。</p><p>When one is big, the other also tends to be big. This is the opposite situation. Here, when one variable.
            here,
            most of the action happens in this quadrant and that quadrant, which means that X times Y, most of the time,
            is
            negative. You get a few positive contributions, but there are few. When you add things up, the negative
            terms
            dominate. And in this case we have covariance of X and Y being negative.</p>
        <h2 id="unknown-265">未知</h2><h2>Unknown</h2>
        <p>因此，正协方差表示一种系统关系，即两个随机变量之间存在正相关。当一个变量很大时，另一个变量也趋于很大。负协方差则相反。当一个变量趋于很大时，另一个变量趋于很小。好的，那么关于协方差还有什么要说的呢？一个观察结果是这样的。X 与 X 本身的协方差是多少？
        </p><p>So a positive covariance indicates a sort of systematic relation, that there’s a positive association between
            the
            two random variables. When one is large, the other also tends to be large. Negative covariance is sort of
            the
            opposite. When one tends to be large, the other variable tends to be small. OK, so what else is there to say
            about the covariance? One observation to make is the following. What’s the covariance of X with X itself?
        </p>
        <p>如果你在这里代入 X，你会看到我们得到的是 X 的期望值减去 X 的平方的期望值。这只是随机变量方差的定义。所以这是需要记住的一个事实。我们有一个计算方差的快捷公式。还有一个类似的计算协方差的快捷公式。具体来说，我们可以用这种特定的方式计算协方差。</p><p>If you plug in X here, you see that what we have is expected value of X minus expected of X squared. And
            that’s
            just the definition of the variance of a random variable. So that’s one fact to keep in mind. We had a
            shortcut
            formula for calculating variances. There’s a similar shortcut formula for calculating covariances. In
            particular, we can calculate covariances in this particular way.</p>
        <p>这只是在需要计算时的一种方便方法。最后，当您想要计算随机变量总和的方差时，协方差非常有用。我们知道，如果两个随机变量是独立的，则总和的方差就是方差的总和。当随机变量是相关的时，情况就不再如此了，我们需要对公式进行一些补充。</p><p>That’s just the convenient way of doing it whenever you need to calculate it. And finally, covariances are
            very
            useful when you want to calculate the variance of a sum of random variables. We know that if two random
            variables are independent, the variance of the sum is the sum of the variances. When the random variables
            are
            dependent, this is no longer true, and we need to supplement the formula a little bit.</p>
        <h2 id="unknown-266">未知</h2><h2>Unknown</h2>
        <p>你们手上的幻灯片上有一个错别字。2 这个项不应该出现在那里。让我们看看这个公式从何而来。假设我们的随机变量独立于非独立变量，我们的随机变量的均值为 0。我们想计算方差。因此，方差将是 (X1 加 Xn) 平方的期望值。你要做的就是展开平方。</p><p>And there’s a typo on the slides that you have in your hands. That term of 2 shouldn’t be there. And let’s
            see
            where that formula comes from. Let’s suppose that our random variables are independent of not independent
            our
            random variables have 0 means. And we want to calculate the variance. So the variance is going to be
            expected
            value of (X1 plus Xn) squared. What you do is you expand the square.</p>
        <p>然后你得到了 Xi 平方和的期望值。然后你得到了所有的交叉项。好的。现在，为了简单起见，我们假设我们有 0 个均值。它的期望值是 X 平方项的期望值之和。这给了我们方差。然后我们就得到了所有可能的交叉项。</p><p>And you get the expected value of the sum of the Xi squared. And then you get all the cross terms. OK. And so
            now, here, let’s assume for simplicity that we have 0 means. The expected value of this is the sum of the
            expected values of the X squared terms. And that gives us the variance. And then we have all the possible
            cross
            terms.</p>
        <p>每个可能的交叉项都是 Xi 乘以 Xj 的期望值。这只是协方差。因此，如果您可以计算所有方差和协方差，那么您也可以计算随机变量总和的方差。现在，如果两个随机变量是独立的，那么您可以查看此表达式。由于独立性，乘积的期望值将是期望值的乘积。</p><p>And each one of the possible cross terms is the expected value of Xi times Xj. This is just the covariance.
            So if
            you can calculate all the variances and the covariances, then you’re able to calculate also the variance of
            a
            sum of random variables. Now, if two random variables are independent, then you look at this expression.
            Because
            of independence, expected value of the product is going to be the product of the expected values.</p>
        <h2 id="unknown-267">未知</h2><h2>Unknown</h2>
        <p>而仅此一项的预期值始终等于 0。您预期的与平均值的偏差就是 0。因此协方差将为 0。因此，独立随机变量导致协方差为 0，尽管相反的事实不一定成立。因此，协方差可以为您提供两个随机变量之间关系的一些指示。协方差在概念上不太方便的地方是它的单位错误。</p><p>And the expected value of just this term is always equal to 0. You’re expected deviation from the mean is
            just 0.
            So the covariance will turn out to be 0. So independent random variables lead to 0 covariances, although the
            opposite fact is not necessarily true. So covariances give you some indication of the relation between two
            random variables. Something that’s not so convenient conceptually about covariances is that it has the wrong
            units.</p>
        <p>这与我们对方差的评论相同。通过考虑标准差（其单位正确），我们解决了方差问题。因此，出于同样的推理，我们希望有一个概念来捕捉两个随机变量之间的关系，从某种意义上说，这与我们正在处理的单位无关。我们希望有一个无量纲量。</p><p>That’s the same comment that we had made regarding variances. And with variances we got out of that issue by
            considering the standard deviation, which has the correct units. So with the same reasoning, we want to have
            a
            concept that captures the relation between two random variables and, in some sense, that doesn’t have to do
            with
            the units that we’re dealing. We want to have a dimensionless quantity.</p>
        <p>这告诉我们两个随机变量之间的关联程度。因此，我们不再只考虑 X 与 Y 的协方差，而是取随机变量，并通过除以各自的标准差来标准化它们，并取其期望值。因此，我们最终得到的是 X 和 Y 的协方差，其单位是 X 的单位乘以 Y 的单位。</p><p>That tells us how strongly two random variables are related to each other. So instead of considering the
            covariance of just X with Y, we take our random variables and standardize them by dividing them by their
            individual standard deviations and take the expectation of this. So what we end up doing is the covariance
            of X
            and Y, which has units that are the units of X times the units of Y.</p>
        <h2 id="unknown-268">未知</h2><h2>Unknown</h2>
        <p>但是除以标准差，这样我们得到一个没有单位的量。这个量我们称之为相关系数。这是一个非常有用的量，是两个随机变量之间关联强度的一个非常有用的度量。它非常有用，因为它总是在 1 和 +1 之间。这是一个代数练习，你们将在复习中看到。你可以这样解释它。</p><p>But divide with a standard deviation, so that we get a quantity that doesn’t have units. This quantity, we
            call
            it the correlation coefficient. And it’s a very useful quantity, a very useful measure of the strength of
            association between two random variables. It’s very informative, because it falls always between 1 and +1.
            This
            is an algebraic exercise that you’re going to see in recitation. And the way that you interpret it is as
            follows.</p>
        <p>如果两个随机变量是独立的，协方差将为 0。相关系数将为 0。因此，相关系数为 0 基本上表示两个随机变量之间缺乏系统关系。另一方面，当 rho 很大时，无论是接近 1 还是接近 1，都表明两个随机变量之间存在很强的关联。极端情况是 rho 取极值。</p><p>If the two random variables are independent, the covariance is going to be 0. The correlation coefficient is
            going to be 0. So 0 correlation coefficient basically indicates a lack of a systematic relation between the
            two
            random variables. On the other hand, when rho is large, either close to 1 or close to 1, this is an
            indication
            of a strong association between the two random variables. And the extreme case is when rho takes an extreme
            value.</p>
        <p>当 rho 的量级等于 1 时，它就变得最大了。在这种情况下，两个随机变量非常相关。有多强？好吧，如果你知道一个随机变量，如果你知道 y 的值，你就可以恢复 x 的值，反之亦然。因此，完全相关的情况是，一个随机变量是另一个随机变量的线性函数。</p><p>When rho has a magnitude equal to 1, it’s as big as it can be. In that case, the two random variables are
            very
            strongly related. How strongly? Well, if you know one random variable, if you know the value of y, you can
            recover the value of x and conversely. So the case of a complete correlation is the case where one random
            variable is a linear function of the other random variable.</p>
        <h2 id="unknown-269">未知</h2><h2>Unknown</h2>
        <p>就散点图而言，这意味着存在一条特定的线，并且唯一可能发生的 (x,y) 对都位于该线上。因此，如果所有可能的 (x,y) 对都位于这条线上，则存在这种关系，相关系数等于 1。相关系数接近 1 的情况是散点图，其中 x 和 y 彼此非常紧密地对齐，可能不完全对齐，但相当紧密。好的，所以明天您将在复习中听到更多关于相关系数和协方差的内容。</p><p>In terms of a scatter plot, this would mean that there’s a certain line and that the only possible (x,y)
            pairs
            that can happen would lie on that line. So if all the possible (x,y) pairs lie on this line, then you have
            this
            relation, and the correlation coefficient is equal to 1. A case where the correlation coefficient is close
            to 1
            would be a scatter plot like this, where the x’s and y’s are quite strongly aligned with each other, maybe
            not
            exactly, but fairly strongly. All right, so you’re going to hear a little more about correlation
            coefficients
            and covariances in recitation tomorrow.</p>
        <h1 id="iterated-expectations">12. 迭代期望</h1><h1>12. Iterated Expectations</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 47:54 <a href="https://youtube.com/watch?v=P7a4bjE6Crk">https://youtube.com/watch?v=P7a4bjE6Crk</a></p><p> 12 years ago (Nov 10, 2012) — 47:54 <a href="https://youtube.com/watch?v=P7a4bjE6Crk">https://youtube.com/watch?v=P7a4bjE6Crk</a></p>
        <h2 id="introduction-1">介绍</h2><h2>Introduction</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。约翰·齐西克利斯：今天我们将结束本课程的核心内容。这些内容与一般概率论有关。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. JOHN TSITSIKLIS: So today we’re going
            to
            finish with the core material of this class. That is the material that has to do with probability theory in
            general.</p>
        <p>然后，在本学期的剩余时间里，我们将研究一些特殊类型的模型，讨论推理。好吧，稍后还会有一个核心材料的小模块。但今天我们基本上完成了第四章。我们要做的是研究一个有点熟悉的概念，即条件期望的概念。</p><p>And then for the rest of the semester we’re going to look at some special types of models, talk about
            inference.
            Well, there’s also going to be a small module of core material coming later. But today we’re basically
            finishing
            chapter four. And what we’re going to do is we’re going to look at a somewhat familiar concept, the concept
            of
            the conditional expectation.</p>
        <p>但我们将从一个略微不同、略微复杂的角度来看待这个问题。除了条件期望，我们还将讨论条件方差。我们将以这种方式表示它。我们将了解它们是什么，这里涉及一些微妙的概念。</p><p>But we’re going to look at it from a slightly different angle, from a slightly more sophisticated angle. And
            together with the conditional expectation we will also talk about conditional variances. It’s something that
            we’re going to denote this way. And we’re going to see what they are, and there are some subtle concepts
            that
            are involved here.</p>
        <p>我们将应用一些我们开发的工具来处理一种特殊的情况，即添加随机变量。但我们添加的是随机数量的随机变量。</p><p>And we’re going to apply some of the tools we’re going to develop to deal with a special type of situation in
            which we’re adding random variables. But we’re adding a random number of random variables.</p>
        <h2 id="conditional-expectations">条件期望</h2><h2>Conditional Expectations</h2>
        <p>好的，让我们开始讨论条件期望。我想你知道它们是什么。假设我们处在离散世界中。xy，或离散随机变量。</p><p>OK, so let’s start talking about conditional expectations. I guess you know what they are. Suppose we are in
            the
            discrete the world. xy, or discrete random variables.</p>
        <p>假设我告诉你随机变量 y 的值，我们定义了 x 的条件期望。我们定义它的方式与普通期望相同，只是我们使用条件 PMF。所以我们使用适用于新宇宙的概率，其中我们被告知随机变量 y 的值。所以到目前为止这仍然是一个熟悉的概念。</p><p>We defined the conditional expectation of x given that I told you the value of the random variable y. And the
            way
            we define it is the same way as an ordinary expectation, except that we’re using the conditional PMF. So
            we’re
            using the probabilities that apply to the new universe where we are told the value of the random variable y.
            So
            this is still a familiar concept so far.</p>
        <p>如果我们处理的是连续随机变量 x，公式是一样的，只是这里我们有一个积分，我们必须使用 x 的条件密度函数。现在我要做的是，我想通过我们上次讨论的例子来简单介绍一下。上次我们讨论了一根具有一定长度的棍子。</p><p>If we’re dealing with the continuous random variable x the formula is the same, except that here we have an
            integral, and we have to use the conditional density function of x. Now what I’m going to do, I want to
            introduce it gently through the example that we talked about last time. So last time we talked about having
            a
            stick that has a certain length.</p>
        <p>我们拿起那根棍子，在任意一个点均匀地随机选择它，然后我们来说明为什么我们选择在那个点折断它。选择了 y 之后，我们就剩下一根棍子了。然后我将在 0 到 y 之间均匀地随机选择一个点，再折断它一次。</p><p>And we take that stick, and we break it at some point that we choose uniformly at random. And let’s denote
            why
            the place where we chose to break it. Having chosen y, then we’re left with a piece of the stick. And I’m
            going
            to choose a place to break it once more uniformly at random between 0 and y.</p>
        <p>所以这是我们要打破它的第二个地方，我们称这个地方为 x。好的，那么如果我告诉你 y 的值，那么 x 的条件期望是什么？我告诉你大写 Y 恰好取一个特定的数值。所以这个大写 Y 现在是一个特定的数值，x 是在这个范围内均匀选择的。
        </p><p>So this is the second place at which we are going to break it, and we call that place x. OK, so what’s the
            conditional expectation of x if I tell you the value of y? I tell you that capital Y happens to take a
            specific
            numerical value. So this capital Y is now a specific numerical value, x is chosen uniformly over this range.
        </p>
        <p>因此 x 的期望值将是 0 到 y 之间这个范围的一半。因此条件期望是 y 除以 2。这里需要注意的是，这个量是一个数字。</p><p>So the expected value of x is going to be half of this range between 0 and y. So the conditional expectation
            is
            little y over 2. The important thing to realize here is that this quantity is a number.</p>
        <p>我告诉你，随机变量取某个数值，比如说 3.5。然后你告诉我，假设随机变量取数值 3.5，x 的期望值为 1.75。所以这是数字之间的等式。另一方面，在进行实验之前，你并不知道 y 会变成什么样子。</p><p>I told you that the random variable took a certain numerical value, let’s say 3.5. And then you tell me given
            that the random variable took the numerical value 3.5 the expected value of x is 1.75. So this is an
            equality
            between numbers. On the other hand, before you do the experiment you don’t know what y is going to turn out
            to
            be.</p>
        <p>所以这个小 y 就是你开始做实验时观察到的数值，你观察到的是大写 Y 的值。所以从某种意义上说，这个量是事先不知道的，它本身就是随机的。所以也许我们可以开始把它看作一个随机变量。换句话说，在我们做实验之前，我会问你给定 y 时 x 的期望值是多少？</p><p>So this little y is the numerical value that has been observed when you start doing the experiments and you
            observe the value of capital Y. So in some sense this quantity is not known ahead of time, it is random
            itself.
            So maybe we can start thinking of it as a random variable. So to put it differently, before we do the
            experiment
            I ask you what’s the expected value of x given y?</p>
        <p>你会回答我，我不知道，这取决于 y 会变成什么样子。因此，给定 y 本身，x 的期望值可以看作是一个随机变量，因为它取决于随机变量大写 Y。因此，这里隐藏着某种关于随机变量而不是数字的陈述。关于随机变量的陈述，我们这样写。</p><p>You’re going to answer me well I don’t know, it depends on what y is going to turn out to be. So the expected
            value of x given y itself can be viewed as a random variable, because it depends on the random variable
            capital
            Y. So hidden here there’s some kind of statement about random variables instead of numbers. And that
            statement
            about random variables, we write it this way.</p>
        <p>通过将期望值（条件期望）视为随机变量而不是数字。当我们不指定具体数字时，它就是一个随机变量，但我们将其视为一个抽象对象。给定随机变量 y 的 x 的期望值是随机变量 y / 2，无论 Y 大写结果是多少。
        </p><p>By thinking of the expected value, the conditional expectation, as a random variable instead of a number.
            It’s a
            random variable when we do not specify a specific number, but we think of it as an abstract object. The
            expected
            value of x given the random variable y is the random variable y over 2 no matter what capital Y turns out to
            be.
        </p>
        <p>因此，我们转而采用一个处理两个数字相等性的语句，并将其变成两个随机变量相等性的语句。好吧，这显然是一个随机变量，因为大写字母 Y 是随机的。这个对象到底是什么？我还没有正式为你定义它。所以现在让我们给出这个将以这种方式表示的对象的正式定义。</p><p>So we turn and take a statement that deals with equality of two numbers, and we make it a statement that’s an
            equality between two random variables. OK so this is clearly a random variable because capital Y is random.
            What
            exactly is this object? I didn’t yet define it for you formally. So let’s now give the formal definition of
            this
            object that’s going to be denoted this way.</p>
        <p>给定随机变量 y 的 x 的条件期望是一个随机变量。它是哪个随机变量？它是当大写 Y 恰好取特定数值小写 y 时取该特定数值的随机变量。具体来说，这是一个随机变量，它是随机变量大写 Y 的函数。在本例中，它由大写 Y 的简单公式给出。</p><p>The conditional expectation of x given the random variable y is a random variable. Which random variable is
            it?
            It’s the random variable that takes this specific numerical value whenever capital Y happens to take the
            specific numerical value little y. In particular, this is a random variable, which is a function of the
            random
            variable capital Y. In this instance, it’s given by a simple formula in terms of capital Y.</p>
        <p>在其他情况下，它可能是一个更复杂的公式。所以，再总结一下，它是一个随机的。条件期望可以被认为是一个随机变量，而不是一个数字。所以在任何特定的环境中，当你得到大写 Y 的值时，条件期望就变成了一个数字。这是这个随机变量的实际值。</p><p>In other situations it might be a more complicated formula. So again, to summarize, it’s a random. The
            conditional expectation can be thought of as a random variable instead of something that’s just a number. So
            in
            any specific context when you’re given the value of capital Y the conditional expectation becomes a number.
            This
            is the realized value of this random variable.</p>
        <p>但在实验开始之前，在你知道大写字母 Y 会是多少之前，你只能说条件期望会是大写字母 Y 的 1/2。这是一个相当微妙的概念，它是一个抽象，但它是一个有用的抽象。我们今天将看到如何使用它。</p><p>But before the experiment starts, before you know what capital Y is going to be, all that you can say is that
            the
            conditional expectation is going to be 1/2 of whatever capital Y turns out to be. This is a pretty subtle
            concept, it’s an abstraction, but it’s a useful abstraction. And we’re going to see today how to use it.</p>
        <p>好吧，我已经指出了条件期望，即取这些数值的随机变量是一个随机变量。如果它是一个随机变量，这意味着它有自己的期望。那么让我们开始思考条件期望的期望会是什么。</p><p>All right, I have made the point that the conditional expectation, the random variable that takes these
            numerical
            values is a random variable. If it is a random variable this means that it has an expectation of its own. So
            let’s start thinking what the expectation of the conditional expectation is going to turn out to be.</p>
        <p>好的，所以条件期望是一个随机变量，一般来说它是我们观察到的随机变量 y 的某个函数。就数值而言，如果大写 Y 恰好取一个特定的数值，那么条件期望也会取一个特定的数值，我们使用相同的函数来评估它。这里的区别在于，这是随机变量的等式，这是数字之间的等式。
        </p><p>OK, so the conditional expectation is a random variable, and in general it’s some function of the random
            variable
            y that we are observing. In terms of numerical values if capital Y happens to take a specific numerical
            value
            then the conditional expectation also takes a specific numerical value, and we use the same function to
            evaluate
            it. The difference here is that this is an equality of random variables, this is an equality between
            numbers.
        </p>
        <p>现在，如果我们想计算条件期望的期望值，我们基本上是在谈论随机变量函数的期望值。我们知道如何计算函数的期望值。例如，如果我们处于离散情况，这将是函数的所有 y 的总和，我们将该函数的期望值乘以 y 取特定数值的概率。</p><p>Now if we want to calculate the expected value of the conditional expectation we’re basically talking about
            the
            expected value of a function of a random variable. And we know how to calculate expected values of a
            function.
            If we are in the discrete case, for example, this would be a sum over all y’s of the function who’s expected
            value we’re taking times the probability that y takes on a specific numerical value.</p>
        <p>好的，但让我们记住 g 是什么。所以 g 是 x 对 y 的条件期望的数值。现在当你看到这个表达式时，你就会认出它。这是我们在总期望定理中得到的表达式。我错过了什么吗？是的，在总期望定理中，为了找到 x 的期望值，我们根据 y 发生的情况将世界划分为不同的场景。</p><p>OK, but let’s remember what g is. So g is the numerical value of the conditional expectation of x with y. And
            now
            when you see this expression you recognize it. This is the expression that we get in the total expectation
            theorem. Did I miss something? Yes, in the total expectation theorem to find the expected value of x, we
            divide
            the world into different scenarios depending on what y happens.</p>
        <p>我们计算每个可能世界中的期望值，然后取加权平均值。所以这是你之前见过的公式，你知道这是 x 的期望值。所以这是我在这里写的内容的一个更长、更详细的推导，但要记住的重要一点是这个故事的寓意，即妙语。</p><p>We calculate the expectation in each one of the possible worlds, and we take the weighted average. So this is
            a
            formula that you have seen before, and you recognize that this is the expected value of x. So this is a
            longer,
            more detailed derivation of what I had written up here, but the important thing to keep in mind is the moral
            of
            the story, the punchline.</p>
        <p>条件期望的期望值就是期望本身。所以这只是我们的总期望定理，但用更抽象的符号表示。这种更抽象的符号很方便，我们一会儿就会看到。好的，我们可以将其应用于我们的棍子示例。如果我们想找到 x 的期望值，那么最后棍子还剩下多少？</p><p>The expected value of the conditional expectation is the expectation itself. So this is just our total
            expectation theorem, but written in more abstract notation. And it comes handy to have this more abstract
            notation, as we’re going to see in a while. OK, we can apply this to our stick example. If we want to find
            the
            expected value of x how much of the stick is left at the end?</p>
        <p>我们可以使用迭代期望定律来计算它。它是条件期望的期望值。我们知道条件期望是 y 除以 2。因此 y 的期望值是 l 除以 2，因为 y 是均匀分布的，所以我们得到 l 除以 4。所以这给了我们上次用相当长的方法得出的相同答案。
        </p><p>We can calculate it using this law of iterated expectations. It’s the expected value of the conditional
            expectation. We know that the conditional expectation is y over 2. So expected value of y is l over 2,
            because y
            is uniform so we get l over 4. So this gives us the same answer that we derived last time in a rather long
            way.
        </p>
        <p>好了，既然我们已经掌握了条件期望，让我们再提高一点标准，谈谈条件方差。因此，条件期望是条件宇宙中的平均值或期望值，在这个宇宙中，你被告知 y 的值。在同一个条件宇宙中，你可以讨论 x 的条件分布，它有一个平均值。条件期望。但 x 的条件分布也有方差。</p><p>All right, now that we have mastered conditional expectations, let’s raise the bar a little more and talk
            about
            conditional variances. So the conditional expectation is the mean value, or the expected value, in a
            conditional
            universe where you’re told the value of y. In that same conditional universe you can talk about the
            conditional
            distribution of x, which has a mean. the conditional expectation. but the conditional distribution of x also
            has
            a variance.</p>
        <p>因此，我们可以讨论该条件宇宙中 x 的方差。条件方差作为一个数字是很自然的事情。它是 x 的方差，只是所有计算都是在条件宇宙中完成的。在条件宇宙中，x 的期望值是条件期望。这是与条件宇宙中平均值的距离的平方。</p><p>So we can talk about the variance of x in that conditional universe. The conditional variance as a number is
            the
            natural thing. It’s the variance of x, except that all the calculations are done in the conditional
            universe. In
            the conditional universe the expected value of x is the conditional expectation. This is the distance from
            the
            mean in the conditional universe squared.</p>
        <p>我们取平方距离的平均值，但使用条件宇宙中适用的概率再次计算。这是数字之间的等式。我告诉你 y 的值，一旦你知道了 y 的值，你就可以继续绘制 x 的条件分布。对于该条件分布，你可以计算出该条件宇宙中 x 的方差。</p><p>And we take the average value of the squared distance, but calculate it again using the probabilities that
            apply
            in the conditional universe. This is an equality between numbers. I tell you the value of y, once you know
            that
            value for y you can go ahead and plot the conditional distribution of x. And for that conditional
            distribution
            you can calculate the number which is the variance of x in that conditional universe.</p>
        <p>现在让我们重复上一张幻灯片中的脑力活动，抽象一些东西，并定义一个随机变量，即条件方差。</p><p>So now let’s repeat the mental gymnastics from the previous slide, and abstract things, and define a random
            variable. the conditional variance.</p>
        <h2 id="conditional-variance">条件方差</h2><h2>Conditional Variance</h2>
        <p>因为我们没有指定大写 Y 的数值，所以它将是一个随机变量。因此我们事先不知道大写 Y 是多少，因此我们事先也不知道条件方差是多少。</p><p>And it’s going to be a random variable because we leave the numerical value of capital Y unspecified. So
            ahead of
            time we don’t know what capital Y is going to be, and because of that we don’t know ahead of time what the
            conditional variance is going to be.</p>
        <p>所以在实验开始前，如果我问你 x 的条件方差是多少？你会告诉我，我不知道，这取决于 y 的结果。它将是某个取决于 y 的东西。所以它是一个随机变量，是 y 的函数。所以更准确地说，当用大写字母以这种符号表示时，条件方差是一个随机变量。</p><p>So before the experiment starts if I ask you what’s the conditional variance of x? You’re going to tell me
            well I
            don’t know, It depends on what y is going to turn out to be. It’s going to be something that depends on y.
            So
            it’s a random variable, which is a function of y. So more precisely, the conditional variance when written
            in
            this notation just with capital letters, is a random variable.</p>
        <p>这是一个随机变量，一旦你知道 Y 的值，它的值就完全确定了。它采用一个特定的数值。如果 Y 碰巧得到一个特定数字的实现，那么方差也会变成一个特定数字。它只是该宇宙中 y 对 x 的条件方差。好的，好的，让我们继续我们在上一张幻灯片中所做的工作。我们有迭代期望定律。</p><p>It’s a random variable whose value is completely determined once you learned the value of capital Y. And it
            takes
            a specific numerical value. If capital Y happens to get a realization that’s a specific number, then the
            variance also becomes a specific number. And it’s just a conditional variance of y over x in that universe.
            All
            right, OK, so let’s continue what we did in the previous slide. We had the law of iterated expectations.</p>
        <p>这告诉我们，条件期望的期望值就是无条件期望。在这种情况下，是否有类似的规则可以适用？因此，您可能会猜测，可以通过取条件方差的期望值来找到 x 的方差。事实证明，这并不正确。就条件量而言，有一个方差公式。但这个公式稍微复杂一些。</p><p>That told us that expected value of a conditional expectation is the unconditional expectation. Is there a
            similar rule that might apply in this context? So you might guess that the variance of x could be found by
            taking the expected value of the conditional variance. It turns out that this is not true. There is a
            formula
            for the variance in terms of conditional quantities. But the formula is a little more complicated.</p>
        <p>如果涉及两个项而不是一个项。所以我们将快速介绍一下这个公式的推导过程。然后，通过例子，我们将尝试解释一下这里的不同项对应的含义。好吧，让我们试着证明这个公式。证明是一种有用的练习，可以确保你理解这里涉及的所有符号。</p><p>If involves two terms instead of one. So we’re going to go quickly through the derivation of this formula.
            And
            then, through examples we’ll try to get some interpretation of what the different terms here correspond to.
            All
            right, so let’s try to prove this formula. And the proof is sort of a useful exercise to make sure you
            understand all the symbols that are involved in here.</p>
        <p>所以证明并不困难，只需要 4 行半的代数运算，只需写下公式即可。但挑战在于确保你在每个点都理解每个对象是什么。所以我们进入方差影响的公式。我们通常知道 x 的方差有一个很好的表达式，我们经常用它来计算它。</p><p>So the proof is not difficult, it’s 4 and 1/2 lines of algebra, of just writing down formulas. But the
            challenge
            is to make sure that at each point you understand what each one of the objects is. So we go into formula for
            the
            variance affects. We know in general that the variance of x has this nice expression that we often use to
            calculate it.</p>
        <p>随机变量平方减去均方后的期望值。这个方差公式当然应该适用于条件宇宙。我的意思是，这是一个关于方差的通用公式。如果我们把自己置于一个条件宇宙中，其中随机变量 y 被赋予给我们，同样的数学应该适用。所以我们应该有一个类似的条件方差公式。它只是相同的公式，但适用于条件宇宙。</p><p>The expected value of the squared of the random variable minus the mean squared. This formula, for the
            variances,
            of course it should apply to conditional universes. I mean it’s a general formula about variances. If we put
            ourselves in a conditional universe where the random variable y is given to us the same math should work. So
            we
            should have a similar formula for the conditional variances. It’s just the same formula, but applied to the
            conditional universe.</p>
        <p>条件宇宙中 x 的方差是条件宇宙中 x 的期望值平方减去条件宇宙中 x 的均值平方。所以这个公式看起来不错。现在让我们取两边的期望值。记住条件方差是一个随机变量，因为它的值取决于我们对大写 Y 的任何实现。所以我们可以在这里取期望值。我们得到方差的期望值。</p><p>The variance of x in the conditional universe is the expected value of x squared. in the conditional
            universe.
            minus the mean of x. in the conditional universe. squared. So this formula looks fine. Now let’s take
            expected
            values of both sides. Remember the conditional variance is a random variable, because its value depends on
            whatever realization we get for capital Y. So we can take expectations here. We get the expected value of
            the
            variance.</p>
        <p>然后我们得到条件期望的期望值。这里我们利用了之前讨论过的事实。条件期望的期望值与无条件期望相同。所以这个项变成了这样。最后，这里我们有一些看起来很奇怪的随机变量，我们取它的期望值。好的，现在我们需要对这个项做些什么。</p><p>Then we have the expected value of a conditional expectation. Here we use the fact that we discussed before.
            The
            expected value of a conditional expectation is the same as the unconditional expectation. So this term
            becomes
            this. And finally, here we just have some weird looking random variable, and we take the expected value of
            it.
            All right, now we need to do something about this term.</p>
        <p>让我们使用相同的规则来写下这个方差。期望的方差有点奇怪，但你要记住，条件期望是随机的，因为 y 是随机的。所以这个东西是一个随机变量，所以这个东西有方差。这个东西的方差是多少？它是这个东西的期望值的平方减去这个东西的期望值的平方。</p><p>Let’s use the same rule up here to write down this variance. So variance of an expectation, that’s kind of
            strange, but you remember that the conditional expectation is random, because y is random. So this thing is
            a
            random variable, so this thing has a variance. What is the variance of this thing? It’s the expected value
            of
            the thing squared minus the square of the expected value of the thing.</p>
        <p>那么这个东西的期望值是多少呢？根据迭代期望定律，这个东西的期望值是无条件期望。这就是为什么我在这里放置无条件期望。所以我再次使用这个关于如何计算方差的一般规则，并应用它来计算条件期望的方差。</p><p>Now what’s the expected value of that thing? By the law of iterated expectations, once more, the expected
            value
            of this thing is the unconditional expectation. And that’s why here I put the unconditional expectation. So
            I’m
            using again this general rule about how to calculate variances, and I’m applying it to calculate the
            variance of
            the conditional expectation.</p>
        <p>现在你注意到，如果你把这两个表达式 c 和 d 相加，我们得到这个加上那个，也就是这个。它等于。这两个项相消，我们得到这个减那个，也就是 x 的方差。这就是证明的结束。这是那些没有传达任何直觉的证明之一。正如我所说，这是一个有用的证明，只是为了确保你理解符号。</p><p>And now you notice that if you add these two expressions c and d we get this plus that, which is this. It’s
            equal
            to. these two terms cancel, we’re left with this minus that, which is the variance of x. And that’s the end
            of
            the proof. This one of those proofs that do not convey any intuition. This, as I said, it’s a useful proof
            to go
            through just to make sure you understand the symbols.</p>
        <p>它开始变得相当混乱，而且有点抽象。所以最好了解发生了什么。现在这个公式背后有直觉，其中一些最好留到我们稍后讨论推理时再讲。这个想法是，你可以将条件期望解释为你试图估计的随机变量的估计值。</p><p>It starts to get pretty confusing, and a little bit on the abstract side. So it’s good to understand what’s
            going
            on. Now there is intuition behind this formula, some of which is better left for later in the class when we
            talk
            about inference. The idea is that the conditional expectation you can interpret it as an estimate of the
            random
            variable that you are trying to.</p>
        <p>基于 y 测量的 x 估计，你可以认为这些方差与估计误差有关。一旦你开始用这些术语思考，就会产生解释。但正如我所说，这最好留到我们开始讨论推理时再讨论。</p><p>An estimate of x based on measurements of y, you can think of these variances as having something to do with
            an
            estimation error. And once you start thinking in those terms an interpretation will come about. But again as
            I
            said this is better left for when we start talking about inference.</p>
        <p>尽管如此，我们还是要通过考虑一个简单示例来直观地了解所有这些公式，在这个示例中，我们将应用迭代期望定律和总方差定律。简单示例就是我们进行一项精彩的实验，即对由许多班级组成的班级进行测验。我们对两个随机变量感兴趣。因此，我们有许多学生，他们都被分配到各个班级。
        </p><p>Nevertheless, we’re going to get some intuition about all these formulas by considering a baby example where
            we’re going to apply the law of iterated expectations, and the law of total variance. So the baby example is
            that we do this beautiful experiment of giving a quiz to a class consisting of many sections. And we’re
            interested in two random variables. So we have a number of students, and they’re all allocated to sections.
        </p>
        <p>实验中，我随机挑选一名学生，并观察两个随机变量。一个是随机挑选的学生的测验成绩，另一个随机变量是我挑选的学生的班级编号。我们给出了关于两个班级的一些统计数据。第一班级有 10 名学生，第二班级有 20 名学生。</p><p>The experiment is that I pick a student at random, and I look at two random variables. One is the quiz score
            of
            the randomly selected student, and the other random variable is the section number of the student that I
            have
            selected. We’re given some statistics about the two sections. Section one has 10 students, section two has
            20
            students.</p>
        <p>第一部分的测验平均分为 90 分。第二部分的测验平均分为 60 分。x 的期望值是多少？如果我随机挑选一名学生，其预期测验分数是多少？好吧，每个学生被选中的概率相同。我对 30 名学生做出了这一假设。我需要将所有学生的测验分数相加。</p><p>The quiz average in section one was 90. Quiz average in section two was 60. What’s the expected value of x?
            What’s the expected quiz score if I pick a student at random? Well, each student has the same probability of
            being selected. I’m making that assumption out of the 30 students. I need to add the quiz scores of all of
            the
            students.</p>
        <p>所以我需要添加第一部分的测验分数，也就是 90 乘以 10。我需要添加该部分的测验分数，也就是 60 乘以 20。我们发现总体平均分数是 70。所以这是通常的无条件期望。让我们看看条件期望，让我们看看我们讨论数值的基本版本。</p><p>So I need to add the quiz scores in section one, which is 90 times 10. I need to add the quiz scores in that
            section, which is 60 times 20. And we find that the overall average was 70. So this is the usual
            unconditional
            expectation. Let’s look at the conditional expectation, and let’s look at the elementary version where we’re
            talking about numerical values.</p>
        <p>如果我告诉你随机选择的学生在第一部分，那么该学生的测验分数的期望值是多少？好吧，根据这些信息，我们从平均分为 90 的那部分随机挑选一名学生。该学生的分数的期望值将是 90。因此，给定 y 的具体值、具体部分，条件期望或测验分数的期望值就是一个具体的数字，即 90。同样，对于第二部分，期望值为 60，这是第二部分的平均分数。</p><p>If I tell you that the randomly selected student was in section one what’s the expected value of the quiz
            score
            of that student? Well, given this information, we’re picking a random student uniformly from that section in
            which the average was 90. The expected value of the score of that student is going to be 90. So given the
            specific value of y, the specific section, the conditional expectation or the expected value of the quiz
            score
            is a specific number, the number 90. Similarly for the second section the expected value is 60, that’s the
            average score in the second section.</p>
        <p>这是基础版本。抽象版本呢？在抽象版本中，条件期望是一个随机变量，因为它取决于具体情况。我挑选的学生在哪个部分？我以 1/3 的概率在第一部分挑选一名学生，在这种情况下，条件期望将是 90，我以 2/3 的概率在第二部分挑选一名学生。</p><p>This is the elementary version. What about the abstract version? In the abstract version the conditional
            expectation is a random variable because it depends. In which section is the student that I picked? And with
            probability 1/3, I’m going to pick a student in the first section, in which case the conditional expectation
            will be 90, and with probability 2/3 I’m going to pick a student in the second section.</p>
        <p>在这种情况下，条件期望将取值为 60。因此，这说明了条件期望是一个随机变量。根据 y 的值，条件期望将以一定的概率为一个或另一个值。现在我们有了条件期望的分布，我们可以计算它的期望值。</p><p>And in that case the conditional expectation will take the value of 60. So this illustrates the idea that the
            conditional expectation is a random variable. Depending on what y is going to be, the conditional
            expectation is
            going to be one or the other value with certain probabilities. Now that we have the distribution of the
            conditional expectation we can calculate the expected value of it.</p>
        <p>而这个随机变量的期望值是 1/3 乘以 90，加上 2/3 乘以 60，结果等于 70。奇迹般地，这个数字和我们上面得到的数字相同。所以这告诉你，你可以通过取每个班的平均分，并根据班里的学生人数对每个班进行加权，来计算一个大班的总体平均分。</p><p>And the expected value of such a random variable is 1/3 times 90, plus 2/3 times 60, and it comes out to
            equal
            70. Which miraculously is the same number that we got up there. So this tells you that you can calculate the
            overall average in a large class by taking the averages in each one of the sections and weighing each one of
            the
            sections according to the number of students that it has.</p>
        <p>因此，这一部分有 90 名学生，但只有 1/3 的学生，因此权重为 1/3。因此，迭代期望定律再一次并不太复杂。你只需查看各部分的平均成绩并将它们相加，即可计算出班级总平均成绩。现在，由于条件期望是一个随机变量，因此它当然有自己的方差。那么让我们计算方差。我们如何计算方差？</p><p>So this section had 90 students but only 1/3 of the students, so it gets a weight of 1/3. So the law of
            iterated
            expectations, once more, is nothing too complicated. It’s just that you can calculate overall class average
            by
            looking at the section averages and combine them. Now since the conditional expectation is a random
            variable, of
            course it has a variance of it’s own. So let’s calculate the variance. How do we calculate variances?</p>
        <p>我们查看这个随机变量的所有可能数值，即 90 和 60。我们查看这些可能数值与这个随机变量的平均值之间的差值，我们发现这个差值是 70。然后我们根据概率对不同的可能数值进行加权。因此，在概率为 1/3 的情况下，条件期望为 90，与平均值相差 20。
        </p><p>We look at all the possible numerical values of this random variable, which are 90 and 60. We look at the
            difference of those possible numerical values from the mean of this random variable, and the mean of that
            random
            variable, we found that’s it’s 70. And then we weight the different possible numerical values according to
            their
            probabilities. So with probability 1/3 the conditional expectation is 90, which is 20 away from the mean.
        </p>
        <p>我们得到了这个平方距离。概率为 2/3 时，条件期望为 60，与平均值相差 10，具有这个平方距离，并且加权 2/3，即概率为 60。因此，您计算这些数字，然后得到方差的值等于 200。好的，现在我们要使用涉及条件​​方差的更复杂的公式。</p><p>And we get this squared distance. With probability 2/3 the conditional expectation is 60, which is 10 away
            from
            the mean, has this squared distance and gets weighed by 2/3, which is the probability of 60. So you do the
            numbers, and you get the value for the variance equal to 200. All right, so now we want to move towards
            using
            that more complicated formula involving the conditional variances.</p>
        <p>好吧，假设有人去计算每个部分内测验分数的方差。有人给了我们这两条信息。在第一部分中，我们取该部分与平均值的差值，假设第二部分中的差值等于 10。所以这些就是各个部分内测验分数的方差。</p><p>OK, suppose someone goes and calculates the variance of the quiz scores inside each one of the sections. So
            someone gives us these two pieces of information. In section one we take the differences from the mean in
            that
            section, and let’s say that the various turns out to be a number equal to 10 similarly in the second
            section. So
            these are the variances of the quiz scores inside individual sections.</p>
        <p>一个条件宇宙中的方差，另一个条件宇宙中的方差。所以，如果我在第一部分中挑选一名学生，而我没有告诉你有关该学生的更多信息，那么该学生的随机分数的方差是多少？方差是 10。我知道原因，但我不认识这个学生。所以分数在那个宇宙中仍然是一个随机变量。它有一个方差，这就是方差。</p><p>The variance in one conditional universe, the variance in the other conditional universe. So if I pick a
            student
            in section one and I don’t tell you anything more about the student, what’s the variance of the random score
            of
            that student? The variance is 10. I know why, but I don’t know the student. So the score is still a random
            variable in that universe. It has a variance, and that’s the variance.</p>
        <p>同样，在另一个宇宙中，测验分数的方差是这个数字，20。再一次，这是数字之间的等式。我已经固定了 y 的具体值。所以我把自己放在一个特定的宇宙中，我可以计算出那个特定宇宙中的方差。如果我没有为大写 Y 指定数值，并且说我不知道​​ Y 会是什么，那么它就是随机的。</p><p>Similarly, in the other universe, the variance of the quiz scores is this number, 20. Once more, this is an
            equality between numbers. I have fixed the specific value of y. So I put myself in a specific universe, I
            can
            calculate the variance in that specific universe. If I don’t specify a numerical value for capital Y, and
            say I
            don’t know what Y is going to be, it’s going to be random.</p>
        <p>那么我将要获得的部分方差类型本身将是随机的。以 1/3 的概率，我在第一部分中挑选一名学生，在这种情况下，根据我所挑选的学生，条件方差将是 10。或者以 2/3 的概率，我挑选 y 等于 2，然后我将自己置于该宇宙中。</p><p>Then what kind of section variance I’m going to get itself will be random. With probability 1/3, I pick a
            student
            in the first section in which case the conditional variance given what I have picked is going to be 10. Or
            with
            probability 2/3 I pick y equal to 2, and I place myself in that universe.</p>
        <p>在那个宇宙中，条件方差是 20。所以你从这里再次看到，条件方差是一个随机变量，它以一定的概率取不同的值。它取哪个值取决于随机变量 Y 的实现。所以如果 Y 是 1，就会发生这种情况；如果 Y 等于 2，就会发生这种情况。一旦你有这种形式的东西。一个以一定概率取值的随机变量。</p><p>And in that universe the conditional variance is 20. So you see again from here that the conditional variance
            is
            a random variable that takes different values with certain probabilities. And which value it takes depends
            on
            the realization of the random variable capital Y. So this happens if capital Y is one, this happens if
            capital Y
            is equal to 2. Once you have something of this form. a random variable that takes values with certain
            probabilities.</p>
        <p>然后你当然可以计算出该随机变量的期望值。不要被这个事实吓到，这个随机变量是由八个或七个符号而不是一个字母组成的字符串描述的。把这整个符号串想象成一个随机变量。例如，你可以称它为 z，使用一个字母。所以 z 是一个随机变量，它取这两个值以及相应的概率。</p><p>Then you can certainly calculate the expected value of that random variable. Don’t get intimidated by the
            fact
            that this random variable, it’s something that’s described by a string of eight symbols, or seven, instead
            of
            just a single letter. Think of this whole string of symbols there as just being a random variable. You could
            call it z for example, use one letter. So z is a random variable that takes these two values with these
            corresponding probabilities.</p>
        <p>因此，我们可以讨论 Z 的期望值，它将是 1/3 乘以 10，2/3 乘以 20，然后我们从这里得到一个特定的数字。现在我们有了计算 x 总体方差的所有要素。上一张幻灯片中的公式告诉我们这一点。我们掌握了所有要素吗？方差的期望值，我们刚刚计算出来了。</p><p>So we can talk about the expected value of Z, which is going to be 1/3 times 10,2/3 times 20, and we get a
            certain number from here. And now we have all the pieces to calculate the overall variance of x. The formula
            from the previous slide tells us this. Do we have all the pieces? The expected value of the variance, we
            just
            calculated it.</p>
        <p>期望值的方差，这是上一张幻灯片中的最后一次计算。我们确实得到了一个数字，它是 200。将这两个数字相加，就得到了总方差。现在，这个练习的有用部分是尝试解释这两个数字，看看它们意味着什么。对于特定的 y，给定 y 的 x 的方差是第一部分内的方差。这是第二部分内的方差。</p><p>The variance of the expected value, this was the last calculation in the previous slide. We did get a number
            for
            it, it was 200. You add the two, you find the total variance. Now the useful piece of this exercise is to
            try to
            interpret these two numbers, and see what they mean. The variance of x given y for a specific y is the
            variance
            inside section one. This is the variance inside section two.</p>
        <p>期望值是各个部分内方差的某种平均值。因此，这个术语告诉我们这门课程的变异性，即它们在各个部分内的分布范围有多广。所以我们有三个部分，而这门课程恰好是。好吧，假设这些部分真的很不一样。这里有本科生，这里有博士后学生。这些是测验分数，即第一部分、第二部分、第三部分。</p><p>The expected value is some kind of average of the variances inside individual sections. So this term tells us
            something about the variability of this course, how widely spread they are within individual sections. So we
            have three sections, and this course happens to be. OK, let’s say the sections are really different. So here
            you
            have undergraduates and here you have post doctoral students. And these are the quiz scores, that’s section
            one,
            section two, section three.</p>
        <p>这是第一部分的平均值。方差与传播有关。第二部分的方差与传播有关，第三部分的方差也与传播有关。条件方差的期望值是我们从各个部分得到的三个方差的加权平均值。因此，部分内的变异性肯定会对本课程的整体变异性产生一定影响。
        </p><p>Here’s the mean of the first section. And the variance has something to do with the spread. The variance in
            the
            second section has something to do with the spread, similarly with the third spread. And the expected value
            of
            the conditional variances is some weighted average of the three variances that we get from individual
            sections.
            So variability within sections definitely contributes something to the overall variability of this course.
        </p>
        <p>但如果你问我整个班级的差异，那么还有第二个影响。这与不同部分之间差异很大有关。这些课程的分数相差甚远。这个术语就是完成这项工作的术语。它查看每个部分内的预期值，这些预期值是这个、这个和那个。</p><p>But if you ask me about the variability over the entire class there’s a second effect. That has to do with
            the
            fact that different sections are very different from each other. That these courses here are far away from
            those
            scores. And this term is the one that does the job. This one looks at the expected values inside each
            section,
            and these expected values are this, this, and that.</p>
        <p>并提出一个问题，它们分布有多广泛？它问的是各个部分内的平均值彼此之间有多大差异？在这张图片中，这个数字会很大，因为不同部分的平均值差别很大。所以这个公式告诉我们，测验分数的整体变异性由两个可以量化和添加的因素组成。一个因素是各个部分内的变异性有多大？</p><p>And asks a question how widely spread are they? It asks how different from each other are the means inside
            individual sections? And in this picture it would be a large number because the difference section means are
            quite different. So the story that this formula is telling us is that the overall variability of the quiz
            scores
            consists of two factors that can be quantified and added. One factor is how much variability is there inside
            individual sections?</p>
        <p>另一个因素是各个部分之间的差异有多大？这两种影响都会影响本课程的整体变化。我们继续看一个数值示例。为了掌握这些计算方法，并应用此公式对随机变量的方差进行分治计算。为了多样化，我们现在将采用连续随机变量。</p><p>And the other factor is how different are the sections from each other? Both effects contribute to the
            overall
            variability of this course. Let’s continue with just one more numerical example. Just to get the hang of
            doing
            these kinds of calculations, and apply this formula to do a divide and conquer calculation of the variance
            of a
            random variable. Just for variety now we’re going to take a continuous random variable.</p>
        <p>有人给你一份这种形式的 PDF，然后他们要求你计算方差。你说这太复杂了，我不想做积分。我能分而治之吗？你说好的，让我做下面的技巧。让我定义一个随机变量 y。如果 x 落在这里，则取值为 1，如果 x 落入第二个区间，则取值为 2。</p><p>Somebody gives you a PDF if this form, and they ask you for the variance. And you say oh that’s too
            complicated,
            I don’t want to do integrals. Can I divide and conquer? And you say OK, let me do the following trick. Let
            me
            define a random variable, y. Which takes the value 1 if x falls in here, and takes the value 2 if x falls in
            the
            second interval.</p>
        <p>让我尝试在条件世界中工作，这样事情可能会更容易，然后将所有东西加起来得到整体方差。所以我用这个特定的方式定义了 y。在这个例子中，y 成为 x 的函数。y 完全由 x 决定。我将通过尝试计算这里涉及的所有项来计算整体方差。让我们开始计算。</p><p>And let me try to work in the conditional world where things might be easier, and then add things up to get
            the
            overall variance. So I have defined y this particular way. In this example y becomes a function of x. y is
            completely determined by x. And I’m going to calculate the overall variance by trying to calculate all of
            the
            terms that are involved here. So let’s start calculating.</p>
        <p>第一个观察结果是，这个事件发生的概率为 1/3，而这个事件发生的概率为 2/3。假设我们处于这个宇宙中，x 的期望值为 1/2，因为我们有一个从 0 到 1 的均匀分布。这里我们有一个从 1 到 2 的均匀分布，因此该宇宙中 x 的条件期望值为 3/2。条件方差如何？</p><p>First observation is that this event has probability 1/3, and this event has probability 2/3. The expected
            value
            of x given that we are in this universe is 1/2, because we have a uniform distribution from 0 to 1. Here we
            have
            a uniform distribution from 1 to 2, so the conditional expectation of x in that universe is 3/2. How about
            conditional variances?</p>
        <p>在 y 等于 1 的世界中，x 在单位间隔内具有均匀分布。x 的方差是多少？现在你可能已经看过那个公式了，它是 1/12。1/12 是单位间隔内均匀分布的方差。当 y 等于 2 时，方差再次是 1/12。因为在这种情况下，x 在单位长度的间隔内再次具有均匀分布。</p><p>In the world who are y is equal to 1 x has a uniform distribution on a unit interval. What’s the variance of
            x?
            By now you’ve probably seen that formula, it’s 1 over 12.1 over 12 is the variance of a uniform distribution
            over a unit interval. When y is equal to 2 the variance is again 1 over 12. Because in this instance again x
            has
            a uniform distribution over an interval of unit length.</p>
        <p>x 的总体期望值是多少？找到总体期望值的方法是考虑条件期望的不同数值。并根据它们的概率对它们进行加权。因此，在概率 1/3 的情况下，条件期望是 1/2。在概率 2/3 的情况下，条件期望是 3/2。结果是 7/6。这是我们需要做的前期工作，现在让我们在这里计算一些东西。</p><p>What’s the overall expected value of x? The way you find the overall expected value is to consider the
            different
            numerical values of the conditional expectation. And weigh them according to their probabilities. So with
            probability 1/3 the conditional expectation is 1/2. And with probability 2/3 the conditional expectation is
            3
            over 2. And this turns out to be 7 over 6. So this is the advance work we need to do, now let’s calculate a
            few
            things here.</p>
        <p>给定 y，x 的期望值的方差是多少？给定 y，x 的期望值是一个随机变量，它以这些概率取这两个值。因此，为了找到方差，我们考虑期望值取 1/2 减去条件期望平均值的数值的概率。条件期望的平均值是多少？它是无条件期望。所以它是 7/6。我们刚刚做了这个计算。</p><p>What’s the variance of the expected value of x given y? Expected value of x given y is a random variable that
            takes these two values with these probabilities. So to find the variance we consider the probability that
            the
            expected value takes the numerical value of 1/2 minus the mean of the conditional expectation. What’s the
            mean
            of the conditional expectation? It’s the unconditional expectation. So it’s 7 over 6. We just did that
            calculation.</p>
        <p>所以我把这个数字放在这里，7/6 的平方。然后还有第二个项，概率为 2/3，条件期望取 3/2 的值，这与平均值相差很大，我们得到了这个贡献。这样我们就计算出了条件期望的方差，这就是这个项。这是什么？有人猜这个数字是什么吗？它是 1/12，为什么？</p><p>So I’m putting here that number, 7 over 6 squared. And then there’s a second term with probability 2/3, the
            conditional expectation takes this value of 3 over 2, which is so much away from the mean, and we get this
            contribution. So this way we have calculated the variance of the conditional expectation, this is this term.
            What is this? Any guesses what this number is? It’s 1 over 12, why?</p>
        <p>在本例中，条件方差无论如何都是 1/12。因此，条件方差是一个确定性随机变量，取一个常数值。因此，这个随机变量的期望值就是 1/12。所以我们得到了我们需要的两个部分，所以我们得到了随机变量 x 的整体方差。</p><p>The conditional variance just happened in this example to be 1 over 12 no matter what. So the conditional
            variance is a deterministic random variable that takes a constant value. So the expected value of this
            random
            variable is just 1 over 12. So we got the two pieces that we need, and so we do have the overall variance of
            the
            random variable x.</p>
        <p>所以这只是一个学术示例，目的是了解如何操纵各种数量。现在让我们利用我们学到的知识和我们拥有的工具来做一些更有趣的事情。好吧，现在你们都爱上了概率。所以周末你们会去书店买概率书。
        </p><p>So this was just an academic example in order to get the hang of how to manipulate various quantities. Now
            let’s
            use what we have learned and the tools that we have to do something a little more interesting. OK, so by now
            you’re all in love with probabilities. So over the weekend you’re going to bookstores to buy probability
            books.
        </p>
        <p>所以你要去随机数量的书店，在每家书店你都会花随机数量的钱。所以让 n 成为你要去的商店数量。所以 n 是一个整数。非负随机变量。也许你知道这个随机变量的分布。</p><p>So you’re going to visit a random number bookstores, and at each one of the bookstores you’re going to spend
            a
            random amount of money. So let n be the number of stores that you are visiting. So n is an integer. non
            negative
            random variable. and perhaps you know the distribution of that random variable.</p>
        <p>每次走进一家商店，你的头脑都会忘记之前做过的事情，你只会购买一定数量的书，这与你当天早些时候买了多少本书无关。这与你逛了多少家商店无关，等等。</p><p>Each time that you walk into a store your mind is clear from whatever you did before, and you just buy a
            random
            number of books that has nothing to do with how many books you bought earlier on the day. It has nothing to
            do
            with how many stores you are visiting, and so on.</p>
        <p>因此，每次你以新人身份进入，购买随机数量的书籍，并花费随机数量的金钱。所以，更准确地说，我做了以下假设。对于每个商店 i。如果你最终访问了第 i 家商店。你花费的金额是一个具有特定分布的随机变量。</p><p>So each time you enter as a brand new person, and buy a random number of books, and spend a random amount of
            money. So what I’m saying, more precisely, is that I’m making the following assumptions. That for each store
            i.
            if you end up visiting the i th store. the amount of money that you spend is a random variable that has a
            certain distribution.</p>
        <p>该分布对于每家商店都是相同的，而且各个商店的 xi 彼此独立。此外，xi 都与 n 无关。因此，我进店后在商店消费多少钱与我逛了多少家商店无关。因此，这就是我们要研究的设置。y 是您实际消费的总金额。</p><p>That distribution is the same for each store, and the xi’s from store to store are independent from each
            other.
            And furthermore, the xi’s are all independent of n.&nbsp;So how much I’m spending at the store. once I get in.
            has
            nothing to do with how many stores I’m visiting. So this is the setting that we’re going to look at. y is
            the
            total amount of money that you did spend.</p>
        <p>它是你在商店消费的总和，但指数上升到大写 N。这里有什么不同？我们处理的是独立随机变量的总和，只不过我们有多少个随机变量不是提前知道的，而是随机选择的。所以它是随机变量的随机数的总和。</p><p>It’s the sum of how much you spent in the stores, but the index goes up to capital N. And what’s the twist
            here?
            It’s that we’re dealing with the sum of independent random variables except that how many random variables
            we
            have is not given to us ahead of time, but it is chosen at random. So it’s a sum of a random number of
            random
            variables.</p>
        <p>我们想计算一些与 y 有关的量，特别是 y 的期望值或 y 的方差。我们该怎么做呢？好的，我们对期望的线性有所了解。总和的期望就是期望的总和。但我们只在它是固定数量的随机变量的总和的情况下使用了该规则。</p><p>We would like to calculate some quantities that have to do with y, in particular the expected value of y, or
            the
            variance of y. How do we go about it? OK, we know something about the linearity of expectations. That
            expectation of a sum is the sum of the expectations. But we have used that rule only in the case where it’s
            the
            sum of a fixed number of random variables.</p>
        <p>因此，x 加 y 加 z 的期望值等于 x 的期望值加上 y 的期望值加上 z 的期望值。我们知道，对于固定数量的随机变量，这是正确的。我们不知道，也不知道对于随机数，它会如何运作。好吧，如果我们对固定随机变量的情况有所了解，那么让我们将自己带到一个条件宇宙中，其中我们要求和的随机变量的数量是固定的。</p><p>So expected value of x plus y plus z is expectation of x, plus expectation of y, plus expectation of z. We
            know
            this for a fixed number of random variables. We don’t know it, or how it would work for the case of a random
            number. Well, if we know something about the case for fixed random variables let’s transport ourselves to a
            conditional universe where the number of random variables we’re summing is fixed.</p>
        <p>因此，让我们尝试通过对要访问的书店数量的不同可能值进行条件化，来打破分而治之的问题。因此，让我们在条件宇宙中工作，找到这个宇宙中的条件期望，然后使用我们的迭代期望定律来查看更普遍的情况。
        </p><p>So let’s try to break the problem divide and conquer by conditioning on the different possible values of the
            number of bookstores that we’re visiting. So let’s work in the conditional universe, find the conditional
            expectation in this universe, and then use our law of iterated expectations to see what happens more
            generally.
        </p>
        <p>如果我告诉你我恰好去了 n 家小商店，其中 n 是一个数字，比如说 10。那么你花费的金额就是 x1 加 x2 一直到 x10，因为我们去了 10 家商店。</p><p>If I told you that I visited exactly little n stores, where little n now is a number, let’s say 10. Then the
            amount of money you’re spending is x1 plus x2 all the way up to x10 given that we visited 10 stores.</p>
        <p>所以我在这里所做的就是用小写 n 替换大写 N，我之所以能做到这一点，是因为我现在处于条件宇宙中，我知道大写 N 是小写 n。现在小写 n 是固定的。我们假设 n 与 xi 无关。因此，在这个固定 n 的宇宙中，这里的信息并没有告诉我有关 x 的值的任何新信息。</p><p>So what I have done here is that I’ve replaced the capital N with little n, and I can do this because I’m now
            in
            the conditional universe where I know that capital N is little n.&nbsp;Now little n is fixed. We have assumed
            that n
            is independent from the xi’s. So in this universe of a fixed n this information here doesn’t tell me
            anything
            new about the values of the x’s.</p>
        <p>如果你正在对独立于你感兴趣的随机变量的随机变量进行条件化，那么条件化就不起作用，因此可以将其丢弃。因此，在这个条件宇宙中，如果你恰好访问 10 家商店，你花费的预期金额就是在 10 家商店花费金额的预期，也就是每家商店预期金额的总和。</p><p>If you’re conditioning random variables that are independent from the random variables you are interested in,
            the
            conditioning has no effect, and so it can be dropped. So in this conditional universe where you visit
            exactly 10
            stores the expected amount of money you’re spending is the expectation of the amount of money spent in 10
            stores, which is the sum of the expected amount of money in each store.</p>
        <p>由于随机变量具有相同的分布，因此每个数字都是相同的。因此，它是您在典型商店花费的金钱预期值的 n 倍。这几乎是显而易见的，无需正式进行。如果我告诉您，您要访问 10 家商店，那么您预计花费的金额是您预计在每家商店单独花费的金额的 10 倍。</p><p>Each one of these is the same number, because the random variables have identical distributions. So it’s n
            times
            the expected value of money you spent in a typical store. This is almost obvious without doing it formally.
            If
            I’m telling you that you’re visiting 10 stores, what you expect to spend is 10 times the amount you expect
            to
            spend in each store individually.</p>
        <p>现在让我们把这个等式用抽象符号重写成随机变量。这是数字之间的等式。假设你访问了 10 家商店，y 的期望值是这个特定数字的 10 倍。让我们把它转换成随机变量。在随机变量符号中，给定商店数量，你花费的金钱的预期值。但不告诉你一个具体的数字。</p><p>Now let’s take this equality here and rewrite it in our abstract notation, in terms of random variables. This
            is
            an equality between numbers. Expected value of y given that you visit 10 stores is 10 times this particular
            number. Let’s translate it into random variables. In random variable notation, the expected value of money
            you’re spending given the number of stores. but without telling you a specific number.</p>
        <p>是商店数量乘以 x 的期望值。因此，这是一个随机变量，当大写 N 等于小写 n 时，它会将其作为数值。这是一个随机变量，根据定义，当大写 N 等于小写 n 时，它会取这个数值。&nbsp;</p><p>Is whatever that number of stores turns out to be times the expected value of x. So this is a random variable
            that takes this as a numerical value whenever capital N happens to be equal to little n.&nbsp;This is a random
            variable, which by definition takes this numerical value whenever capital N is equal to little n.&nbsp;</p>
        <p>因此，无论大写 N 是哪个特定值，小写 n，它都会认为这个等于那个。因此，这个随机变量的值将等于那个随机变量。因此，作为随机变量，这两个随机变量彼此相等。现在我们使用迭代期望定律。
        </p><p>So no matter what capital N happens to be what specific value, little n, it takes this is equal to that.
            Therefore the value of this random variable is going to be equal to that random variable. So as random
            variables, these two random variables are equal to each other. And now we use the law of iterated
            expectations.
        </p>
        <p>迭代期望定律告诉我们，y 的总体期望值是条件期望的期望值。我们有一个条件期望的公式。它是 x 期望值的 n 倍。现在 x 的期望值是一个数字。某个随机变量的期望值乘以一个数字等于该随机变量的期望值乘以该数字本身。我们可以在期望之外取一个数字。</p><p>The law of iterated expectations tells us that the overall expected value of y is the expected value of the
            conditional expectation. We have a formula for the conditional expectation. It’s n times expected value of
            x.
            Now the expected value of x is a number. Expected value of something random times a number is expected value
            of
            the random variable times the number itself. We can take a number outside the expectation.</p>
        <p>因此，x 的期望值被提取出来。这就是结论，总体而言，您预期要花费的金额等于您预期平均要访问的商店数量，以及您预期平均在每家商店花费的金额。您可能已经猜到了这就是答案。</p><p>So expected value of x gets pulled out. And that’s the conclusion, that overall the expected amount of money
            you’re going to spend is equal to how many stores you expect to visit on the average, and how much money you
            expect to spend on each one on the average. You might have guessed that this is the answer.</p>
        <p>如果你预计要逛 10 家商店，并且预计在每家商店花费 100 美元，那么是的，你今天预计会花费 1,000 美元。如果你告诉你的哈佛朋友这个故事，他们不会对你留下深刻印象。这是推理平均而言确实能给你合理答案的情况之一。</p><p>If you expect to visit 10 stores, and you expect to spend $100 on each store, then yes, you expect to spend
            $1,000 today. You’re not going to impress your Harvard friends if you tell them that story. It’s one of the
            cases where reasoning, on the average, does give you the plausible answer.</p>
        <p>但如果你告诉你的哈佛朋友，我可以计算出我可以花多少钱的方差，他们一定会对你刮目相看。我们将应用这个公式，困难在于理清所有这些术语，以及它们的含义。所以让我们从这个术语开始。</p><p>But you will be able to impress your Harvard friends if you tell them that I can actually calculate the
            variance
            of how much I can spend. And we’re going to work by applying this formula that we have, and the difficulty
            is
            basically sorting out all those terms here, and what they mean. So let’s start with this term.</p>
        <p>因此，假设您访问了 n 家商店，则 y 的预期值是 x 的预期值的 n 倍。这就是我们在上一张幻灯片中所做的。所以这是一个随机变量，它有一个方差。方差是什么？是 x 的预期值的 n 倍的方差。记住 x 的预期值是一个数字。所以我们处理的是数字的 n 倍的方差。</p><p>So the expected value of y given that you’re visiting n stores is n times the expected value of x. That’s
            what we
            did in the previous slide. So this thing is a random variable, it has a variance. What is the variance? Is
            the
            variance of n times the expected value of x. Remember expected value of x is a number. So we’re dealing with
            the
            variance of n times a number.</p>
        <p>当你将一个随机变量乘以一个常数时会发生什么？方差变为前一个方差乘以常数的平方。因此，这个方差是 n 乘以我们这里得到的常数的平方的方差。因此，这告诉我们给定 n 时 y 的预期值的方差。&nbsp;</p><p>What happens when you multiply a random variable by a constant? The variance becomes the previous variance
            times
            the constant squared. So the variance of this is the variance of n times the square of that constant that we
            had
            here. So this tells us the variance of the expected value of y given n.&nbsp;</p>
        <p>这是你花费金额的可变性的一部分，它归因于你访问的商店数量的随机性或可变性。因此，这两个术语的解释是，你花费的金额是随机的，这归因于商店数量的随机性以及单个商店内部的随机性。好吧，在我告诉你你访问了多少家商店之后。</p><p>This is the part of the variability of how much money you’re spending, which is attributed to the randomness,
            or
            the variability, in the number of stores that you are visiting. So the interpretation of the two terms is
            there’s randomness in how much you’re going to spend, and this is attributed to the randomness in the number
            of
            stores together with the randomness inside individual stores. Well, after I tell you how many stores you’re
            visiting.</p>
        <p>现在让我们来处理这个术语。单个商店内的方差。让我们慢慢来。如果我告诉你，你正好访问了 n 家小商店，那么 y 就是你在这 n 家小商店里花了多少钱。你正在处理 n 个小随机变量的总和。n 个小随机变量的总和的方差是多少？它是它们的方差的总和。</p><p>So now let’s deal with this term. the variance inside individual stores. Let’s take it slow. If I tell you
            that
            you’re visiting exactly little n stores, then y is how much money you spent in those little n stores. You’re
            dealing with the sum of little n random variables. What is the variance of the sum of little n random
            variables?
            It’s the sum of their variances.</p>
        <p>因此，每家商店都会贡献 x 的方差，而您要添加超过 n 家商店。如果我告诉您商店数量，这就是花费的方差。现在让我们将其转换为随机变量符号。这是一个随机变量，只要大写 N 等于小写 n，它就会取此数值。这是一个随机变量，只要大写 N 等于小写 n，它就会取此数值。这等于那个。</p><p>So each store contributes a variance of x, and you’re adding over little n stores. That’s the variance of
            money
            spent if I tell you the number of stores. Now let’s translate this into random variable notation. This is a
            random variable that takes this numerical value whenever capital N is equal to little n.&nbsp;This is a random
            variable that takes this numerical value whenever capital N is equal to little n.&nbsp;This is equal to that.</p>
        <p>因此，无论 y 发生什么，这两个值始终相等。因此，我们在随机变量之间有一个相等性。现在我们取两者的期望值。方差的期望值就是这个的期望值。好吧，在这里考虑方差的期望值可能看起来令人困惑，但 x 的方差是一个数字，而不是一个随机变量。你可以把它看作一个常数。</p><p>Therefore, these two are always equal, no matter what happens to y. So we have an equality here between
            random
            variables. Now we take expectations of both. Expected value of the variance is expected value of this. OK it
            may
            look confusing to think of the expected value of the variance here, but the variance of x is a number, not a
            random variable. You think of it as a constant.</p>
        <p>因此，它的期望值乘以常数的 n 倍，等于常数本身的 n 倍的期望值。现在我们也得到了第二项，现在我们将所有项放在一起，加上那个，得到 y 的总体方差表达式。正如我之前所说，y 的总体变异性与你在典型商店内消费金额的变异性有关。以及你访问的商店数量的变异性。好的，今天就讲到这里。下次我们将彻底改变话题。</p><p>So its expected value of n times a constant gives us the expected value of n times the constant itself. So
            now we
            got the second term as well, and now we put everything together, this plus that to get an expression for the
            overall variance of y. Which again, as I said before, the overall variability in y has to do with the
            variability of how much you spent inside the typical store. And the variability in the number of stores that
            you
            are visiting. OK, so this is it for today. We’ll change subjects quite radically from next time.</p>
        <h1 id="bernoulli-process">13.伯努利过程</h1><h1>13. Bernoulli Process</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 50:58 <a href="https://youtube.com/watch?v=gMTiAeE0NCw">https://youtube.com/watch?v=gMTiAeE0NCw</a></p><p> 12 years ago (Nov 10, 2012) — 50:58 <a href="https://youtube.com/watch?v=gMTiAeE0NCw">https://youtube.com/watch?v=gMTiAeE0NCw</a></p>
        <h2 id="intro-2">简介</h2><h2>Intro</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：现在您已经了解了基本概率论中几乎所有可能的技巧，包括如何计算分布等等。您拥有做几乎任何事情的基本工具。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So by now you have seen
            pretty
            much every possible trick there is in basic probability theory, about how to calculate distributions, and so
            on.
            You have the basic tools to do pretty much anything.</p>
        <p>那么接下来会发生什么呢？嗯，概率对于发展推理科学很有用，这是我们学期末要回顾的一个主题。</p><p>So what’s coming after this? Well, probability is useful for developing the science of inference, and this is
            a
            subject to which we’re going to come back at the end of the semester.</p>
        <h2 id="random-processes">随机过程</h2><h2>Random Processes</h2>
        <p>另一章，也就是我们接下来几周要讲的内容，是处理随时间演变的现象。也就是所谓的随机过程。那么这是什么呢？</p><p>Another chapter, which is what we will be doing over the next few weeks, is to deal with phenomena that
            evolve in
            time. So so called random processes or stochastic processes. So what is this about?</p>
        <p>因此在现实世界中，你不会只是抛出两个随机变量然后回家。相反，世界会继续发展。因此，你生成随机变量，然后得到更多随机变量，事物会随着时间而发展。随机过程应该是捕捉随机现象随时间演变的模型。这就是我们要做的。现在，当我们有时间演变时，从数学上讲，你可以使用离散时间或连续时间。</p><p>So in the real world, you don’t just throw two random variables and go home. Rather the world goes on. So you
            generate the random variable, then you get more random variables, and things evolve in time. And random
            processes are supposed to be models that capture the evolution of random phenomena over time. So that’s what
            we
            will be doing. Now when we have evolution in time, mathematically speaking, you can use discrete time or
            continuous time.</p>
        <p>当然，离散时间更简单。这就是我们要开始的地方。我们将从最容易、最简单的随机过程开始，即所谓的伯努利过程，它只不过是一系列的抛硬币过程。你不断地抛硬币，永远不停地抛下去。这就是伯努利过程。所以从某种意义上说，它是你已经见过的东西。</p><p>Of course, discrete time is easier. And that’s where we’re going to start from. And we’re going to start from
            the
            easiest, simplest random process, which is the so called Bernoulli process, which is nothing but just a
            sequence
            of coin flips. You keep flipping a coin and keep going forever. That’s what the Bernoulli process is. So in
            some
            sense it’s something that you have already seen.</p>
        <p>但是，我们在这里要介绍一些额外的概念，这些概念在我们继续讨论连续时间过程时会非常有用和相关。因此，我们将定义伯努利过程，讨论该过程的一些基本属性，推导出一些公式，并利用其特殊结构来做一些非常有趣的事情。顺便问一下，伯努利这个词是怎么来的？</p><p>But we’re going to introduce a few additional ideas here that will be useful and relevant as we go along and
            we
            move on to continuous time processes. So we’re going to define the Bernoulli process, talk about some basic
            properties that the process has, and derive a few formulas, and exploit the special structure that it has to
            do
            a few quite interesting things. By the way, where does the word Bernoulli come from?</p>
        <p>伯努利家族是 18 世纪左右的一个数学家家族，是瑞士的数学家和科学家。伯努利家族人数众多，有些人甚至有相同的名字。历史学家甚至很难弄清楚他们到底做了什么。但无论如何，你可以想象，在餐桌上，他们可能正在抛硬币和做伯努利试验。所以也许那是他们的消遣。</p><p>Well the Bernoulli’s were a family of mathematicians, Swiss mathematicians and scientists around the 1700s.
            There
            were so many of them that actually. and some of them had the same first name. historians even have
            difficulty of
            figuring out who exactly did what. But in any case, you can imagine that at the dinner table they were
            probably
            flipping coins and doing Bernoulli trials. So maybe that was their pass time.</p>
        <h2 id="bernoulli-process-1">伯努利过程</h2><h2>Bernoulli Process</h2>
        <p>好的。那么伯努利过程是什么？</p><p>OK. So what is the Bernoulli process?</p>
        <p>伯努利过程不过是一系列独立的伯努利试验，你可以将其视为抛硬币。因此，你可以认为每次试验的结果都是正面或反面。也许谈论成功和失败比谈论正面或反面更方便一些。或者，如果你希望使用数值，则使用 1 表示成功，0 表示失败。</p><p>The Bernoulli process is nothing but a sequence of independent Bernoulli trials that you can think of as coin
            flips. So you can think the result of each trial being heads or tails. It’s a little more convenient maybe
            to
            talk about successes and failures instead of heads or tails. Or if you wish numerical values, to use a 1 for
            a
            success and 0 for a failure.</p>
        <p>因此，模型认为，这些试验中的每一次都具有相同的成功概率 p。另一个假设是，这些试验在统计上是相互独立的。那么，伯努利试验的例子有哪些呢？你每周买一张彩票，你要么赢，要么输。据推测，这些试验是相互独立的。如果是同一种彩票，那么每周中奖的概率应该相同。
        </p><p>So the model is that each one of these trials has the same probability of success, p.&nbsp;And the other
            assumption is
            that these trials are statistically independent of each other. So what could be some examples of Bernoulli
            trials? You buy a lottery ticket every week and you win or lose. Presumably, these are independent of each
            other. And if it’s the same kind of lottery, the probability of winning should be the same during every
            week.
        </p>
        <p>也许你想模拟金融市场。一个粗略的模型可能是，在任何一天，道琼斯指数都会以一定的概率上涨或下跌。这个概率肯定在 0.5 左右。这是金融市场的粗略模型。你可能会说，可能还有更多内容。生活没有那么简单。但实际上这是一个相当合理的模型。</p><p>Maybe you want to model the financial markets. And a crude model could be that on any given day the Dow Jones
            is
            going to go up or down with a certain probability. Well that probability must be somewhere around 0.5, or
            so.
            This is a crude model of financial markets. You say, probably there is more into them. Life is not that
            simple.
            But actually it’s a pretty reasonable model.</p>
        <p>要建立更复杂的模型来做出比单纯的正面和反面更好的预测需要做大量工作。现在更有趣的是，也许是我们将在本课中处理的例子。伯努利过程是用于设施中任何类型的到达流的良好模型。所以它可能是一家银行，而你正坐在银行门口。</p><p>It takes quite a bit of work to come up with more sophisticated models that can do better predictions than
            just
            pure heads and tails. Now more interesting, perhaps to the examples we will be dealing with in this class. a
            Bernoulli process is a good model for streams of arrivals of any kind to a facility. So it could be a bank,
            and
            you are sitting at the door of the bank.</p>
        <p>每一秒，你都会检查这一秒内是否有顾客进来。或者你可以想想服务器上的任务到达情况。或者任何其他类型的请求到达服务系统的情况。因此，请求或任务会在随机时间到达。你将时间分成多个时间段。每个时间段内，有事发生或没有事发生。</p><p>And at every second, you check whether a customer came in during that second or not. Or you can think about
            arrivals of jobs to a server. Or any other kind of requests to a service system. So requests, or jobs,
            arrive at
            random times. You split the time into time slots. And during each time slot something comes or something
            does
            not come.</p>
        <p>对于许多应用来说，合理的假设是，任何给定时隙的到达都独立于其他时隙的到达。因此，每个时隙都可以看作是一次试验，其中要么有事发生，要么没有事发生。不同的试验彼此独立。现在我们在这里做出两个假设。一个是独立性假设。另一个是这个数字 p，即成功概率，是常数。
        </p><p>And for many applications, it’s a reasonable assumption to make that arrivals on any given slot are
            independent
            of arrivals in any other time slot. So each time slot can be viewed as a trial, where either something comes
            or
            doesn’t come. And different trials are independent of each other. Now there’s two assumptions that we’re
            making
            here. One is the independence assumption. The other is that this number, p, probability of success, is
            constant.
        </p>
        <p>现在，如果您考虑一下银行的例子，如果您早上 9:30 站在银行外面，您会看到以一定频率到达的客人。如果您中午 12:00 站在银行外面，到达的客人可能更频繁。这意味着给定的时间段在中午左右到达的概率更高。
        </p><p>Now if you think about the bank example, if you stand outside the bank at 9:30 in the morning, you’ll see
            arrivals happening at a certain rate. If you stand outside the bank at 12:00 noon, probably arrivals are
            more
            frequent. Which means that the given time slot has a higher probability of seeing an arrival around noon
            time.
        </p>
        <p>这意味着，如果谈论的是全天，那么假设 p 为常数可能在该设置下不正确。因此，上午成功或到达的概率将小于中午。但是，如果谈论的是某个时间段，比如 10:00 到 10:15，那么所有时段到达的概率可能都相同，这是一个很好的近似值。</p><p>This means that the assumption of a constant p is probably not correct in that setting, if you’re talking
            about
            the whole day. So the probability of successes or arrivals in the morning is going to be smaller than what
            it
            would be at noon. But if you’re talking about a time period, let’s say 10:00 to 10:15, probably all slots
            have
            the same probability of seeing an arrival and it’s a good approximation.</p>
        <p>因此，我们坚持假设 p 是常数，不会随时间而变化。既然我们有了模型，我们该如何处理它呢？好吧，我们开始讨论它的统计特性。这里有两种略有不同的观点来思考什么是随机过程。最简单的版本是将随机过程视为随机变量的序列。我们知道什么是随机变量。</p><p>So we’re going to stick with the assumption that p is constant, doesn’t change with time. Now that we have
            our
            model what do we do with it? Well, we start talking about the statistical properties that it has. And here
            there’s two slightly different perspectives of thinking about what a random process is. The simplest version
            is
            to think about the random process as being just a sequence of random variables. We know what random
            variables
            are.</p>
        <p>我们知道什么是多重随机变量。所以这只是一个与一堆随机变量相关的实验。那么一旦你有了随机变量，你会本能地做什么？你谈论这些随机变量的分布。我们已经为伯努利过程指定了每个 Xi 都是伯努利随机变量，成功概率等于 p。这指定了随机变量 X 或 Xt 在一般时间 t 的分布。</p><p>We know what multiple random variables are. So it’s just an experiment that has associated with it a bunch of
            random variables. So once you have random variables, what do you do instinctively? You talk about the
            distribution of these random variables. We already specified for the Bernoulli process that each Xi is a
            Bernoulli random variable, with probability of success equal to p.&nbsp;That specifies the distribution of the
            random
            variable X, or Xt, for general time t.</p>
        <p>然后你就可以计算期望值和方差等等。所以期望值是，概率为 p，你得到 1。概率为 1 p，你得到 0。所以期望值等于 p。然后我们之前已经看到伯努利随机变量方差的公式，即 p 乘以 1 p。&nbsp;</p><p>Then you can calculate expected values and variances, and so on. So the expected value is, with probability
            p,
            you get a 1. And with probability 1 p, you get a 0. So the expected value is equal to p.&nbsp;And then we have
            seen
            before a formula for the variance of the Bernoulli random variable, which is p times 1 p.&nbsp;</p>
        <p>这样，我们现在基本上掌握了随机变量 Xt 的所有统计属性，并且每个 t 都有这些属性。这足以作为随机过程的概率描述吗？嗯，还不够。你需要知道不同随机变量之间的关系。如果你谈论的是一般随机过程，你会想知道一些事情。</p><p>So this way we basically now have all the statistical properties of the random variable Xt, and we have those
            properties for every t. Is this enough of a probabilistic description of a random process? Well, no. You
            need to
            know how the different random variables relate to each other. If you’re talking about a general random
            process,
            you would like to know things.</p>
        <p>例如，X2、X5 和 X7 的联合分布。例如，这可能是您感兴趣的内容。指定它的方法是给出这些随机变量的联合 PMF。您必须对您感兴趣的随机变量的每个集合或任何子集执行此操作。因此，要对随机过程进行完整的描述，您需要为我指定所有可能的联合分布。
        </p><p>For example, the joint distribution of X2, with X5, and X7. For example, that might be something that you’re
            interested in. And the way you specify it is by giving the joint PMF of these random variables. And you have
            to
            do that for every collection, or any subset, of the random variables you are interested in. So to have a
            complete description of a random processes, you need to specify for me all the possible joint distributions.
        </p>
        <p>一旦你有了所有可能的联合分布，那么原则上你就可以回答你可能感兴趣的任何问题了。我们如何解决伯努利过程的这个问题？我没有明确地给你联合分布。但我隐含地给了你。这是因为我告诉你不同的随机变量是相互独立的。</p><p>And once you have all the possible joint distributions, then you can answer, in principle, any questions you
            might be interested in. How did we get around this issue for the Bernoulli process? I didn’t give you the
            joint
            distributions explicitly. But I gave them to you implicitly. And this is because I told you that the
            different
            random variables are independent of each other.</p>
        <p>因此，至少对于伯努利过程，我们做出独立性假设，我们知道这将是 PMF 的乘积。既然我已经告诉过您各个 PMF 是什么，这意味着您会自动知道所有联合 PMF。我们可以基于此开始讨论。好的。因此，这是对随机过程的一种看法，它只是一组随机变量。</p><p>So at least for the Bernoulli process, where we make the independence assumption, we know that this is going
            to
            be the product of the PMFs. And since I have told you what the individual PMFs are, this means that you
            automatically know all the joint PMFs. And we can go to business based on that. All right. So this is one
            view
            of what a random process is, just a collection of random variables.</p>
        <p>还有一种观点更加抽象，如下所示。整个过程应被视为一个长期实验。因此，我们回到第一章的概率模型观点。因此，必须涉及样本空间。样本空间是什么？如果我进行无限、长期的实验，即抛出无数枚硬币，实验的典型结果将是 0 和 1 的序列。</p><p>There’s another view that’s a little more abstract, which is the following. The entire process is to be
            thought
            of as one long experiment. So we go back to the chapter one view of probabilistic models. So there must be a
            sample space involved. What is the sample space? If I do my infinite, long experiment of flipping an
            infinite
            number of coins, a typical outcome of the experiment would be a sequence of 0’s and 1’s.</p>
        <h2 id="sample-space-1">样本空间</h2><h2>Sample Space</h2>
        <p>所以这可能是实验的一个可能结果，只是一个无限的 0 和 1 序列。我的样本空间是这种所有可能结果的集合。这是另一个可能的结果，等等。本质上，我们处理的是样本空间，即所有 0 和 1 序列的空间。我们对实验中可能发生的事情做出某种概率假设。</p><p>So this could be one possible outcome of the experiment, just an infinite sequence of 0’s and 1’s. My sample
            space is the set of all possible outcomes of this kind. Here’s another possible outcome, and so on. And
            essentially we’re dealing with a sample space, which is the space of all sequences of 0’s and 1’s. And we’re
            making some sort of probabilistic assumption about what may happen in that experiment.</p>
        <p>因此，我们可能感兴趣的一个特定序列是获得所有 1 的序列。因此，这是永远为您提供 1 的序列。一旦您认识到这是我们的样本空间。它是所有无限序列的空间。您可以开始提出与无限序列有关的问题。例如，获得由所有 1 组成的无限序列的概率是多少？</p><p>So one particular sequence that we may be interested in is the sequence of obtaining all 1’s. So this is the
            sequence that gives you 1’s forever. Once you take the point of view that this is our sample space. its the
            space of all infinite sequences. you can start asking questions that have to do with infinite sequences.
            Such as
            the question, what’s the probability of obtaining the infinite sequence that consists of all 1’s?</p>
        <p>那么这个概率是多少呢？让我们看看如何计算它。因此，获得所有 1 的概率肯定小于或等于在前 10 次投掷中获得 1 的概率。好的。这要求发生更多的事情。如果这个事件是真的，那么这也是真的。因此，这个事件的概率小于那个事件的概率。这个事件包含在那个事件中。</p><p>So what is this probability? Let’s see how we could calculate it. So the probability of obtaining all 1’s is
            certainly less than or equal to the probability of obtaining 1’s, just in the first 10 tosses. OK. This is
            asking for more things to happen than this. If this event is true, then this is also true. Therefore the
            probability of this is smaller than the probability of that. This event is contained in that event.</p>
        <p>这就意味着这一点。所以我们有这个不等式。那么在 10 次试验中获得 1 的概率是多少？这只是 p 的 10 次方，因为试验是独立的。当然，我在这里选择 10 没有任何理由。如果我使用任意数字 k，同样的论点也会成立。而且这对所有 k 都必须成立。所以这个概率小于 p 的 k 次方，无论我选择什么 k。</p><p>This implies this. So we have this inequality. Now what’s the probability of obtaining 1’s in 10 trials? This
            is
            just p to the 10th because the trials are independent. Now of course there’s no reason why I chose 10 here.
            The
            same argument goes through if I use an arbitrary number, k. And this has to be true for all k. So this
            probability is less than p to the k, no matter what k I choose.</p>
        <p>因此，当 k 趋于无穷大时，这个值必须小于或等于这个值的极限。对于所有 k，这个值都小于这个值。让 k 趋于无穷大，取任意大的 k，这个数字就会变得任意小。它会趋于 0。这就证明了无限 1 序列的概率等于 0。因此对两边取极限。</p><p>Therefore, this must be less than or equal to the limit of this, as k goes to infinity. This is smaller than
            that
            for all k’s. Let k go to infinity, take k arbitrarily large, this number is going to become arbitrarily
            small.
            It goes to 0. And that proves that the probability of an infinite sequence of 1’s is equal to 0. So take
            limits
            of both sides.</p>
        <p>它会小于或等于极限。我不应该在这里取极限。当 k 趋向无穷大（即 0）时，概率小于或等于 p 到 k 的极限。因此，这正式证明了全 1 序列的概率为 0。如果有无数次抛硬币，那么所有抛硬币都出现正面的概率是多少？</p><p>It’s going to be less than or equal to the limit. I shouldn’t take a limit here. The probability is less than
            or
            equal to the limit of p to the k, as k goes to infinity, which is 0. So this proves in a formal way that the
            sequence of all 1’s has 0 probability. If you have an infinite number of coin flips, what’s the probability
            that
            all of the coin flips result in heads?</p>
        <p>发生这种情况的概率等于零。所以这个特定序列的概率为 0。当然，我在这里假设 p 小于 1，严格小于 1。现在有趣的是，如果你看任何其他无限序列，并尝试计算该无限序列的概率，你会得到 (1 p) 乘以 1、1 p 乘以 1、1 p、乘以 p 乘以 p、乘以 1 p 等等的乘积。</p><p>The probability of this happening is equal to zero. So this particular sequence has 0 probability. Of course,
            I’m
            assuming here that p is less than 1, strictly less than 1. Now the interesting thing is that if you look at
            any
            other infinite sequence, and you try to calculate the probability of that infinite sequence, you would get a
            product of (1 p) times 1,1 p times 1,1 p, times p times p, times 1 p and so on.</p>
        <p>你不断将小于 1 的数字相乘。再次，我假设 p 在 0 和 1 之间。所以 1 p 小于 1，p 小于 1。你不断将小于 1 的数字相乘。如果你将无限多个这样的数字相乘，无限乘积将变为 0。因此，此样本空间中的任何单个序列实际上都有 0 概率。这可能有点违反直觉。</p><p>You keep multiplying numbers that are less than 1. Again, I’m making the assumption that p is between 0 and
            1. So
            1 p is less than 1, p is less than 1. You keep multiplying numbers less than 1. If you multiply infinitely
            many
            such numbers, the infinite product becomes 0. So any individual sequence in this sample space actually has 0
            probability. And that is a little bit counter intuitive perhaps.</p>
        <p>但这种情况更像我们处理连续随机变量的情况。所以如果你能画一个连续随机变量，每个可能结果的概率都是 0。这很好。但所有结果加起来仍然有正概率。所以这里的情况非常相似。所以 0 和 1 的无限序列空间，那个样本空间非常像一个连续空间。</p><p>But the situation is more like the situation where we deal with continuous random variables. So if you could
            draw
            a continuous random variable, every possible outcome has 0 probability. And that’s fine. But all of the
            outcomes
            collectively still have positive probability. So the situation here is very much similar. So the space of
            infinite sequences of 0’s and 1’s, that sample space is very much like a continuous space.</p>
        <p>如果你想进一步推论这个类比，你可以把它看作是一个实数的展开。或者一个实数的二进制表示。取一个实数，用二进制写下来，你会得到一个无限的 0 和 1 序列。所以你可以把这里的每个可能结果本质上看作一个实数。
        </p><p>If you want to push that analogy further, you could think of this as the expansion of a real number. Or the
            representation of a real number in binary. Take a real number, write it down in binary, you are going to get
            an
            infinite sequence of 0’s and 1’s. So you can think of each possible outcome here essentially as a real
            number.
        </p>
        <p>因此，进行无数次抛硬币的实验与随机选取一个实数的实验有点相似。当你随机选取实数时，任何特定的实数都有 0 的概率。同样，任何特定的无限序列都有 0 的概率。因此，如果我们进一步推论这个类比，我们可以做一些有趣的事情。但我们不会进一步推论。</p><p>So the experiment of doing an infinite number of coin flips is sort of similar to the experiment of picking a
            real number at random. When you pick real numbers at random, any particular real number has 0 probability.
            So
            similarly here, any particular infinite sequence has 0 probability. So if we were to push that analogy
            further,
            there would be a few interesting things we could do. But we will not push it further.</p>
        <p>这只是为了告诉大家，一旦开始讨论涉及无限时间范围的随机过程，事情就会变得非常微妙和有趣。因此，即使在简单的伯努利过程的背景下，事情也会变得有趣。为了让大家预览接下来的内容，今天我们将只讨论伯努利过程。在下一堂课之前，你应该确认一下。</p><p>This is just to give you an indication that things can get pretty subtle and interesting once you start
            talking
            about random processes that involve forever, over the infinite time horizon. So things get interesting even
            in
            this context of the simple Bernoulli process. Just to give you a preview of what’s coming further, today
            we’re
            going to talk just about the Bernoulli process. And you should make sure before the next lecture.</p>
        <p>我想在考试和下一堂课之间。理解我们今天所做的一切。因为下次我们将再次做所有事情，但在连续时间内。在连续时间内，事情会变得更加微妙和困难。但我们将以我们对离散时间情况的理解为基础。</p><p>I guess between the exam and the next lecture. to understand everything we do today. Because next time we’re
            going to do everything once more, but in continuous time. And in continuous time, things become more subtle
            and
            a little more difficult. But we are going to build on what we understand for the discrete time case.</p>
        <p>现在，伯努利过程及其连续时间模拟都具有我们称之为无记忆性的属性，即过去发生的任何事情都不会影响未来。稍后在本课中，我们将讨论更一般的随机过程，即所谓的马尔可夫链，其中存在一定的时间依赖性。也就是说，过去发生的事情会对未来可能发生的事情产生一定影响。</p><p>Now both the Bernoulli process and its continuous time analog has a property that we call memorylessness,
            whatever happened in the past does not affect the future. Later on in this class we’re going to talk about
            more
            general random processes, so called Markov chains, in which there are certain dependences across time. That
            is,
            what has happened in the past will have some bearing on what may happen in the future.</p>
        <p>所以这就像抛硬币，下一次抛硬币的结果与前一次抛硬币的结果有某种相关性。这为我们提供了更丰富的模型。一旦我们做到这一点，我们基本上就涵盖了所有可能的模型。因此，对于实际有用且可以操纵的随机过程，马尔可夫链是一类相当通用的模型。</p><p>So it’s like having coin flips where the outcome of the next coin flip has some dependence on the previous
            coin
            flip. And that gives us a richer class of models. And once we get there, essentially we will have covered
            all
            possible models. So for random processes that are practically useful and which you can manipulate, Markov
            chains
            are a pretty general class of models.</p>
        <p>几乎任何随时间演变的现实世界现象都可以用马尔可夫链进行近似建模。因此，尽管这是概率中的一流学科，但我们在这个方向上已经取得了很大进展。</p><p>And almost any real world phenomenon that evolves in time can be approximately modeled using Markov chains.
            So
            even though this is a first class in probability, we will get pretty far in that direction.</p>
        <h2 id="bernoulli-questions">伯努利问题</h2><h2>Bernoulli Questions</h2>
        <p>好的。现在让我们开始进行一些计算并回答一些有关伯努利过程的问题。
        </p><p>All right. So now let’s start doing a few calculations and answer some questions about the Bernoulli process.
        </p>
        <p>因此，再次强调，用与伯努利过程相对应的模型来思考，最好的方法是用作业到达设施的方式来思考。你可以问两种类型的问题。在给定的时间内，有多少作业到达？或者反过来说，对于给定数量的作业，它们需要多长时间才能到达？所以我们将处理这两个问题，从第一个开始。</p><p>So again, the best way to think in terms of models that correspond to the Bernoulli process is in terms of
            arrivals of jobs to a facility. And there’s two types of questions that you can ask. In a given amount of
            time,
            how many jobs arrived? Or conversely, for a given number of jobs, how much time did it take for them to
            arrive?
            So we’re going to deal with these two questions, starting with the first.</p>
        <p>对于给定的时间量，即给定的时间段数，我们有多少次到达？这些 Xi 中有多少恰好是 1？我们固定时间段数，假设是 n 个时间段，然后测量成功次数。这是一个非常熟悉的随机变量。n 次独立抛硬币或 n 次独立试验中的成功次数是二项式随机变量。</p><p>For a given amount of time. that is, for a given number of time periods. how many arrivals have we had? How
            many
            of those Xi’s happen to be 1’s? We fix the number of time slots. let’s say n time slots. and you measure the
            number of successes. Well this is a very familiar random variable. The number of successes in n independent
            coin
            flips. or in n independent trials. is a binomial random variable.</p>
        <p>所以我们知道它的分布是由二项式 PMF 给出的，它就是这样的，k 从 0 到 n。现在我们知道了关于这个随机变量的一切。我们知道它的期望值是 n 乘以 p。我们知道方差，即 n 乘以 p，乘以 1 p。所以这里没有什么新东西。这是最简单的部分。现在让我们看看相反的问题。</p><p>So we know its distribution is given by the binomial PMF, and it’s just this, for k going from 0 up to n.&nbsp;And
            we
            know everything by now about this random variable. We know its expected value is n times p.&nbsp;And we know the
            variance, which is n times p, times 1 p.&nbsp;So there’s nothing new here. That’s the easy part. So now let’s
            look at
            the opposite kind of question.</p>
        <p>现在，我们不再固定时间并询问有多少人到达，而是固定到达人数并询问花费了多少时间。让我们从第一个到达的时间开始。这样流程就开始了。我们得到了我们的时段。</p><p>Instead of fixing the time and asking how many arrivals, now let us fix the number of arrivals and ask how
            much
            time did it take. And let’s start with the time until the first arrival. So the process starts. We got our
            slots.</p>
        <p>我们可能看到一串 0，然后在某个时刻我们得到了 1。我们得到 1 之前所进行的试验次数，我们称之为 T1。这是第一次到达的时间。</p><p>And we see, perhaps, a sequence of 0’s and then at some point we get a 1. The number of trials it took until
            we
            get a 1, we’re going to call it T1. And it’s the time of the first arrival.</p>
        <h2 id="probability-distribution">概率分布</h2><h2>Probability Distribution</h2>
        <p>好的。T1 的概率分布是什么？它是什么样的随机变量？我们之前已经讨论过这个问题。</p><p>OK. What is the probability distribution of T1? What kind of random variable is it? We’ve gone through this
            before.</p>
        <p>第一次到达发生在时间 t 的事件是前 t 1 次试验失败，而第 t 次试验恰好成功的事件。因此，对于在时间段 5 发生的第一次成功，这意味着前 4 个时间段失败，而第 5 个时间段成功。</p><p>The event that the first arrival happens at time little t is the event that the first t 1 trials were
            failures,
            and the trial number t happens to be a success. So for the first success to happen at time slot number 5, it
            means that the first 4 slots had failures and the 5th slot had a success.</p>
        <p>所以发生这种情况的概率就是前 t 1 次试验失败，而第 1 次试验成功的概率。这是 t 等于 1、2 等的公式。所以我们知道这个分布是什么。这就是所谓的几何分布。让我先简单说一下。过去，我们确实计算过几何分布的期望值，它是 1/p。&nbsp;</p><p>So the probability of this happening is the probability of having failures in the first t 1 trials, and
            having a
            success at trial number 1. And this is the formula for t equal 1,2, and so on. So we know what this
            distribution
            is. It’s the so called geometric distribution. Let me jump this through this for a minute. In the past, we
            did
            calculate the expected value of the geometric distribution, and it’s 1/p.&nbsp;</p>
        <p>这意味着如果 p 很小，则预计需要很长时间才能首次成功。然后还有一个 T1 方差公式，我们从未在课堂上正式推导过，但它在你的教科书中，碰巧就是这样。好的。所以到目前为止没有什么新东西。</p><p>Which means that if p is small, you expect to take a long time until the first success. And then there’s a
            formula also for the variance of T1, which we never formally derived in class, but it was in your textbook
            and
            it just happens to be this. All right. So nothing new until this point.</p>
        <h2 id="memoryless-property">无记忆特性</h2><h2>Memoryless Property</h2>
        <p>现在我们来谈谈这个属性，无记忆属性。我们在讨论时提到过这个属性。</p><p>Now, let’s talk about this property, the memorylessness property. We kind of touched on this property when we
            discussed.</p>
        <p>当我们在课堂上推导 T1 的期望值时。那么无记忆性是什么呢？它本质上是独立性的结果。如果我告诉你在某个时间之前抛硬币的结果，由于独立性，这不会给你关于那个时间之后抛硬币的任何信息。</p><p>When we did the derivation in class of the expected value of T1. Now what is the memoryless property? It’s
            essentially a consequence of independence. If I tell you the results of my coin flips up to a certain time,
            this, because of independence, doesn’t give you any information about the coin flips after that time.</p>
        <p>因此，知道这里有很多 0 并不会改变我对未来抛硬币结果的看法，因为未来的抛硬币结果将只是独立的抛硬币结果，并且抛出反面的概率为 p。因此，这是我对特定时间做出的陈述。也就是说，你抛硬币到 12 点。然后在 12 点，你开始观察。</p><p>So knowing that we had lots of 0’s here does not change what I believe about the future coin flips, because
            the
            future coin flips are going to be just independent coin flips with a given probability, p, for obtaining
            tails.
            So this is a statement that I made about a specific time. That is, you do coin flips until 12 o’clock. And
            then
            at 12 o’clock, you start watching.</p>
        <p>无论 12 点之前发生什么，在 12 点之后，您将看到的只是一系列独立的伯努利试验，概率相同，p。过去发生的一切无关紧要。现在，我们不再讨论您开始观看的固定时间，而是想一想这样一种情况：您的姐姐坐在隔壁房间，抛硬币，直到她观察到第一次成功，然后叫您进屋。</p><p>No matter what happens before 12 o’clock, after 12:00, what you’re going to see is just a sequence of
            independent
            Bernoulli trials with the same probability, p.&nbsp;Whatever happened in the past is irrelevant. Now instead of
            talking about the fixed time at which you start watching, let’s think about a situation where your sister
            sits
            in the next room, flips the coins until she observes the first success, and then calls you inside.</p>
        <p>然后你开始观察这个时间之后的情况。你会看到什么？嗯，你会看到一次抛硬币，成功概率为 p。你会看到另一次试验，成功概率为 p，这些试验都是相互独立的。所以从那个时间开始，你将看到的只是一系列独立的伯努利试验，就好像这个过程是从这个时候开始的一样。</p><p>And you start watching after this time. What are you’re going to see? Well, you’re going to see a coin flip
            with
            probability p of success. You’re going to see another trial that has probability p as a success, and these
            are
            all independent of each other. So what you’re going to see starting at that time is going to be just a
            sequence
            of independent Bernoulli trials, as if the process was starting at this time.</p>
        <p>第一次成功需要多长时间与之后发生的事情没有任何关系。之后发生的事情仍然是一系列独立的抛硬币。这个故事实际上更加普遍。所以你的妹妹看着抛硬币，在某个时候告诉你，哦，这里发生了一些非常有趣的事情。我得到了这一连串一百个 1。快来看吧。</p><p>How long it took for the first success to occur doesn’t have any bearing on what is going to happen
            afterwards.
            What happens afterwards is still a sequence of independent coin flips. And this story is actually even more
            general. So your sister watches the coin flips and at some point tells you, oh, something really interesting
            is
            happening here. I got this string of a hundred 1’s in a row.come and watch.</p>
        <p>现在，当你进入那里并开始观察时，你期望看到一些不寻常的事情吗？在你被叫去之前就发生了一些不寻常的事情。这是否意味着你之后会看到不寻常的事情？不。之后，你将看到的只是一系列独立的硬币翻转。之前发生的一些奇怪的事情与未来会发生什么无关。</p><p>Now when you go in there and you start watching, do you expect to see something unusual? There were unusual
            things that happened before you were called in. Does this means that you’re going to see unusual things
            afterwards? No.&nbsp;Afterwards, what you’re going to see is, again, just a sequence of independent coin flips.
            The
            fact that some strange things happened before doesn’t have any bearing as to what is going to happen in the
            future.</p>
        <p>因此，如果赌场中的轮盘是正确制造的，连续出现 3 个红色的事实不会影响下一轮是红色还是黑色的概率。因此，无论过去发生什么。无论它有多么不寻常。当你被叫进去的时候，未来将要发生的事情将只是独立的伯努利试验，概率相同，p。&nbsp;</p><p>So if the roulettes in the casino are properly made, the fact that there were 3 reds in a row doesn’t affect
            the
            odds of whether in the next roll it’s going to be a red or a black. So whatever happens in the past. no
            matter
            how unusual it is. at the time when you’re called in, what’s going to happen in the future is going to be
            just
            independent Bernoulli trials, with the same probability, p.&nbsp;</p>
        <p>唯一会改变这个故事的情况是，如果你的姐姐有一点先见之明。所以你的姐姐可以展望未来，知道接下来的 10 次抛硬币都会是正面，并在这 10 次抛硬币发生之前打电话给你。如果她打电话给你，那么你会看到什么？</p><p>The only case where this story changes is if your sister has a little bit of foresight. So your sister can
            look
            ahead into the future and knows that the next 10 coin flips will be heads, and calls you before those 10
            flips
            will happen. If she calls you in, then what are you going to see?</p>
        <p>你不会看到独立的伯努利试验，因为她有通灵能力，她知道接下来的 1 会是 1。她叫你进来，你会看到一连串 1。所以不再是独立的伯努利试验了。那么这里的细微差别是什么呢？</p><p>You’re not going to see independent Bernoulli trials, since she has psychic powers and she knows that the
            next
            ones would be 1’s. She called you in and you will see a sequence of 1’s. So it’s no more independent
            Bernoulli
            trials. So what’s the subtle difference here?</p>
        <p>未来与过去是相互独立的，前提是叫你开始观察的时间是由一个没有任何远见、看不到未来的人决定的。如果你被叫去，只是基于迄今为止发生的事情，那么你就没有任何关于未来的信息。这里的情况是一个特殊情况。你有你的硬币翻转。</p><p>The future is independent from the past, provided that the time that you are called and asked to start
            watching
            is determined by someone who doesn’t have any foresight, who cannot see the future. If you are called in,
            just
            on the basis of what has happened so far, then you don’t have any information about the future. And one
            special
            case is the picture here. You have your coin flips.</p>
        <p>一旦你看到有事情发生，一旦你看到成功，你就会被叫去。你是根据过去发生的事情被叫去的，但没有任何远见。</p><p>Once you see a one that happens, once you see a success, you are called in. You are called in on the basis of
            what happened in the past, but without any foresight.</p>
        <h2 id="random-variable-1">随机变量</h2><h2>Random Variable</h2>
        <p>好的。这个微妙的区别将使我们的下一个示例变得有趣和微妙。那么问题来了。你每天都会买一张彩票，所以我们有一个在时间中运行的伯努利过程。</p><p>OK. And this subtle distinction is what’s going to make our next example interesting and subtle. So here’s
            the
            question. You buy a lottery ticket every day, so we have a Bernoulli process that’s running in time.</p>
        <p>您对第一个亏损日的长度感兴趣。这意味着什么？假设一个典型的事件序列可能是这样的。那么我们在这里讨论什么？我们正在查看第一个亏损日字符串，其中亏损日表示 0。因此亏损日字符串就是这里的字符串。我们将该字符串的长度称为 L。</p><p>And you’re interested in the length of the first string of losing days. What does that mean? So suppose that
            a
            typical sequence of events could be this one. So what are we discussing here? We’re looking at the first
            string
            of losing days, where losing days means 0’s. So the string of losing days is this string here. Let’s call
            the
            length of that string, L.</p>
        <p>我们感兴趣的是随机变量，也就是这个间隔的长度。它是什么样的随机变量？好的。以下是思考这个问题的一种可能方式。好的。从这个时间开始，一直到这个时间，我们在看什么？我们看时间，从这里开始，直到第一次成功。所以过去并不重要。从这里开始，我们进行抛硬币，直到第一次成功。</p><p>We’re interested in the random variable, which is the length of this interval. What kind of random variable
            is
            it? OK. Here’s one possible way you might think about the problem. OK. Starting from this time, and looking
            until this time here, what are we looking at? We’re looking at the time, starting from here, until the first
            success. So the past doesn’t matter. Starting from here we have coin flips until the first success.</p>
        <p>伯努利过程第一次成功所需的时间。我们刚刚讨论过，这是一个几何随机变量。所以你的第一个猜想是，这里的这个随机变量比我们感兴趣的随机变量长 1，可能是一个几何随机变量。如果是这样，那么你可以说随机变量 L 是一个几何，减 1。这是正确答案吗？</p><p>The time until the first success in a Bernoulli process. we just discussed that it’s a geometric random
            variable.
            So your first conjecture would be that this random variable here, which is 1 longer than the one we are
            interested in, that perhaps is a geometric random variable. And if this were so, then you could say that the
            random variable, L, is a geometric, minus 1. Can that be the correct answer?</p>
        <p>几何随机变量，它取什么值？它的值为 1、2.3，等等。1 减去几何随机变量，它的值为 0、1.2，等等。随机变量 L 可以是 0 吗？不能。随机变量 L 是亏损天数的字符串长度。</p><p>A geometric random variable, what values does it take? It takes values 1,2.3, and so on. 1 minus a geometric
            would take values from 0,1.2, and so on. Can the random variable L be 0? No.&nbsp;The random variable L is the
            length
            of a string of losing days.</p>
        <p>因此 L 的最小值就是 1。如果你只输了一天，然后开始赢钱，L 就等于 1。所以根据定义，L 不能为 0，这意味着根据定义，L + 1 不能为 1。但如果 L +1 是几何的，它就等于 1。因此，这个随机变量 L + 1 不是几何的。好吧。为什么它不是几何的？</p><p>So the shortest that L could be, would be just 1. If you get just one losing day and then you start winning,
            L
            would be equal to 1. So L cannot be 0 by definition, which means that L + 1 cannot be 1, by definition. But
            if L
            +1 were geometric, it could be equal to 1. Therefore this random variable, L + 1, is not a geometric. OK.
            Why is
            it not geometric?</p>
        <p>我从这个时间开始观察。从这个时间到第一次成功，这应该是一个几何随机变量。问题在哪里？如果我被要求从这个时间开始观察，那是因为我妹妹知道下一次失败了。这是失败串开始的时间。为了知道他们应该从这里开始观察，这就像我被告知下一次失败一样。</p><p>I started watching at this time. From this time until the first success, that should be a geometric random
            variable. Where’s the catch? If I’m asked to start watching at this time, it’s because my sister knows that
            the
            next one was a failure. This is the time where the string of failures starts. In order to know that they
            should
            start watching here, it’s the same as if I’m told that the next one is a failure.</p>
        <p>因此，要求在此时开始观察意味着有人观察了未来。在这种情况下，这些不再是独立的伯努利试验。事实上，它们不是。如果你现在开始观察，你肯定会下一次失败。下一次不是独立的伯努利试验。这就是为什么声称这个 L + 1 是几何的论点是不正确的。</p><p>So to be asked to start watching at this time requires that someone looked in the future. And in that case,
            it’s
            no longer true that these will be independent Bernoulli trials. In fact, they’re not. If you start watching
            here, you’re certain that the next one is a failure. The next one is not an independent Bernoulli trial.
            That’s
            why the argument that would claim that this L + 1 is geometric would be incorrect.</p>
        <p>那么如果这不是正确答案，那么哪个是正确答案？正确答案如下。你的姐姐正在观察。你的姐姐看到了第一次失败，然后告诉你，好吧，失败或亏损的日子已经开始了。进来观察。所以你从这个时候开始观察。你开始观察直到第一次成功。这将是一个几何随机变量。所以从这里到这里，这将是几何的。</p><p>So if this is not the correct answer, which is the correct answer? The correct answer goes as follows. Your
            sister is watching. Your sister sees the first failure, and then tells you, OK, the failures. or losing
            days.
            have started.come in and watch. So you start to watching at this time. And you start watching until the
            first
            success comes. This will be a geometric random variable. So from here to here, this will be geometric.</p>
        <p>事情就这样发生了。你被要求开始观察。开始观察后，未来只是一系列独立的伯努利试验。第一次失败发生之前的时间，这将是一个参数为 p 的几何随机变量。然后你会注意到感兴趣的间隔与这个间隔的长度完全相同。它在一个时间步后开始，在一个时间步后结束。</p><p>So things happen. You are asked to start watching. After you start watching, the future is just a sequence of
            independent Bernoulli trials. And the time until the first failure occurs, this is going to be a geometric
            random variable with parameter p.&nbsp;And then you notice that the interval of interest is exactly the same as
            the
            length of this interval. This starts one time step later, and ends one time step later.</p>
        <p>所以结论是 L 实际上是几何的，参数为 p。好的，看起来我漏了一张幻灯片。我可以从这里偷懒一下吗？好的。既然我们处理了第一次到达的时间，我们可以开始讨论第二次到达的时间，等等。我们如何定义这些？</p><p>So conclusion is that L is actually geometric, with parameter p.&nbsp;OK, it looks like I’m missing one slide. Can
            I
            cheat a little from here? OK. So now that we dealt with the time until the first arrival, we can start
            talking
            about the time until the second arrival, and so on. How do we define these?</p>
        <p>第一个到达发生后，我们将得到一系列没有到达的时间段，然后下一个到达将会发生。所以我们称这个时间为。或者第一个到达后到下一个到达的时间段数。我们称之为 T2。这是第二个到达间隔时间，即到达之间的时间。
        </p><p>After the first arrival happens, we’re going to have a sequence of time slots with no arrivals, and then the
            next
            arrival is going to happen. So we call this time that elapses. or number of time slots after the first
            arrival
            until the next one. we call it T2. This is the second inter arrival time, that is, time between arrivals.
        </p>
        <p>一旦到达，我们就会等待，看看第三次到达还需要多少次。我们把这个时间称为 T3。我们感兴趣的是第 k 次到达的时间，也就是前 k 次到达间隔时间的总和。例如，假设 Y3 是第三次到达的时间。</p><p>Once this arrival has happened, then we wait and see how many more it takes until the third arrival. And we
            call
            this time here, T3. We’re interested in the time of the k th arrival, which is going to be just the sum of
            the
            first k inter arrival times. So for example, let’s say Y3 is the time that the third arrival comes.</p>
        <p>Y3 就是 T1、T2 和 T3 的总和。所以我们感兴趣的是这个随机变量 Y3，它是到达间隔时间的总和。要了解它是哪种随机变量，我想我们应该了解这些将是哪种随机变量。那么 T2 是哪种随机变量？你的妹妹一直在抛硬币，直到第一次观察到成功。</p><p>Y3 is just the sum of T1, plus T2, plus T3. So we’re interested in this random variable, Y3, and it’s the sum
            of
            inter arrival times. To understand what kind of random variable it is, I guess we should understand what
            kind of
            random variables these are going to be. So what kind of random variable is T2? Your sister is doing her coin
            flips until a success is observed for the first time.</p>
        <p>根据迄今为止发生的情况，你被叫进房间。你开始观察，直到再次观察到成功。所以在你开始观察之后，你所看到的只是一系列独立的伯努利试验。所以每一个都有成功的概率 p。第一次成功所需的时间，这个数字，T2，将再次成为另一个几何随机变量。</p><p>Based on that information about what has happened so far, you are called into the room. And you start
            watching
            until a success is observed again. So after you start watching, what you have is just a sequence of
            independent
            Bernoulli trials. So each one of these has probability p of being a success. The time it’s going to take
            until
            the first success, this number, T2, is going to be again just another geometric random variable.</p>
        <p>就好像这个过程才刚刚开始。你被叫进房间后，你没有任何先见之明，你对未来一无所知，除了这些将是独立的伯努利试验这一事实之外。因此，T2 本身将是几何的，具有相同的参数 p。然后你可以继续论证，并论证 T3 也是几何的，具有相同的参数 p。&nbsp;</p><p>It’s as if the process just started. After you are called into the room, you have no foresight, you don’t
            have
            any information about the future, other than the fact that these are going to be independent Bernoulli
            trials.
            So T2 itself is going to be geometric with the same parameter p.&nbsp;And then you can continue the arguments and
            argue that T3 is also geometric with the same parameter p.&nbsp;</p>
        <p>此外，无论发生什么，你被叫去医院需要多长时间，都不会改变未来会发生什么的统计数据。所以未来发生的一切都与过去无关。所以 T1、T2 和 T3 是独立的随机变量。所以结论是，第三个到达的时间是 3 个独立几何随机变量的总和，具有相同的参数。这在更普遍的情况下是正确的。</p><p>Furthermore, whatever happened, how long it took until you were called in, it doesn’t change the statistics
            about
            what’s going to happen in the future. So whatever happens in the future is independent from the past. So T1,
            T2,
            and T3 are independent random variables. So conclusion is that the time until the third arrival is the sum
            of 3
            independent geometric random variables, with the same parameter. And this is true more generally.</p>
        <p>第 k 次到达的时间将是 k 个独立随机变量的总和。因此，一般来说，Yk 将是 T1 加 Tk，其中 Ti 是几何的，具有相同的参数 p，并且是独立的。那么现在有什么比尝试找到随机变量 Yk 的分布更自然的呢？我们如何找到它？所以我为你固定了 k。</p><p>The time until the k th arrival is going to be the sum of k independent random variables. So in general, Yk
            is
            going to be T1 plus Tk, where the Ti’s are geometric, with the same parameter p, and independent. So now
            what’s
            more natural than trying to find the distribution of the random variable Yk? How can we find it? So I fixed
            k
            for you.</p>
        <p>假设 k 为 100。我想知道 100 位顾客到达需要多长时间。我们如何找到 Yk 的分布？一种方法是使用这个可爱的卷积公式。取一个几何，将其与另一个几何进行卷积，你会得到一个结果。将得到的那个结果再与一个几何进行卷积，重复 99 次，这样就可以得到 Yk 的分布。</p><p>Let’s say k is 100. I’m interested in how long it takes until 100 customers arrive. How can we find the
            distribution of Yk? Well one way of doing it is to use this lovely convolution formula. Take a geometric,
            convolve it with another geometric, you get something. Take that something that you got, convolve it with a
            geometric once more, do this 99 times, and this gives you the distribution of Yk.</p>
        <p>所以这绝对是可行的，而且非常繁琐。让我们尝试使用捷径来找到 Yk 的分布。所以 Yk 等于 t 的概率。所以我们试图找到 Yk 的 PMF。k 已经为我们固定了。我们想要计算 t 的各个值的概率，因为这将为我们提供 Yk 的 PMF。好的。这个事件是什么？</p><p>So that’s definitely doable, and it’s extremely tedious. Let’s try to find the distribution of Yk using a
            shortcut. So the probability that Yk is equal to t. So we’re trying to find the PMF of Yk. k has been fixed
            for
            us. And we want to calculate this probability for the various values of t, because this is going to give us
            the
            PMF of Yk. OK. What is this event?</p>
        <p>在时间 t 时，第 k 次到达需要什么？要做到这一点，我们需要两件事。在前 t 1 个时隙中，我们应该得到多少次到达？k 1。然后在最后一个时隙中，我们再得到一次到达，这就是第 k 次到达。所以，这就是在从 1 到 t 的时间间隔内有 k 1 次到达的概率。然后，在时间 t 时到达。</p><p>What does it take for the k th arrival to be at time t? For that to happen, we need two things. In the first
            t 1
            slots, how many arrivals should we have gotten? k 1. And then in the last slot, we get one more arrival, and
            that’s the k th one. So this is the probability that we have k 1 arrivals in the time interval from 1 up to
            t.
            And then, an arrival at time t.</p>
        <p>这是唯一可能的情况，即第 k 次到达发生在时间 t。我们需要在时间 t 到达。在此之前，我们需要恰好有 k 1 次到达。现在这是一个指代 t 1 的事件。在之前的时间段中，我们恰好有 k 1 次到达。然后在最后一个时间段，我们又得到一次到达。
        </p><p>That’s the only way that it can happen, that the k th arrival happens at time t. We need to have an arrival
            at
            time t. And before that time, we need to have exactly k 1 arrivals. Now this is an event that refers. t 1.
            In
            the previous time slots we had exactly k 1 arrivals. And then at the last time slot we get one more arrival.
        </p>
        <p>现在有趣的是，这里的这个事件与从时间 1 到时间 t 1 发生的事情有关。这个事件与时间 t 发生的事情有关。不同的时间段彼此独立。所以这个事件和那个事件是独立的。所以这意味着我们可以将它们的概率相乘。所以取这个的概率。那是什么？</p><p>Now the interesting thing is that this event here has to do with what happened from time 1 up to time t 1.
            This
            event has to do with what happened at time t. Different time slots are independent of each other. So this
            event
            and that event are independent. So this means that we can multiply their probabilities. So take the
            probability
            of this. What is that?</p>
        <p>嗯，在一定数量的时间段内有一定数量的乘客到达的概率，这些只是二项式概率。因此，在 t 1 个时间段中，恰好有 k 1 个乘客到达，p 比 k 1，（1 p）比 t 1（k 1），这给出了 t k。然后我们乘以这个概率，到达的概率在时间 t 等于 p。因此，这是数字 PMF 的公式。</p><p>Well probability of having a certain number of arrivals in a certain number of time slots, these are just the
            binomial probabilities. So this is, out of t 1 slots, to get exactly k 1 arrivals, p to the k 1, (1 p) to
            the t
            1 (k 1), this gives us t k. And then we multiply with this probability, the probability of an arrival, at
            time t
            is equal to p.&nbsp;And so this is the formula for the PMF of the number.</p>
        <p>直到第 k 次到达所需的时间。它是否与您讲义中的公式一致？或者它不在那里？它不在那里。好的。是的。好的。那么这就是公式，它对 t 的什么值成立？为了获得 k 次到达，至少需要 k 个时间段，因此当 k 大于或等于 t 时，这个公式应该成立。当 t 大于或等于 k ​​时。好的。</p><p>Of the time it takes until the k th arrival happens. Does it agree with the formula in your handout? Or its
            not
            there? It’s not there. OK. Yeah. OK. So that’s the formula and it is true for what values of t? It takes at
            least k time slots in order to get k arrivals, so this formula should be true for k larger than or equal to
            t.
            For t larger than or equal to k. All right.</p>
        <p>因此，这给出了随机变量 Yk 的 PMF。当然，我们可能还对 Yk 的均值和方差感兴趣。但这要简单得多。由于 Yk 是独立随机变量的总和，因此 Yk 的预期值将恰好是典型 t 的预期值的 k 倍。</p><p>So this gives us the PMF of the random variable Yk. Of course, we may also be interested in the mean and
            variance
            of Yk. But this is a lot easier. Since Yk is the sum of independent random variables, the expected value of
            Yk
            is going to be just k times the expected value of your typical t.</p>
        <p>因此 Yk 的期望值将恰好是 k 乘以 1/p，即几何的平均值。方差也类似，它将是几何方差的 k 乘以。因此，我们已了解了关于第一个到达所需时间的分布的所有信息。</p><p>So the expected value of Yk is going to be just k times 1/p, which is the mean of the geometric. And
            similarly
            for the variance, it’s going to be k times the variance of a geometric. So we have everything there is to
            know
            about the distribution of how long it takes until the first arrival comes.</p>
        <h2 id="bernoulli-processes">伯努利过程</h2><h2>Bernoulli Processes</h2>
        <p>好的。最后，我们再做一些有关伯努利过程的事情。</p><p>OK. Finally, let’s do a few more things about the Bernoulli process.</p>
        <p>同时讨论几个过程是很有趣的。因此，在拆分伯努利过程的情况下，您有到达服务器的到达。这是哪些位置到达的示意图。但实际上您可能有两个服务器。每当到达系统时，您都会抛硬币，并以某个概率 q 将其发送到一个服务器。以概率 1 q 将其发送到另一个服务器。</p><p>It’s interesting to talk about several processes at the time. So in the situation here of splitting a
            Bernoulli
            process is where you have arrivals that come to a server. And that’s a picture of which slots get arrivals.
            But
            actually maybe you have two servers. And whenever an arrival comes to the system, you flip a coin and with
            some
            probability, q, you send it to one server. And with probability 1 q, you send it to another server.</p>
        <p>因此，只有一个到达流，但有两个可能的服务器。每当有数据到达时，您要么将其发送到这里，要么将其发送到那里。每次您通过抛出一个具有自身偏差 q 的独立硬币来决定将其发送到哪里。决定将其发送到哪里的抛硬币被认为与到达过程本身无关。因此，发生了两次抛硬币。</p><p>So there is a single arrival stream, but two possible servers. And whenever there’s an arrival, you either
            send
            it here or you send it there. And each time you decide where you send it by flipping an independent coin
            that
            has its own bias q. The coin flips that decide where do you send it are assumed to be independent from the
            arrival process itself. So there’s two coin flips that are happening.</p>
        <p>在每个时间段，都会抛硬币来决定此过程中是否有到达，抛硬币的参数为 p。如果有到达，则抛另一枚硬币，概率为 q，1 q，决定是将其发送到上面还是下面。那么该服务器会看到什么样的到达过程？</p><p>At each time slot, there’s a coin flip that decides whether you have an arrival in this process here, and
            that
            coin flip is with parameter p.&nbsp;And if you have something that arrives, you flip another coin with
            probabilities
            q, and 1 q, that decides whether you send it up there or you send it down there. So what kind of arrival
            process
            does this server see?</p>
        <p>在任何给定的时间段，有概率 p 有数据到达这里。还有另一个概率 q 有数据被发送到那里。因此，该服务器在任何给定时间看到数据到达的概率是 p 乘以 q。因此，此处的这个过程将是伯努利过程，但参数不同，即 p 乘以 q。
        </p><p>At any given time slot, there’s probability p that there’s an arrival here. And there’s a further probability
            q
            that this arrival gets sent up there. So the probability that this server sees an arrival at any given time
            is p
            times q. So this process here is going to be a Bernoulli process, but with a different parameter, p times q.
        </p>
        <p>而下面这个，使用相同的参数，将是参数为 p 乘以 (1 q) 的伯努利过程。因此，通过获取到达的伯努利流并将其分成两部分，您将得到两个独立的伯努利过程。这将是一个伯努利过程，那将是伯努利过程。好吧，实际上，我跑得太快了。如何验证它是伯努利过程？</p><p>And this one down here, with the same argument, is going to be Bernoulli with parameter p times (1 q). So by
            taking a Bernoulli stream of arrivals and splitting it into two, you get two separate Bernoulli processes.
            This
            is going to be a Bernoulli process, that’s going to be a Bernoulli process. Well actually, I’m running a
            little
            too fast. What does it take to verify that it’s a Bernoulli process?</p>
        <p>在每个时间段，它都是 0 或 1。如果是 1，您将看到概率为 p 乘以 q 的到达。我们还需要验证什么才能说这是一个伯努利过程？我们需要确保在这个过程中发生的任何事情，在不同的时间段，在统计上都是相互独立的。这个属性正确吗？</p><p>At each time slot, it’s a 0 or 1. And it’s going to be a 1, you’re going to see an arrival with probability p
            times q. What else do we need to verify, to be able to tell. to say that it’s a Bernoulli process? We need
            to
            make sure that whatever happens in this process, in different time slots, are statistically independent from
            each other. Is that property true?</p>
        <p>例如，无论是否收到到达消息，这个时间段内发生的事情是否与该时间段内发生的事情无关？答案是肯定的，原因如下。这个时间段内发生的事情与此时与原始过程相关的抛硬币以及决定将事物发送到何处的抛硬币有关。</p><p>For example, what happens in this time slot whether you got an arrival or not, is it independent from what
            happened at that time slot? The answer is yes for the following reason. What happens in this time slot has
            to do
            with the coin flip associated with the original process at this time, and the coin flip that decides where
            to
            send things.</p>
        <p>那个时间段发生的事情与这里的抛硬币有关，还有额外的抛硬币决定如果有东西到达，要把它送到哪里。现在所有这些抛硬币都是相互独立的。决定我们在这里是否有到达的抛硬币与决定我们在那里是否有到达的抛硬币是独立的。</p><p>What happens at that time slot has to do with the coin flip here, and the additional coin flip that decides
            where
            to send it if something came. Now all these coin flips are independent of each other. The coin flips that
            determine whether we have an arrival here is independent from the coin flips that determined whether we had
            an
            arrival there.</p>
        <p>你可以概括这个论点并得出结论，事实上，这里的每个时间段都与其他时间段无关。这确实使其成为伯努利过程。原因是，在原始过程中，每个时间段都与其他时间段无关。我们用来决定将物品发送到何处的额外假设是抛硬币，它们也是相互独立的。</p><p>And you can generalize this argument and conclude that, indeed, every time slot here is independent from any
            other time slot. And this does make it a Bernoulli process. And the reason is that, in the original process,
            every time slot is independent from every other time slot. And the additional assumption that the coin flips
            that we’re using to decide where to send things, these are also independent of each other.</p>
        <p>因此，我们在这里使用独立事物的函数保持独立的基本属性。这有相反的情况。您可以做相反的事情，而不是将一个流分成两个流。您可以从两个到达流开始。假设您有男性到达，也有女性到达，但您不关心性别。</p><p>So we’re using here the basic property that functions of independent things remain independent. There’s a
            converse picture of this. Instead of taking one stream and splitting it into two streams, you can do the
            opposite. You could start from two streams of arrivals. Let’s say you have arrivals of men and you have
            arrivals
            of women, but you don’t care about gender.</p>
        <p>你唯一要记录的是，在给定的时间段内，是否有人到达。请注意，这里可能有一名男性到达，也可能有一名女性到达。我们只需用 1 记录，表示有一名到达。因此，在合并过程中，我们不会跟踪总共有多少人到达。我们只需记录是否有人到达。</p><p>And the only thing you record is whether, in a given time slot, you had an arrival or not. Notice that here
            we
            may have an arrival of a man and the arrival of a woman. We just record it with a 1, by saying there was an
            arrival. So in the merged process, we’re not keeping track of how many arrivals we had total. We just record
            whether there was an arrival or not an arrival.</p>
        <p>因此，当且仅当其中一个或两个流都有到达时，才会记录到达。因此，我们称其为两个伯努利过程的合并。两个过程，两个到达过程。因此，让我们假设这个到达过程独立于那个到达过程。那么，这里的典型时隙会发生什么？我将看到一个到达，除非这些都没有到达。</p><p>So an arrival gets recorded here if, and only if, one or both of these streams had an arrival. So that we
            call a
            merging of two Bernoull. of two processes, of two arrival processes. So let’s make the assumption that this
            arrival process is independent from that arrival process. So what happens at the typical slot here? I’m
            going to
            see an arrival, unless none of these had an arrival.</p>
        <p>因此，典型时隙中到达的概率将是 1 减去不到达的概率。不到达事件对应于第一个过程没有到达，第二个过程也没有到达。因此，当且仅当第一个过程没有到达且第二个过程没有到达时，合并过程才没有到达。</p><p>So the probability of an arrival in a typical time slot is going to be 1 minus the probability of no arrival.
            And
            the event of no arrival corresponds to the first process having no arrival, and the second process having no
            arrival. So there’s no arrival in the merged process if, and only if, there’s no arrival in the first
            process
            and no arrival in the second process.</p>
        <p>我们假设这两个过程是独立的，这就是为什么我们可以在这里乘以概率。然后你可以取这个公式，它简化为 p + q，减去 p 乘以 q。因此，合并过程的每个时间段都有一定的概率看到到达。合并过程是伯努利过程吗？是的，在你验证了不同时间段相互独立的附加属性之后，它就是伯努利过程。为什么它们是独立的？
        </p><p>We’re assuming that the two processes are independent and that’s why we can multiply probabilities here. And
            then
            you can take this formula and it simplifies to p + q, minus p times q. So each time slot of the merged
            process
            has a certain probability of seeing an arrival. Is the merged process a Bernoulli process? Yes, it is after
            you
            verify the additional property that different slots are independent of each other. Why are they independent?
        </p>
        <p>这个位置发生的事情与那个位置以及下面的那个位置有关。这两个位置。所以这里发生的事情与这里和那里发生的事情有关。这个位置发生的事情与这里和那里发生的事情有关。现在，这里和那里发生的一切都与这里和那里发生的一切都无关。因此，这里发生的事情与那里发生的事情无关。因此，独立性得以保留。
        </p><p>What happens in this slot has to do with that slot, and that slot down here. These two slots. so what happens
            here, has to do with what happens here and there. What happens in this slot has to do with whatever happened
            here and there. Now, whatever happens here and there is independent from whatever happens here and there.
            Therefore, what happens here is independent from what happens there. So the independence property is
            preserved.
        </p>
        <p>这个合并过程的不同时隙是相互独立的。因此，合并过程本身就是伯努利过程。所以请消化这两张合并和分裂的图片，因为我们将在连续时间内重新审视它们，其中的事情比这更微妙。好的。祝你考试顺利，一周后见。</p><p>The different slots of this merged process are independent of each other. So the merged process is itself a
            Bernoulli process. So please digest these two pictures of merging and splitting, because we’re going to
            revisit
            them in continuous time where things are little subtler than that. OK. Good luck on the exam and see you in
            a
            week.</p>
        <h1 id="poisson-process-i">14. 泊松过程 I</h1><h1>14. Poisson Process I</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 52:44 <a href="https://youtube.com/watch?v=jsqSScywvMc">https://youtube.com/watch?v=jsqSScywvMc</a></p><p> 12 years ago (Nov 10, 2012) — 52:44 <a href="https://youtube.com/watch?v=jsqSScywvMc">https://youtube.com/watch?v=jsqSScywvMc</a></p>
        <h2 id="unknown-270">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：上次我们开始讨论随机过程。随机过程是随时间演变的随机实验。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So last time we started
            talking about random processes. A random process is a random experiment that evolves over time.</p>
        <p>从概念上讲，重要的是要认识到这是一个具有多个阶段的单一概率实验。实际上，它有无数个阶段。我们讨论了最简单的随机过程，即伯努利过程，它只不过是伯努利试验的序列。伯努利试验的无限序列。例如，反复抛硬币。</p><p>And conceptually, it’s important to realize that it’s a single probabilistic experiment that has many stages.
            Actually, it has an infinite number of stages. And we discussed the simplest random process there is, the
            Bernoulli process, which is nothing but the sequence of Bernoulli trials. an infinite sequence of Bernoulli
            trials. For example, flipping a coin over and over.</p>
        <p>一旦我们理解了这个过程的原理，我们想要做的就是进入伯努利过程的连续时间版本。这就是我们所说的泊松过程。对于泊松过程，我们将做与伯努利过程完全相同的事情。</p><p>Once we understand what’s going on with that process, then what we want is to move into a continuous time
            version
            of the Bernoulli process. And this is what we will call the Poisson process. And for the Poisson process,
            we’re
            going to do exactly the same things that we did for the Bernoulli process.</p>
        <h2 id="unknown-271">未知</h2><h2>Unknown</h2>
        <p>也就是说，讨论给定时间段内的到达次数，并讨论连续到达之间的时间，以及到达间隔时间的分布。那么，让我们先快速回顾一下上次讨论的内容。首先，关于语言的说明。如果你想到抛硬币，我们就会讨论正面和反面。如果你把它们看作一系列试验，你就可以谈论成功和失败。</p><p>That is, talk about the number of arrivals during a given time period, and talk also about the time between
            consecutive arrivals, and for the distribution of inter arrival times. So let’s start with a quick review of
            what we discussed last time. First, a note about language. If you think of coin tosses, we then talk about
            heads
            and tails. If you think of these as a sequence of trials, you can talk about successes and failures.</p>
        <p>我们将使用的语言将更多地是到达语言。也就是说，如果在给定的时间范围内成功，则表示某事已到达。如果失败，则表示什么也没有到达。这种语言更方便、更自然，尤其是当我们谈论连续时间时。谈论到达而不是成功。</p><p>The language that we will be using will be more the language of arrivals. That is, if in a given slot you
            have a
            success, you say that something arrived. If you have a failure, nothing arrived. And that language is a
            little
            more convenient and more natural, especially when we talk about continuous time. to talk about arrivals
            instead
            of successes.</p>
        <p>但无论如何，对于伯努利过程，我们暂时保留成功的语言。而在离散时间中工作，我们有时间段。在每个时间段内，我们有一个独立的伯努利试验。成功的概率为 p。不同的时间段彼此独立。并且这个概率 p 对于任何给定的时间段都是相同的。</p><p>But in any case, for the Bernoulli process let’s keep, for a little bit, the language of successes. Whereas
            working in discrete time, we have time slots. During each time slot, we have an independent Bernoulli trial.
            There is probability p of having a success. Different slots are independent of each other. And this
            probability
            p is the same for any given time slot.</p>
        <h2 id="unknown-272">未知</h2><h2>Unknown</h2>
        <p>因此，对于此过程，我们将讨论一个感兴趣的随机变量，如下所示。如果我们有 n 个时间段或 n 次试验，将有多少次到达？或者有多少次成功？嗯，这只是由二项式 PMF 给出的。n 次试验中的成功次数是一个具有二项式 PMF 的随机变量，我们知道这是什么。然后我们讨论了到达间隔时间。</p><p>So for this process we will discuss the one random variable of interest, which is the following. If we have n
            time slots, or n trials, how many arrivals will there be? Or how many successes will there be? Well, this is
            just given by the binomial PMF. Number of successes in n trials is a random variable that has a binomial
            PMF,
            and we know what this is. Then we talked about inter arrival times.</p>
        <p>第一次到达的时间服从几何分布。我们之前已经看到过。现在，如果你开始考虑第 k 次到达的时间，我们用 Yk 表示，这是第一次到达的时间。然后在第一次到达之后，你必须等待一段时间，直到第二次到达，依此类推。</p><p>The time until the first arrival happens has a geometric distribution. And we have seen that from some time
            ago.
            Now if you start thinking about the time until k arrivals happen, and we denote that by Yk, this is the time
            until the first arrival happens. And then after the first arrival happens, you have to wait some time until
            the
            second arrival happens, and so on.</p>
        <p>然后是从第 (k 1) 次到达到第 k 次到达的时间。这里需要注意的是，由于该过程具有无记忆性，因此一旦第一次到达，就好像我们从头开始，我们将一直抛硬币直到下一次到达。因此，下一次到达所需的时间也将是一个几何随机变量。</p><p>And then the time from the (k 1)th arrival, until arrival number k. The important thing to realize here is
            that
            because the process has a memorylessness property, once the first arrival comes, it’s as if we’re starting
            from
            scratch and we will be flipping our coins until the next arrival comes. So the time it will take until the
            next
            arrival comes will also be a geometric random variable.</p>
        <h2 id="unknown-273">未知</h2><h2>Unknown</h2>
        <p>而且由于不同的时隙是独立的，因此第一次到达后发生的任何事情都与之前发生的任何事情无关。因此 T1 和 T2 将是独立的随机变量。同样，一直到 Tk。因此，直到第 k 次到达的时间是独立几何随机变量的总和，具有相同的参数 p。我们上次看到，我们可以找到 Yk 的概率分布。</p><p>And because different slots are independent, whatever happens after the first arrival is independent from
            whatever happened before. So T1 and T2 will be independent random variables. And similarly, all the way up
            to
            Tk. So the time until the k th arrival is a sum of independent geometric random variables, with the same
            parameter p.&nbsp;And we saw last time that we can find the probability distribution of Yk.</p>
        <p>Yk 取 t 值的概率等于。这里有一个组合因子，然后你得到 p 的 k 倍，(1 p) 的 (tk) 倍，这个公式对 t 等于 k、k+1 等都成立。这个分布有一个名字。它被称为帕斯卡 PMF。这就是关于伯努利过程的全部知识。一个重要的评论是要明白这个无记忆属性到底是什么意思。</p><p>The probability that Yk takes a value of t is equal to. there’s this combinatorial factor here, and then you
            get
            p to the k, (1 p) to the (t k), and this formula is true for t equal to k, k+1, and so on. And this
            distribution
            has a name. It’s called the Pascal PMF. So this is all there is to know about the Bernoulli process. One
            important comment is to realize what exactly this memorylessness property is saying.</p>
        <p>上次我讨论过这个问题。让我重申一下。我们有一个伯努利过程，它是一系列伯努利试验。这些是 (0,1) 随机变量，它们会永远持续下去。所以有人在看这部伯努利试验电影 B_t。在某个时候，他们说他们认为，或者发生了一些有趣的事情，你为什么不进来开始观看呢？</p><p>So I discussed it a little bit last time. Let me reiterate it. So we have a Bernoulli process, which is a
            sequence of Bernoulli trials. And these are (0,1) random variables that keep going on forever. So someone is
            watching this movie of Bernoulli trials B_t. And at some point, they say they think, or something
            interesting
            has happened, why don’t you come in and start watching?</p>
        <h2 id="unknown-274">未知</h2><h2>Unknown</h2>
        <p>所以在某个时间 t，他们会让你进来开始观察。所以你进来后看到的将是未来的试验。所以实际上你将看到的是一个随机过程，它的第一个随机变量将是你看到的第一个变量，B_(t +1)。第二个将是这个，依此类推。</p><p>So at some time t, they tell you to come in and start watching. So what you will see once you come in will be
            this future trials. So actually what you will see is a random process, whose first random variable is going
            to
            be the first one that you see, B_(t +1). The second one is going to be this, and so on.</p>
        <p>所以这就是被要求进来并开始观察的人所看到的过程。并且声称这个过程本身就是伯努利过程，前提是叫你进房间的人不展望未来。叫你进房间的人只根据他们迄今为止所看到的情况决定叫你进来。</p><p>So this is the process that’s seen by the person who’s asked to come in and start watching at that time. And
            the
            claim is that this process is itself a Bernoulli process, provided that the person who calls you into the
            room
            does not look into the future. The person who calls you into the room decides to call you in only on the
            basis
            of what they have seen so far.</p>
        <p>例如，叫你进房间的人可能有一条规则，即只要我看到连续 3 次正面朝上，我就会叫另一个人进来。所以如果他们使用这条特定规则，就意味着当你被叫进来时，前 3 次都是正面朝上。但这不会给你任何关于未来的信息。所以未来的试验将只是独立的伯努利试验。</p><p>So for example, who calls you into the room might have a rule that says, as soon as I see a sequence of 3
            heads,
            I ask the other person to come in. So if they use that particular rule, it means that when you’re called in,
            the
            previous 3 were heads. But this doesn’t give you any information about the future. And so the future ones
            will
            be just independent Bernoulli trials.</p>
        <h2 id="unknown-275">未知</h2><h2>Unknown</h2>
        <p>另一方面，如果叫你进来的人以前看过这部电影，并且他们使用了一个规则，例如，我会在 3 个头第一次出现之前叫你进来。那么这个人叫你进来是基于这两个头会是三个头的知识。如果他们有这样的远见。如果他们能展望未来。
        </p><p>If on the other hand, the person who calls you in has seen the movie before and they use a rule, such as, for
            example, I call you in just before 3 heads show up for the first time. So the person calls you in based on
            knowledge that these two would be three heads. If they have such foresight. if they can look into the
            future.
        </p>
        <p>然后 X1、X2、X3，它们肯定是三次正面，所以它们不对应于随机独立的伯努利试验。所以换言之，这个过程是无记忆的。过去发生了什么并不重要。即使你被叫进房间并在随机时间开始观察，只要那个随机时间是根据迄今为止发生的事情以因果关系确定的，情况也是如此。</p><p>Then X1, X2, X3, they’re certain to be three heads, so they do not correspond to random independent Bernoulli
            trials. So to rephrase this, the process is memoryless. It does not matter what has happened in the past.
            And
            that’s true even if you are called into the room and start watching at a random time, as long as that random
            time is determined in a causal way on the basis of what has happened so far.</p>
        <p>因此，你被叫进房间，这是因果关系，仅基于迄今为止发生的事情。从那个时间开始，你将看到的仍然是一系列独立的伯努利试验。这就是我们在这里使用的论点，本质上，是为了论证这个 T2 是一个独立于 T1 的随机变量。所以一个人在看电影时，看到了第一次成功。这是基于他们所看到的内容。</p><p>So you are called into the room in a causal manner, just based on what’s happened so far. What you’re going
            to
            see starting from that time will still be a sequence of independent Bernoulli trials. And this is the
            argument
            that we used here, essentially, to argue that this T2 is an independent random variable from T1. So a person
            is
            watching the movie, sees the first success. And on the basis of what they have seen.</p>
        <h2 id="unknown-276">未知</h2><h2>Unknown</h2>
        <p>他们刚刚看到第一次成功。他们请你进来。你进来了。你将看到一系列伯努利试验。你等待这么久，直到下一次成功。你看到的是一个伯努利过程，好像这个过程刚刚开始。这让我们相信这应该是与这个相同类型的几何随机变量，与之前发生的事情无关。</p><p>They have just seen the first success. they ask you to come in. You come in. What you’re going to see is a
            sequence of Bernoulli trials. And you wait this long until the next success comes in. What you see is a
            Bernoulli process, as if the process was just starting right now. And that convinces us that this should be
            a
            geometric random variable of the same kind as this one, as independent from what happened before.</p>
        <p>好的。所以这就是关于伯努利过程的所有内容。加上我们在上节课结束时所做的两件事，我们将两个独立的伯努利过程合并在一起，我们得到了一个伯努利过程。如果我们有一个伯努利过程，我们通过抛硬币将它分开，并将事物以某种方式发送，那么我们就会得到两个独立的伯努利过程。</p><p>All right. So this is pretty much all there is to know about the Bernoulli process. Plus the two things that
            we
            did at the end of the last lecture where we merge two independent Bernoulli processes, we get a Bernoulli
            process. If we have a Bernoulli process and we split it by flipping a coin and sending things one way or the
            other, then we get two separate Bernoulli processes.</p>
        <p>我们看到，所有这些都延续到了连续时间。我们今天的任务基本上就是处理这些连续时间变化。因此，泊松过程是伯努利过程的连续时间版本。这就是将其视为伯努利过程的动机。所以，你有一个工作是坐在银行门外的人。</p><p>And we see that all of these carry over to the continuous time. And our task for today is basically to work
            these
            continuous time variations. So the Poisson process is a continuous time version of the Bernoulli process.
            Here’s
            the motivation for considering it a Bernoulli process. So you have that person whose job is to sit outside
            the
            door of a bank.</p>
        <h2 id="unknown-277">未知</h2><h2>Unknown</h2>
        <p>他们有一张长表，每过一秒钟，就会有人进来，就标记一个 X，如果在这段时间内没有人进来，就标记其他内容。现在，银行经理是一位受过科学训练的人，他想要非常准确的结果。所以他们会告诉你，不要使用一秒钟的时间段，而要使用毫秒时间段。所以你有所有这些时间段，无论在这段时间内是否有人进来，你都可以继续填写。</p><p>And they have this long sheet, and for every one second slot, they mark an X if a person came in, or they
            mark
            something else if no one came in during that slot. Now the bank manager is a really scientifically trained
            person and wants very accurate results. So they tell you, don’t use one second slots, use milliseconds
            slots. So
            you have all those slots and you keep filling if someone arrived or not during that slot.</p>
        <p>那么，你想出了一个主意。为什么要使用毫秒时段并在每个时段中不断插入叉号或零？如果我只记录人们进​​来的确切时间，那就简单多了。所以时间是连续的。我不会在每个时段都做某事。但我没有标记时间轴，而是标记了顾客到达的时间。所以实际上不需要时段。</p><p>Well then you come up with an idea. Why use millisecond slots and keep putting crosses or zero’s into each
            slot?
            It’s much simpler if I just record the exact times when people came in. So time is continuous. I don’t keep
            doing something at every time slot. But instead of the time axis, I mark the times at which customers
            arrive. So
            there’s no real need for slots.</p>
        <p>您唯一需要的信息是什么时候有人到达。现在，我们想对这种在连续时间内发生的过程进行建模，但其特点与伯努利过程相同。这就是我们想要开发的模型。好的。那么我们将拥有哪些属性？首先，我们假设相同长度的间隔以相同的方式概率地表现。</p><p>The only information that you want is when did we have arrivals of people. And we want to now model a process
            of
            this kind happening in continuous time, that has the same flavor, however, as the Bernoulli process. So
            that’s
            the model we want to develop. OK. So what are the properties that we’re going to have? First, we’re going to
            assume that intervals over the same length behave probabilistically in an identical fashion.</p>
        <h2 id="unknown-278">未知</h2><h2>Unknown</h2>
        <p>那么这是什么意思呢？考虑某个给定长度的间隔。在该长度的间隔内，到达的次数是随机的。而该随机到达次数将具有概率分布。因此，该概率分布。让我们用这个符号来表示它。我们固定 t，我们固定持续时间。所以这是固定的。我们看看不同的 k。</p><p>So what does that mean? Think of an interval of some given length. During the interval of that length,
            there’s
            going to be a random number of arrivals. And that random number of arrivals is going to have a probability
            distribution. So that probability distribution. let’s denote it by this notation. We fix t, we fix the
            duration.
            So this is fixed. And we look at the different k’s.</p>
        <p>0 次到达的概率、1 次到达的概率、2 次到达的概率等等。所以这个东西本质上是一个 PMF。所以它应该具有这样的属性：这个 P_(k, tau) 的所有 k 的总和应该等于 1。现在，这个符号中隐藏着一个时间同质性的假设。</p><p>The probability of having 0 arrivals, the probability of 1 arrival, the probability of 2 arrivals, and so on.
            So
            this thing is essentially a PMF. So it should have the property that the sum over all k’s of this P_(k, tau)
            should be equal to 1. Now, hidden inside this notation is an assumption of time homogeneity.</p>
        <p>也就是说，到达人数的概率分布仅取决于间隔的长度，而不取决于间隔在时间轴上的确切位置。也就是说，如果我取一个长度为 tau 的间隔，我会询问这个间隔内的到达人数。然后我取另一个长度为 tau 的间隔，我会询问该间隔内的到达人数。</p><p>That is, this probability distribution for the number of arrivals only depends on the length of the interval,
            but
            not the exact location of the interval on the time axis. That is, if I take an interval of length tau, and I
            ask
            about the number of arrivals in this interval. And I take another interval of length tau, and I ask about
            the
            number of arrivals during that interval.</p>
        <h2 id="unknown-279">未知</h2><h2>Unknown</h2>
        <p>到达这里的人数和到达那里的人数具有相同的概率分布，用这种方式表示。因此，到达这里的统计行为与到达那里的统计行为相同。这与伯努利过程有什么关系？它非常类似于伯努利过程的假设，即在不同的时段，我们具有相同的成功概率。每个时段在概率上看起来都与其他时段一样。</p><p>Number of arrivals here, and number of arrivals there have the same probability distribution, which is
            denoted
            this way. So the statistical behavior of arrivals here is the same as the statistical behavioral of arrivals
            there. What’s the relation with the Bernoulli process? It’s very much like the assumption. the Bernoulli
            process. that in different slots, we have the same probability of success. Every slot looks
            probabilistically as
            any other slot.</p>
        <p>因此，类似地，任何长度为 tau 的间隔在概率上看起来都与任何其他长度为 tau 的间隔一样。并且该间隔内的到达次数是由这些概率描述的随机变量。这里的到达次数是由这些相同概率描述的随机变量。所以这是我们的第一个假设。然后还有什么？在伯努利过程中，我们假设不同的时间段彼此独立。</p><p>So similarly here, any interval of length tau looks probabilistically as any other interval of length tau.
            And
            the number of arrivals during that interval is a random variable described by these probabilities. Number of
            arrivals here is a random variable described by these same probabilities. So that’s our first assumption.
            Then
            what else? In the Bernoulli process we had the assumption that different time slots were independent of each
            other.</p>
        <p>这里我们没有时间段，但我们仍然可以用类似的方式思考并施加以下假设，即这些联合时间间隔在统计上是独立的。这是什么意思？在这个间隔内到达的随机数，以及在这个间隔内到达的随机数，以及在这个间隔内到达的随机数。所以这是三个不同的随机变量。这三个随机变量彼此独立。</p><p>Here we do not have time slots, but we can still think in a similar way and impose the following assumption,
            that
            these joint time intervals are statistically independent. What does that mean? Does a random number of
            arrivals
            during this interval, and the random number of arrivals during this interval, and the random number of
            arrivals
            during this interval. so these are three different random variables. these three random variables are
            independent of each other.</p>
        <h2 id="unknown-280">未知</h2><h2>Unknown</h2>
        <p>我们在这里收到的到达次数与在那里收到的到达次数无关。所以这类似于说不同的时间段是独立的。这就是我们在离散时间中所做的。连续时间模拟就是这种独立性假设。例如，具体来说，这里的到达次数与那里的到达次数无关。所以这是关于这个过程的两个基本假设。</p><p>How many arrivals we got here is independent from how many arrivals we got there. So this is similar to
            saying
            that different time slots were independent. That’s what we did in discrete time. The continuous time analog
            is
            this independence assumption. So for example, in particular, number of arrivals here is independent from the
            number of arrivals there. So these are two basic assumptions about the process.</p>
        <p>现在，为了最终写下关于这个概率分布的公式。这是我们的下一个目标，我们想具体谈谈这个到达人数的分布。我们需要在问题中添加一些结构。我们将做出以下假设。如果我们查看长度为 delta 的时间间隔。delta 现在应该是一个小数字，所以像这样的图片。</p><p>Now in order to write down a formula, eventually, about this probability distribution. which is our next
            objective, we would like to say something specific about this distribution of number of arrivals. we need to
            add
            a little more structure into the problem. And we’re going to make the following assumption. If we look at
            the
            time interval of length delta. and delta now is supposed to be a small number, so a picture like this.</p>
        <p>在非常小的时间间隔内，我们得到恰好一个到达的概率是 lambda 乘以 delta。Delta 是间隔的长度，lambda 是比例因子，是到达过程的强度。lambda 越大意味着在小间隔内更有可能得到一个到达。因此，有 1 个到达的概率是 lambda 乘以 delta。剩余的概率为 0 个到达。</p><p>During a very small time interval, there is a probability that we get exactly one arrival, which is lambda
            times
            delta. Delta is the length of the interval and lambda is a proportionality factor, which is sort of the
            intensity of the arrival process. Bigger lambda means that a little interval is more likely to get an
            arrival.
            So there’s a probability lambda times delta of 1 arrival. The remaining probability goes to 0 arrivals.</p>
        <h2 id="unknown-281">未知</h2><h2>Unknown</h2>
        <p>当 delta 很小时，2 次到达的概率可以近似为 0。所以这是对在很小的时隙内发生的情况的描述。现在，当 delta 非常小时，这在某种限制意义上应该是正确的。所以这个陈述的确切版本是，这是一个等式，加上 delta 平方项的顺序。所以这是一个近似等式。</p><p>And when delta is small, the probability of 2 arrivals can be approximated by 0. So this is a description of
            what
            happens during a small, tiny slot. Now this is something that’s supposed to be true in some limiting sense,
            when
            delta is very small. So the exact version of this statement would be that this is an equality, plus order of
            delta squared terms. So this is an approximate equality.</p>
        <p>近似意味着在小 delta 的极限下，主导项。常数和一阶项由此给出。现在，当 delta 非常小时，delta 中的二阶项无关紧要。它们与一阶项相比很小。所以我们忽略这一点。因此，您可以考虑一个精确的关系，即概率由此给出，加上 delta 平方项。</p><p>And what approximation means is that in the limit of small deltas, the dominant terms. the constant and the
            first
            order term are given by this. Now when delta is very small, second order terms in delta do not matter. They
            are
            small compared to first order terms. So we ignore this. So you can either think in terms of an exact
            relation,
            which is the probabilities are given by this, plus delta squared terms.</p>
        <p>或者，如果你想更宽松一点，你可以在这里写成近似等式。理解就是这个等式成立。当 delta 趋近于 0 时，近似变得越来越正确。因此，该陈述的另一个版本是，如果取 delta 趋近于 0 时的极限 p，即在长度为 delta 的间隔内有 1 个到达的概率，除以 delta，这等于 lambda。</p><p>Or if you want to be a little more loose, you just write here, as an approximate equality. And the
            understanding
            is that this equality holds. approximately becomes more and more correct as delta goes to 0. So another
            version
            of that statement would be that if you take the limit as delta goes to 0, of p, the probability of having 1
            arrival in an interval of length delta, divided by delta, this is equal to lambda.</p>
        <h2 id="unknown-282">未知</h2><h2>Unknown</h2>
        <p>因此，这将是我们在此假设的确切陈述的一个版本。因此，我们将这个 lambda 称为到达率，或过程的强度。显然，如果你将 lambda 加倍，那么在一段很小的间隔内，你期望获得到达的概率可能会加倍。因此，从某种意义上说，我们的到达过程强度增加了一倍。</p><p>So that would be one version of an exact statement of what we are assuming here. So this lambda, we call it
            the
            arrival rate, or the intensity of the process. And clearly, if you double lambda, then a little interval is
            likely you expect to get the probability of obtaining an arrival during that interval has doubled. So in
            some
            sense we have twice as intense arrival process.</p>
        <p>如果您查看 delta 间隔内的到达次数，该随机变量的预期值是多少？以概率 lambda delta 计算，我们得到 1 次到达。以剩余概率计算，我们得到 0 次到达。所以它只是 lambda 乘以 delta。因此，在小间隔内的预期到达次数是 lambda 乘以 delta。因此，预期到达次数与 lambda 成正比，这也是我们将 lambda 称为到达率的原因。
        </p><p>If you look at the number of arrivals during delta interval, what is the expected value of that random
            variable?
            Well with probability lambda delta we get 1 arrival. And with the remaining probability, we get 0 arrivals.
            So
            it’s just lambda times delta. So expected number of arrivals during a little interval is lambda times delta.
            So
            expected number of arrivals is proportional to lambda, and that’s again why we call lambda the arrival rate.
        </p>
        <p>如果你把 delta 代入这个等式的分母，它告诉你 lambda 是单位时间内的预期到达次数。所以到达率是单位时间内的预期到达次数。这再次证明了为什么我们称 lambda 为这个过程的强度。好的。那么我们现在在哪里？</p><p>If you send delta to the denominator in this equality, it tells you that lambda is the expected number of
            arrivals per unit time. So the arrival rate is expected number of arrivals per unit time. And again, that
            justifies why we call lambda the intensity of this process. All right. So where are we now?</p>
        <h2 id="unknown-283">未知</h2><h2>Unknown</h2>
        <p>对于伯努利过程，给定长度为 n 的间隔内到达的次数具有 PMF，我们知道它是二项式 PMF。连续时间过程的相应 PMF 公式是什么？我们想以某种方式使用我们的假设并得出这个数量的公式。所以这告诉我们在某个一般长度的间隔内到达次数的分布。</p><p>For the Bernoulli process, the number of arrivals during a given interval of length n had the PMF that we
            knew it
            was the binomial PMF. What is the formula for the corresponding PMF for the continuous time process? Somehow
            we
            would like to use our assumptions and come up with the formula for this quantity. So this tells us about the
            distribution of number of arrivals during an interval of some general length.</p>
        <p>我们已经假设了短间隔内到达的次数。长间隔由许多短间隔组成，所以也许这是可行的方法。取一个大间隔，并将其分成许多短间隔。这样我们就得到了时间轴。我们有一个长度为 tau 的间隔。我将把它分成许多长度为 delta 的小间隔。</p><p>We have made assumptions about the number of arrivals during an interval of small length. An interval of big
            length is composed of many intervals of small length, so maybe this is the way to go. Take a big interval,
            and
            split it into many intervals of small length. So we have here our time axis. And we have an interval of
            length
            tau. And I’m going to split it into lots of little intervals of length delta.</p>
        <p>那么我们将有多少个间隔呢？间隔数将是总时间除以 delta。现在，在每个小间隔期间会发生什么？只要间隔很小，那么在一个间隔期间，您将有 0 个或 1 个到达。在小间隔期间超过 1 个到达的概率可以忽略不计。</p><p>So how many intervals are we going to have? The number of intervals is going to be the total time, divided by
            delta. Now what happens during each one of these little intervals? As long as the intervals are small, what
            you
            have is that during an interval, you’re going to have either 0 or 1 arrival. The probability of more than 1
            arrival during a little interval is negligible.</p>
        <h2 id="unknown-284">未知</h2><h2>Unknown</h2>
        <p>因此，从这张图片中，您基本上可以看到一个由许多次试验组成的伯努利过程。在每一次试验中，我们都有一个成功概率，即 lambda 乘以 delta。这里的不同小间隔彼此独立。这是我们的假设之一，即这些联合时间间隔是独立的。因此，大致而言，我们得到的是一个伯努利过程。我们有独立性。我们有感兴趣的时隙数。</p><p>So with this picture, you have essentially a Bernoulli process that consists of so many trials. And during
            each
            one of those trials, we have a probability of success, which is lambda times delta. Different little
            intervals
            here are independent of each other. That’s one of our assumptions, that these joint time intervals are
            independent. So approximately, what we have is a Bernoulli process. We have independence. We have the number
            of
            slots of interest.</p>
        <p>在每个时间段内，我们都有一定的成功概率。因此，如果我们将其视为泊松过程的另一个良好近似值。随着 delta 趋近于 0，近似值变得越来越准确
        </p><p>And during each one of the slots we have a certain probability of success. So if we think of this as another
            good
            approximation of the Poisson process. with the approximation becoming more and more accurate as delta goes
            to 0
        </p>
        <p>我们应该做的是采用伯努利过程中到达次数的 PMF 公式，然后取 delta 趋近于 0 时的极限。因此，在伯努利过程中，k 次到达的概率为 n 选 k，然后得到 p 的 k 次方。现在在我们的例子中，我们有 lambda 乘以 delta，delta 是 tau 除以 n。&nbsp;</p><p>What we should do would be to take the formula for the PMF of number of arrivals in a Bernoulli process, and
            then
            take the limit as delta goes to 0. So in the Bernoulli process, the probability of k arrivals is n choose k,
            and
            then you have p to the k. Now in our case, we have here lambda times delta, delta is tau over n.&nbsp;</p>
        <h2 id="unknown-285">未知</h2><h2>Unknown</h2>
        <p>Delta 是 tau 除以 n，所以 p 是 lambda 乘以 tau 除以 n。所以这里是 p lambda tau 除以 n 的 k 次方，然后乘以 1 减去这个。这就是 1 减去 p 的 n 次方 k。所以这是伯努利过程的精确公式。</p><p>Delta is tau over n, so p is lambda times tau divided by n.&nbsp;So here’s our p Lambda tau over n to the power k,
            and
            then times one minus this. this is our one minus p.&nbsp;to the power n k. So this is the exact formula for the
            Bernoulli process.</p>
        <p>对于泊松过程，我们的做法是采用该公式，让 delta 趋于 0。当 delta 趋于 0 时，n 趋于无穷大。这就是我们要取的极限。另一方面，这个表达式 lambda 乘以 tau。lambda 乘以 tau，它会是什么？lambda 乘以 tau 等于 n 乘以 p。n 乘以 p，这是我想要的吗？不，让我们看看。lambda tau 是 np。是的。</p><p>For the Poisson process, what we do is we take that formula and we let delta go to 0. As delta goes to 0, n
            goes
            to infinity. So that’s the limit that we’re taking. On the other hand, this expression lambda times tau.
            lambda
            times tau, what is it going to be? Lambda times tau is equal to n times p.&nbsp;n times p, is that what I want?
            No,
            let’s see. Lambda tau is np. Yeah.</p>
        <p>所以 lambda tau 是 np。好的。所以我们有这个关系，lambda tau 等于 np。这两个数字相等是有道理的。np 是伯努利过程中预期的成功次数。Lambda tau。由于 lambda 是到达率，并且您有 tau 的总时间，因此您可以将 lambda tau 视为伯努利过程中预期的到达次数。</p><p>So lambda tau is np. All right. So we have this relation, lambda tau equals np. These two numbers being equal
            kind of makes sense. np is the expected number of successes you’re going to get in the Bernoulli process.
            Lambda
            tau. since lambda is the arrival rate and you have a total time of tau, lambda tau you can think of it as
            the
            number of expected arrivals in the Bernoulli process.</p>
        <h2 id="unknown-286">未知</h2><h2>Unknown</h2>
        <p>我们对泊松过程进行​​伯努利近似。我们采用伯努利公式，然后取 n 趋于无穷时的极限。现在 λ tau 除以 n 等于 p，因此很明显这个项会给我们什么。这只是 p 的 k 次方。实际上需要做的工作比这多一点。</p><p>We’re doing a Bernoulli approximation to the Poisson process. We take the formula for the Bernoulli, and now
            take
            the limit as n goes to infinity. Now lambda tau over n is equal to p, so it’s clear what this term is going
            to
            give us. This is just p to the power k. It will actually take a little more work than that.</p>
        <p>现在我不打算做代数运算，但我只告诉您，当 n 趋于无穷大时，可以取此公式中的极限。这将为您提供另一个公式，即泊松 PMF 的最终公式。需要注意的一点是，这里有一个类似于 1 减去 n 的常数的 n 次方。&nbsp;</p><p>Now I’m not going to do the algebra, but I’m just telling you that one can take the limit in this formula
            here,
            as n goes to infinity. And that will give you another formula, the final formula for the Poisson PMF. One
            thing
            to notice is that here you have something like 1 minus a constant over n, to the power n.&nbsp;</p>
        <p>你可能还记得微积分中有一个这样的公式，即它收敛到 e 的负 c。如果你还记得微积分中的这个公式，那么你就会期望在这里，在极限中，你会得到类似于 e 的负 lambda tau 的值。所以事实上，我们会得到这样一个项。需要做一些工作来找到这个表达式的极限，乘以这个表达式。</p><p>And you may recall from calculus a formula of this kind, that this converges to e to the minus c.&nbsp;If you
            remember
            that formula from calculus, then you will expect that here, in the limit, you are going to get something
            like an
            e to the minus lambda tau. So indeed, we will get such a term. There is some work that needs to be done to
            find
            the limit of this expression, times that expression.</p>
        <h2 id="unknown-287">未知</h2><h2>Unknown</h2>
        <p>代数并不难，它就在课本里。我们就不多花时间做这件事了。我先给你最后得到的公式。最后得到的公式是这种形式。所以这里重要的不是你要从这个公式到那个公式要进行的具体代数运算。这很简单。</p><p>The algebra is not hard, it’s in the text. Let’s not spend more time doing this. But let me just give you the
            formula of what comes at the end. And the formula that comes at the end is of this form. So what matters
            here is
            not so much the specific algebra that you will do to go from this formula to that one. It’s kind of
            straightforward.</p>
        <p>重要的是，根据定义，泊松过程可以用伯努利过程来近似，其中我们有大量的时间槽。n 趋向于无穷大。而我们在每个时间槽中成功的概率非常小。因此，虽然有大量的时间槽，但每个时间槽中成功的概率很小。并且随着时间槽越来越小，我们取极限。</p><p>What’s important is the idea that the Poisson process, by definition, can be approximated by a Bernoulli
            process
            in which we have a very large number of slots. n goes to infinity. Whereas we have a very small probability
            of
            success during each time slot. So a large number of slots, but tiny probability of success during each slot.
            And
            we take the limit as the slots become smaller and smaller.</p>
        <p>因此，通过这种近似，我们最终得到了这个特定公式。这就是所谓的泊松 PMF。现在这个函数 P 有两个参数。重要的是要意识到，当你把它看作 PMF 时，你将 t 固定为 tau。对于固定的 tau，现在这就是 PMF。</p><p>So with this approximation we end up with this particular formula. And this is the so called Poisson PMF. Now
            this function P here has two arguments. The important thing to realize is that when you think of this as a
            PMF,
            you fix t to tau. And for a fixed tau, now this is a PMF.</p>
        <h2 id="unknown-288">未知</h2><h2>Unknown</h2>
        <p>正如我之前所说，k 之和必须等于 1。因此，对于给定的 tau，这些概率加起来等于 1。这个公式有点混乱，但也不是太混乱。人们可以毫不费力地使用它。这个 PMF 的均值和方差是多少？那么预期到达人数是多少？</p><p>As I said before, the sum over k has to be equal to 1. So for a given tau, these probabilities add up to 1.
            The
            formula is moderately messy, but not too messy. One can work with it without too much pain. And what’s the
            mean
            and variance of this PMF? Well what’s the expected number of arrivals?</p>
        <p>如果你想想这个伯努利类比，我们知道伯努利过程中的预期到达次数是 n 乘以 p。在我们在这个过程中使用的近似值中，n 乘以 p 与 lambda tau 相同。这就是为什么我们得到 lambda tau 作为预期到达次数。这里我使用 t 而不是 tau。预期到达次数是 lambda t。</p><p>If you think of this Bernoulli analogy, we know that the expected number of arrivals in the Bernoulli process
            is
            n times p.&nbsp;In the approximation that we’re using in these procedure, n times p is the same as lambda tau.
            And
            that’s why we get lambda tau to be the expected number of arrivals. Here I’m using t instead of tau. The
            expected number of arrivals is lambda t.</p>
        <p>因此，如果将时间加倍，预计到达人数也会加倍。如果将到达率加倍，预计到达人数也会加倍。方差公式如何？伯努利过程的方差是 np，乘以 1 减 p。极限中这等于多少？在我们取的极限中，当 delta 趋于零时，p 也趋于零。</p><p>So if you double the time, you expect to get twice as many arrivals. If you double the arrival rate, you
            expect
            to get twice as many arrivals. How about the formula for the variance? The variance of the Bernoulli process
            is
            np, times one minus p.&nbsp;What does this go to in the limit? In the limit that we’re taking, as delta goes to
            zero,
            then p also goes to zero.</p>
        <h2 id="unknown-289">未知</h2><h2>Unknown</h2>
        <p>任何给定时段的成功概率都会趋近于零。因此，该项变得微不足道。因此，它变为 n 乘以 p，也就是 lambda t 或 lambda tau。因此，方差不再是伯努利过程的更复杂公式，而是简化为 lambda t。有趣的是，泊松过程中的方差与预期值完全相同。</p><p>The probability of success in any given slot goes to zero. So this term becomes insignificant. So this
            becomes n
            times p, which is again lambda t, or lambda tau. So the variance, instead of having this more complicated
            formula of the variance is the Bernoulli process, here it gets simplified and it’s lambda t. So
            interestingly,
            the variance in the Poisson process is exactly the same as the expected value.</p>
        <p>所以你可以把这看作是一个有趣的巧合。现在我们要采用这个公式，看看如何使用它。首先，我们要举一个非常简单、直接的例子。15 年前，当这个例子被提出时，电子邮件的发送速度是每小时 5 封。我希望今天也是这样。现在，电子邮件的发送速度，比如说白天。</p><p>So you can look at this as just some interesting coincidence. So now we’re going to take this formula and see
            how
            to use it. First we’re going to do a completely trivial, straightforward example. So 15 years ago when that
            example was made, email was coming at a rate of five messages per hour. I wish that was the case today. And
            now
            emails that are coming in, let’s say during the day.</p>
        <p>电子邮件的到达率在一天中的不同时间可能有所不同。但如果你固定一个时间段，比如说下午 1:00 到 2:00，那么到达率可能就是恒定的。而且电子邮件的到达可以用泊松过程很好地建模。说到建模，不仅仅是电子邮件的到达。只要电子邮件的到达是完全随机的，没有任何额外的结构，泊松过程就是这些到达的一个很好的模型。</p><p>The arrival rates of emails are probably different in different times of the day. But if you fix a time slot,
            let’s say 1:00 to 2:00 in the afternoon, there’s probably a constant rate. And email arrivals are reasonably
            well modeled by a Poisson process. Speaking of modeling, it’s not just email arrivals. Whenever arrivals
            happen
            in a completely random way, without any additional structure, the Poisson process is a good model of these
            arrivals.</p>
        <h2 id="unknown-290">未知</h2><h2>Unknown</h2>
        <p>因此，车祸发生的时间是一个泊松过程。如果你有一个非常非常弱的光源，每次只发射一个光子，这些光子发射的时间也可以用泊松过程很好地模拟出来。所以它是完全随机的。或者如果你有一种放射性物质，每次一个原子都会随机变化。所以这是一个非常缓慢的放射性衰变。</p><p>So the times at which car accidents will happen, that’s a Poisson processes. If you have a very, very weak
            light
            source that’s shooting out photons, just one at a time, the times at which these photons will go out is well
            modeled again by a Poisson process. So it’s completely random. Or if you have a radioactive material where
            one
            atom at a time changes at random times. So it’s a very slow radioactive decay.</p>
        <p>这些阿尔法粒子（或我们所发射的任何粒子）的发射时间将再次由泊松过程描述。因此，如果您的到达或发射发生在完全随机的时间，并且偶尔会收到到达或事件，那么泊松过程就是这些事件的一个非常好的模型。回到电子邮件。以每天每小时五封邮件的速度接收它们。</p><p>The time at which these alpha particles, or whatever we get emitted, again is going to be described by a
            Poisson
            process. So if you have arrivals, or emissions, that happen at completely random times, and once in a while
            you
            get an arrival or an event, then the Poisson process is a very good model for these events. So back to
            emails.
            Get them at a rate of five messages per day, per hour.</p>
        <p>30 分钟就是半小时。所以我们得到的是 lambda t，到达的总数量是。预期到达的数量是。lambda 是 5，t 是半，如果我们谈论的是小时。所以 lambda t 是 2 的 0.5 次方。在长度为 t 的时间间隔内，没有新消息的概率是零的概率，在我们的例子中，是半。</p><p>In 30 minutes this is half an hour. So what we have is that lambda t, total number of arrivals is. the
            expected
            number of arrivals is. lambda is five, t is one half, if we talk about hours. So lambda t is two to the 0.5.
            The
            probability of no new messages is the probability of zero, in time interval of length t, which, in our case,
            is
            one half.</p>
        <h2 id="unknown-291">未知</h2><h2>Unknown</h2>
        <p>然后我们回顾一下上一张幻灯片中的公式，零次到达的概率是 lambda t 的零次方，除以零的阶乘，然后是 lambda t 的 e。然后代入我们得到的数字。lambda t 的零次方是 1。零的阶乘是 1。</p><p>And then we look back into the formula from the previous slide, and the probability of zero arrivals is
            lambda t
            to the power zero, divided by zero factorial, and then an e to the lambda t. And you plug in the numbers
            that we
            have. Lambda t to the zero power is one. Zero factorial is one.</p>
        <p>因此，我们只剩下 e 的负 2.5 倍。这个数字是 0.08。同样，您可以要求您在半小时内收到一条消息的概率。那就是。半小时内收到一条消息的概率。将是 lambda t 的一次方，除以 1 的阶乘，e 的负 lambda t，也就是。因为我们现在得到了额外的 lambda t 因子。</p><p>So we’re left with e to the minus 2.5. And that number is 0.08. Similarly, you can ask for the probability
            that
            you get exactly one message in half an hour. And that would be. the probability of one message in one half
            an
            hour. is going to be lambda t to the first power, divided by 1 factorial, e to the minus lambda t, which. as
            we
            now get the extra lambda t factor.</p>
        <p>将是 2.5，e 的负 2.5 次方。数值答案是 0.20。这就是我们在上一张幻灯片中介绍的泊松分布的 PMF 公式的使用方法。好的。这就是到达次数的分布。上次我们还做了什么？上次我们还讨论了第 k 次到达所需的时间。好的。</p><p>Is going to be 2.5, e to the minus 2.5. And the numerical answer is 0.20. So this is how you use the PMF
            formula
            for the Poisson distribution that we had in the previous slide. All right. So this was all about the
            distribution of the number of arrivals. What else did we do last time? Last time we also talked about the
            time
            it takes until the k th arrival. OK.</p>
        <h2 id="unknown-292">未知</h2><h2>Unknown</h2>
        <p>让我们试着弄清楚这个特定分布的一些情况。我们可以使用与上次完全相同的参数来推导出第 k 次到达时间的分布。所以现在第 k 次到达时间是一个连续随机变量。所以它有一个 PDF。因为我们处于连续时间中，所以到达可能发生在任何时间。所以 Yk 是一个连续随机变量。</p><p>So let’s try to figure out something about this particular distribution. We can derive the distribution of
            the
            time of the k th arrival by using the exact same argument as we did last time. So now the time of the k th
            arrival is a continuous random variable. So it has a PDF. Since we are in continuous time, arrivals can
            happen
            at any time. So Yk is a continuous random variable.</p>
        <p>但现在让我们考虑一个长度为小 delta 的时间间隔。并使用我们通常对 PDF 的解释。在特定时间评估的随机变量的 PDF 乘以 delta，这是 Yk 落在这个小间隔内的概率。所以正如我之前所说，这是思考 PDF 的最佳方式。PDF 为您提供小间隔的概率。现在让我们尝试计算这个概率。</p><p>But now let’s think of a time interval of length little delta. And use our usual interpretation of PDFs. The
            PDF
            of a random variable evaluated at a certain time times delta, this is the probability that the Yk falls in
            this
            little interval. So as I’ve said before, this is the best way of thinking about PDFs. PDFs give you
            probabilities of little intervals. So now let’s try to calculate this probability.</p>
        <p>为了使第 k 次到达发生在这个小间隔内，我们需要两件事。我们需要一个到达发生在这个间隔内，并且我们需要 k 减 1 次到达发生在这个间隔内。好的。你会告诉我，但我们可能让 k 减 1 次到达发生在这里，让第 k 次到达发生在这里。原则上，这是可能的。</p><p>For the k th arrival to happen inside this little interval, we need two things. We need an arrival to happen
            in
            this interval, and we need k minus one arrivals to happen during that interval. OK. You’ll tell me, but it’s
            possible that we might have the k minus one arrival happen here, and the k th arrival to happen here. In
            principle, that’s possible.</p>
        <h2 id="unknown-293">未知</h2><h2>Unknown</h2>
        <p>但在极限情况下，当我们将 delta 取得很小的时候，在同一个小时隙中有两个到达的概率可以忽略不计。因此，假设在同一个小时隙中不会发生两个到达，那么要在这里发生第 k 个到达，我们必须在此间隔内有 k - 1。</p><p>But in the limit, when we take delta very small, the probability of having two arrivals in the same little
            slot
            is negligible. So assuming that no two arrivals can happen in the same mini slot, then for the k th one to
            happen here, we must have k minus one during this interval.</p>
        <p>现在，因为我们假设这些联合间隔彼此独立，所以这分解为在从零到 t 的间隔内恰好有 k 减 1 个到达的概率乘以在那个小间隔内恰好有 1 个到达的概率，即 lambda delta。</p><p>Now because we have assumed that these joint intervals are independent of each other, this breaks down into
            the
            probability that we have exactly k minus one arrivals, during the interval from zero to t, times the
            probability
            of exactly one arrival during that little interval, which is lambda delta.</p>
        <p>我们在上一张幻灯片中确实有一个公式，即 lambda t，到 k 减 1，除以 k 减一个阶乘，乘以 e 减去 lambda t。然后 lambda 乘以 delta。我漏掉了什么吗？是的，好的。好的。现在你用那个 delta 取消这个 delta。这给了我们一个 PDF 公式，用于计算第 k 次到达的时间。当然，这个 PDF 取决于数字 k。</p><p>We do have a formula for this from the previous slide, which is lambda t, to the k minus 1, over k minus one
            factorial, times e to minus lambda t. And then lambda times delta. Did I miss something? Yeah, OK. All
            right.
            And now you cancel this delta with that delta. And that gives us a formula for the PDF of the time until the
            k
            th arrival. This PDF, of course, depends on the number k.</p>
        <h2 id="unknown-294">未知</h2><h2>Unknown</h2>
        <p>第一次到达将发生在这个时间范围内的某个时间。所以这是它的 PDF。第二次到达当然会发生在稍后。PDF 是这样的。所以它更有可能在这些时间附近​​发生。第三次到达有这个 PDF，所以它更有可能在这些时间附近​​发生。如果你取 k 等于 100，你可能会得到一个 PDF。</p><p>The first arrival is going to happen somewhere in this range of time. So this is the PDF that it has. The
            second
            arrival, of course, is going to happen later. And the PDF is this. So it’s more likely to happen around
            these
            times. The third arrival has this PDF, so it’s more likely to happen around those times. And if you were to
            take
            k equal to 100, you might get a PDF.</p>
        <p>第 k 次到达极不可能发生在一开始，它可能发生在未来的某个地方。因此，根据我们讨论的具体到达，它具有不同的概率分布。当然，第 100 次到达的时间预计会比第一次到达的时间大得多。顺便说一句，第一次到达的时间有一个 PDF，其形式非常简单。</p><p>It’s extremely unlikely that the k th arrival happens in the beginning, and it might happen somewhere down
            there,
            far into the future. So depending on which particular arrival we’re talking about, it has a different
            probability distribution. The time of the 100th arrival, of course, is expected to be a lot larger than the
            time
            of the first arrival. Incidentally, the time of the first arrival has a PDF whose form is quite simple.</p>
        <p>如果在这里让 k 等于 1，这个项就消失了。那个项变成了 1。剩下的只有 lambda，e 的负 lambda。你知道，这是指数分布。所以泊松过程中第一次到达的时间是指数分布。伯努利过程中第一次到达的时间是多少？它是一个几何分布。嗯，并非巧合的是，这两个看起来很像另一个。
        </p><p>If you let k equal to one here, this term disappears. That term becomes a one. You’re left with just lambda,
            e to
            the minus lambda. And you recognize it, it’s the exponential distribution. So the time until the first
            arrival
            in a Poisson process is an exponential distribution. What was the time of the first arrival in the Bernoulli
            process? It was a geometric distribution. Well, not coincidentally, these two look quite a bit like the
            other.
        </p>
        <h2 id="unknown-295">未知</h2><h2>Unknown</h2>
        <p>几何分布具有这种形状。指数分布具有那种形状。几何分布只是指数分布的离散版本。在伯努利分布的情况下，我们处于离散时间中。我们有一个 PMF，表示首次到达的时间，它是几何的。在泊松分布的情况下，我们得到的是几何的极限，因为您让这些线越来越近，这给出了指数分布。
        </p><p>A geometric distribution has this kind of shape. The exponential distribution has that kind of shape. The
            geometric is just a discrete version of the exponential. In the Bernoulli case, we are in discrete time. We
            have
            a PMF for the time of the first arrival, which is geometric. In the Poisson case, what we get is the limit
            of
            the geometric as you let those lines become closer and closer, which gives you the exponential distribution.
        </p>
        <p>现在，泊松过程具有伯努利过程的所有无记忆性。人们可以就此图进行论证。由于泊松过程是伯努利过程的极限，因此伯努利过程中的任何定性过程对于泊松过程都有效。特别是，我们具有这种无记忆性。让泊松过程运行一段时间，然后开始观察它。</p><p>Now the Poisson process shares all the memorylessness properties of the Bernoulli process. And the way one
            can
            argue is just in terms of this picture. Since the Poisson process is the limit of Bernoulli processes,
            whatever
            qualitative processes you have in the Bernoulli process remain valid for the Poisson process. In particular
            we
            have this memorylessness property. You let the Poisson process run for some time, and then you start
            watching
            it.</p>
        <p>过去发生的一切对未来都没有影响。从现在开始，未来将要发生的事情再次用泊松过程来描述，在长度为 delta 的每个小时间段内，都有 lambda delta 的概率到达。并且这个 lambda delta 可能是一样的。始终是 lambda delta。无论过去发生了什么。</p><p>What ever happened in the past has no bearing about the future. Starting from right now, what’s going to
            happen
            in the future is described again by a Poisson process, in the sense that during every little slot of length
            delta, there’s going to be a probability of lambda delta of having an arrival. And that probably lambda
            delta is
            the same. is always lambda delta. no matter what happened in the past of the process.</p>
        <h2 id="unknown-296">未知</h2><h2>Unknown</h2>
        <p>具体来说，我们可以用这个论点来说明，第 k 次到达的时间就是第一次到达所需的时间。好的，让我对 k 等于 2 的情况进行同样的操作。然后在第一次到达之后，等待一段时间，直到第二次到达。现在，一旦第一次到达，就过去了。你开始观察。</p><p>And in particular, we could use this argument to say that the time until the k th arrival is the time that it
            takes for the first arrival to happen. OK, let me do it for k equal to two. And then after the first arrival
            happens, you wait a certain amount of time until the second arrival happens. Now once the first arrival
            happened, that’s in the past. You start watching.</p>
        <p>从现在开始，您将拥有长度为 delta 的迷你时隙，每个迷你时隙的成功概率为 lambda delta。这就像我们从头开始泊松过程一样。因此从那时开始，到下一次到达的时间将再次呈指数分布，它不关心过去发生了什么，也不关心第一次到达花了多长时间。</p><p>From now on you have mini slots of length delta, each one having a probability of success lambda delta. It’s
            as
            if we started the Poisson process from scratch. So starting from that time, the time until the next arrival
            is
            going to be again an exponential distribution, which doesn’t care about what happened in the past, how long
            it
            took you for the first arrival.</p>
        <p>因此，这两个随机变量将是独立的，并且是指数的，具有相同的参数 lambda。因此，除其他事项外，我们在这里所做的就是，我们基本上推导出了 k 个独立指数之和的 PDF。第 k 次到达的时间是 k 个到达间隔时间之和。由于无记忆性，到达间隔时间都是彼此独立的。它们都具有相同的指数分布。
        </p><p>So these two random variables are going to be independent and exponential, with the same parameter lambda. So
            among other things, what we have done here is we have essentially derived the PDF of the sum of k
            independent
            exponentials. The time of the k th arrival is the sum of k inter arrival times. The inter arrival times are
            all
            independent of each other because of memorylessness. And they all have the same exponential distribution.
        </p>
        <h2 id="unknown-297">未知</h2><h2>Unknown</h2>
        <p>顺便说一句，这为您提供了一种模拟泊松过程的方法。如果您想在计算机上模拟它，您可以选择将时间分成非常小的时间段。对于每个微小的时间段，使用随机数生成器来决定是否有到达。为了获得非常准确的结果，您必须使用非常小的时间段。因此，这将需要大量计算。</p><p>And by the way, this gives you a way to simulate the Poisson process. If you wanted to simulate it on your
            computer, you would have one option to break time into tiny, tiny slots. And for every tiny slot, use your
            random number generator to decide whether there was an arrival or not. To get it very accurate, you would
            have
            to use tiny, tiny slots. So that would be a lot of computation.</p>
        <p>模拟泊松过程的更巧妙的方法是使用随机数生成器从指数分布中生成样本，并将其称为首次到达时间。然后返回随机数生成器，再次从相同的指数分布中生成另一个独立样本。这是第一次到达和第二次到达之间的时间，然后继续这样做。因此，作为一种快速总结，这就是大局。</p><p>The more clever way of simulating the Poisson process is you use your random number generator to generate a
            sample from an exponential distribution and call that your first arrival time. Then go back to the random
            number
            generator, generate another independent sample, again from the same exponential distribution. That’s the
            time
            between the first and the second arrival, and you keep going that way. So as a sort of a quick summary, this
            is
            the big picture.</p>
        <p>这张表并没有告诉你任何新东西。但最好将其作为参考，并查看它，以确保您了解所有不同的框的含义。基本上，伯努利过程在离散时间内运行。泊松过程在连续时间内运行。到达率、每次试验的 p 或单位时间的强度都有类似之处。我们确实推导了或勾勒出了到达次数 PMF 的推导过程。</p><p>This table doesn’t tell you anything new. But it’s good to have it as a reference, and to look at it, and to
            make
            sure you understand what all the different boxes are. Basically the Bernoulli process runs in discrete time.
            The
            Poisson process runs in continuous time. There’s an analogy of arrival rates, p per trial, or intensity per
            unit
            time. We did derive, or sketched the derivation for the PMF of the number of arrivals.</p>
        <h2 id="unknown-298">未知</h2><h2>Unknown</h2>
        <p>泊松分布，也就是我们得到的分布，这个 Pk 为 t。当我们以这种特定方式取极限时，Pk 和 t 就是二项式的极限，因为 delta 趋向于零，n 趋向于无穷大。几何在极限中变成指数。还有第 k 次到达时间的分布。上次我们有一个伯努利过程的闭式公式。</p><p>And the Poisson distribution, which is the distribution that we get, this Pk of t. Pk and t is the limit of
            the
            binomial when we take the limit in this particular way, as delta goes to zero, and n goes to infinity. The
            geometric becomes an exponential in the limit. And the distribution of the time of the k th arrival. we had
            a
            closed form formula last time for the Bernoulli process.</p>
        <p>这次我们得到了泊松过程的闭式公式。我们实际上使用了完全相同的参数来得到这两个闭式公式。好的。现在我们来谈谈添加或合并泊松过程。这里有两个陈述。一个与添加泊松随机变量有关，只是随机变量。另一个陈述与添加泊松过程有关。第二个陈述比第一个陈述更大。</p><p>We got the closed form formula this time for the Poisson process. And we actually used exactly the same
            argument
            to get these two closed form formulas. All right. So now let’s talk about adding or merging Poisson
            processes.
            And there’s two statements that we can make here. One has to do with adding Poisson random variables, just
            random variables. There’s another statement about adding Poisson processes. And the second is a bigger
            statement
            than the first.</p>
        <p>但这只是热身。让我们从第一个陈述开始。因此，断言是独立泊松随机变量的总和是泊松的。好的。假设我们有一个泊松过程，其速率为。为了简单起见。lambda 为 1。我取从零到二的间隔。然后取从二到五的间隔。这个间隔内的到达次数。我们将其称为从零到二的 n。</p><p>But this is a warm up. Let’s work with the first statement. So the claim is that the sum of independent
            Poisson
            random variables is Poisson. OK. So suppose that we have a Poisson process with rate. just for simplicity.
            lambda one. And I take the interval from zero to two. And that take then the interval from two until five.
            The
            number of arrivals during this interval. let’s call it n from zero to two.</p>
        <h2 id="unknown-299">未知</h2><h2>Unknown</h2>
        <p>是一个泊松随机变量，其参数或均值为 2。此间隔内的到达次数为 n，从时间 2 到时间 5。这又是一个均值为 3 的泊松随机变量，因为到达率为 1，间隔持续时间为 3。这两个随机变量是独立的。它们服从我们之前推导的泊松分布。</p><p>Is going to be a Poisson random variable, with parameter, or with mean, two. The number of arrivals during
            this
            interval is n from time two until five. This is again a Poisson random variable with mean equal to three,
            because the arrival rate is 1 and the duration of the interval is three. These two random variables are
            independent. They obey the Poisson distribution that we derived before.</p>
        <p>如果将它们相加，得到的就是从 0 到 5 的时间间隔内到达的人数。那么这个随机变量具有什么样的分布呢？这是泊松过程中一定长度的时间间隔内的到达人数。因此，这也是均值为 5 的泊松过程。</p><p>If you add them, what you get is the number of arrivals during the interval from zero to five. Now what kind
            of
            distribution does this random variable have? Well this is the number of arrivals over an interval of a
            certain
            length in a Poisson process. Therefore, this is also Poisson with mean five.</p>
        <p>因为对于泊松过程，我们知道到达的人数是泊松的，这是泊松的，而且总到达人数也是泊松的。这就确定了泊松加上泊松随机变量的总和会给我们另一个泊松随机变量。因此，添加泊松随机变量会给我们一个泊松随机变量。但现在我要做一个更一般的陈述，即它不仅仅是固定时间间隔内的到达人数。</p><p>Because for the Poisson process we know that this number of arrivals is Poisson, this is Poisson, but also
            the
            number of overall arrivals is also Poisson. This establishes that the sum of a Poisson plus a Poisson random
            variable gives us another Poisson random variable. So adding Poisson random variables gives us a Poisson
            random
            variable. But now I’m going to make a more general statement that it’s not just number of arrivals during a
            fixed time interval.</p>
        <h2 id="unknown-300">未知</h2><h2>Unknown</h2>
        <p>它不仅仅是给定时间间隔内的到达次数。而是如果你将两个不同的泊松过程相加，那么这个过程本身就是泊松过程，因为这个过程将满足泊松过程的所有假设。所以故事是这样的，你有一个红色灯泡，它以 lambda 1 的速率随机闪烁。这是一个泊松过程。</p><p>It’s not just numbers of arrivals for given time intervals. but rather if you take two different Poisson
            processes and add them up, the process itself is Poisson in the sense that this process is going to satisfy
            all
            the assumptions of a Poisson process. So the story is that you have a red bulb that flashes at random times
            at
            the rate of lambda one. It’s a Poisson process.</p>
        <p>你有一个独立的过程，其中绿色灯泡随机闪烁。而你恰好是色盲，所以你只能看到闪烁的东西。因此，这两个过程被假定为独立的泊松过程。我们能对你观察到的过程说些什么呢？那么在你观察到的过程中，如果你取一个长度为 delta 的典型时间间隔，那么在那个小时间间隔内会发生什么？</p><p>You have an independent process where a green bulb flashes at random times. And you happen to be color blind,
            so
            you just see when something is flashing. So these two are assumed to be independent Poisson processes. What
            can
            we say about the process that you observe? So in the processes that you observe, if you take a typical time
            interval of length little delta, what can happen during that little time interval?</p>
        <p>红色灯泡可能会有东西闪烁。所以红色灯泡会闪烁。或者红色灯泡不会闪烁。对于另一个灯泡，绿色灯泡，有两种可能性。绿色灯泡闪烁。另一种可能性是绿色灯泡不闪烁。好的。因此，在一个小的时间段内可能发生的情况有四种可能性。红色灯泡闪烁和绿色灯泡闪烁的概率，这个概率是多少？</p><p>The red process may have something flashing. So red flashes. Or the red does not. And for the other bulb, the
            green bulb, there’s two possibilities. The green one flashes. And the other possibility is that the green
            does
            not. OK. So there’s four possibilities about what can happen during a little slot. The probability that the
            red
            one flashes and the green one flashes, what is this probability?</p>
        <h2 id="unknown-301">未知</h2><h2>Unknown</h2>
        <p>第一个灯闪烁的概率是 lambda 1 delta，第二个灯闪烁的概率是 lambda 2 delta。我在这里乘以概率是因为我假设这两个过程是独立的。好的。现在红色灯闪烁的概率是 lambda 1 delta。但是绿色灯不闪烁的概率是 1 减去 lambda 2 delta。这里的概率是红色灯不闪烁的概率乘以绿色灯闪烁的概率。
        </p><p>It’s lambda one delta that the first one flashes, and lambda two delta that the second one does. I’m
            multiplying
            probabilities here because I’m making the assumption that the two processes are independent. OK. Now the
            probability that the red one flashes is lambda one delta. But the green one doesn’t is one, minus lambda two
            delta. Here the probability would be that the red one does not, times the probability that the green one
            does.
        </p>
        <p>然后，我们得到了它们都不闪烁的概率，也就是剩下的那个。但它是 1 减去 lambda 1 delta，乘以 1 减去 lambda 2 delta。现在我们把 delta 看作很小的。因此，考虑一下 delta 趋于零的情况，但保留一阶项。我们保留 delta 项，但丢弃 delta 平方项。</p><p>And then here we have the probability that none of them flash, which is whatever is left. But it’s one minus
            lambda one delta, times one minus lambda two delta. Now we’re thinking about delta as small. So think of the
            case where delta goes to zero, but in a way that we keep the first order terms. We keep the delta terms, but
            we
            throw away the delta squared terms.</p>
        <p>当 delta 变小时，Delta 平方项比 delta 项小得多。如果我们这样做。如果我们只保留 delta 项的顺序。这个项实际上就消失了。这是 delta 平方。所以我们让它为零。因此，在短时间内同时出现红色和绿色闪光的概率可以忽略不计。我们在这里得到了什么？Lambda delta 乘以 1 仍然存在，但这个乘以那个不存在。所以我们可以把它扔掉。</p><p>Delta squared terms are much smaller than the delta terms when delta becomes small. If we do that. if we only
            keep the order of delta terms. this term effectively disappears. This is delta squared. So we make it zero.
            So
            the probability of having simultaneously a red and a green flash during a little interval is negligible.
            What do
            we get here? Lambda delta times one survives, but this times that doesn’t. So we can throw that away.</p>
        <h2 id="unknown-302">未知</h2><h2>Unknown</h2>
        <p>因此，我们得到的近似值是 lambda 1 delta。同样，这里这个值消失了。我们剩下 lambda 2 delta。这就是剩下的，剩下的。那么我们得到了什么？看到闪光的概率是红色或绿色，即 lambda 1 delta 加上 lambda 2 delta。</p><p>So the approximation that we get is lambda one delta. Similarly here, this goes away. We’re left with a
            lambda
            two delta. And this is whatever remains, whatever is left. So what do we have? That there is a probability
            of
            seeing a flash, either a red or a green, which is lambda one delta, plus lambda two delta.</p>
        <p>因此，如果我们在这里取一个长度为 delta 的小间隔，它将看到一个到达概率约为 lambda 1，加上 lambda 2，delta。因此，这个合并过程中的每个时隙都有一个到达概率，其速率是这两个过程的速率之和。所以这是泊松过程定义的一部分。还有一些事情需要验证。</p><p>So if we take a little interval of length delta here, it’s going to see an arrival with probability
            approximately
            lambda one, plus lambda two, delta. So every slot in this merged process has an arrival probability with a
            rate
            which is the sum of the rates of these two processes. So this is one part of the definition of the Poisson
            process. There’s a few more things that one would need to verify.</p>
        <p>即，相同长度的间隔具有相同的概率分布，并且不同的时隙彼此独立。这可以从这里开始论证，因为此过程中的不同间隔彼此独立。这里的不同间隔彼此独立。不难论证合并过程中的不同间隔也将彼此独立。</p><p>Namely, that intervals of the same length have the same probability distribution and that different slots are
            independent of each other. This can be argued by starting from here because different intervals in this
            process
            are independent from each other. Different intervals here are independent from each other. It’s not hard to
            argue that different intervals in the merged process will also be independent of each other.</p>
        <h2 id="unknown-303">未知</h2><h2>Unknown</h2>
        <p>所以最后得出的结论是，这个过程是一个泊松过程，总速率等于两个过程速率之和。现在，如果我告诉你，在合并过程中，某个时刻发生了一个到达事件，那么它从这里来的可能性有多大？可能性有多大？我们来看这幅图。假设发生了一个到达事件。</p><p>So the conclusion that comes at the end is that this process is a Poisson process, with a total rate which is
            equal to the sum of the rate of the two processes. And now if I tell you that an arrival happened in the
            merged
            process at a certain time, how likely is it that it came from here? How likely is it? We go to this picture.
            Given that an arrival occurred.</p>
        <p>哪个事件是这个或那个事件发生的。它来自第一个过程（红色过程）的概率是多少？嗯，它是这个的概率除以这个的概率，再乘以那个。假设这个事件发生了，你想找到这个子事件的条件概率。所以我们要问的是，在这两个事件的总概率中，这个概率的多少部分被分配到这里？</p><p>Which is the event that this or that happened. what is the probability that it came from the first process,
            the
            red one? Well it’s the probability of this divided by the probability of this, times that. Given that this
            event
            occurred, you want to find the conditional probability of that sub event. So we’re asking the question, out
            of
            the total probability of these two, what fraction of that probability is assigned here?</p>
        <p>这是我们忽略其他项后的 lambda 1 增量。这是 lambda 2 增量。因此，该分数将是 lambda 1 除以 lambda 1 加上 lambda 2。这说明了什么？如果 lambda 1 和 lambda 2 相等，考虑到我看到这里有一辆车到达，那么它有同样的可能性是红色或绿色。</p><p>And this is lambda one delta, after we ignore the other terms. This is lambda two delta. So that fraction is
            going to be lambda one, over lambda one plus lambda two. What does this tell you? If lambda one and lambda
            two
            are equal, given that I saw an arrival here, it’s equally likely to be red or green.</p>
        <h2 id="unknown-304">未知</h2><h2>Unknown</h2>
        <p>但如果红色的到达率更高，当我在这里看到到达时，这个数字很可能很大。所以它更有可能来自红色过程。好的，我们下次继续这个故事并做一些应用。</p><p>But if the reds have a much higher arrival rate, when I see an arrival here, it’s more likely this number
            will be
            large. So it’s more likely to have come from the red process. OK so we’ll continue with this story and do
            some
            applications next time.</p>
        <h1 id="poisson-process-ii">15. 泊松过程 II</h1><h1>15. Poisson Process II</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 49:28 <a href="https://youtube.com/watch?v=XsYXACeIklU">https://youtube.com/watch?v=XsYXACeIklU</a></p><p> 12 years ago (Nov 10, 2012) — 49:28 <a href="https://youtube.com/watch?v=XsYXACeIklU">https://youtube.com/watch?v=XsYXACeIklU</a></p>
        <h2 id="intro-3">简介</h2><h2>Intro</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.</p>
        <h2 id="outline-1">大纲</h2><h2>Outline</h2>
        <p>约翰·齐茨克利斯：今天我们将结束对泊松过程的讨论。我们将了解它的一些属性，解决一些有趣的问题，其中一些问题比其他问题更有趣。</p><p>JOHN TSITSIKLIS: Today we’re going to finish our discussion of the Poisson process. We’re going to see a few
            of
            its properties, do a few interesting problems, some more interesting than others.</p>
        <p>通过几个例子，我们将讨论泊松过程中发生的一些相当奇怪的事情。</p><p>So go through a few examples and then we’re going to talk about some quite strange things that happen with
            the
            Poisson process.</p>
        <h2 id="poisson-process">泊松过程</h2><h2>Poisson Process</h2>
        <p>所以，首先要记住什么是泊松过程。可以说，这是一个客户到达的模型，在某种意义上，完全随机，也就是说客户可以在任何时间点到达。所有时间点的概率都是相等的。</p><p>So the first thing is to remember what the Poisson processes is. It’s a model, let’s say, of arrivals of
            customers that are, in some sense, quote unquote, completely random, that is a customer can arrive at any
            point
            in time. All points in time are equally likely.</p>
        <p>不同的时间点与其他时间点是相互独立的。因此，我现在得到一个到达点这一事实并不能告诉我在其他时间是否也会有到达点。从某种意义上说，它是伯努利过程的连续时间版本。因此，思考泊松过程的最佳方式是将时间划分为非常小的时隙。</p><p>And different points in time are sort of independent of other points in time. So the fact that I got an
            arrival
            now doesn’t tell me anything about whether there’s going to be an arrival at some other time. In some sense,
            it’s a continuous time version of the Bernoulli process. So the best way to think about the Poisson process
            is
            that we divide time into extremely tiny slots.</p>
        <p>在每个时间段内，都有到达的独立可能性。不同的时间段彼此独立。另一方面，当时间段很小的时候，在这个小时间段内获得到达的概率本身也会很小。因此，我们将这些属性捕获到泊松过程的正式定义中。我们有一个概率质量函数，用于给定长度间隔内的到达次数 k。</p><p>And in each time slot, there’s an independent possibility of having an arrival. Different time slots are
            independent of each other. On the other hand, when the slot is tiny, the probability for obtaining an
            arrival
            during that tiny slot is itself going to be tiny. So we capture these properties into a formal definition
            what
            the Poisson process is. We have a probability mass function for the number of arrivals, k, during an
            interval of
            a given length.</p>
        <p>这就是到达次数分布的基本描述。因此 tau 是固定的。k 是参数。因此当我们将所有 k 相加时，这些概率的总和必须等于 1。这里隐藏着一个时间同质性假设，即唯一重要的是时间间隔的持续时间，而不是时间间隔在实轴上的位置。</p><p>So this is the sort of basic description of the distribution of the number of arrivals. So tau is fixed. And
            k is
            the parameter. So when we add over all k’s, the sum of these probabilities has to be equal to 1. There’s a
            time
            homogeneity assumption, which is hidden in this, namely, the only thing that matters is the duration of the
            time
            interval, not where the time interval sits on the real axis.</p>
        <p>然后我们有一个独立性假设。不相交的间隔在统计上是相互独立的。因此，你给我的关于此时间间隔内到达的任何信息都不会改变我对另一个时间间隔内将发生什么事情的信念。因此，这是伯努利过程中不同时间段相互独立这一想法的概括。</p><p>Then we have an independence assumption. Intervals that are disjoint are statistically independent from each
            other. So any information you give me about arrivals during this time interval doesn’t change my beliefs
            about
            what’s going to happen during another time interval. So this is a generalization of the idea that we had in
            Bernoulli processes that different time slots are independent of each other.</p>
        <p>然后，为了指定这个函数，即到达人数的分布，我们分阶段进行。我们首先为时间间隔非常小的情况指定这个函数。我会告诉您这些概率是多少。然后，基于这些概率，我们进行一些计算，并找到一般持续时间间隔的到达人数分布公式。</p><p>And then to specify this function, the distribution of the number of arrivals, we sort of go in stages. We
            first
            specify this function for the case where the time interval is very small. And I’m telling you what those
            probabilities will be. And based on these then, we do some calculations and to find the formula for the
            distribution of the number of arrivals for intervals of a general duration.</p>
        <p>因此，对于一个很小的持续时间 delta，获得 1 次到达的概率是 lambda delta。剩余的概率分配给我们在该间隔内没有到达的事件。在极小的间隔内获得超过 1 次到达的概率本质上是 0。当我们说本质上时，它意味着模数，即 delta 平方的阶数。当 delta 非常小时，任何 delta 平方的东西都可以忽略。</p><p>So for a small duration, delta, the probability of obtaining 1 arrival is lambda delta. The remaining
            probability
            is assigned to the event that we get to no arrivals during that interval. The probability of obtaining more
            than
            1 arrival in a tiny interval is essentially 0. And when we say essentially, it’s means modular, terms that
            of
            order delta squared. And when delta is very small, anything which is delta squared can be ignored.</p>
        <p>因此，就 delta 平方项而言，这就是在小间隔内发生的情况。现在，如果我们知道小间隔内到达人数的概率分布。我们可以利用这一点来获得几个间隔内到达人数的分布。我们怎么做呢？大间隔由许多小间隔组成。</p><p>So up to delta squared terms, that’s what happened during a little interval. Now if we know the probability
            distribution for the number of arrivals in a little interval. We can use this to get the distribution for
            the
            number of arrivals over several intervals. How do we do that? The big interval is composed of many little
            intervals.</p>
        <p>每个小间隔都独立于任何其他小间隔，因此这就像我们进行了一系列伯努利试验。每次伯努利试验都与一个小间隔相关联，并且在该小间隔内获得成功或到达的概率很小。另一方面，当 delta 较小时，如果您将一个大间隔切分，则会得到大量的小间隔。</p><p>Each little interval is independent from any other little interval, so is it is as if we have a sequence of
            Bernoulli trials. Each Bernoulli trial is associated with a little interval and has a small probability of
            obtaining a success or an arrival during that mini slot. On the other hand, when delta is small, and you
            take a
            big interval and chop it up, you get a large number of little intervals.</p>
        <p>因此，我们本质上看到的是一个伯努利过程，其中试验次数巨大，但每次试验成功的概率却很小。平均试验次数最终与间隔长度成正比。如果间隔是原来的两倍，那么在这些小试验中，试验次数就增加了一倍，因此预期到达次数也会相应增加。</p><p>So what we essentially have here is a Bernoulli process, in which is the number of trials is huge but the
            probability of success during any given trial is tiny. The average number of trials ends up being
            proportional
            to the length of the interval. If you have twice as large an interval, it’s as if you’re having twice as
            many
            over these mini trials, so the expected number of arrivals will increase proportionately.</p>
        <p>还有这个参数 lambda，我们将其解释为单位时间内的预期到达人数。它以概率的形式出现。当你将 lambda 加倍时，这意味着在短时间间隔内到达的概率是原来的两倍。因此，你也会期望到达的人数是原来的两倍。这就是为什么在长度为 tau 的时间间隔内，预期到达人数也与这个参数 lambda 成比例。</p><p>There’s also this parameter lambda, which we interpret as expected number of arrivals per unit time. And it
            comes
            in those probabilities here. When you double lambda, this means that a little interval is twice as likely to
            get
            an arrival. So you would expect to get twice as many arrivals as well. That’s why the expected number of
            arrivals during an interval of length tau also scales proportional to this parameter lambda.</p>
        <p>出乎意料的是，到达人数的方差也与平均值相同。这是泊松过程的一个特点。因此，这是思考泊松过程的一种方式，即从小间隔的角度考虑，每个间隔的成功概率都很小。我们认为与该过程相关的分布由这个特定的 PMF 描述。</p><p>Somewhat unexpectedly, it turns out that the variance of the number of arrivals is also the same as the mean.
            This is a peculiarity that happens in the Poisson process. So this is one way of thinking about Poisson
            process,
            in terms of little intervals, each one of which has a tiny probability of success. And we think of the
            distribution associated with that process as being described by this particular PMF.</p>
        <p>所以这是固定持续时间 tau 间隔内到达次数的 PMF。</p><p>So this is the PMF for the number of arrivals during an interval of a fixed duration, tau.</p>
        <h2 id="poisson-pmf">泊松 PMF</h2><h2>Poisson PMF</h2>
        <p>它是一个 PMF，可覆盖整个非负整数范围。因此，在一定长度的间隔内，您可以获得的到达次数可以是任意的。您可以获得任意数量的到达。当然，获得无数次到达的概率很小。但原则上，这是可能的。</p><p>It’s a PMF that extends all over the entire range of non negative integers. So the number of arrivals you can
            get
            during an interval for certain length can be anything. You can get as many arrivals as you want. Of course
            the
            probability of getting a zillion arrivals is going to be tiny. But in principle, this is possible.</p>
        <p>这是因为，即使间隔是固定长度的，在某种意义上，它也由无数个迷你时隙组成。您可以将其划分、切碎，变成任意数量的迷你时隙。因此，原则上，每个迷你时隙都有可能到达。原则上，可以得到任意数量的到达。因此，当您查看时，这里的特定公式并不是很直观。但它是一个合法的 PMF。</p><p>And that’s because an interval, even if it’s a fixed length, consists of an infinite number of mini slots in
            some
            sense. You can divide, chop it up, into as many mini slots as you want. So in principle, it’s possible that
            every mini slot gets an arrival. In principle, it’s possible to get an arbitrarily large number of arrivals.
            So
            this particular formula here is not very intuitive when you look at it. But it’s a legitimate PMF.</p>
        <p>它被称为泊松 PMF。PMF 描述了到达的数量。</p><p>And it’s called the Poisson PMF. It’s the PMF that describes the number of arrivals.</p>
        <h2 id="exponential-distribution">指数分布</h2><h2>Exponential Distribution</h2>
        <p>这就是思考泊松过程的一种方式，其中感兴趣的基本对象是 PMF，然后尝试处理它。还有另一种思考泊松过程中发生的事情的方式。这与让事物随时间演变有关。</p><p>So that’s one way of thinking about the Poisson process, where the basic object of interest would be this PMF
            and
            you try to work with it. There’s another way of thinking about what happens in the Poisson process. And this
            has
            to do with letting things evolve in time.</p>
        <p>从时间 0 开始。将有一个时间，在该时间第一次到达，并将该时间称为时间 T1。该时间结果呈指数分布，参数为 lambda。一旦到达，就好像过程重新开始一样。理解为什么会出现这种情况的最好方法是通过与伯努利过程的类比来思考。</p><p>You start at time 0. There’s going to be a time at which the first arrival occurs, and call that time T1.
            This
            time turns out to have an exponential distribution with parameter lambda. Once you get an arrival, it’s as
            if
            the process starts fresh. The best way to understand why this is the case is by thinking in terms of the
            analogy
            with the Bernoulli process.</p>
        <p>如果你相信伯努利过程的这个说法，那么由于这是一个极限情况，它也应该是正确的。所以从这个时间开始，我们将等待一段随机的时间，直到我们得到第二个到达。这个随机的时间，我们称之为 T2。这一次，T2 也将具有具有相同参数 lambda 的指数分布。这两个将彼此独立。好吗？</p><p>If you believe that statement for the Bernoulli process, since this is a limiting case, it should also be
            true.
            So starting from this time, we’re going to wait a random amount of time until we get the second arrival This
            random amount of time, let’s call it T2. This time, T2 is also going to have an exponential distribution
            with
            the same parameter, lambda. And these two are going to be independent of each other. OK?</p>
        <p>因此，泊松过程具有伯努利过程所具有的所有无记忆性。这种特性的另一种思考方式是什么？想象一下这样一个过程：你有一个灯泡。灯泡烧坏的时间，你可以用指数随机变量来建模。假设他们告诉你，到目前为止，我们处于某个时间 T。</p><p>So the Poisson process has all the same memorylessness properties that the Bernoulli process has. What’s
            another
            way of thinking of this property? So think of a process where you have a light bulb. The time at the light
            bulb
            burns out, you can model it by an exponential random variable. And suppose that they tell you that so far,
            we’re
            are sitting at some time, T.</p>
        <p>我告诉你，灯泡还没有烧坏。这对灯泡的未来意味着什么？到目前为止，它们还没有烧坏，这是好消息还是坏消息？你宁愿保留这个已经工作了 t 倍并且仍然没问题的灯泡吗？还是你宁愿使用一个在那个时间点上全新的灯泡？</p><p>And I tell you that the light bulb has not yet burned out. What does this tell you about the future of the
            light
            bulb? Is the fact that they didn’t burn out, so far, is it good news or is it bad news? Would you rather
            keep
            this light bulb that has worked for t times steps and is still OK? Or would you rather use a new light bulb
            that
            starts new at that point in time?</p>
        <p>由于无记忆特性，灯泡的过去并不重要。因此，从统计学上讲，这个灯泡的未来与新灯泡的未来相同。对于它们两者而言，烧坏的时间将呈现指数分布。因此，人们描述这种情况的一种方式是说，二手灯泡和新灯泡一样好。</p><p>Because of the memorylessness property, the past of that light bulb doesn’t matter. So the future of this
            light
            bulb is statistically the same as the future of a new light bulb. For both of them, the time until they burn
            out
            is going to be described an exponential distribution. So one way that people described the situation is to
            say
            that used is exactly as good as a new.</p>
        <p>所以，一个用过的灯泡并不比一个新的灯泡差。一个用过的灯泡并不比一个新的灯泡好。所以一个还没有烧坏的旧灯泡和一个新灯泡一样好。所以，这是另一种思考泊松过程中无记忆性的方法。回到这张图片。第二次到达的时间是两个独立指数随机变量的总和。</p><p>So a used on is no worse than a new one. A used one is no better than a new one. So a used light bulb that
            hasn’t
            yet burnt out is exactly as good as a new light bulb. So that’s another way of thinking about the
            memorylessness
            that we have in the Poisson process. Back to this picture. The time until the second arrival is the sum of
            two
            independent exponential random variables.</p>
        <p>因此，原则上，您可以使用卷积公式来找到 T1 加 T2 的分布，这就是我们所说的 Y2，即第二次到达的时间。</p><p>So, in principle, you can use the convolution formula to find the distribution of T1 plus T2, and that would
            be
            what we call Y2, the time until the second arrival.</p>
        <h2 id="erlang-distribution">Erlang 分布</h2><h2>Erlang Distribution</h2>
        <p>但是还有一种直接的方法可以得到 Y2 的分布，这是我们上次在黑板上做的计算。实际上，我们做得更普遍。我们找到了案例到达的时间。</p><p>But there’s also a direct way of obtaining to the distribution of Y2, and this is the calculation that we did
            last time on the blackboard. And actually, we did it more generally. We found the time until the case
            arrival
            occurs.</p>
        <p>它有一个闭式公式，称为具有 k 个自由度的 Erlang 分布。让我们看看这里发生了什么。这是一个分布，什么类型的？它是一个连续分布。它是一个概率密度函数。这是因为时间是一个连续随机变量。时间是连续的。到达可能发生在任何时间。所以我们讨论的是 PDF。这个 k 只是分布的参数。</p><p>It has a closed form formula, which is called the Erlang distribution with k degrees of freedom. So let’s see
            what’s going on here. It’s a distribution Of what kind? It’s a continuous distribution. It’s a probability
            density function. This is because the time is a continuous random variable. Time is continuous. Arrivals can
            happen at any time. So we’re talking about the PDF. This k is just the parameter of the distribution.</p>
        <p>我们讨论的是第 k 次到达，所以 k 是一个固定数字。Lambda 是分布的另一个参数，即到达率。所以它是 Y 的 PDF，而 lambda 和 k 是分布的参数。好的。这就是我们上次所知道的。为了练习一下，让我们做一道不太难的题目，只是为了看看我们如何使用我们已有的各种公式。</p><p>We’re talking about the k th arrival, so k is a fixed number. Lambda is another parameter of the
            distribution,
            which is the arrival rate So it’s a PDF over the Y’s, whereas lambda and k are parameters of the
            distribution.
            OK. So this was what we knew from last time. Just to get some practice, let us do a problem that’s not too
            difficult, but just to see how we use the various formulas that we have.</p>
        <h2 id="fishing">钓鱼</h2><h2>Fishing</h2>
        <p>所以泊松是一位数学家，但泊松在法语中也有鱼的意思。所以泊松指的是钓鱼。我们假设鱼是按照泊松过程捕获的。这个假设还不错。在任何给定的时间点，你都有很小的概率捕获到鱼。而你现在是否能捕获到鱼与以后是否能捕获到鱼在某种程度上是相互独立的。</p><p>So Poisson was a mathematician, but Poisson also means fish in French. So Poisson goes fishing. And let’s
            assume
            that fish are caught according to a Poisson process. That’s not too bad an assumption. At any given point in
            time, you have a little probability that a fish would be caught. And whether you catch one now is sort of
            independent about whether at some later time a fish will be caught or not.</p>
        <p>所以让我们做这样的假设。假设游戏规则是，你。鱼以每小时 0.6 条的固定速率捕捞。不管怎样，你都要钓 2 个小时。然后有两种可能性。如果我钓到了一条鱼，我就停下来回家。所以如果钓到了一些鱼，那么在这段时间内至少有一条鱼到达，我就回家。</p><p>So let’s just make this assumption. And suppose that the rules of the game are that you. Fish are being
            called it
            the certain rate of 0.6 per hour. You fish for 2 hours, no matter what. And then there are two
            possibilities. If
            I have caught a fish, I stop and go home. So if some fish have been caught, so there’s at least 1 arrival
            during
            this interval, I go home.</p>
        <p>或者如果什么都没钓到，我会继续钓鱼，直到钓到鱼为止。然后我就回家。这就是对即将发生的事情的描述。现在让我们开始问各种问题。我钓鱼超过 2 小时的可能性有多大？</p><p>Or if nothing has being caught, I continue fishing until I catch something. And then I go home. So that’s the
            description of what is going to happen. And now let’s starts asking questions of all sorts. What is the
            probability that I’m going to be fishing for more than 2 hours?</p>
        <p>我将钓鱼超过 2 个小时，前提是在这 2 小时内没有钓到任何鱼，在这种情况下，我必须继续钓鱼。因此，这只是这个数量。在接下来的 2 小时内钓到 2 条鱼的概率是钓到 0 条鱼的概率，根据我们已有的公式，这将是 e 的负 λ 乘以我们有多少时间。</p><p>I will be fishing for more than 2 hours, if and only if no fish were caught during those 2 hours, in which
            case,
            I will have to continue. Therefore, this is just this quantity. The probability of catching 2 fish in. of
            catching 0 fish in the next 2 hours, and according to the formula that we have, this is going to be e to the
            minus lambda times how much time we have.</p>
        <p>还有另一种思考方式。我钓鱼超过 2 小时的概率是第一次捕获发生在时间 2 之后的概率，这将是首次到达时间的密度从 2 到无穷大的积分。而该密度是一个指数。因此，你对指数进行积分，当然，你会得到相同的答案。好的。这很简单。</p><p>There’s another way of thinking about this. The probability that I fish for more than 2 hours is the
            probability
            that the first catch happens after time 2, which would be the integral from 2 to infinity of the density of
            the
            first arrival time. And that density is an exponential. So you do the integral of an exponential, and, of
            course, you would get the same answer. OK. That’s easy.</p>
        <p>那么钓鱼时间超过 2 小时但少于 5 小时的概率是多少？要发生这种情况需要什么？要做到这一点，我们需要在时间 0 到 2 之间捕获 0 条鱼，并在 2 到 5 之间的某个时间捕获第一条鱼。所以如果你。思考这里发生的事情的一种方式可能是说有一个泊松过程会永远持续下去。</p><p>So what’s the probability of fishing for more than 2 but less than 5 hours? What does it take for this to
            happen?
            For this to happen, we need to catch 0 fish from time 0 to 2 and catch the first fish sometime between 2 and
            5.
            So if you. one way of thinking about what’s happening here might be to say that there’s a Poisson process
            that
            keeps going on forever.</p>
        <p>但是，一旦我钓到第一条鱼，我就不再继续钓鱼，去钓其他鱼，而是直接回家。现在我在第 5 个时间之前就回家了，这意味着，如果我待到第 5 个时间，我至少会钓到一条鱼。</p><p>But as soon as I catch the first fish, instead of continuing fishing and obtaining those other fish I just go
            home right now. Now the fact that I go home before time 5 means that, if I were to stay until time 5, I
            would
            have caught at least 1 fish.</p>
        <p>我可能捕获了超过 1 条。所以这里感兴趣的事件是第一次捕获发生在时间 2 和 5 之间。因此，计算这个数量的一种方法是。</p><p>I might have caught more than 1. So the event of interest here is that the first catch happens between times
            2
            and 5. So one way of calculating this quantity would be.</p>
        <p>这是第一次捕获发生在 2 到 5 之间的概率。另一种处理方法是说，这是我在前 2 个小时内捕获 0 条鱼的概率，然后是我在接下来的 3 个小时内捕获至少 1 条鱼的概率。这。这是什么？接下来 3 个小时内捕获 0 条鱼的概率就是这段时间内捕获 0 条鱼的概率。</p><p>Its the probability that the first catch happens between times 2 and 5. Another way to deal with it is to
            say,
            this is the probability that I caught 0 fish in the first 2 hours and then the probability that I catch at
            least
            1 fish during the next 3 hours. This. What is this? The probability of 0 fish in the next 3 hours is the
            probability of 0 fish during this time.</p>
        <p>1 减去这个就是在 2 到 5 之间捕获至少 1 条鱼、至少有 1 条鱼到达的概率。如果在 2 到 5 之间至少有 1 条鱼到达，那么我到 5 的时候就已经回家了。所以，如果你代入数字等，这两个答案当然会相同。现在，我捕获至少 2 条鱼的概率是多少？</p><p>1 minus this is the probability of catching at least 1 fish, of having at least 1 arrival, between times 2
            and 5.
            If there’s at least 1 arrival between times 2 and 5, then I would have gone home by time 5. So both of
            these, if
            you plug in numbers and all that, of course, are going to give you the same answer. Now next, what’s the
            probability that I catch at least 2 fish?</p>
        <p>我们处于哪种场景？在这种情况下，我钓到第一条鱼后就回家了。因此，为了钓到至少 2 条鱼，必须在这种情况下。因此，这与我在前 2 个时间步骤中钓到至少 2 条鱼的事件相同。</p><p>In which scenario are we? Under this scenario, I go home when I catch my first fish. So in order to catch at
            least 2 fish, it must be in this case. So this is the same as the event that I catch at least 2 fish during
            the
            first 2 time steps.</p>
        <p>所以它将是从 2 到无穷大的概率，我抓到 2 条鱼的概率，或者抓到 3 条鱼的概率，或者抓到更多条鱼的概率。所以它是这个数量。k 是我抓到的鱼的数量。至少是 2，所以 k 从 2 到无穷大。这些是在这个间隔内抓到 k 条鱼的概率。
        </p><p>So it’s going to be the probability from 2 to infinity, the probability that I catch 2 fish, or that I catch
            3
            fish, or I catch more than that. So it’s this quantity. k is the number of fish that I catch. At least 2, so
            k
            goes from 2 to infinity. These are the probabilities of catching a number k of fish during this interval.
        </p>
        <p>如果你想要一个更简单的形式，没有无限的总和，那么这将是 1 减去捕获 0 条鱼的概率，再减去捕获 1 条鱼的概率，在长度为 2 的时间间隔内。另一种思考方式。我将捕获 2 条鱼，至少 2 条鱼，当且仅当在此过程中捕获的第二条鱼发生在时间 2 之前。所以这是思考同一事件的另一种方式。</p><p>And if you want a simpler form without an infinite sum, this would be 1 minus the probability of catching 0
            fish,
            minus the probability of catching 1 fish, during a time interval of length 2. Another way to think of it.
            I’m
            going to catch 2 fish, at least 2 fish, if and only if the second fish caught in this process happens before
            time 2. So that’s another way of thinking about the same event.</p>
        <p>因此，这将是随机变量 Y2（到达第二条鱼的时间）小于或等于 2 的概率。好的。下一个有点棘手。在这里我们需要做一些分而治之。总的来说，在这次探险中，预计捕获的鱼的数量是多少？一种思考方法是尝试使用总期望定理。</p><p>So it’s going to be the probability that the random variable Y2, the arrival time over the second fish, is
            less
            than or equal to 2. OK. The next one is a little trickier. Here we need to do a little bit of divide and
            conquer. Overall, in this expedition, what the expected number of fish to be caught? One way to think about
            it
            is to try to use the total expectations theorem.</p>
        <p>考虑一下在这种情况下预期的鱼的数量，或者在这种情况下预期的鱼的数量。这比我要做的事情要复杂一些。我要做的事情是按以下方式思考。预期的鱼的数量是第 0 到第 2 次之间捕获的鱼的预期数量加上第 2 次之后捕获的鱼的预期数量。那么第 0 到第 2 次之间捕获的鱼的预期数量是多少？</p><p>And think of expected number of fish, given this scenario, or expected number of fish, given this scenario.
            That’s a little more complicated than the way I’m going to do it. The way I’m going to do is to think as
            follows. Expected number of fish is the expected number of fish caught between times 0 and 2 plus expected
            number of fish caught after time 2. So what’s the expected number caught between time 0 and 2?</p>
        <p>这是 lambda t。所以 lambda 是 0.6 乘以 2。这是在 0 到 2 之间捕获的鱼的预期数量。现在让我们考虑一下之后捕获的鱼的预期数量。之后捕获了多少条鱼？这取决于具体情况。如果我们处于这种情况，我们回家了，捕获了 0 条鱼。如果我们处于这种情况，那么我们继续钓鱼，直到捕获一条鱼。</p><p>This is lambda t. So lambda is 0.6 times 2. This is the expected number of fish that are caught between times
            0
            and 2. Now let’s think about the expected number of fish caught afterwards. How many fish are being caught
            afterwards? Well it depends on the scenario. If we’re in this scenario, we’ve gone home and we catch 0. If
            we’re
            in this scenario, then we continue fishing until we catch one.</p>
        <p>因此，在第 2 次之后捕获的鱼的预期数量将是这种情况的概率乘以 1。这种情况的概率是他们所说的前 2 个时间步骤中没有鱼的概率乘以 1，如果我继续，这就是我将捕获的鱼的数量。我们可以用完全相同的方式计算预期的总捕鱼时间。我直接跳到最后一个。</p><p>So the expected number of fish to be caught after time 2 is going to be the probability of this scenario
            times 1.
            And the probability of that scenario is the probability that they call it’s 0 fish during the first 2 time
            steps
            times 1, which is the number of fish I’m going to catch if I continue. The expected total fishing time we
            can
            calculate exactly the same way. I’m jumping to the last one.</p>
        <p>我的总钓鱼时间有 2 个时间步长。不管怎样，我都会钓鱼 2 个时间步长。然后，如果我钓到 0 条鱼（这种情况发生的概率是这样的），我的预期时间就是从现在开始的预期时间，也就是这个带有参数 lambda 的几何随机变量的预期值。所以预期时间是 1/lambda。</p><p>My total fishing time has a period of 2 time steps. I’m going to fish for 2 time steps no matter what. And
            then
            if I caught 0 fish, which happens with this probability, my expected time is going to be the expected time
            from
            here onwards, which is the expected value of this geometric random variable with parameter lambda. So the
            expected time is 1 over lambda.</p>
        <p>在我们的例子中，这个是 1/0.6。最后，如果我告诉你我已经钓鱼 4 个小时了，但到目前为止什么都没钓到，你预计这个数量是多少？同样，对于使用的泊松过程来说，这个数量和新的一样好。这个过程没有任何记忆。因为过去发生的事情对未来并不重要。</p><p>And in our case this, is 1/0.6. Finally, if I tell you that I have been fishing for 4 hours and nothing has
            been
            caught so far, how much do you expect this quantity to be? Here is the story that, again, that for the
            Poisson
            process used is as good as new. The process does not have any memory. Given what happens in the past doesn’t
            matter for the future.</p>
        <p>就好像这个过程在这个时间点重新开始。所以这一次将再次是具有相同参数 lambda 的相同指数分布的随机变量。因此，无论过去发生了什么，到达的预期时间都是具有参数 lambda 的指数分布。从现在开始展望未来，就好像这个过程刚刚开始。
        </p><p>It’s as if the process starts new at this point in time. So this one is going to be, again, the same
            exponentially distributed random variable with the same parameter lambda. So expected time until an arrival
            comes is an exponential distribut has an exponential distribution with parameter lambda, no matter what has
            happened in the past. Starting from now and looking into the future, it’s as if the process has just
            started.
        </p>
        <p>所以它将是 1/lambda，也就是 1/0.6。好的。</p><p>So it’s going to be 1 over lambda, which is 1/0.6. OK.</p>
        <h2 id="merged-process">合并流程</h2><h2>Merged Process</h2>
        <p>现在我们的下一个示例将更加复杂或微妙。但在开始示例之前，让我们回顾一下我们上次讨论的关于合并泊松独立泊松过程的内容。除了以这种方式绘制图片，我们还可以采用另一种方式绘制它。</p><p>Now our next example is going to be a little more complicated or subtle. But before we get to the example,
            let’s
            refresh our memory about what we discussed last time about merging Poisson independent Poisson processes.
            Instead of drawing the picture that way, another way we could draw it could be this.</p>
        <p>我们有一个速率为 lambda1 的泊松过程和一个速率为 lambda2 的泊松过程。它们每个都有到达点。然后我们形成合并过程。合并过程记录两个过程中的任意一个到达点。这个过程和那个过程被认为是相互独立的。现在，这个过程和那个过程中的不同时间是相互独立的。
        </p><p>We have a Poisson process with rate lambda1, and a Poisson process with rate lambda2. They have, each one of
            these, have their arrivals. And then we form the merged process. And the merged process records an arrival
            whenever there’s an arrival in either of the two processes. This process in that process are assumed to be
            independent of each other. Now different times in this process and that process are independent of each
            other.
        </p>
        <p>因此，这两个时间间隔内发生的事情与这两个时间间隔内发生的事情无关。这两个时间间隔决定了这里发生的事情。这两个时间间隔决定了那里发生的事情。因此，由于这些是独立于这些的，这意味着这也独立于那个。因此，对于合并过程，独立性假设得到满足。合并过程最终是一个泊松过程。</p><p>So what happens in these two time intervals is independent from what happens in these two time intervals.
            These
            two time intervals to determine what happens here. These two time intervals determine what happens there. So
            because these are independent from these, this means that this is also independent from that. So the
            independence assumption is satisfied for the merged process. And the merged process turns out to be a
            Poisson
            process.</p>
        <p>如果你想找到该过程的到达率，你可以进行如下论证。在长度为 delta 的短时间间隔内，我们有 lambda1 delta 概率到达该过程。我们有 lambda2 delta 概率到达该过程，加上 delta 中的二阶项，我们将其忽略。</p><p>And if you want to find the arrival rate for that process, you argue as follows. During a little interval of
            length delta, we have probability lambda1 delta of having an arrival in this process. We have probability
            lambda2 delta of an arrival in this process, plus second order terms in delta, which we’re ignoring.</p>
        <p>然后你进行计算，你会发现在这个过程中，你会得到一个到达概率，即 lambda1 加上 lambda2，再次忽略 delta 中的二阶项。因此，合并过程是一个泊松过程，其到达率是各个过程到达率的总和。我们在上节课结束时进行了计算。</p><p>And then you do the calculation and you find that in this process, you’re going to have an arrival
            probability,
            which is lambda1 plus lambda2, again ignoring second order in delta. terms that are second order in delta.
            So
            the merged process is a Poisson process whose arrival rate is the sum of the arrival rates of the individual
            processes. And the calculation we did at the end of the last lecture.</p>
        <p>如果我告诉你新来者发生在这里，那么它来自哪里？它是从这里来的还是从那里来的？如果 lambda1 等于 lambda2，那么根据对称性，你会说它从这里或从那里来的概率是相等的。但如果这个 lambda 比那个 lambda 大得多，那么他们看到新来者的可能性就更大。</p><p>If I tell you that the new arrival happened here, where did that arrival come from? Did it come from here or
            from
            there? If the lambda1 is equal to lambda2, then by symmetry you would say that it’s equally likely to have
            come
            from here or to come from there. But if this lambda is much bigger than that lambda, the fact that they saw
            an
            arrival is more likely to have come from there.</p>
        <p>计算这个概率的公式如下。这是我的到来来自这个特定溪流而不是那个特定溪流的概率。所以当一个到来者到来时，你会问，这个到来者的来源是什么？这就像我用这些概率抛硬币一样。根据硬币的结果，我会告诉你它来自那里或它来自那里。</p><p>And the formula that captures this is the following. This is the probability that my arrival has come from
            this
            particular stream rather than that particular stream. So when an arrival comes and you ask, what is the
            origin
            of that arrival? It’s as if I’m flipping a coin with these odds. And depending on outcome of that coin, I’m
            going to tell you came from there or it came from there.</p>
        <p>因此，到达的起点要么是这条河道，要么是那条河道。这就是到达起点是那条河道的概率。现在，如果我们看两个不同的到达，并询问它们的起点。那么让我们考虑一下这个到达的起点，并将其与那个到达的起点进行比较。这个到达的起点是随机的。它可能是这个，也可能是那个。</p><p>So the origin of an arrival is either this stream or that stream. And this is the probability that the origin
            of
            the arrival is that one. Now if we look at 2 different arrivals, and we ask about their origins. So let’s
            think
            about the origin of this arrival and compare it with the origin that arrival. The origin of this arrival is
            random. It could be right be either this or that.</p>
        <p>这是相关概率。到达的起点是随机的。它可能在这里，也可能在那里，同样，相关概率也相同。问题是，这个到达的起点是依赖于还是独立于那个到达的起点？论点是这样的。不同的时间是独立的。在这个时间段内，过程中发生的任何事情都独立于那个时间段内的过程。</p><p>And this is the relevant probability. The origin of that arrival is random. It could be either here or is
            there,
            and again, with the same relevant probability. Question. The origin of this arrival, is it dependent or
            independent from the origin that arrival? And here’s how the argument goes. Separate times are independent.
            Whatever has happened in the process during this set of times is independent from whatever happened in the
            process during that set of times.</p>
        <p>因为不同的时间彼此之间没有关系，所以到达这里的起点与到达那里的起点没有任何关系。因此，不同到达的起点也是独立的随机变量。所以如果我告诉你。是的。好的。</p><p>Because different times have nothing to do with each other, the origin of this, of an arrival here, has
            nothing
            to do with the origin of an arrival there. So the origins of different arrivals are also independent random
            variables. So if I tell you that. yeah. OK.</p>
        <p>因此，每次在合并过程中有到达时，就好像您在抛硬币来确定到达的数来自哪里，并且这些硬币彼此独立。好的。好的。现在我们将利用我们对合并过程的了解来解决这个问题，如果不使用泊松过程的思想，这个问题将更难解决。</p><p>So it as if that each time that you have an arrival in the merge process, it’s as if you’re flipping a coin
            to
            determine where did that arrival came from and these coins are independent of each other. OK. OK. Now we’re
            going to use this. what we know about merged processes to solve the problem that would be harder to do, if
            you
            were not using ideas from Poisson processes.</p>
        <p>因此，问题的公式与泊松过程无关。公式如下。我们有 3 个灯泡。每个灯泡都是独立的，并且会在呈指数分布的时间熄灭。所以有 3 个灯泡。它们开始使用，然后在某个时间点熄灭或烧坏。因此，让我们将这个视为 X，这个视为 Y，这个视为 Z。</p><p>So the formulation of the problem has nothing to do with the Poisson process. The formulation is the
            following.
            We have 3 light bulbs. And each light bulb is independent and is going to die out at the time that’s
            exponentially distributed. So 3 light bulbs. They start their lives and then at some point they die or burn
            out.
            So let’s think of this as X, this as Y, and this as Z.</p>
        <p>我们感兴趣的是最后一个灯泡烧坏的时间。所以我们感兴趣的是 3 个随机变量 X、Y 和 Z 中的最大值。特别是，我们想找到这个最大值的期望值。好的。因此，您可以进行派生分布，使用期望值规则，或任何您想要的方法。您可以使用您手中已有的工具来获得这个答案。</p><p>And we’re interested in the time until the last light bulb burns out. So we’re interested in the maximum of
            the 3
            random variables, X, Y, and Z. And in particular, we want to find the expected value of this maximum. OK. So
            you
            can do derived distribution, use the expected value rule, anything you want. You can get this answer using
            the
            tools that you already have in your hands.</p>
        <p>但现在让我们看看如何将这幅图与泊松图联系起来，并以非常简单的方式得出答案。什么是指数随机变量？指数随机变量是涉及整个泊松过程的长剧的第一幕。所以指数随机变量是泊松电影的第一幕。这里也是一样。</p><p>But now let us see how we can connect to this picture with a Poisson picture and come up with the answer in a
            very simple way. What is an exponential random variable? An exponential random variable is the first act in
            the
            long play that involves a whole Poisson process. So an exponential random variable is the first act of a
            Poisson
            movie. Same thing here.</p>
        <p>你可以把这个随机变量看作是某个正在运行的泊松过程的一部分。所以它是这个更大图景的一部分。我们仍然对 3 中的最大值感兴趣。其他到达不会影响我们的答案。只是，从概念上讲，我们可以把指数随机变量看作是嵌入在一个更大的泊松图中。所以我们有 3 个并行运行的泊松过程。</p><p>You can think of this random variable as being part of some Poisson process that has been running. So it’s
            part
            of this bigger picture. We’re still interested in the maximum of the 3. The other arrivals are not going to
            affect our answers. It’s just, conceptually speaking, we can think of the exponential random variable as
            being
            embedded in a bigger Poisson picture. So we have 3 Poisson process that are running in parallel.</p>
        <p>让我们将最后一次烧毁的预期时间分成几部分，即第一次烧毁的时间、第一次烧毁到第二次烧毁的时间以及第二次烧毁到第三次烧毁的时间。然后找出每个部分的预期值。我们能说出这个预期值是什么吗？这是所有这 3 个泊松过程中的第一个到达值。</p><p>Let us split the expected time until the last burnout into pieces, which is time until the first burnout,
            time
            from the first until the second, and time from the second until the third. And find the expected values of
            each
            one of these pieces. What can we say about the expected value of this? This is the first arrival out of all
            of
            these 3 Poisson processes.</p>
        <p>这是同时观察所有这些过程时发生的第一个事件。因此，3 个泊松过程并行运行。我们感兴趣的是其中一个过程（其中任何一个过程）到达所需的时间。换言之。我们合并了 3 个泊松过程，并要求在合并过程中观察到到达所需的时间。当 3 个过程中的 1 个过程首次到达时，合并过程将获得其首次到达。</p><p>It’s the first event that happens when you look at all of these processes simultaneously. So 3 Poisson
            processes
            running in parallel. We’re interested in the time until one of them, any one of them, gets in arrival.
            Rephrase.
            We merged the 3 Poisson processes, and we ask for the time until we observe an arrival in the merged
            process.
            When 1 of the 3 gets an arrival for the first time, the merged process gets its first arrival.</p>
        <p>那么，第一次烧毁前的时间期望值是多少？它将是泊松随机变量的期望值。因此，第一次烧毁将有一个期望值，即。好的。这是一个泊松过程。这三个过程的合并过程具有集体到达率，即 lambda 的 3 倍。因此，这是指数分布的参数，它描述了合并过程中第一次到达前的时间。</p><p>So what’s the expected value of this time until the first burnout? It’s going to be the expected value of a
            Poisson random variable. So the first burnout is going to have an expected value, which is. OK. It’s a
            Poisson
            process. The merged process of the 3 has a collective arrival rate, which is 3 times lambda. So this is the
            parameter over the exponential distribution that describes the time until the first arrival in the merged
            process.</p>
        <p>这个随机变量的期望值是 1/lambda。当你有一个带有参数 lambda 的指数随机变量时，这个随机变量的期望值是 1/lambda。这里我们讨论的是速率为 3 lambda 的过程中的首次到达时间。首次到达的预期时间是 1/(3 lambda)。好的。所以此时，这个灯泡，这个到达发生了，这个灯泡已经烧坏了。</p><p>And the expected value of this random variable is 1 over that. When you have an exponential random variable
            with
            parameter lambda, the expected value of that random variable is 1 over lambda. Here we’re talking about the
            first arrival time in a process with rate 3 lambda. The expected time until the first arrival is 1 over (3
            lambda). Alright. So at this time, this bulb, this arrival happened, this bulb has been burned.</p>
        <p>所以我们不再关心那个灯泡了。我们从此时开始，展望未来。这个灯泡已经烧坏了。所以从现在开始，我们只展望未来。我们得到了什么？我们有两个灯泡在烧。我们有一个泊松过程，这是如果我们继续更换灯泡，灯泡会发生什么情况的更大图景。另一个泊松过程。这两个过程再次是独立的。</p><p>So we don’t care about that bulb anymore. We start at this time, and we look forward. This bulb has been
            burned.
            So let’s just look forward from now on. What have we got? We have two bulbs that are burning. We have a
            Poisson
            process that’s the bigger picture of what could happen to that light bulb, if we were to keep replacing it.
            Another Poisson process. These two processes are, again, independent.</p>
        <p>从此时到彼时，需要多长时间？这是这个过程记录到达或那个过程记录到达的时间。这与这两个过程合并后的过程记录到达的时间相同。所以我们讨论的是合并过程中第一次到达的预期时间。合并过程是泊松分布。它是具有 2 lambda 速率的泊松分布。所以这额外的时间是需要的。</p><p>From this time until that time, how long does it take? It’s the time until either this process records an
            arrival
            or that process records and arrival. That’s the same as the time that the merged process of these two
            records an
            arrival. So we’re talking about the expected time until the first arrival in a merged process. The merged
            process is Poisson. It’s Poisson with rate 2 lambda. So that extra time is going to take.</p>
        <p>预期值将是 1 除以 (泊松过程的速率)。因此 1 除以 (2 lambda) 是这个随机变量的预期值。因此，此时，这个灯泡也烧坏了。因此，我们从此时开始观察。图片的这一部分消失了。从此时开始，直到剩下的灯泡烧坏为止的预期值是多少？</p><p>The expected value is going to be 1 over the (rate of that Poisson process). So 1 over (2 lambda) is the
            expected
            value of this random variable. So at this point, this bulb now is also burned. So we start looking from this
            time on. That part of the picture disappears. Starting from this time, what’s the expected value until that
            remaining light bulb burns out?</p>
        <p>好吧，正如我们之前所说，在泊松过程或指数随机变量中，我们有无记忆性。旧灯泡和新灯泡一样好。所以这就像我们从头开始一样。所以这将是一个带有参数 lambda 的指数随机变量。它的预期值将是 1/lambda。
        </p><p>Well, as we said before, in a Poisson process or with exponential random variables, we have memorylessness. A
            used bulb is as good as a new one. So it’s as if we’re starting from scratch here. So this is going to be an
            exponential random variable with parameter lambda. And the expected value of it is going to be 1 over
            lambda.
        </p>
        <p>因此，用这种特殊方法解决这个问题的妙处在于，我们不用任何微积分、不用积分、不用计算任何形式的期望，就能解决所有问题。你在泊松世界中遇到的大多数非平凡问题基本上都涉及这类技巧。
        </p><p>So the beauty of approaching this problem in this particular way is, of course, that we manage to do
            everything
            without any calculus at all, without striking an integral, without trying to calculate expectations in any
            form.
            Most of the non trivial problems that you encounter in the Poisson world basically involve tricks of these
            kind.
        </p>
        <p>你有一个问题并尝试重新表述它，尝试思考在泊松设置中可能发生的情况，使用无记忆，使用合并等等。</p><p>You have a question and you try to rephrase it, trying to think in terms of what might happen in the Poisson
            setting, use memorylessness, use merging, et cetera, et cetera.</p>
        <h2 id="splitting-process">分裂过程</h2><h2>Splitting Process</h2>
        <p>现在我们讨论合并。事实证明，泊松过程的分裂也很好。这里的故事与伯努利过程完全相同。所以我有一个泊松过程。</p><p>Now we talked about merging. It turns out that the splitting of Poisson processes also works in a nice way.
            The
            story here is exactly the same as for the Bernoulli process. So I’m having a Poisson process.</p>
        <p>每次，以某个速率 lambda，每次有数据到达，我都会将其发送到那个流，并以某个概率 P 记录这里的数据到达。然后，我会以某个概率 1 减 P 将其发送到另一个流。因此，这取决于我抛硬币的结果，要么发生这种情况，要么发生那种情况。</p><p>And each time, with some rate lambda, and each time that an arrival comes, I’m going to send it to that
            stream
            and the record an arrival here with some probability P. And I’m going to send it to the other stream with
            some
            probability 1 minus P. So either of this will happen or that will happen, depending on the outcome of the
            coin
            flip that I do.</p>
        <p>每次发生该事件时，我都会抛硬币并决定是在这里还是在那里记录。这称为将泊松过程分成两部分。我们在这里得到了什么样的过程？如果你查看长度 delta 的小间隔，这个小间隔到达的概率是多少？</p><p>Each time that then arrival occurs, I flip a coin and I decide whether to record it here or there. This is
            called
            splitting a Poisson process into two pieces. What kind of process do we get here? If you look at the little
            interval for length delta, what’s the probability that this little interval gets an arrival?</p>
        <p>这是到达的概率，即 lambda delta 乘以到达后抛硬币结果为该点的概率，这样我就能到达那里。所以这意味着这个小间隔的概率为 lambda delta P。或者更明确地说，我应该把它写成 lambda P 乘以 delta。所以每个小间隔的到达概率都与 delta 成正比。</p><p>It’s the probability that this one gets an arrival, which is lambda delta times the probability that after I
            get
            an arrival my coin flip came out to be that way, so that it sends me there. So this means that this little
            interval is going to have probability lambda delta P. Or maybe more suggestively, I should write it as
            lambda P
            times delta. So every little interval has a probability of an arrival proportional to delta.</p>
        <p>比例因子是 lambda P。所以 lambda P 是这个过程的速率。然后你进行与伯努利过程相同的心理练习，以证明这里的不同间隔是独立的等等。这样就完成了对这个过程是否是泊松过程的检查。</p><p>The proportionality factor is lambda P. So lambda P is the rate of that process. And then you go through the
            mental exercise that you went through for the Bernoulli process to argue that a different intervals here are
            independent and so on. And that completes checking that this process is going to be a Poisson process.</p>
        <p>因此，当你通过每次发生某事时进行独立抛硬币来拆分泊松过程时，你得到的过程仍然是泊松过程，但速率当然会降低。因此，有时人们还会使用“稀疏”一词来代替“拆分”一词。也就是说，在到达的事件中，你保留一些，但丢弃一些。</p><p>So when you split a Poisson process by doing independent coin flips each time that something happens, the
            processes that you get is again a Poisson process, but of course with a reduced rate. So instead of the word
            splitting, sometimes people also use the words thinning out. That is, out of the arrivals that came, you
            keep a
            few but throw away a few.</p>
        <h2 id="random-incidence">随机发生</h2><h2>Random Incidence</h2>
        <p>好的。</p><p>OK.</p>
        <p>现在，本讲座的最后一个主题是一种相当奇怪的现象，被称为随机事件。故事是这样的。自古以来，公交车就在马萨诸塞州大道上运行。运营公交车的公交公司声称，公交车的运行符合泊松过程，每小时有 4 辆公交车。因此，预计公交车到达的时间间隔为 15 分钟。好的。好吧。</p><p>So now the last topic over this lecture is a quite curious phenomenon that goes under the name of random
            incidents. So here’s the story. Buses have been running on Mass Ave. from time immemorial. And the bus
            company
            that runs the buses claims that they come as a Poisson process with some rate, let’s say, of 4 buses per
            hour.
            So that the expected time between bus arrivals is going to be 15 minutes. OK. Alright.</p>
        <p>人们一直在抱怨他们来这里。他们认为公交车耗时太长。所以你被要求调查。是公司吗？它是否按照承诺运营。所以你派了一名卧底特工去检查公交车的到达间隔时间。是 15 分钟吗？还是更长？</p><p>So people have been complaining that they have been showing up there. They think the buses are taking too
            long.
            So you are asked to investigate. Is the company. Does it operate according to its promises or not. So you
            send
            an undercover agent to go and check the interarrival times of the buses. Are they 15 minutes? Or are they
            longer?</p>
        <p>于是你戴上墨镜，在某个时间出现在公交车站。然后你去问沙拉三明治卡车上的那个人，距离上一辆车到站已经过了多久？当然，那个人是 FBI 的工作人员，对吧？他们会告诉你，嗯，比如说，距离上一辆车到站已经过了 12 分钟。然后你说，“哦，12 分钟。</p><p>So you put your dark glasses and you show up at the bus stop at some random time. And you go and ask the guy
            in
            the falafel truck, how long has it been since the last arrival? So of course that guy works for the FBI,
            right?
            So they tell you, well, it’s been, let’s say, 12 minutes since the last bus arrival. And then you say, "Oh,
            12
            minutes.</p>
        <p>平均时间为 15 分钟。所以现在公交车随时都会到。”对吗？不，你不会这么想。这是一个泊松过程。距离上一班公交车到达的时间有多久并不重要。所以你不会犯这个错误。你不用预测要等多久，而是坐在那里等着，然后测量时间。然后你会发现，假设是 11 分钟。</p><p>Average time is 15. So a bus should be coming any time now." Is that correct? No, you wouldn’t think that
            way.
            It’s a Poisson process. It doesn’t matter how long it has been since the last bus arrival. So you don’t go
            through that fallacy. Instead of predicting how long it’s going to be, you just sit down there and wait and
            measure the time. And you find that this is, let’s say, 11 minutes.</p>
        <p>然后你去找老板汇报，“嗯，确实花了。我去了那里，从上一班公交车到下一班公交车的时间是 23 分钟。比他们说的 15 分钟还要多。”然后你再去重复一遍。你日复一日地做。你记录下这段时间长度的统计数据。然后你告诉你的老板，这远不止 15 分钟。往往是 30 分钟左右。</p><p>And you go to your boss and report, “Well, it took. I went there and the time from the previous bus to the
            next
            one was 23 minutes. It’s more than the 15 that they said.” So go and do that again. You go day after day.
            You
            keep these statistics of the length of this interval. And you tell your boss it’s a lot more than 15. It
            tends
            to be more like 30 or so.</p>
        <p>所以公交公司在欺骗我们。公交公司真的按照他们承诺的速度运行泊松公交吗？让我们分析一下这种情况，并计算出这个间隔的平均长度应该是多少。最简单的说法是，这个间隔是一个到达间隔时间。如果公交公司确实按照这些到达间隔时间运行泊松过程，那么平均到达间隔时间为 15 分钟。</p><p>So the bus company is cheating us. Does the bus company really run Poisson buses at the rate that they have
            promised? Well let’s analyze the situation here and figure out what the length of this interval should be,
            on
            the average. The naive argument is that this interval is an interarrival time. And interarrival times, on
            the
            average, are 15 minutes, if the company runs indeed Poisson processes with these interarrival times.</p>
        <p>但实际上情况要更微妙一些，因为这不是典型的到达间隔。这个到达间隔由两部分组成。我们把它们称为 T1 和 T1 prime。你能告诉我关于这两个随机变量的什么信息吗？T1 是什么样的随机变量？从这个时间开始，对于泊松过程，过去并不重要。它是到达发生之前的时间。</p><p>But actually the situation is a little more subtle because this is not a typical interarrival interval. This
            interarrival interval consists of two pieces. Let’s call them T1 and T1 prime. What can you tell me about
            those
            two random variables? What kind of random variable is T1? Starting from this time, with the Poisson process,
            the
            past doesn’t matter. It’s the time until an arrival happens.</p>
        <p>因此，T1 将是一个具有参数 lambda 的指数随机变量。因此，具体来说，T1 的期望值本身将是 15。随机变量 T1 prime 怎么样？它是什么样的随机变量？这就像在时间上倒退的泊松过程中的第一次到达。在时间上倒退的泊松过程是什么样的过程？让我们想想抛硬币。</p><p>So T1 is going to be an exponential random variable with parameter lambda. So in particular, the expected
            value
            of T1 is going to be 15 by itself. How about the random variable T1 prime. What kind of random variable is
            it?
            This is like the first arrival in a Poisson process that runs backwards in time. What kind of process is a
            Poisson process running backwards in time? Let’s think of coin flips.</p>
        <p>假设你有一部关于抛硬币的电影。由于某种意外，这部引人入胜的电影，你恰好倒着看。从统计上看，它会有什么不同吗？不会。它只是随机抛硬币的序列。因此，反向运行的伯努利过程在统计上与正向运行的伯努利过程相同。泊松过程是伯努利的一个极限。所以，泊松过程也是一样。</p><p>Suppose you have a movie of coin flips. And for some accident, that fascinating movie, you happen to watch it
            backwards. Will it look any different statistically? No.&nbsp;It’s going to be just the sequence of random coin
            flips. So a Bernoulli process that’s runs in reverse time is statistically identical to a Bernoulli process
            in
            forward time. The Poisson process is a limit of the Bernoulli. So, same story with the Poisson process.</p>
        <p>如果你把它倒过来，它看起来是一样的。所以回顾过去，这是一个泊松过程。T1 prime 是这个倒退过程中第一次到达的时间。所以 T1 prime 也将是一个具有相同参数 lambda 的指数随机变量。T1 prime 的预期值为 15。结论是，这个间隔的预期长度将是 30 分钟。</p><p>If you run it backwards in time it looks the same. So looking backwards in time, this is a Poisson process.
            And
            T1 prime is the time until the first arrival in this backward process. So T1 prime is also going to be an
            exponential random variable with the same parameter, lambda. And the expected value of T1 prime is 15.
            Conclusion is that the expected length of this interval is going to be 30 minutes.</p>
        <p>而这位代理人发现平均值为 30 的事实并不与公交公司声称他们正在运行泊松公交车，其 lambda 速率等于 4 的说法相矛盾。好吧。也许公司可以这样。他们可以在法庭上为自己辩护。但这里有一个令人费解的地方。到达间隔时间有多长？是 15 分钟吗？还是 30 分钟？平均而言。</p><p>And the fact that this agent found the average to be something like 30 does not contradict the claims of the
            bus
            company that they’re running Poisson buses with a rate of lambda equal to 4. OK. So maybe the company can
            this
            way. they can defend themselves in court. But there’s something puzzling here. How long is the interarrival
            time? Is it 15? Or is it 30? On the average.</p>
        <p>问题是，我们所说的典型到达间隔时间是什么意思。当我们说典型时，我们指的是某种平均值。但平均值是多少呢？这里有两种不同的平均值计算方法。给公交车编号。有 100 号公交车。有 101 号公交车、102 号公交车、110 号公交车等等。一种考虑平均值的方法是随机选择一个公交车号。</p><p>The issue is what do we mean by a typical interarrival time. When we say typical, we mean some kind of
            average.
            But average over what? And here’s two different ways of thinking about averages. You number the buses. And
            you
            have bus number 100. You have bus number 101, bus number 102, bus number 110, and so on. One way of thinking
            about averages is that you pick a bus number at random.</p>
        <p>假设我选择那辆公交车，所有公交车被选中的概率都差不多。然后我测量这个到达间隔时间。对于一辆典型的公交车来说。然后，从这里到那里，对于泊松过程，预期时间必须是 1/lambda。但我们在这个实验中做的是不同的。我们不是随机选择一辆公交车。我们随机选择一个时间。</p><p>I pick, let’s say, that bus, all buses being sort of equally likely to be picked. And I measure this
            interarrival
            time. So for a typical bus. Then, starting from here until there, the expected time has to be 1 over lambda,
            for
            the Poisson process. But what we did in this experiment was something different. We didn’t pick a bus at
            random.
            We picked a time at random.</p>
        <p>如果情况是这样的，我更有可能选择这个间隔，因此是这个到达间隔时间，而不是那个间隔。因为，这个间隔对应的时间很少。所以如果我随机选择一个时间，在某种意义上，比如说均匀的，这样所有时间都有同等的可能性，我更有可能落入一个大间隔而不是一个小间隔。</p><p>And if the picture is, let’s say, this way, I’m much more likely to pick this interval and therefore this
            interarrival time, rather than that interval. Because, this interval corresponds to very few times. So if
            I’m
            picking a time at random and, in some sense, let’s say, uniform, so that all times are equally likely, I’m
            much
            more likely to fall inside a big interval rather than a small interval.</p>
        <p>因此，一个人在随机时间出现在公交车站。他们以有偏见的方式选择一个间隔，偏向于更长的间隔。这就是为什么他们观察到的是一个随机变量，其预期值比普通预期值更大。所以这里的微妙之处在于要意识到我们正在讨论两种不同类型的实验。</p><p>So a person who shows up at the bus stop at a random time. They’re selecting an interval in a biased way,
            with
            the bias favor of longer intervals. And that’s why what they observe is a random variable that has a larger
            expected value then the ordinary expected value. So the subtlety here is to realize that we’re talking
            between
            two different kinds of experiments.</p>
        <p>随机选择公交车号与随机选择间隔并偏向较长间隔。人们使用泊松过程和一般随机过程可以编造出许多悖论，这些悖论通常与此类故事有关。我们在这个特定示例中遇到的现象通常也会在遇到其他类型的到达过程时出现。</p><p>Picking a bus number at random verses picking an interval at random with a bias in favor of longer intervals.
            Lots of paradoxes that one can cook up using Poisson processes and random processes in general often have to
            do
            with the story of this kind. The phenomenon that we had in this particular example also shows up in general,
            whenever you have other kinds of arrival processes.</p>
        <p>因此，泊松过程是最简单的到达过程，其中到达间隔时间为指数随机变量。还有一类更大的模型。它们被称为更新过程，其中，我们再次有一个连续到达的序列，到达间隔时间是相同分布且独立的，但它们可能来自一般分布。</p><p>So the Poisson process is the simplest arrival process there is, where the interarrival times are exponential
            random variables. There’s a larger class of models. They’re called renewal processes, in which, again, we
            have a
            sequence of successive arrivals, interarrival times are identically distributed and independent, but they
            may
            come from a general distribution.</p>
        <p>因此，为了在更简单的环境中说明上一个示例的相同观点，假设公交车到达间隔时间为 5 分钟或 10 分钟。因此，您会得到一些长度为 5 分钟的间隔。您会得到一些长度为 10 分钟的间隔。假设这些间隔的可能性相同。因此，从长远来看，我们拥有的 5 分钟间隔并不等于 10 分钟间隔。</p><p>So to make the same point of the previous example but in a much simpler setting, suppose that bus
            interarrival
            times are either 5 or 10 minutes apart. So you get some intervals that are of length 5. You get some that
            are of
            length 10. And suppose that these are equally likely. So we have not exactly In the long run, we have as
            many 5
            minute intervals as we have 10 minute intervals.</p>
        <p>因此，平均到达间隔时间为 7.5。但如果一个人在随机时间出现，他们会看到什么？5 和 10 的数量一样多吗？但每 10 个覆盖的空间都是两倍。</p><p>So the average interarrival time is 7 and 1/2. But if a person shows up at a random time, what are they going
            to
            see? Do we have as many 5s as 10s? But every 10 covers twice as much space.</p>
        <p>因此，如果我在随机时间出现，则有 2/3 的概率落在持续时间为 10 的区间内。并且有 1/3 的概率落在持续时间为 5 的区间内。这是因为，在整个实线中，2/3 被长度为 10 的区间覆盖，只是因为它们更长。1/3 由较小的区间覆盖。</p><p>So if I show up at a random time, I have probability 2/3 falling inside an interval of duration 10. And I
            have
            one 1/3 probability of falling inside an interval of duration 5. That’s because, out of the whole real line,
            2/3
            of it is covered by intervals of length 10, just because they’re longer. 1/3 is covered by the smaller
            intervals.</p>
        <p>现在，如果我落入长度为 10 的区间，并测量我落入的区间的长度，结果将是 10。但是，如果我落入长度为 5 的区间，并测量它的长度，结果将是 5。当然，这与 7.5 不同。好的。哪个数字应该更大？</p><p>Now if I fall inside an interval of length 10 and I measure the length of the interval that I fell into,
            that’s
            going to be 10. But if I fall inside an interval of length 5 and I measure how long it is, I’m going to get
            a 5.
            And that, of course, is going to be different than 7.5. OK. And which number should be bigger?</p>
        <p>第二个数字更大，因为这个数字偏向于较长的间隔。因此，这再次说明了在这种随机事件现象中会得到不同的结果。因此，底线是，如果您谈论典型的到达间隔时间，则必须非常精确地指定我们所说的典型。因此，典型意味着某种随机性。</p><p>It’s the second number that’s bigger because this one is biased in favor of the longer intervals. So that’s,
            again, another illustration of the different results that you get when you have this random incidence
            phenomenon. So the bottom line, again, is that if you talk about a typical interarrival time, one must be
            very
            precise in specifying what we mean typical. So typical means sort of random.</p>
        <p>但要使用“随机”这个词，你必须非常准确地说明你使用的随机实验是什么。如果你不小心，你可能会陷入明显的困惑，例如以下情况。假设有人告诉你平均家庭规模是 4 人，但平均每个人生活在一个 6 人的家庭中。这兼容吗？</p><p>But to use the word random, you must specify very precisely what is the random experiment that you are using.
            And
            if you’re not careful, you can get into apparent puzzles, such as the following. Suppose somebody tells you
            the
            average family size is 4, but the average person lives in a family of size 6. Is that compatible?</p>
        <p>平均家庭规模为 4 人，但一般人平均生活在 6 人的家庭中。是的。这里没有矛盾。我们谈论的是两个不同的实验。</p><p>Family size is 4 on the average, but typical people live, on the average, in families of size 6. Well yes.
            There’s no contradiction here. We’re talking about two different experiments.</p>
        <p>在一项实验中，我随机挑选一个家庭，告诉你这个家庭的平均成员数为 4。在另一项实验中，我随机挑选一个人，告诉你这个人的家庭平均成员数为 6。这其中的奥秘是什么？如果我随机挑选一个人，大家庭更有可能被选中。因此，人们倾向于选择大家庭。</p><p>In one experiment, I pick a family at random, and I tell you the average family is 4. In another experiment,
            I
            pick a person at random and I tell you that this person, on the average, will be in their family of size 6.
            And
            what is the catch here? That if I pick a person at random, large families are more likely to be picked. So
            there’s a bias in favor of large families.</p>
        <p>或者如果你想调查一下，比如说，你所在的城市火车挤不挤？公交车挤不挤？一种选择是随机挑选一辆公交车，检查它有多拥挤。另一种选择是挑选一个典型的人，问他们：“你今天坐过公交车吗？公交车挤不挤？”好吧，假设在这个城市，有一辆公交车非常拥挤，其他公交车都空无一人。如果你问一个人。</p><p>Or if you want to survey, let’s say, are trains crowded in your city? Or are buses crowded? One choice is to
            pick
            a bus at random and inspect how crowded it is. Another choice is to pick a typical person and ask them, “Did
            you
            ride the bus today? Was it’s crowded?” Well suppose that in this city there’s one bus that’s extremely
            crowded
            and all the other buses are completely empty. If you ask a person.</p>
        <p>“你的公交车挤吗？”他们会告诉你，“是的，我的公交车很挤。”空车上没有证人可以为他们作证。因此，通过抽样人员而不是抽样公交车，你将得到不同的结果。而在过程工业中，如果你的工作是检查和检验饼干，你将面临一个很大的困境。你想知道一块典型的饼干上有多少巧克力片吗？</p><p>“Was your bus crowded?” They will tell you, “Yes, my bus was crowded.” There’s no witness from the empty
            buses to
            testify in their favor. So by sampling people instead of sampling buses, you’re going to get different
            result.
            And in the process industry, if your job is to inspect and check cookies, you will be faced with a big
            dilemma.
            Do you want to find out how many chocolate chips there are on a typical cookie?</p>
        <p>您要采访饼干还是采访巧克力片，并询问他们饼干上还有多少其他巧克力片？在这些情况下，您会得到不同的答案。因此，道德是，您必须非常精确地制定您的采样程序。您会得到不同的答案。
        </p><p>Are you going to interview cookies or are you going to interview chocolate chips and ask them how many other
            chips where there on your cookie? And you’re going to get different answers in these cases. So moral is, one
            has
            to be very precise on how you formulate the sampling procedure that you have. And you’ll get different
            answers.
        </p>
        <h1 id="markov-chains-i">16. 马尔可夫链 I</h1><h1>16. Markov Chains I</h1>
        <p><img src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2MBERISGBUYLxoaL2NCOEJjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAACAwEBAQAAAAAAAAAAAAACAwABBAUGB//EADkQAAICAQMBBQYFAwMFAQEAAAABAgMRBBIhMQUTQVFhBiIyUnHBFDRygZEVI0IzU2IkkqGx0UOC/8QAGAEBAQEBAQAAAAAAAAAAAAAAAAECAwT/xAAcEQEBAQEAAwEBAAAAAAAAAAAAARECEiExUQP/2gAMAwEAAhEDEQA/APJdlfDM3s5/ZfwzN2SCymQgHT7MeaZLyZtRzuy5czR0UWCw0UkGii0g0UgkULtXQFcDbV7qFJckUaWAkUgkAcQ0gUMRUWgkiINARLkNIpINICJBJESDSKKSCSLSLSKIohJECwBRWA8F4ADBA8E2hQ4JgPBMFAYJgMmCAcFYDwTAC2uSNB4BwADRW0YCyoBgsZgpoilMrGBj6gtBAAyDwC0AtoFjGgWiBTBYySAYC2LaGsB9CBbQOA2CwAaBfQNgPoQSv4xjBqXLDZQEuhwPalY0lP6/segOB7Vflaf1/YlHJ7L+GZuMHZnwyNxkWUTJQG7sx4vkvNHVSON2fLGqj6naRYCQaQCGLlFFpchpFJBooG1e4LSHWL3GLiBEg0iINAXFDEUgkgCSDSBQaKLSDSKQaCrSCSIkEkVERMrOMrISQnuF+Inc0nx7v1AesA1WKalLpFPCfmKrslPuVjmWWyoUOen7lrDhP/xkKdbaq4b+sU8P0GtqKy2kvUx2UTjp7q8ZdsuMeBqtpc6lDK4x1WQCUoNpKSf0Yq7UOq6qtQcna2lyVXpHCaluTx6E1WnndqNPNJba5Nvn0wBUdZCdcpKL3Rn3e3zY225UyrjOL994z4JgXUqt1ThDiE9zS/gmqpnqa5xwkksw56sodW3OOdrjz4hPgz2z1CjFqKyo5aXmNuqdtSisL6gFx5lSaSyxFelnCyMt0eAr5J21wjJ7tyzjyIhjaUox8WBbbCrDm8ZeApr/AKir9/8A0K1lcrq51JNZWd3qFFGcZyko87epeDM7J91VGX9p7W5fVDHvt0kWl70knjOAG4KlhdTJCm9TTaeE/nHW7+/qxnbnkqJGSsTcfPBGhdHN17XwuSx/A5oigwC0GCwFtANDGC1yQLaAaGsBoBTQDQ1oBhCmgcDRbAW0A0NYDIJWuobKrXulgAzge1X5Sn9f2PQtHn/av8pT+v7Cjj9m/DI3GDs3pI3GRCEKIH6N41Vb9Tvrqecpltug/wDkj0cfAsBIZHoBEYUFFDELiMXUokl7jFRQ+S9x/QTEA0gkikGkAUUEikGkUFENAxDiASQaRWAkUFFBpAoJFFpFotFgCq4qe7HIxIpBJARLkvBaIFTBC8EwBMEwQhUVghZAKwVsW7dhZ8yyEUOFnPiWWQBVlcZ8SSf1KwhjBaAHBTWVgIoIWoRisRWEUw2gWFBgBoYC0ELaAaGsWwAYEugxgMilsCQb6gMIBgtBMpgLYDGMBkBwXuEYUeIIpgLZ5/2s/KU/r+x6Fnnvaz8pT+v7Eo43ZvSRtMPZ3SRuMiEIUQFF4kvqekreYR+h5rPJ6PTPNFb/AOJYHxDQMUGkaBRGIBIZEAmuGISNAlAWkMSBig0ii0MSBQaANIOIKGRRRaDRSRJx3Qa9ADQaRhrjbXVXtUt217s+YUZ6lNJpvL64A3JF4M+LPxKfOMr+PE1IqqwEiBJBELJgsCmVgImCiiELIKIVZlQbTwzLXqpKqM5+85RTwkBrwTBmhrFJxWx5bG3XqqUU03u8gsHgoDvf70YLo02MKBBYYDIKZQvUWSqipRSeXjkB6qMXifXOMIaGtAtA13wtliPPGSu+jK11r4kBeCn0DBaCFtC2hrAkAtoBoY0BIKVJAMaxbRlC2C0G0CwAkgWGwGUNXwoBjfBC5IgWzz3tb+Vp/X9j0LPPe1n5Sn9f2JRxezukjaYuz+kjaZEKLKAtM9D2fLOjrfoedR3+yXnRx9G0IN6DQCDRoMQcQEMiUEhKXLHxEv439SA0g0Ag4lBoJAoNAEkMiDENFBoJIFBoC0GkCg0UEi0RItBUCwUggiEIQKogNtkaq5Tm8RSyzLDUai1ZrpSg+jlL7FxG0sxuerj/APlGX0kX+McOLKrE/SORitLJsjjojN+Nr8VNf/ywvxunSbdkUl5jKad3cVhqK/gkoqXVJ4Ew1feR3VVucPMZXdG2GYP6p+BAe2OU8cojRCAU0A0GwWAuyEZrEllZyLlp6pNycVlvI5lFCoUwrbcI4B7qCm5qPvPxHMpkwAwWGYO1NQ9PppSXUB9lkYfFJIDvIS6SR4bW9oXWzeZszrW3xWFNmPJH0DOfEGR4iHbGsgsK1mqj2hvj/qYki+UHqJAs5dHbtFuFPMWb67oWxzB5KCYDDbQDAFsAKQPiA9i5BsXIgFnnva38rT+v7HoGee9rPytP6/sKOJ2f0ZtMXZ/SRsMCyEIQQ7fYss6eS8pHEOt2JLiyP0ZR2UGhaGI0DiMQuPUYihkUJkv7jHx6Cp/GwLQSBQaANBoBBoA4hoGIaKDQaAQaKDQUQUGgoi0UWBeCymZ79bVp5xhZnM+mFko0kfHiYZdoOUoxposm5eLWEJbtv5tt2rGdkH/7ZZA/tOSehtw0+Puaa0sL6HH1KVLnWotUzW1+j8zVLWT3RppSdjeOeiXmXxHQlJJZbwJ/GafxsiIlQvi1Vu7/AI9F/Axe8lGqnMfCUuET1EBfrq4xcq4b1544OHqblqZwucI+9xGMXl8G/tiuyzTx08J/3LJY2x44Gdn9j1aWhRSzZ878GblkBaG22SqnLEISXwo1RnD8clBrMoPckJjoZx92SlJLyfAyGjlXLvYNKzGMeGPIz1ZVbCCqrlZlNbZrrFhW2Rqhul0MCygVdBpNPqshAUwXwEVIoAphAMCmcjty2mGkkrZpPwRt7R1cdHpZWy8Oh4DX623W3udkm/JGOqM9sk7HjpkApshhELKIBeTt9h6lxs2ylwzhmrRW91fF+BYPZN5BZVUlOqMl4okjYFsqPMkRkh8aIHMUxjFyKAkee9q/ytP6/segZ5/2r/K0/r+xKOLoOjNhj0HRmwwIQosgh0+xJf3rF5xOWdDsd41ePOLKPQLkZEVEbE0Dj1GIWhiKGroLs+MOPQCz4kBEHEBBoA0MQuIxFBoNAINAMQSBQaKCQa6AoJAEXwV0Rhvvlfa6KJbdvx2eXoWK6Ckn4nO7RnGvUaWxtJKxpv0aM9lkqLU9GpW4WJrPBz7I29o6yMLXNxgnLjwZvnhHbWonc8aevOP85cI89ZZf2N2qq7LW6rVlN9DovtC/TwVSokprjjnJm1nZ2p7Vo/uQ2TXMXLqLz+DTqdd3kU5Vvuo8vH+T8MA6R305lKEpRn70pRXK9DF2NfTprLKu0bPfqXu7uiO7ou1NBrZbKbY7l4Pgecir0+ppUVLu5crhyWQrda5Jqtbc+L/+DZaCmTyk8eSfAcNLXDpBE3kYtHpG9TLUz3Z24WTpYL+nBZm3VV0IXnJRELuojbhp7ZrpJGeU+8So1Huzymn4SNjFW1Rsi4yWfJ+QUm+iUsuHy7UvId0S+giNk6HsueY+E/8A6O3ZCLBfUtsEooGZYnU2RrqlKTwkupKPI+1HaLuu/DQfuw6nnTV2hZ3ussmnnMjKckQosoCiyEAgUc5BCQHrOyZbtHHk1yOd2FzpDotmwDJX8RGFWveYBSFsaxbKFs8/7V/laf1/Y9DI897V/laf1/YlHE0HRmwx6HozYYEIUWQQ29lPGth65RiNPZ7xrK/qUemiMiLiMRoGhiyLQcQGxBt6xCiDd/iUVEYhSDiwHRCQEWGgGINAINFBoYhaGRKBut7qKwt0pPEV5lJavGd1f0wDY9mqplL4eV+4+vdvnnp4AZNXrWtJbGKcb0sbfuc2qOs1NSjpa+7pXXfw5vxZ1GpWdoS7mcYuMMTeM854HKvUrpdH/tLLiuZPs3W91FRuj6xjwkatL2bOmtRdmF47fE1bNT/uw/7S1DU/70P+0150FXpqodILPmNUUuiE7dR072H/AGl4vXW2HP8AxM6PMdudmyhXqL9Rt3SnmO3qea2WQlmGYnuO0+ybNT3molqMuMW1HHB52EoNKNsUt3RnLpqc6yU9r9oadx26iaS8Gz3vZWtXaGgqv/yaxL6nirNAocrmJp7I19nZmqVaklTa+c9EJVsx7gCyyNVblIyXS1VumlLSW1Sm/hYCWsVVctZKt7ZpvYjaNSV8lndGPpgKuyXed3YkpYymujQLVn4mM44dbjyBdP8A6mpRjucct48EVGkpiXqYJ4kpR+sS1dXL4Zxf7gXOMZJxkk0/AyNT0r4zOry8YmzwAaBgYzU4qUXlFsyalS00J30LOOXX5k0OthrNPvxskuJRfVDUaJPg8f7Sdp2Tvemg3GMevqeuU4SlhSTfocT2i0Wnlo7L3HFiXUnXweJfUEKRRzQJCyYAohZQFlooZTBzsjFLLYHpexI40Kfmzexekp7nTQh4pDGdADCr8QZB19ACYthsBgBI897V/laf1/Y9Czz3tX+Wp/X9iUcTQ9Ga0ZND0ZrMCEIQCxumltvg/VCQoPEk/Ug9bHAxCK23FP0HR6GoGRGRFIYmaDY8Et5iiRJb8AC0wkAg0FNgNQqHQYghkRiFRDRQ1BoCISKq5wjZFxmspgKiaWFdJIZENAJ/BwiswbjPxkur+oSsuqeLI74/NHr/AAPIgKrthYswkmMM9mnjN7otwn5xK722n/Wjuj80QGTrbm7E+duEhOnruccW5W1vBprnCyOYSTQQCoafFMq92crB5TtfQvTSVDi3HrGSPYoydo6GOtqSziUeUzPU1vmvK03Ren2zeJdMMw6pKSaimdKdNmntddscNeJU6VbiK8Tlnt0+xx9J2lq9BPNNsl6N8HotD7WVWR2ayvD810Zz7+zoSjtx+5ydTo50POMo65Y5V7unXaK2nvKtYoQ8nLobaVXtzXJSz/knnJ8wy+iyatJ2lqtFNOq2Sx4eA1nX0lpeJktnQrXCVa46tI83pva6xcamlS9YnZ0XaOi7QVjhPbOSw1Lhl1WqNVcm+6nKPjxInd3RXF270lEvT0qlyalui8JDnyUcy/U9oV6lRjplZU1y0w661qL5ylXsjDhx6ZfqbJyjCLlJpJeLMFWsr/EXOuTnXLlyS4TCHumqyOYra1wpR8DNqVDUdnWRvWcJp/sNjdGFe2vM5vlYQrWbtP2Xa0t09rb+rFHgLVFWSUemeBYyzO5t8ZFtnJEKJlEyBCEIgIdfsKmM790lnHQ5J6XsKjbp+8fj0LB1H0BYTBZsAMr+EW+oyHwARgSDYtgBI8/7Vflaf1/Y9BI897Vflaf1/YzRxdD4msx6HxNeTIshRZBCIhEUeq07zRB+cUPj0Mmge7R1P/ia48FgZENARYaNBkC7H/bBQU/9NgLiMQuIxAHFjExUQ0A1MOLFoOIDkwhcWGaBxDQtBJhTEywUy8gEWUi8gKnp4t7oZhLzXiV3tlX+rHdH5o//AAcWUVCcbFmMk0EJnp4ye6D2S84gq2yp4tjmPzR+5Ay/TVXxxZFP1OHZRDTXzjFPGeMnejZGyOYSTXoK1OmhqIYksPwYxrXFccoy31RfDjlYNt1VmnbjJNrwkZ2nJNLqzpsxXLsqim/7cVjwMWr0E4JWRxh+B25Ux3LL9WVKqM053LKfEYmbymPLSWHh9SKTi8ptP0O3foq7E3sS9Tl6nSOrlZaOd5Zxs0fb+t0qUd/eR8p8nV0HtQrdQ46qChB9GvA8oV0JuD6HZD8bW43SUapLiKfLH00V6eqNVUUoo+dR1V8cYumsdPeOhV7Ra+uMV3m5LzLOjXuDPrLK69POVjSjg8pZ7TayfwqMTPZq9TqY/wB6yUvQl6We2HtJxd0nWntb4MLZ1rKe8jhr9zl2R2yaMLecAWuGV4ERWTIyyFkUnhjfAIi6nrOx7VZoo+nB5NHU7H1600nCx+7I1B6WQDZIzVkVKL4ZTNCmxsOIIUxsfhQEkLYUhbAGTPP+1P5Wr9f2O/Loef8Aaj8tT+v7Eo4ui8TWZNF0ZrMCEIQgha6gloD0fZcs6Kv0yjcjndjv/o/pJm9M1A1DExKYyLKGphS+B/QCLD8GULiw4sTFjEA1MOLFIOLAdEOPUUmMiVTEMTFphJhDMhJgIsoYi8gIIKNMJC8lp8gDHUR95yW1KW3LJLUR3RUXnJVtMLY7ZcLOeCV0QrbaX8gSOpU65Sgmml0YcLd0lFrrFSCWEsYREknnxACdEXLdW3XPzQPfTqeLo8fNHoOzyRgBJV3Qw8STOXrNHKlb68uJuuoeHKh7J44x0Zjov1tVbr1lSc28RlF8MarnYW+Pqy5LMm/2watVo51TVmU8vnHgZP8AJo3KoLIZWZdF0QqyqMo42rJokt/0RVvCUV4lVyNToIcPHPXg5+p06rWY/uju3yjznhdDl2USuy8tLwwc+ozXMIabNHKCbTXHmZmmjGIOqDlLPgjp1V5RmonCWnccYaN1azBY4aRjp14jPrpKmn3fiZxZcs6evi5yw/AyOnj1ETplxgtcjJUsBLa+SsAfAcHmIMseAdfwhkRMkIB1uze03Viq3mHn5HcjZGyKlF5R403dn66enmot5g+pqUekY5P3UZYTjZBSi8pmhvg0I2LkE2LYAyOB7T/lqv1/Y7sjg+0/5ar9f2M0cfRdGajJo+jNZkQsohBCyiAdzsWWdPNf8jpo5HYsvcsXqjqo1AxMYhSDTNByGLlCUw4sKUhqYldWMTIGphpiojEyoamHEUmHFlDohoUmGmAxMJMBFplDEy0wMlphTMlpi8hJgGi+AUy8lBcEByTJARGwckZRMmfVJ7YWRWe7luwPByQZ7IRve9WLa10Offp/ek6cyjHq0dOVFMpZ2LIyMYqO1JYKuvP74xi8vDBc5WL+3HOF1Z1rez6JtyjHEmYZ1SqbrawvBmpVlYPwzk82PPougfcLolgtuVCxLMsjYyUmpLo3guKwW6ROWWIs0cXDO1L9up2JwTzwA61mI8THEs0aoe58N+HkOq27fUfrK5Sz59TLXw+Tz/0mNcei9XXxuxkxvMVlnUUvew+UKm9O21xl8YOcrV51ypTXmDLElnBL47LpY6Z4F7uTblfQJR8gorESnLL4CDKEIQIhEUWBt0OunppJN5h5HparY3VRnF8M8ajqdk651TVU37j6ehqUd6QuTDznoLZoC2cH2m/LVfr+x3JHC9pfy1X6/sZo5Gi6M1mTR9GazIhCEIIQhCjqdjSxKxHXTOJ2PLF8l5xO0iwGmMixURiZQ2LGRYlMOJQD+NhxYEn77LTIGpjExSkGiqdFhIUmHFgNi8BqRnlVGbTln+RtcVCOF0KDldGEoqX+XQKOoracs+7nGX4iLqY3Sr3dIvOPMB6R7FBSzGMsxX2A2xug03uXAW9QjmUlgw/h5d5WksR/z9UaboK1QXGIyTAb30G8b1kuN0H0mmZPwmbpyWMSec+Qfc2bIr3U4NNY8cFg1d9BYzJc8dSd/B4xLOXjgzVadxac8PDk8Y8wYaWSeMpR3JpIDX3sMv3lx1CVsJLKkmY3pZSjtcktsWlhdcjoVbJykse9jjywBpyTIGS8gTJRMg5AtsmQckyBeQLa43RxIvJMgcu/Tyqbyt0fMTHak0ujOxJJpp8mHUaPGZ1fwblajP1X7EjzGIEZPc0wov3PozTTNd/qtGXWVLCnCPPiP3brW/DI3Znbxy+WZs1HH3SXgEn3iWV0NmvhGKUkufERXFNNnm65x15rnavTb5ZiuTItJZKajFZbO2od5Pu61ul/6Nun0UaVlpOT8TXHNrPUjz92hengt3UyNNPDPS9pVwlS+VlHn749PQ31zjlSSEKyc2VlEZMgQOt4mgAofEgPV0Szp4N+RJMGj8vD6Fs6AZM4XtJ+Wq/X9jts4ntJ+Wq/X9jNHI0fiajLo/E1GRCyEIIQogG7st41S9Uztp5OB2e8auB3E+TUDUw4sUnwGmUOixiYhMOLAqbxNhRYux++SLCnIZFiYsNMB6YSYpMNMB0WHkUmGmaDUw4sSmGmENyTxAUi0yqNMtMDJNy6ZAbksXuwslQtUoqXg+mQG5LyJ7xd6o+LWV6hbknywGZAstjWk5eJFLyM997jqIVTgtsvhk34gaK7Y2Z2vIWTPDdCxf20k+G0xza8wCKyDkHvU7HBdUssBhTBzkgEyUDKyMZxg/il0KVicnHo49QMus0+P7kF9UYZWYhLHmjsNqSfqcfW0d1fx8MuUalXSKY7rEvNmndiTl+wqn3FOb8EK0uqjqlJR42vDRpWbtbXVxg1DDl5GfRQ1GtSUF3dfi/FnRs7O0yuXu54yzTH3FiKSXoZ8dvtqVKqKtLXhdfF+Ym6/Pwvgu2csilFNPczfz4jJqJOSfkc+6GV6HUujx7pz7NyfwnPpmufOO14Fmm+UWvUzHKsoWUQgsOv/Uj9RYUXiSaA9XXxTBegMhels7zSwefAJs2KZxPaT8tV+v7HZkcX2i/LVfr+xKOVo+hqMuj6M0mRZCiwIQhCB+keNTX9TupnnqXi6L9TvplgagkxaYSZoNTGRYlMOLAuz4kVF4BtfKInwFNTGKQmIaYDkw0JTGRZUNTGJiUw4soamGmKTCTCmphJikwkwDyKdWLJ2ct491B5LTKF5lYlX0zB59GTupWQrz7uyO1oasKWcdeoWQFKEnZSn/8AmuWFdS7Jbt2PTA3JWQBpr7qON2ReqlXYlVJrPXPkObwmzJC5Sr3zr3S+gGlXV5jBTTbKur7ycHn3U+eTPKzFU7IQUZLzRrTzgDNZZanFRi+Jf+C3XJu5Z/1Emn5GgmRgXVW63LnKaWAtrTy5sLcsFNgKujOVlcoJYi8vIi9Tk7JRTy0or+TW2C2Bm/uVWxXLj4+gjWSc5Za4Twje+TFrcJxSRYOdrLZU6G2cY5fQ5PYdmdZJt4yuh25xTpw+U2c6PZ1f4yVkJOCjhpLzFiuruyyNlQWI+pZuKXJZYqUeOR8inXtWXzJgZmvQz2rHODdKOF0EyhvUl0YwcjUVRmsqOGYJxcHhnYtr2SUZPqZdXpW+UuTl1Gcc8hHw8FHNFloEtAdvsm3NMoPwNrOR2TPFzXmjqtm4JI4vtF+Xq/X9jsNnG9ofy1X6/sSjl6Q1dTLpOhpMiyEyV4EFkKIAcXiS+p3oPMU/Q8+up3KnmuP0RQ5MJMBFoocmGmKTDUiiWvmIKbJY+hSYDUHGQqLCTAcmGmJTDTCnphpiNySy3gKFsZfC0/oUPUg0zNZb3cN+MrJX4lKyUX0Syn5gbEwkzE9VhJ7H1S49Q4apSslBppqW3JdGtMJMzzujW4qXWXCRX4qCk08ppZA1plpmRamHPDzwsEr1SlHO15y+AjXkvJlerrXm1xyvUbVbGyLa4w8cgNIsLokZvxcNqbT6Z48iS1W2UVtbzLbwBoeHw1kvjjBmlqYxr3SXGWg67d8prGNpVNJkzS1cVGUlGT2+Qcr4xcU8rcA7JGzMtXFxbcZLEtqXmMjappOOcPxANsrJWSslBZMOufv/ALGtsw6x5n+whCJP3KxdT3Ny82S97aM+KL06xXH6Gmj0WwFNKWJcPwGJe7lmlSEMvL8AXLLc/LhBWSarSXj4i+vHgiitrxkVcnhTiuV1Q9/CKk8P6kGSWy7EZfQxW2Spk6Zcr/HJttjiTlEx6+PewU49cGKywa+ChfwsJpMzDtTb3rg/FRwxJxqLKZMkIjTop7L4s7jZ5yDxJM78Jbq4v0NQE2cj2g/L1fr+x1Wzk9v/AJev9X2FHL0nQ1GXSdGajIspFFkEZCiyi11OxppqVMMPojjDabpUyzF/UDtphJmei+N0U0+fIdkoYmGmKTDiwCsfH7gJl2P3QEyhqYaYlMNMgcmEmKUgkyqdxJYYUIxj0SQtMJMobhSg0+jFrTQdcINv3XnJaYaYFypjLd7zWWn/AAWqoqWcvLlu/ctSCTyATgpTjN9Y9Ba00Nzbb6NfyMUi1Iop0xlLKbT45+hFp4xXEpJ85fnkPJMgVGiCTS6cP+BkIqClj/J5YKYW4AFRBRcecNY/YvuYtdWnu3ZXmXkvdhALnpoSWG3jnP7jYRUM48cGV2Xbn0xk07uChf4aLc25P3lgj06lNSc28Y/8DM8kbIFS0ykmnJ/FuXoxkIbUlubwXkrJRbZWQWytwFuRi1TzYam8mLUP+6WEZ9TL+wl5vA6r4PRGa7LhF+UkbKlw4+ZqNJhOfK4DSwuH+zFyTivoGpZSZtQ3bnW1Fcg1yzFPzCm8JioPa0gGzeIme2TWGNl0EzSlBogkcT9eDBcnXJ1vzyjRRPu54bL11Skty6mb8RwNQkrpJCh2rji3OBBwrCFlEIDj1R3auKo/Q4MXydjS295SvNFgccrt/wDL1/q+x1MnK7d/L1/q+xaOdpDQZ9J0NJkUWQhBCEIBCEIAddkq5bovk6mm1Ublh8S8jkZCjJxeU8Mo76DTMGk1isShPiXn5m1MoKfMRaYcn7rF5APISYvISYDUwkxSYUXyVTkwlIUpBJgOTDTwITGJlDG3teHzgRprbpwrnnMWnuG9VjwLrjGEdsVheQEjKT0+9SxKSygqLm61OckvDHkwYwjFY6pPK9C+7h5dXlgNlao1ua5QMNTuaWOpIpRhtxwXwnwkAU7dsox+Z4JTY3KyDedj6i3HNim2+OgVcVDOHlt5bKJ+LWfh/wDI2eLK2nxlAceSKm2ovb18AFaZZc6bE90P8s9UaVHu63GLf7mWDugv8Nz8RtUnKGJtOWcPACtPdfOFc85jzuyh2+b029T96Syi4RjCO2KwvIpVxSS64fHoBVN+6pTk/i4248Q5WpV71ygHXH/D3XnJcUowUeqAGOpU5KKiy5WZm608PGS/dT4SBlGLzjhtYyUBDUblFbW20J1Ek5proaFCKjiK6LBjuj3eF5CEKk9+nk11jyaqp5Sa8VkTRFLh9GXQvddfSUH/AODcaaJPcsAQ4e0Dc0+URyTaZoHY+BW19W+gU5JY8wcOXMugBbsxFZymSTUegtTwQBbH3Wy1craVnquGG3mPJh3dzc4PpLlEqMfaMcTi/MxHU1sN9WV1RzDj0lUQhDKLNmgscbHHwZiN2kjGuHeyLBulJRTb6HG7Xv72MYromOv1MrW0uImDWfBH6gVozSZdJ0NRBCFFkEIREAhCEAhCEAtcPJ0NJrU8Qtf0ZziFHoOHHgDJztLrXWtk3lG6MlJZXQoZkiYGS0wGphJi8lpgNUg0xKYaZVOTCTEp+oSkEPUi1ISpBbih2S0xKkGpAN3FpitxNwDcl5FKRNwUzIM05Yw8YZWS9xUDGpqae7xDqThKece88lbibgGbuS85Fbi9wUzJTYG4rcEFkjYG4rIBZMmsZocjLq/hbXURYNpwhGS5RdnDjfHw+L6Aaa1W17ZeAcfck4S+F9DrGjPdmk8lSrWeGZ4T7mx1P4XzF/Yc5FVTW3wyRyzHJTl6ipTAXfN5whMZscmm22JnFbjKHxeYLBk1de+OV8S6GiptRFWP3mSoyd6pUvPVLk5zNmqqeXOP7oxnKpVEIQyiIZK2Ukk3whaLAszaz4I/U0GfWfBH6gVpehoyZ9L0NKQEQWCYwQghCFAQjIRgQhRALIUQosdRqJVPD5iIIB2IWKSzF8BpnJpulU+OhvrvhNZykBpyWmI76HzItXw+ZFGhMJMzd/X86LWor+dDRpyGmZPxNXzov8VT86A1qQSkYvxlK/zRf46hf5gbVILcYPx9GfjL/qFHzf8AgujepF7jn/1Gj5n/AAT+pUfM/wCBo6O4m4539To+Z/wV/VKF4v8AgarpqZe45f8AVKPN/wAE/qtC8/4LqOpuJuOZ/VqfX+Cf1anwT/gaOnuJuOZ/Vqf+X8Ff1an1/gaOnvJvOX/VafJ/wU+1avJjR1dxW45f9Upfgyf1Wn1/gaOk5AWPdE5/9Uq9f4J/VKemGNDqZbLX68GtTUvdl18GcyNqtW+JoqujZHa3yjpzWo0aiDnDHSS5Qqu2W33vDqTvJV8PlATsjnclh+KNKc5+QpvLwUoqa3Vv6oCanHoBcp7ZAylliZyln1Lrk118SDRCXu5yKtll9OBscKAix4b5IhM+jOfZjvHg16izKxH9zC3lnLpEIiEMoshRYEM+s+CP1NBn1nwR+oE0ZrMmj8TVkCymTJTIIQosCFFlAQhCFEIQgEIQgEIQgELyUQCyEIBMkKLAjIQgELRRALIUQC8kyUQCyMhAITwIRgQmSEKIQhAKIQhBCZIUA2GolWtvgHHVpctPJkkysl1XQh2jKL+ZeTNdespsWcpPyOIs5IWd0124zhGbxPCfK5ClqYxXNkWcPnHV/wAhdTXma6Fusrb45GRm1jcuH0Zyk+Vk6qcXBY8hLaa0J4iZr7FFNsK2yNcOWc6du+WWW1Vynuz6ii5PJWTnWUwXghCCEIQCGfWfBH6mgz6v4I/UCaToacGTT8QGtvzAcTxE59SZ9QH8FceYnJAHceZMrzEEIH5XmTK8xBYDsrzImvNCSih+V5lZXmKKAdleZNy8xJAHbo+ZN0fMSQB25eZNy8xJAHbl5k3LzElgN3LzJuXmKJgBu+JN8fMVggDd8SboiifsA3fEm+IogDd8Sb4isEAbviTfHzFlYKHb4+ZN8RRMMBu+JW+PmKwTBA3fEm+PmKwTADNy8ybkKwWgLZRbKAhMkIBMl7uCigGVtb1u6Gx6iMF7v7GKteITLKLttlZLMmAii1jAF5XmTKFkIG7l5kyvMUQBuV5kyvMUQBuV5mfV/BH6hitR8K+oDNJGMo+9LBo7qv8A3CyAV3dXzsnd0/OyyAVsp+Zk20/NIhCCYo85ExT6kIUT+x6kzT5MhAJmn5WTdT8pCEE3VfKTdV8hCAVvq+Qm+r5CyFE31fITvK/9shAJ3lfyE7yv/bRCATvIfIid7D5EQgE72HyInew+REIBO+j8iJ30fkRCATvo/Iid9H5EQgE76PyonfR+RFkArvl8qL75fKiEAnfr5UTvl8qIQonff8UX36+VEIBXfr5UTv18qIQUTvl8qK75fKiEII7l8qKdqaxtRCALZRCAQhCARlEIA1LCwUyEAEJTil8GSyATvIfIid5D5EWQCd5X/tk31f7ZZAK31f7ZN1PyFkArdT8hn1kq3BbI45LIB//Z">12 年前 (2012 年 11 月 10 日) — 52:06 <a href="https://youtube.com/watch?v=IkbkEtOOC1Y">https://youtube.com/watch?v=IkbkEtOOC1Y</a></p><p> 12 years ago (Nov 10, 2012) — 52:06 <a href="https://youtube.com/watch?v=IkbkEtOOC1Y">https://youtube.com/watch?v=IkbkEtOOC1Y</a></p>
        <h2 id="unknown-305">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：我们现在要开始新的一章。我们将讨论马尔可夫过程。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So we’re going to start
            now
            with a new chapter. We’re going to talk about Markov processes.</p>
        <p>好消息是，这个主题在很多方面比泊松过程更加直观和简单。希望这会很有趣。马尔可夫过程是一类一般的随机过程。从某种意义上说，它比伯努利过程和泊松过程更复杂，因为现在我们将在不同时间之间建立依赖关系，而不是无记忆过程。因此，基本思想如下。</p><p>The good news is that this is a subject that is a lot more intuitive and simple in many ways than, let’s say,
            the
            Poisson processes. So hopefully this will be enjoyable. So Markov processes is, a general class of random
            processes. In some sense, it’s more elaborate than the Bernoulli and Poisson processes, because now we’re
            going
            to have dependencies between difference times, instead of having memoryless processes. So the basic idea is
            the
            following.</p>
        <p>例如，在物理学中，你会写下具有一般形式的系统演化方程。系统一秒钟后的新状态是旧状态的某个函数。因此，牛顿方程和物理学中所有方程都允许你写出这种方程。因此，如果一个粒子以一定的速度移动并且位于某个位置，你可以预测它稍后会到达什么位置。</p><p>In physics, for example, you write down equations for how a system evolves that has the general form. The new
            state of a system one second later is some function of old state. So Newton’s equations and all that in
            physics
            allow you to write equations of this kind. And so if that a particle is moving at a certain velocity and
            it’s at
            some location, you can predict when it’s going to be a little later.</p>
        <h2 id="unknown-306">未知</h2><h2>Unknown</h2>
        <p>马尔可夫过程具有相同的特点，只是方程中还包含一些随机性。这就是马尔可夫过程的本质。它描述了系统或某些变量的演变，但存在一些噪声，因此运动本身有点随机。所以这是一个相当通用的框架。
        </p><p>Markov processes have the same flavor, except that there’s also some randomness thrown inside the equation.
            So
            that’s what Markov process essentially is. It describes the evolution of the system, or some variables, but
            in
            the presence of some noise so that the motion itself is a bit random. So this is a pretty general framework.
        </p>
        <p>因此，只要正确定义状态概念，几乎任何你能想到的有用或有趣的随机过程都可以描述为马尔可夫过程。因此，我们要做的是，通过讨论超市收银台来介绍马尔可夫过程类。然后，我们将从示例中进行抽象，以便获得更通用的定义。</p><p>So pretty much any useful or interesting random process that you can think about, you can always described it
            as
            a Markov process if you define properly the notion of the state. So what we’re going to do is we’re going to
            introduce the class of Markov processes by, example, by talking about the checkout counter in a supermarket.
            Then we’re going to abstract from our example so that we get a more general definition.</p>
        <p>然后我们会做一些事情，比如如果我们从特定状态开始，如何预测 n 个时间步骤后会发生什么。然后讨论一下马尔可夫过程或马尔可夫链的一些结构特性。下面是我们的例子。你去超市的收银台，站在那里观察来来的顾客。</p><p>And then we’re going to do a few things, such as how to predict what’s going to happen n time steps later, if
            we
            start at the particular state. And then talk a little bit about some structural properties of Markov
            processes
            or Markov chains. So here’s our example. You go to the checkout counter at the supermarket, and you stand
            there
            and watch the customers who come.</p>
        <h2 id="unknown-307">未知</h2><h2>Unknown</h2>
        <p>因此，顾客来了，他们排队，然后一次只为一位顾客提供服务。因此，我们将从超市收银台的角度进行讨论，但同样的情况也适用于任何服务系统。您可能有一个服务器，任务到达该服务器，然后被放入队列中，服务器一次处理一个任务。
        </p><p>So customers come, they get in queue, and customers get served one at a time. So the discussion is going to
            be in
            terms of supermarket checkout counters, but the same story applies to any service system. You may have a
            server,
            jobs arrive to that server, they get put into the queue, and the server processes those jobs one at a time.
        </p>
        <p>现在要建立概率模型，我们需要对顾客到达和离开做出一些假设。我们希望一开始就尽可能简单。因此，我们假设顾客按照伯努利过程到达，参数为 b。本质上，这与假设连续顾客到达之间的时间是参数为 b 的几何随机变量相同。这是思考到达过程的另一种方式。</p><p>Now to make a probabilistic model, we need to make some assumption about the customer arrivals and the
            customer
            departures. And we want to keep things as simple as possible to get started. So let’s assume that customers
            arrive according to a Bernoulli process with some parameter b. So essentially, that’s the same as the
            assumption
            that the time between consecutive customer arrivals is a geometric random variable with parameter b. Another
            way
            of thinking about the arrival process.</p>
        <p>事情并非如此，但从数学角度来看，想象一下某人抛硬币，其偏差等于 b。只要硬币正面朝上，就会有顾客到来。所以，这就像大自然在抛硬币，决定顾客的到来。所以我们知道，抛硬币来决定顾客的到来，与几何间隔时间是一样的。</p><p>That’s not how it happens, but it’s helpful, mathematically, is to think of someone who’s flipping a coin
            with
            bias equal to b. And whenever the coin lands heads, then a customer arrives. So it’s as if there’s a coin
            flip
            being done by nature that decides the arrivals of the customers. So we know that coin flipping to determine
            the
            customer arrivals is the same as having geometric inter arrival times.</p>
        <h2 id="unknown-308">未知</h2><h2>Unknown</h2>
        <p>我们从对伯努利过程的研究中知道了这一点。好的。现在来看看客户服务时间。我们假设。好的。如果队列中没有客户，没有人得到服务，那么当然，没有人会离开队列。但如果队列中有客户，那么该客户就会开始得到服务，并且服务时间是随机的。</p><p>We know that from our study of the Bernoulli process. OK. And now how about the customer service times. We’re
            going to assume that. OK. If there is no customer in queue, no one being served, then of course, no one is
            going
            to depart from the queue. But if there a customer in queue, then that customer starts being served, and is
            going
            to be served for a random amount of time.</p>
        <p>我们假设店员服务顾客所花的时间服从几何分布，且参数 q 已知。因此，服务顾客所花的时间是随机的，因为他们的购物车中放入的商品数量、需要取出的优惠券数量等都是随机的。所以它是随机的。在现实世界中，它服从某种概率分布。</p><p>And we make the assumption that the time it takes for the clerk to serve the customer has a geometric
            distribution with some known parameter q. So the time it takes to serve a customer is random, because it’s
            random how many items they got in their cart, and how many coupons they have to unload and so on. So it’s
            random. In the real world, it has some probability distribution.</p>
        <p>我们不必关心现实世界中究竟会怎样，但作为建模近似或只是为了开始，我们假设客户服务时间可以通过几何分布很好地描述，参数为 q。从数学上讲，考虑客户服务的一种等效方式是再次以抛硬币的方式进行思考。也就是说，店员有一枚带有偏差的硬币，店员在每个时间段都会抛硬币。
        </p><p>Let’s not care exactly about what it would be in the real world, but as a modeling approximation or just to
            get
            started, let’s pretend that customer service time are well described by a geometric distribution, with a
            parameter q. An equivalent way of thinking about the customer service, mathematically, would be, again, in
            terms
            of coin flipping. That is, the clerk has a coin with a bias, and at each time slot the clerk flips the coin.
        </p>
        <h2 id="unknown-309">未知</h2><h2>Unknown</h2>
        <p>服务以概率 q 结束。以概率 1 q 继续服务流程。我们要做的一个假设是，这里发生的决定到达的抛硬币都是相互独立的。决定服务结束的抛硬币也是相互独立的。但这里涉及的抛硬币也与那里发生的抛硬币无关。</p><p>With probability q, service is over. With probability 1 q, you continue the service process. An assumption
            that
            we’re going to make is that the coin flips that happen here to determine the arrivals, they’re all
            independent
            of each other. The coin flips that determine the end of service are also independent from each other. But
            also
            the coin flips involved here are independent from the coin flips that happened there.</p>
        <p>因此，顾客的到达方式与服务流程无关。好的。假设现在您要回答以下问题。时间是晚上 7:00。顾客在此特定时间离开的可能性有多大？嗯，您会说，这要视情况而定。如果当时队列为空，那么您肯定不会有顾客离开。</p><p>So how arrivals happen is independent with what happens at the service process. OK. So suppose now you want
            to
            answer a question such as the following. The time is 7:00 PM. What’s the probability that the customer will
            be
            departing at this particular time? Well, you say, it depends. If the queue is empty at that time, then
            you’re
            certain that you’re not going to have a customer departure.</p>
        <p>但如果队列不空，那么有概率 q 会在那个时间出发。所以，这类问题的答案与当时系统的状态有关。这取决于队列是什么。如果我问你，7:10 时队列会空吗？嗯，这个问题的答案取决于 7 点时队列是否很大。</p><p>But if the queue is not empty, then there is probability q that a departure will happen at that time. So the
            answer to a question like this has something to do with the state of the system at that time. It depends
            what
            the queue is. And if I ask you, will the queue be empty at 7:10? Well, the answer to that question depends
            on
            whether at 7 o’clock whether the queue was huge or not.</p>
        <h2 id="unknown-310">未知</h2><h2>Unknown</h2>
        <p>因此，了解队列当前的状态可以让我了解未来可能发生的情况。那么系统的状态是什么？因此我们开始使用这个术语。状态基本上对应于任何相关的事情。现在发生的任何事都与未来可能发生的事情有关。</p><p>So knowing something about the state of the queue right now gives me relevant information about what may
            happen
            in the future. So what is the state of the system? Therefore we’re brought to start using this term. So the
            state basically corresponds to anything that’s relevant. Anything that’s happening right now that’s kind of
            relevant to what may happen in the future.</p>
        <p>知道现在队列的大小，对我预测 2 分钟后可能发生的事情很有用。因此，在这个特定示例中，对于状态，一个合理的选择是仅计算队列中有多少客户。假设我们的超市建筑不太大，因此只能容纳 10 个人。所以我们将限制状态。</p><p>Knowing the size of the queue right now, is useful information for me to make predictions about what may
            happen 2
            minutes later from now. So in this particular example, a reasonable choice for the state is to just count
            how
            many customers we have in the queue. And let’s assume that our supermarket building is not too big, so it
            can
            only hold 10 people. So we’re going to limit the states.</p>
        <p>我们不会从 0 到无穷大，而是将模型截断为 10。因此，我们有 11 种可能的状态，分别对应排队中的 0 名顾客、1 名顾客、2 名顾客等等，一直到 10。假设商店无法处理超过 10 名顾客，这些就是系统的不同可能状态。因此，这是第一步，写下我们系统的可能状态集。</p><p>Instead of going from 0 to infinity, we’re going to truncate our model at ten. So we have 11 possible states,
            corresponding to 0 customers in queue, 1 customer in queue, 2 customers, and so on, all the way up to 10. So
            these are the different possible states of the system, assuming that the store cannot handle more than 10
            customers. So this is the first step, to write down the set of possible states for our system.</p>
        <h2 id="unknown-311">未知</h2><h2>Unknown</h2>
        <p>接下来要做的就是开始描述状态之间的可能转换。在任何给定的时间步骤中，可能发生什么事情？我们可以有客户到达，这将使状态上升 1。我们可以有客户离开，这将使状态下降 1。也有可能什么都没有发生，在这种情况下状态保持不变。</p><p>Then the next thing to do is to start describing the possible transitions between the states. At any given
            time
            step, what are the things that can happen? We can have a customer arrival, which moves the state 1 higher.
            We
            can have a customer departure, which moves the state 1 lower. There’s a possibility that nothing happens, in
            which case the state stays the same.</p>
        <p>还可能同时有到达和离开，在这种情况下，状态仍然保持不变。让我们写出一些代表性概率。如果我们有 2 个客户，那么在此步骤中我们下降的概率，这就是服务完成但没有客户到达的概率。所以这是与此转换相关的概率。</p><p>And there’s also the possibility of having simultaneously an arrival and a departure, in which case the state
            again stays the same. So let’s write some representative probabilities. If we have 2 customers, the
            probability
            that during this step we go down, this is the probability that we have a service completion, but to no
            customer
            arrival. So this is the probability associated with this transition.</p>
        <p>另一种可能性是，有顾客到达，概率为 p，而没有顾客离开，因此特定转换的概率就是这个数字。最后，我们保持相同状态的概率，这可能以两种可能的方式发生。一种方式是，我们同时有顾客到达和离开。</p><p>The other possibility is that there’s a customer arrival, which happens with probability p, and we do not
            have a
            customer departure, and so the probability of that particular transition is this number. And then finally,
            the
            probability that we stay in the same state, this can happen in 2 possible ways. One way is that we have an
            arrival and a departure simultaneously.</p>
        <h2 id="unknown-312">未知</h2><h2>Unknown</h2>
        <p>另一种可能性是，没有到达和离开，因此状态保持不变。因此，从任何其他状态（状态 3 或状态 9 等）开始，这些转换概率都是相同的。转换概率在边界处（即此图的边界处）会略有不同，因为如果您处于状态 0，则不会有任何客户离开。</p><p>And the other possibility is that we have no arrival and no departure, so that the state stays the same. So
            these
            transition probabilities would be the same starting from any other states, state 3, or state 9, and so on.
            Transition probabilities become a little different at the borders, at the boundaries of this diagram,
            because if
            you’re in a state 0, then you cannot have any customer departures.</p>
        <p>没有人需要服务，但有概率 p 会有顾客到达，在这种情况下，系统中的顾客数量变为 1。然后概率 1 p，什么也不会发生。对于离开，同样，如果系统已满，就没有空间容纳另一个到达。但我们可能会有概率为 q 的离开，而概率为 1 q 的什么也不会发生。所以这是用转移概率注释的完整转移图。</p><p>There’s no one to be served, but there is a probability p that the customer arrives, in which case the number
            of
            customers in the system goes to 1. Then probability 1 p, nothing happens. Similarly with departures, if the
            system is full, there’s no room for another arrival. But we may have a departure that happens with
            probability
            q, and nothing happens with probability 1 q. So this is the full transition diagram annotated with
            transition
            probabilities.</p>
        <p>这是离散时间、有限状态马尔可夫链的完整描述。所以这是一个完整的概率模型。一旦你掌握了所有这些信息，你就可以开始计算，并试图预测未来会发生什么。现在让我们从这个例子中抽象出来，得出一个更通用的定义。</p><p>And this is a complete description of a discrete time, finite state Markov chain. So this is a complete
            probabilistic model. Once you have all of these pieces of information, you can start calculating things, and
            trying to predict what’s going to happen in the future. Now let us abstract from this example and come up
            with a
            more general definition.</p>
        <h2 id="unknown-313">未知</h2><h2>Unknown</h2>
        <p>因此，我们有状态的概念，它描述了我们正在研究的系统的当前情况。当前状态是随机的，所以我们将其视为一个随机变量 Xn 是状态，并在系统开始运行后进行转换。因此，系统开始在某个初始状态 X0 运行，经过 n 次转换后，它移动到状态 Xn。现在我们有一组可能的状态。</p><p>So we have this concept of the state which describes the current situation in the system that we’re looking
            at.
            The current state is random, so we’re going to think of it as a random variable Xn is the state, and
            transitions
            after the system started operating. So the system starts operating at some initial state X0, and after n
            transitions, it moves to state Xn. Now we have a set of possible states.</p>
        <p>状态 1、状态 2、状态 3，以及一般而言的状态 I 和状态 j。为了简单起见，我们假设可能状态的集合是有限集。您可以想象，我们可以拥有状态空间无限的系统。它可以是离散的，也可以是连续的。但所有这些都更加困难和复杂。</p><p>State 1 state 2, state 3, and in general, state I and state j. To keep things simple, we assume that the set
            of
            possible states is a finite set. As you can imagine, we can have systems in which the state space is going
            to be
            infinite. It could be discrete, or continuous. But all that is more difficult and more complicated.</p>
        <p>从最简单的设置开始是有意义的，我们只需处理有限状态空间。时间是离散的，所以我们可以考虑在开始时，经过 1 次转换、2 次转换等等之后的状态。所以我们处于离散时间中，并且在许多状态下都是有限的。因此系统从某个地方开始，在每个时间步骤中，状态都在这里。</p><p>It makes sense to start from the simplest possible setting where we just deal with the finite state space.
            And
            time is discrete, so we can think of this state in the beginning, after 1 transition, 2 transitions, and so
            on.
            So we’re in discrete time and we have finite in many states. So the system starts somewhere, and at every
            time
            step, the state is, let’s say, here.</p>
        <h2 id="unknown-314">未知</h2><h2>Unknown</h2>
        <p>一声哨响，状态跳转到下一个随机状态。所以它可能会移动到这儿，也可能移动到那儿，也可能移动到这儿，也可能停留在原地。所以一种可能的转换就是你跳转之前的转换，然后就落在你开始的地方。现在我们要描述这些转换的统计数据。
        </p><p>A whistle blows, and the state jumps to a random next state. So it may move here, or it may move there, or it
            may
            move here, or it might stay in the place. So one possible transition is the transition before you jump, and
            just
            land in the same place where you started from. Now we want to describe the statistics of these transitions.
        </p>
        <p>如果我处于那个状态，下次我处于那个状态的可能性有多大？我们通过写下转移概率来描述这种转移的统计数据，即从状态 3 到状态 1 的转移概率。因此，这个转移概率被认为是一个条件概率。</p><p>If I am at that state, how likely is it to that, next time, I’m going to find myself at that state? Well, we
            describe the statistics of this transition by writing down a transition probability, the transition
            probability
            of going from state 3 to state 1. So this transition probability is to be thought of as a conditional
            probability.</p>
        <p>假设我现在处于状态 I，那么下次我处于状态 j 的概率是多少？假设我现在处于状态 3，那么 P31 就是下次我处于状态 1 的概率。类似地，我们有一个概率 P3i，即假设我现在处于状态 3，那么下次我处于状态 i 的概率。</p><p>Given that right now I am at state I what is the probability that next time I find myself at state j? So
            given
            that right now I am at state 3, P31 is the probability that the next time I’m going to find myself at state
            1.
            Similarly here, we would have a probability P3i, which is the probability that given that right now I’m at
            state
            3, next time I’m going to find myself at state i.</p>
        <h2 id="unknown-315">未知</h2><h2>Unknown</h2>
        <p>现在，我们可以从原则上写下这些条件概率，但我们需要做出……所以，你可能认为这是一个定义，但我们需要做出一个额外的大假设，这就是假设一个过程是马尔可夫过程。这就是所谓的马尔可夫性质，它是这样说的。让我先用文字来描述一下。</p><p>Now one can write such conditional probabilities down in principle, but we need to make. so you might think
            of
            this as a definition here, but we need to make one additional big assumption, and this is the assumption
            that to
            make a process to be a Markov process. This is the so called Markov property, and here’s what it says. Let
            me
            describe it first in words here.</p>
        <p>每次我发现自己处于状态 3 时，下次我发现自己处于状态 1 的概率就是这个特定的数字，无论我如何到达那里。也就是说，这个转换概率不受过程的过去的影响。它不关心我使用什么路径到达状态 3。从数学上讲，它意味着以下内容。你有这个从状态 I 跳转到状态 j 的转换概率。</p><p>Every time that I find myself at state 3, the probability that next time I’m going to find myself at state 1
            is
            this particular number, no matter how I got there. That is, this transition probability is not affected by
            the
            past of the process. It doesn’t care about what path I used to find myself at state 3. Mathematically, it
            means
            the following. You have this transition probability that from state I jump to state j.</p>
        <p>假设我给了你一些额外的信息，告诉你过去发生的所有事情，所有发生的事情，你是如何到达状态 i 的？我们假设这些关于过去的信息与对未来的预测无关，只要你知道你现在在哪里。</p><p>Suppose that I gave you some additional information, that I told you everything else that happened in the
            past of
            the process, everything that happened, how did you get to state i? The assumption we’re making is that this
            information about the past has no bearing in making predictions about the future, as long as you know where
            you
            are right now.</p>
        <h2 id="unknown-316">未知</h2><h2>Unknown</h2>
        <p>所以如果我告诉你，现在你处于状态 i，顺便说一下，你是沿着一条特定的路径到达那里的，你可以忽略你所走的特定路径的额外信息。你只考虑你现在在哪里。</p><p>So if I tell you, right now, you are at state i, and by the way, you got there by following a particular
            path,
            you can ignore the extra information of the particular path that you followed. You only take into account
            where
            you are right now.</p>
        <p>因此，每次你发现自己处于该状态时，无论你是如何到达那里的，下次你都会以 P31 的概率发现自己处于状态 1。因此，只要你知道自己现在处于什么位置，过去就不会影响未来。要实现此属性，你需要以正确的方式谨慎选择你的状态。
        </p><p>So every time you find yourself at that state, no matter how you got there, you will find yourself next time
            at
            state 1 with probability P31. So the past has no bearing into the future, as long as you know where you are
            sitting right now. For this property to happen, you need to choose your state carefully in the right way.
        </p>
        <p>从这个意义上讲，各州需要包含与系统未来相关的所有信息。任何不在州内的信息都不会发挥作用，但州需要拥有所有相关信息，以确定下一步将发生哪种转变。</p><p>In that sense, the states needs to include any information that’s relevant about the future of the system.
            Anything that’s not in the state is not going to play a role, but the state needs to have all the
            information
            that’s relevant in determining what kind of transitions are going to happen next.</p>
        <h2 id="unknown-317">未知</h2><h2>Unknown</h2>
        <p>举个例子，在讨论马尔可夫过程之前，仅从确定性世界来看，如果你有一个在空中飞舞的球，你想预测它的未来。如果我告诉你球的状态是球在特定时间的位置，这足以让你预测球的下一步去向吗？不能。&nbsp;</p><p>So to take an example, before you go to Markov process, just from the deterministic world, if you have a ball
            that’s flying up in the air, and you want to make predictions about the future. If I tell you that the state
            of
            the ball is the position of the ball at the particular time, is that enough for you to make predictions
            where
            the ball is going to go next? No.&nbsp;</p>
        <p>你需要知道位置和速度。如果你知道位置和速度，你就可以预测未来。所以飞行中的球的状态是位置和速度。</p><p>You need to know both the position and the velocity. If you know position and velocity, you can make
            predictions
            about the future. So the state of a ball that’s flying is position together with velocity.</p>
        <p>如果你只知道位置，那信息就不够了，因为如果我告诉你当前位置，然后告诉你过去的位置，你就可以利用过去位置的信息来完成轨迹并做出预测。所以如果你不知道速度，过去的信息是有用的。但如果同时知道位置和速度，你就不关心你是如何到达那里的，也不关心你什么时候出发的。</p><p>If you were to just take position, that would not be enough information, because if I tell you current
            position,
            and then I tell you past position, you could use the information from the past position to complete the
            trajectory and to make the prediction. So information from the past is useful if you don’t know the
            velocity.
            But if both position and velocity, you don’t care how you got there, or what time you started.</p>
        <h2 id="unknown-318">未知</h2><h2>Unknown</h2>
        <p>通过位置和速度，你可以预测未来。因此，解决这类问题需要一定的技巧，或者说一定的思维要素，即非机械方面，以确定哪个是正确的状态变量。当你定义系统的状态时，你需要以一种包含所有积累的与未来相关的信息的方式来定义它。</p><p>From position and velocity, you can make predictions about the future. So there’s a certain art, or a certain
            element of thinking, a non mechanical aspect into problems of this kind, to figure out which is the right
            state
            variable. When you define the state of your system, you need to define it in such a way that includes all
            information that has been accumulated that has some relevance for the future.</p>
        <p>因此，提出马尔可夫模型的一般过程是首先做出一个重大决定，即你的状态变量将是什么。然后写下它是否可能是不同状态的图片。然后确定可能的转换。因此，有时你将得到的图表不会包含所有可能的弧。你只会显示与可能的转换相对应的弧。</p><p>So the general process for coming up with a Markov model is to first make this big decision of what your
            state
            variable is going to be. Then you write down if it may be a picture of the different states. Then you
            identify
            the possible transitions. So sometimes the diagram that you’re going to have will not include all the
            possible
            arcs. You would only show those arcs that correspond to transitions that are possible.</p>
        <p>例如，在超市示例中，我们没有从状态 2 到状态 5 的转换，因为那不可能发生。任何时候都只能有 1 个到达。因此，在图中，我们仅显示了可能的转换。然后，对于每个可能的转换，您都可以使用模型的描述来找出正确的转换概率。因此，您可以通过写下转换概率来获得图表。</p><p>For example, in the supermarket example, we did not have a transition from state 2 to state 5, because that
            cannot happen. You can only have 1 arrival at any time. So in the diagram, we only showed the possible
            transitions. And for each of the possible transitions, then you work with the description of the model to
            figure
            out the correct transition probability. So you got the diagram by writing down transition probabilities.</p>
        <h2 id="unknown-319">未知</h2><h2>Unknown</h2>
        <p>好的，假设您得到了马尔可夫模型。您将用它做什么？那么，我们需要模型做什么？我们需要模型来做出预测，做出概率预测。例如，我告诉您该过程从该状态开始。您让它运行一段时间。您认为从现在起 10 个时间步骤后它会在哪里？您可能想回答这个问题。</p><p>OK, so suppose you got your Markov model. What will you do with it? Well, what do we need models for? We need
            models in order to make predictions, to make probabilistic predictions. So for example, I tell you that the
            process started in that state. You let it run for some time. Where do you think it’s going to be 10 time
            steps
            from now? That’s a question that you might want to answer.</p>
        <p>由于这个过程是随机的，你无法告诉我它到底会在哪里。但也许你可以告诉我概率。你可以告诉我，在多大的概率下，这个状态会在那里。在多大的概率下，这个状态会在那里，等等。所以我们的第一个练习是计算这些概率，即在未来几个步骤中，这个过程可能发生的情况。</p><p>Since the process is random, there’s no way for you to tell me exactly where it’s going to be. But maybe you
            can
            give me probabilities. You can tell me, with so much probability, the state would be there. With so much
            probability, the state would be there, and so on. So our first exercise is to calculate those probabilities
            about what may happen to the process a number of steps in the future.</p>
        <p>在这里加一些符号会很方便。有人告诉我们这个过程从特定状态 i 开始。我们让这个过程运行 n 次转换。它可能会落在某个状态 j，但它将落在的状态 j 将是随机的。所以我们想给出概率。告诉我，n 次步骤之后的状态成为特定状态 j 的概率是多少？</p><p>It’s handy to have some notation in here. So somebody tells us that this process starts at the particular
            state
            i. We let the process run for n transitions. It may land at some state j, but that state j at which it’s
            going
            to land is going to be random. So we want to give probabilities. Tell me, with what probability the state, n
            times steps later, is going to be that particular state j?</p>
        <h2 id="unknown-320">未知</h2><h2>Unknown</h2>
        <p>简写符号是使用这个符号表示从状态 i 开始，经过 n 步后到达状态 j 的转移概率。因此，这两个指标的排序方式，其思考方式是从 i 到达 j。因此，如果前面有 n 步，那么从 I 到达 j 的概率就是这个。</p><p>The shorthand notation is to use this symbol here for the n step transition probabilities that you find
            yourself
            at state j given that you started at state i. So the way these two indices are ordered, the way to think
            about
            them is that from i, you go to j. So the probability that from I you go to j if you have n steps in front of
            you.</p>
        <p>当然，有些转移概率很容易写出来。例如，在 0 次转移中，您将回到起点。因此，如果 I 等于 j，则该概率等于 1；如果 I 不同于 j，则该概率等于 0。这个很容易写出来。</p><p>Some of these transition probabilities are, of course easy to write. For example, in 0 transitions, you’re
            going
            to be exactly where you started. So this probability is going to be equal to 1 if I is equal to j, And 0 if
            I is
            different than j. That’s an easy one to write down.</p>
        <p>如果只有 1 个转换，假设您从状态 i 开始，那么 1 步后您处于状态 j 的概率是多少？这是什么？这些只是我们在问题描述中给出的普通 1 步转换概率。因此，根据定义，1 步转换概率具有这种形式。这个等式是正确的，因为我们定义这两个量的方式。</p><p>If you have only 1 transition, what’s the probability that 1 step later you find yourself in state j given
            that
            you started at state i? What is this? These are just the ordinary 1 step transition probabilities that we
            are
            given in the description of the problem. So by definition, the 1 step transition probabilities are of this
            form.
            This equality is correct just because of the way that we defined those two quantities.</p>
        <h2 id="unknown-321">未知</h2><h2>Unknown</h2>
        <p>现在我们想谈谈当 n 为较大数字时 n 步转移概率。好的。在这里，我们将使用总概率定理。因此，我们将在两种不同的场景中进行条件设定，并通过考虑此事件发生的不同方式来分解此数量的计算。那么感兴趣的事件是什么？感兴趣的事件如下。在时间 0 时，我们开始 i。</p><p>Now we want to say something about the n step transition probabilities when n is a bigger number. OK. So
            here,
            we’re going to use the total probability theorem. So we’re going to condition in two different scenarios,
            and
            break up the calculation of this quantity, by considering the different ways that this event can happen. So
            what
            is the event of interest? The event of interest is the following. At time 0 we start i.</p>
        <p>我们感兴趣的是，在时间 n 时，落在特定状态 j 上。现在，这个事件可以以几种不同的方式发生，以许多不同的方式发生。但让我们将它们分成几个子组。一个组或一种场景如下。在前 n 1 个时间步骤中，事情发生了，不知何故你最终处于状态 1。然后从状态 1，在下一个时间步骤中，你转换到状态 j。</p><p>We are interested in landing at time n at the particular state j. Now this event can happen in several
            different
            ways, in lots of different ways. But let us group them into subgroups. One group, or one sort of scenario,
            is
            the following. During the first n 1 time steps, things happen, and somehow you end up at state 1. And then
            from
            state 1, in the next time step you make a transition to state j.</p>
        <p>此处的特定弧线实际上对应着许多不同的可能场景、不同点或不同转换。在 n 1 个时间步骤中，有很多种可能的方式让您最终到达状态 1。状态空间中的不同路径。但所有这些路径加在一起都具有一个概率，即 (n 1) 步骤转换概率，即从状态 i 开始，您最终到达状态 1，然后还有其他可能场景。</p><p>This particular arc here actually corresponds to lots and lots of different possible scenarios, or different
            spots, or different transitions. In n 1 time steps, there’s lots of possible ways by which you could end up
            at
            state 1. Different paths through the state space. But all of them together collectively have a probability,
            which is the (n 1) step transition probability, that from state i, you end up at state 1 And then there’s
            other
            possible scenarios.</p>
        <h2 id="unknown-322">未知</h2><h2>Unknown</h2>
        <p>也许在前 n 1 个时间步骤中，你会遵循到达状态 m 的轨迹。然后从状态 m 开始，你进行了这种转换，最后到达状态 j。因此，此图将从 I 到 j 的所有可能轨迹集合分解为不同的集合，其中每个集合都与最后一个时间步骤之前的状态（即时间 n 之前的状态）有关。&nbsp;</p><p>Perhaps in the first n 1 time steps, you follow the trajectory that took you at state m. And then from state
            m,
            you did this transition, and you ended up at state j. So this diagram breaks up the set of all possible
            trajectories from I to j into different collections, where each collection has to do with which one happens
            to
            be the state just before the last time step, just before time n.&nbsp;</p>
        <p>我们将以时间 n 1 的状态为条件。因此，最终达到状态 j 的总概率是不同场景的概率之和，即可以达到状态 j 的不同方式。如果我们考虑这种场景，该场景发生的概率是多少？</p><p>And we’re going to condition on the state at time n 1. So the total probability of ending up at state j is
            the
            sum of the probabilities of the different scenarios the different ways that you can get to state j. If we
            look
            at that type of scenario, what’s the probability of that scenario happening?</p>
        <p>概率为 Ri1(n 1)，我在时间 n 1 时处于状态 1。这只是根据这些多步转换概率的定义。这是转换次数。从状态 i 到状态 1 的概率。然后假设我发现自己处于状态 1，概率为 P1j，这是转换概率，下次我将发现自己处于状态 j。</p><p>With probability Ri1(n 1), I find myself at state 1 at time n 1. This is just by the definition of these
            multi
            step transition probabilities. This is the number of transitions. The probability that from state i, I end
            up at
            state 1. And then given that I found myself at state 1, with probability P1j, that’s the transition
            probability,
            next time I’m going to find myself at state j.</p>
        <h2 id="unknown-323">未知</h2><h2>Unknown</h2>
        <p>因此，这两个的乘积就是我之前从状态 I 经过状态 1 到达状态 j 的总概率。现在我们究竟在哪里使用了马尔可夫假设？无论我们使用哪条特定路径从 I 到达状态 1，我下次进行这种转换的概率都是相同的数字，即 P1j。
        </p><p>So the product of these two is the total probability of my getting from state I to state j through state 1 at
            the
            time before. Now where exactly did we use the Markov assumption here? No matter which particular path we
            used to
            get from I to state 1, the probability that next I’m going to make this transition is that same number, P1j.
        </p>
        <p>因此，该数字与我为了到达那里而遵循的特定路径无关。如果我们没有马尔可夫假设，我们应该考虑这里所有可能的单独轨迹，然后我们需要使用与该特定轨迹相对应的转移概率。但由于马尔可夫假设，唯一重要的是我们现在处于状态 1。我们如何到达那里并不重要。</p><p>So that number does not depend on the particular path that I followed in order to get there. If we didn’t
            have
            the Markov assumption, we should have considered all possible individual trajectories here, and then we
            would
            need to use the transition probability that corresponds to that particular trajectory. But because of the
            Markov
            assumption, the only thing that matters is that right now we are at state 1. It does not matter how we got
            there.</p>
        <p>所以现在，一旦你看到这个场景，然后是这个场景，然后是那个场景，然后你把这些不同场景的概率加起来，你就会得到这个公式，这是一个递归。它告诉我们，一旦你计算了 (n 1) 步转移概率，那么你也可以计算 n 步转移概率。这是一个你同时对所有 i 和 j 执行或运行的递归。这是固定的。</p><p>So now once you see this scenario, then this scenario, and that scenario, and you add the probabilities of
            these
            different scenarios, you end up with this formula here, which is a recursion. It tells us that once you have
            computed the (n 1) step transition probabilities, then you can compute also the n step transition
            probabilities.
            This is a recursion that you execute or you run for all i’s and j’s simultaneously. That is fixed.</p>
        <h2 id="unknown-324">未知</h2><h2>Unknown</h2>
        <p>对于特定的 n，您可以计算所有可能的 i、j、k 的量。您有所有这些量，然后使用此方程式再次找到所有可能的 i 和 j 的那些数字。现在这个公式始终是正确的，并且公式背后有一个大想法。现在这个公式有变体，取决于您是否对略有不同的东西感兴趣。</p><p>And for a particular n, you calculate this quantity for all possible i’s, j’s, k’s. You have all of those
            quantities, and then you use this equation to find those numbers again for all the possible i’s and j’s. Now
            this is formula which is always true, and there’s a big idea behind the formula. And now there’s variations
            of
            this formula, depending on whether you’re interested in something that’s slightly different.</p>
        <p>例如，如果你有一个随机的初始状态，有人会给你初始状态的概率分布，所以有人告诉你，以这样的概率，你将从状态 1 开始。以这个概率，你将从状态 2 开始，依此类推。你想找到在时间 n 时你处于状态 j 的概率。同样，全概率定理，你以初始状态为条件。
        </p><p>So for example, if you were to have a random initial state, somebody gives you the probability distribution
            of
            the initial state, so you’re told that with probability such and such, you’re going to start at state 1.
            With
            that probability, you’re going to start at state 2, and so on. And you want to find the probability at the
            time
            n you find yourself at state j. Well again, total probability theorem, you condition on the initial state.
        </p>
        <p>有了这个概率，你就会发现自己处于那个特定的初始状态，并且假设这是你的初始状态，那么这就是 n 个时间步骤后你发现自己处于状态 j 的概率。现在再次基于相同的想法，你可以通过在不同时间进行调节来运行这种类型的每个递归。所以这是一个变体。你从状态 i 开始。</p><p>With this probability you find yourself at that particular initial state, and given that this is your initial
            state, this is the probability that n time steps later you find yourself at state j. Now building again on
            the
            same idea, you can run every recursion of this kind by conditioning at different times. So here’s a
            variation.
            You start at state i.</p>
        <h2 id="unknown-325">未知</h2><h2>Unknown</h2>
        <p>经过 1 个时间步骤后，你会发现自己处于状态 1，概率为 pi1，然后你会发现自己处于状态 m，概率为 Pim。一旦发生这种情况，你就会遵循一些轨迹。并且有可能在 n 个 1 个时间步骤后最终处于状态 j。这种情况可能以多种可能的方式发生。从状态 1 到状态 j 有很多可能的路径。</p><p>After 1 time step, you find yourself at state 1, with probability pi1, and you find yourself at state m with
            probability Pim. And once that happens, then you’re going to follow some trajectories. And there is a
            possibility that you’re going to end up at state j after n 1 time steps. This scenario can happen in many
            possible ways. There’s lots of possible paths from state 1 to state j.</p>
        <p>从状态 1 到状态 j 有很多路径。所有这些转换的总体概率是多少？这是从状态 1 开始，在 n 1 个时间步骤内到达状态 j 的事件。因此，这里的概率为 R1j，为 n 1。下面也是一样。然后，使用与之前相同的思维方式，我们得到公式 Rij(n) 是 Pik 所有 k 的总和，然后是 Rkj(n 1)。</p><p>There’s many paths from state 1 to state j. What is the collective probability of all these transitions? This
            is
            the event that, starting from state 1, I end up at state j in n 1 time steps. So this one has here
            probability
            R1j of n 1. And similarly down here. And then by using the same way of thinking as before, we get the
            formula
            that Rij(n) is the sum over all k’s of Pik, and then the Rkj(n 1).</p>
        <p>因此，这个公式看起来与这个公式几乎相同，但实际上有所不同。指标和计算方式略有不同，但基本思想是一样的。在这里，我们使用全概率理论，通过条件化时间范围结束前 1 步的状态。在这里，我们使用全概率定理，通过条件化第一次转换后的状态。因此，这个总体思想有不同的变化。
        </p><p>So this formula looks almost the same as this one, but it’s actually different. The indices and the way
            things
            work out are a bit different, but the basic idea is the same. Here we use the total probability theory by
            conditioning on the state just 1 step before the end of our time horizon. Here we use total probability
            theorem
            by conditioning on the state right after the first transition. So this generally idea has different
            variations.
        </p>
        <h2 id="unknown-326">未知</h2><h2>Unknown</h2>
        <p>它们都是有效的，根据你正在处理的情况，你可能想要使用其中一种或另一种。让我们用一个例子来说明这些计算。在这个例子中，我们只有 2 个状态，有人给了我们转换概率，也就是那些特定的数字。让我们写下这些方程。所以从状态 1 开始，我在 n 个时间步骤后发现自己处于状态 1 的概率。
        </p><p>They’re all valid, and depending on the context that you’re dealing with, you might want to work with one of
            these or another. So let’s illustrate these calculations in terms of an example. So in this example, we just
            have 2 states, and somebody gives us transition probabilities to be those particular numbers. Let’s write
            down
            the equations. So the probability that starting from state 1, I find myself at state 1 n time steps later.
        </p>
        <p>这可能以两种方式发生。在时间 n 1 时，我可能会发现自己处于状态 2。然后从状态 2 转换回状态 1，这种情况发生的概率为 0.2。为什么我要把 2 放在那里呢？另一种方式是从状态 1，经过 n 1 步进入状态 1，然后从状态 1 停留在原地，这种情况发生的概率为 0.5。所以这是针对 R11(n) 的。</p><p>This can happen in 2 ways. At time n 1, I might find myself at state 2. And then from state 2, I make a
            transition back to state 1, which happens with probability. why’d I put 2 there anyway, 0.2. And another way
            is
            that from state 1, I go to state 1 in n 1 steps, and then from state 1 I stay where I am, which happens with
            probability 0.5. So this is for R11(n).</p>
        <p>现在 R12(n)，我们可以为它写一个类似的递归。另一方面，这些似乎是概率。时间 n 的状态将是状态 1 或状态 2。因此这两个数字需要加到 1，所以我们可以将其写为 1 R11(n)。这是一个足够的递归，可以随着时间的推移传播 R11 和 R12。</p><p>Now R12(n), we can write a similar recursion for this one. On the other hand, seems these are probabilities.
            The
            state at time n is going to be either state 1 or state 2. So these 2 numbers need to add to 1, so we can
            just
            write this as 1 R11(n). And this is an enough of a recursion to propagate R11 and R12 as time goes on.</p>
        <h2 id="unknown-327">未知</h2><h2>Unknown</h2>
        <p>因此，在 n 1 次转换之后，要么我发现自己处于状态 2，然后有一个转换点，我会转到状态 1，要么我发现自己处于状态 1，这是该概率，从那里，我有 0.5 的概率留在原地。现在让我们开始计算。</p><p>So after n 1 transitions, either I find myself in state 2, and then there’s a point to transition that I go
            to 1,
            or I find myself in state 1, which with that probability, and from there, I have probability 0.5 of staying
            where I am. Now let’s start calculating.</p>
        <p>正如我们之前讨论过的，如果我从状态 1 开始，经过 0 次转换后，我肯定会处于状态 1，而且肯定不会处于状态 1。如果我从状态 1 开始，我肯定不会在那时处于状态，而我肯定我现在处于状态 1。在我从状态 1 开始进行转换后，有 0.5 的概率我停留在状态 1。有 0.5 的概率我停留在状态 2。如果我从状态 2 开始，那么我在 1 个时间步内转到状态 1 的概率是这个转换的概率为 0.2，另一个转换的概率为 0.8。好的。
        </p><p>As we discussed before, if I start at state 1, after 0 transitions I’m certain to be at state,and I’m certain
            not
            to be at state 1. If I start from state 1, I’m certain to not to be at state at that time, and I’m certain
            that
            I am right now, it’s state 1. After I make transition, starting from state 1, there’s probability 0.5 that I
            stay at state 1. And there’s probability 0.5 that I stay at state 2. If I were to start from state 2, the
            probability that I go to 1 in 1 time step is this transition that has probability 0.2, and the other 0.8.
            OK.
        </p>
        <p>因此，如果我们要计算下一个项，计算现在会变得更加有趣。在时间 2 时，我发现自己处于状态 1 的可能性有多大？为了处于状态 1，这可以通过两种方式实现。要么第一个转换让我留在原地，要么第二个转换也一样。所以这些对应于这个 0.5，即第一个转换将我带到了那里，而下一个转换也是同一种。</p><p>So the calculation now becomes more interesting, if we want to calculate the next term. How likely is that at
            time 2, I find myself at state 1? In order to be here at state 1, this can happen in 2 ways. Either the
            first
            transition left me there, and the second transition is the same. So these correspond to this 0.5, that the
            first
            transition took me there, and the next transition was also of the same kind.</p>
        <h2 id="unknown-328">未知</h2><h2>Unknown</h2>
        <p>这是一种可能性。但还有另一种情况。为了在时间 2 处于状态 1，也可以这样发生。所以这就是在 1 次转换之后我到达那里的事件。而下一次转换恰好是这个。</p><p>That’s one possibility. But there’s another scenario. In order to be at state 1 at time 2 this can also
            happen
            this way. So that’s the event that, after 1 transition, I got there. And the next transition happened to be
            this
            one.</p>
        <p>所以这相当于 0.5 乘以 0.2。这相当于将到达那里的 1 步转移概率乘以我从状态 2 移动到状态 1 的概率，在本例中为 0.2。所以基本上我们取这个数字，乘以 0.2，然后将这两个数字相加。</p><p>So this corresponds to 0.5 times 0.2. It corresponds to taking the 1 step transition probability of getting
            there, times the probability that from state 2 I move to state 1, which in this case, is 0.2. So basically
            we
            take this number, multiplied with 0.2, and then add those 2 numbers.</p>
        <p>将它们相加后，结果为 0.35。同样，这里的结果为 0.65。现在继续递归，我们继续做同样的事情。我们将这个数字乘以 0.5，再加上这个数字乘以 0.2。将它们相加，得到下一个条目。继续这样做，继续这样做，最终你会注意到数字开始稳定在 2/7 的极限值。让我们来验证一下。</p><p>And after you add them, you get 0.35. And similarly here, you’re going to get 0.65. And now to continue with
            the
            recursion, we keep doing the same thing. We take this number times 0.5 plus this number times 0.2. Add them
            up,
            you get the next entry. Keep doing that, keep doing that, and eventually you will notice that the numbers
            start
            settling into a limiting value at 2/7. And let’s verify this.</p>
        <h2 id="unknown-329">未知</h2><h2>Unknown</h2>
        <p>如果这个数字是 2/7，下一个数字会是多少？下一个数字是 2/7（不是 2.7），是 2/7。这是我处于该状态的概率，乘以 0.5。这是下一个将我带到状态 1 的转换，再加上 5/7。这将是我处于状态 2 的剩余概率</p><p>If this number is 2/7, what is the next number going to be? The next number is going to be 2/7 (not 2.7) it’s
            going to be 2/7. That’s the probability that I find myself at that state, times 0.5. that’s the next
            transition
            that takes me to state 1 plus 5/7. that would be the remaining probability that I find myself in state 2</p>
        <p>乘以 1/5。这样，我得到的结果是 2/7。这个计算基本上说明，如果这个数字变成了 2/7，那么下一个数字也将是 2/7。当然，这个数字必须是 5/7。这个数字也必须是相同的 5/7。因此，经过很长一段时间后，我发现自己处于状态 1 的概率会稳定在某个稳定状态值。</p><p>Times 1/5. And so that gives me, again, 2/7. So this calculation basically illustrates, if this number has
            become
            2/7, then the next number is also going to be 2/7. And of course this number here is going to have to be
            5/7.
            And this one would have to be again, the same, 5/7. So the probability that I find myself at state 1, after
            a
            long time has elapsed, settles into some steady state value.</p>
        <p>这是一个有趣的现象。我们只是做了这个观察。现在我们也可以从状态 2 开始计算概率。在这里，你做计算，我不会做。但你做完之后，你会发现这个概率也稳定在 2/7，这个也稳定在 5/7。所以这里的数字和那些数字是一样的。它们之间有什么区别？</p><p>So that’s an interesting phenomenon. We just make this observation. Now we can also do the calculation about
            the
            probability, starting from state 2. And here, you do the calculations I’m not going to do them. But after
            you do
            them, you find this probability also settles to 2/7 and this one also settles to 5/7. So these numbers here
            are
            the same as those numbers. What’s the difference between these?</p>
        <h2 id="unknown-330">未知</h2><h2>Unknown</h2>
        <p>这是我从状态 1 开始，最终发现自己处于状态 1 的概率。这是我从状态 2 开始，最终发现自己处于状态 1 的概率。无论我从哪里开始，这些概率都是相同的。</p><p>This is the probability that I find myself at state 1 given that I started at 1. This is the probability that
            I
            find myself at state 1 given that I started at state 2. These probabilities are the same, no matter where I
            started from.</p>
        <p>因此，这个数值示例说明了这样一个观点：在链运行了很长时间之后，链的状态与链的初始状态无关。因此，如果你从这里开始，你知道你将在这里停留一段时间，进行一些转换，因为这个概率很小。因此，初始状态确实会告诉你一些事情。</p><p>So this numerical example sort of illustrates the idea that after the chain has run for a long time, what the
            state of the chain is, does not care about the initial state of the chain. So if you start here, you know
            that
            you’re going to stay here for some time, a few transitions, because this probability is kind of small. So
            the
            initial state does that’s tell you something.</p>
        <p>但从长远来看，这种转变将会发生。这种转变将会发生。有很多随机性进入，这些随机性会冲走系统初始状态可能出现的任何信息。我​​们这样描述这种情况：马尔可夫链最终会进入稳定状态。稳定状态意味着什么？</p><p>But in the very long run, transitions of this kind are going to happen. Transitions of that kind are going to
            happen. There’s a lot of randomness that comes in, and that randomness washes out any information that could
            come from the initial state of the system. We describe this situation by saying that the Markov chain
            eventually
            enters a steady state. Where a steady state, what does it mean it?</p>
        <h2 id="unknown-331">未知</h2><h2>Unknown</h2>
        <p>这是否意味着状态本身会变得稳定并停在一处？不，链的状态会一直跳跃。链的状态会不断转换，会在 1 和 2 之间来回转换。因此，状态本身（Xn）在任何意义上都不会变得稳定。变得稳定的是描述 Xn 的概率。</p><p>Does it mean the state itself becomes steady and stops at one place? No, the state of the chain keeps jumping
            forever. The state of the chain will keep making transitions, will keep going back and forth between 1 and
            2. So
            the state itself, the Xn, does not become steady in any sense. What becomes steady are the probabilities
            that
            describe Xn.</p>
        <p>也就是说，经过很长一段时间后，你处于状态 1 的概率会变成一个常数 2/7，而你处于状态 2 的概率会变成一个常数。因此跳跃会不断发生，但在任何给定时间，如果你问现在我处于状态 1 的概率是多少，答案将是 2/7。顺便问一下，这些数字有意义吗？</p><p>That is, after a long time elapses, the probability that you find yourself at state 1 becomes a constant 2/7,
            and
            the probability that you find yourself in state 2 becomes a constant. So jumps will keep happening, but at
            any
            given time, if you ask what’s the probability that right now I am at state 1, the answer is going to be 2/7.
            Incidentally, do the numbers sort of makes sense?</p>
        <p>为什么这个数字比那个数字大？因为这个州比那个州更棘手。一旦你进入这里，就很难出来。所以当你进入这里时，你会在这里花很多时间。这个州比较容易出来，因为概率是 0.5，所以当你进入那里时，你往往会更快地出来。</p><p>Why is this number bigger than that number? Well, this state is a little more sticky than that state. Once
            you
            enter here, it’s kind of harder to get out. So when you enter here, you spend a lot of time here. This one
            is
            easier to get out, because the probability is 0.5, so when you enter there, you tend to get out faster.</p>
        <h2 id="unknown-332">未知</h2><h2>Unknown</h2>
        <p>因此，你会不断从一个状态转移到另一个状态，但你往往会在那个状态上花费更多时间，这反映在这个概率大于那个概率上。因此，无论你从哪里开始，都有 5/7 的概率在这里，2/7 的概率在那里。因此，在这个例子中发生了一些非常好的事情。问题是，对于一般的马尔可夫链来说，事情是否总是如此美好。</p><p>So you keep moving from one to the other, but you tend to spend more time on that state, and this is
            reflected in
            this probability being bigger than that one. So no matter where you start, there’s 5/7 probability of being
            here, 2/7 probability being there. So there were some really nice things that happened in this example. The
            question is, whether things are always as nice for general Markov chains.</p>
        <p>发生的两件好事如下。随着我们继续进行这种计算，这个数字会稳定下来。极限存在。发生的另一件事是这个数字与那个数字相同，这意味着初始状态并不重要。情况总是如此吗？当 n 趋于无穷大时，转移概率是否总是会收敛到某个值？</p><p>The two nice things that happened where the following. as we keep doing this calculation, this number settles
            to
            something. The limit exists. The other thing that happens is that this number is the same as that number,
            which
            means that the initial state does not matter. Is this always the case? Is it always the case that as n goes
            to
            infinity, the transition probabilities converge to something?</p>
        <p>如果它们确实收敛到某个值，那么极限是否不受链开始的初始状态 I 的影响？因此从数学上讲，我们提出的问题是 Rij(n) 是否收敛到某个值。它收敛到的那个值是否只与 j 有关。它是你处于状态 j 的概率，并且该概率与初始状态无关。</p><p>And if they do converge to something, is it the case that the limit is not affected by the initial state I at
            which the chain started? So mathematically speaking, the question we are raising is whether Rij(n) converges
            to
            something. And whether that something to which it converges to has only to do with j. It’s the probability
            that
            you find yourself at state j, and that probability doesn’t care about the initial state.</p>
        <h2 id="unknown-333">未知</h2><h2>Unknown</h2>
        <p>所以问题是从长远来看，初始状态是否会被遗忘。所以答案是，通常，或者对于好的链，这两件事都是正确的。你得到的极限不依赖于初始状态。但如果你的链有一些特殊或独特的结构，这可能不会发生。所以让我们首先考虑收敛问题。</p><p>So it’s the question of whether the initial state gets forgotten in the long run. So the answer is that
            usually,
            or for nice chains, both of these things will be true. You get the limit which does not depend on the
            initial
            state. But if your chain has some peculiar or unique structure, this might not happen. So let’s think first
            about the issue of convergence.</p>
        <p>因此，当 n 以稳定值趋于无穷大时，收敛实际上意味着以下情况。如果我告诉你已经过去了很多时间，那么你告诉我，好吧，概率的状态等于该值，而不必查看你的时钟。如果没有收敛，则意味着 Rij 可以不断上下波动，而不会稳定下来。</p><p>So convergence, as n goes to infinity at a steady value, really means the following. If I tell you a lot of
            time
            has passed, then you tell me, OK, the state of the probabilities are equal to that value without having to
            consult your clock. If you don’t have convergence, it means that Rij can keep going up and down, without
            settling to something.</p>
        <p>因此，为了让你告诉我 Rij 的值，你需要查看你的时钟，检查它现在是上升还是下降。因此，当你没有收敛时，你可能会得到某种周期性行为，这个例子说明了这一点。那么在这个例子中发生了什么？从状态 2 开始，下次你到这里或那里，概率是一半。</p><p>So in order for you to tell me the value of Rij, you need to consult your clock to check if, right now, it’s
            up
            or is it down. So there’s some kind of periodic behavior that you might get when you do not get convergence,
            and
            this example here illustrates it. So what’s happened in this example? Starting from state 2, next time you
            go
            here, or there, with probability half.</p>
        <h2 id="unknown-334">未知</h2><h2>Unknown</h2>
        <p>然后下一次，无论你在哪里，你都会回到状态 2。所以这个链有一些随机性，但随机性是有限的。你出去，你进来。你出去，你进来。所以有一个重复的周期性模式。</p><p>And then next time, no matter where you are, you move back to state 2. So this chain has some randomness, but
            the
            randomness is kind of limited type. You go out, you come in. You go out, you come in. So there’s a periodic
            pattern that gets repeated.</p>
        <p>这意味着如果你在偶数步之后从状态 2 开始，你肯定会回到状态 2。所以这里的概率是 1。另一方面，如果转换次数是奇数，你就不可能回到初始状态。如果你从这里开始，偶数次你会在这里，奇数次你会在那里或那里。</p><p>It means that if you start at state 2 after an even number of steps, you are certain to be back at state 2.
            So
            this probability here is 1. On the other hand, if the number of transitions is odd, there’s no way that you
            can
            be at your initial state. If you start here, at even times you would be here, at odd times you would be
            there or
            there.</p>
        <p>所以这个概率是 0。当 n 趋向无穷大时，这些概率，即 n 步转移概率不会收敛到任何值。它一直在 0 和 1 之间交替。所以收敛失败了。如果你的链具有周期性结构，这就是收敛失败的主要机制。下次我们将讨论，如果没有周期性，那么我们就不会遇到收敛问题。</p><p>So this probability is 0. As n goes to infinity, these probabilities, the n step transition probability does
            not
            converge to anything. It keeps alternating between 0 and 1. So convergence fails. This is the main mechanism
            by
            which convergence can fail if your chain has a periodic structure. And we’re going to discuss next time
            that, if
            periodicity absent, then we don’t have an issue with convergence.</p>
        <h2 id="unknown-335">未知</h2><h2>Unknown</h2>
        <p>第二个问题是，如果我们有收敛，那么初始状态是否重要。在上一个链中，你可以在数字上在状态 1 和 2 之间来回移动，你会发现初始状态并不重要。但你可以考虑初始状态确实重要的情况。看看这个链。如果你从状态 1 开始，你就会永远停留在状态 1。没有办法逃脱。</p><p>The second question if we have convergence, whether the initial state matters or not. In the previous chain,
            where you could keep going back and forth between states 1 and 2 numerically, one finds that the initial
            state
            does not matter. But you can think of situations where the initial state does matter. Look at this chain
            here.
            If you start at state 1, you stay at state 1 forever. There’s no way to escape.</p>
        <p>所以这意味着 R11(n) 对于所有 n 都是 1。如果你从状态 3 开始，你将在阶段 3 和阶段 4 之间移动，但没有办法朝那个方向移动，所以你没有办法转到状态 1。因此，R31 对于所有 n 都是 0。好的，所以这是一个初始状态很重要的情况。当 n 趋向无穷大时，R11 趋向极限，因为它是常数。</p><p>So this means that R11(n) is 1 for all n.&nbsp;If you start at state 3, you will be moving between stage 3 and 4,
            but
            there’s no way to go in that direction, so there’s no way that you go to state 1. And for that reason, R31
            is 0
            for all n.&nbsp;OK So this is a case where the initial state matters. R11 goes to a limit, as n goes to infinity,
            because it’s constant.</p>
        <p>它始终为 1，因此极限为 1。R31 也有一个极限。它始终为 0。因此，这些是发现自己处于状态 1 的长期概率。但这些长期概率会受到您从哪里开始的影响。如果您从这里开始，您确信从长远来看您会在这里。如果您从这里开始，您确信从长远来看您不会在那里。因此，初始状态在这里确实很重要。</p><p>It’s always 1 so the limit is 1. R31 also has a limit. It’s 0 for all times. So these are the long term
            probabilities of finding yourself at state 1. But those long term probabilities are affected by where you
            started. If you start here, you’re sure that’s, in the long term, you’ll be here. If you start here, you’re
            sure
            that, in the long term, you will not be there. So the initial state does matter here.</p>
        <h2 id="unknown-336">未知</h2><h2>Unknown</h2>
        <p>这是一种某些状态无法从某些其他状态访问的情况，因此它与我们的马尔可夫链的图形结构有关。最后让我们在这里回答这个问题，至少对于较大的 n。如果你从状态 2 开始，你认为长期会发生什么？</p><p>And this is a situation where certain states are not accessible from certain other states, so it has
            something to
            do with the graph structure of our Markov chain. Finally let’s answer this question here, at least for large
            n’s. What do you think is going to happen in the long term if you start at state 2?</p>
        <p>如果你从状态 2 开始，你可能会在状态 2 上停留一段随机的时间，但最终这种转变会发生，或者那种转变会发生。由于对称性，你从状态 2 逃逸到这个方向或那个方向的概率是 1/2，因此当转变发生时，转变发生在那个方向的概率是 1/2。因此，对于较大的 N，你可以确定转变确实会发生。</p><p>If you start at state 2, you may stay at state 2 for a random amount of time, but eventually this transition
            will
            happen, or that transition would happen. Because of the symmetry, you are as likely to escape from state 2
            in
            this direction, or in that direction, so there’s probability 1/2 that, when the transition happens, the
            transition happens in that direction. So for large N, you’re certain that the transition does happen.</p>
        <p>假设转变已经发生，那么转变发生的概率为 1/2。因此，在这里您可以清楚地看到，处于特定状态的概率很大程度上取决于您的出发点。</p><p>And given that the transition has happened, it has probability 1/2 that it has gone that particular way. So
            clearly here, you see that the probability of finding yourself in a particular state is very much affected
            by
            where you started from.</p>
        <h2 id="unknown-337">未知</h2><h2>Unknown</h2>
        <p>因此，我们接下来要做的是从这两个例子中抽象出来，并描述与周期性有关的一般结构属性，这些属性与某些状态下发生的事情有关，而其他状态下无法访问。我们将在下次讨论周期性。但让我们谈谈我们遇到的第二种现象。</p><p>So what we want to do next is to abstract from these two examples and describe the general structural
            properties
            that have to do with periodicity, and that have to do with what happened here with certain states, not being
            accessible from the others. We’re going to leave periodicity for next time. But let’s talk about the second
            kind
            of phenomenon that we have.</p>
        <p>因此，我们在这里要做的是将转换图中的状态分为两种类型：循环状态和瞬时状态。因此，如果以下条件成立，则称该状态为循环状态。如果从状态 i 开始，您可以前往某些地方，但无论前往哪里，都有返回的途径。那么循环状态的一个例子是什么？就是这个。从这里开始，您可以前往其他地方。</p><p>So here, what we’re going to do is to classify the states in a transition diagram into two types, recurrent
            and
            transient. So a state is said to be recurrent if the following is true. If you start from the state i, you
            can
            go to some places, but no matter where you go, there is a way of coming back. So what’s an example for the
            recurrent state? This one. Starting from here, you can go elsewhere.</p>
        <p>你可以进入状态 7。你可以进入状态 6。这就是你能去的所有地方。但无论你去哪里，都有一条路可以带你回到那里。所以无论你去哪里，都有机会，有办法回到你开始的地方。这些状态我们称之为循环状态。因此，状态 8 是循环的。所有这些都是循环的。所以这是循环的，这是循环的。</p><p>You can go to state 7. You can go to state 6. That’s all where you can go to. But no matter where you go,
            there
            is a path that can take you back there. So no matter where you go, there is a chance, and there is a way for
            returning where you started. Those states we call recurrent. And by this, 8 is recurrent. All of these are
            recurrent. So this is recurrent, this is recurrent.</p>
        <h2 id="unknown-338">未知</h2><h2>Unknown</h2>
        <p>而且这个状态 5 也是循环的。除了回到 5 本身，你无法从 5 去往任何地方。无论你去哪里，你都可以回到你开始的地方。所以这是循环的。如果它不是循环的，我们说它是瞬态的。那么瞬态是什么意思呢？你需要把这个定义反过来。</p><p>And this state 5 is also recurrent. You cannot go anywhere from 5 except to 5 itself. Wherever you can go,
            you
            can go back to where you start. So this is recurrent. If it is not the recurrent, we say that it is
            transient.
            So what does transient mean? You need to take this definition, and reverse it.</p>
        <p>瞬态意味着，从 i 出发，有一个地方你可以去，但从那里你不能回来。如果它是循环的，无论你去哪里，你都可以回来。瞬态意味着有一些地方你可以去，但从那里你不能回来。所以状态 1 是循环的，因为从这里出发，你有可能到达那里，然后就没有回头路了。</p><p>Transient means that, starting from i, there is a place to which you could go, and from which you cannot
            return.
            If it’s recurrent, anywhere you go, you can always come back. Transient means there are places where you can
            go
            from which you cannot come back. So state 1 is recurrent because starting from here, there’s a possibility
            that
            you get there, and then there’s no way back.</p>
        <p>状态 4 是循环的，从 4 开始，你可以去某个地方，对不起，是短暂的，对。状态 4 是从这里开始的短暂的，你可以去某些地方，但你不能再回来了。在这个特定的图中，所有这 4 个状态都是短暂的。现在，如果状态是短暂的，就意味着有一条路可以去某个地方，但你会被卡住而无法到达。</p><p>State 4 is recurrent, starting from 4, there’s somewhere you can go and. sorry, transient, correct. State 4
            is
            transient starting from here, there are places where you could go, and from which you cannot come back. And
            in
            this particular diagram, all these 4 states are transients. Now if the state is transient, it means that
            there
            is a way to go somewhere where you’re going to get stuck and not to be able to come.</p>
        <h2 id="unknown-339">未知</h2><h2>Unknown</h2>
        <p>只要您的状态一直在这里循环，最终就会发生其中一种转变，一旦发生，您就无法返回。因此，该瞬时状态只能被访问有限次数。您将无法返回到该状态。</p><p>As long as your state keeps circulating around here, eventually one of these transitions is going to happen,
            and
            once that happens, then there’s no way that you can come back. So that transient state will be visited only
            a
            finite number of times. You will not be able to return to it.</p>
        <p>从长远来看，你肯定会摆脱瞬时状态，进入某种循环状态，并永远停留在这种状态中。那么，让我们看看，在这个图中，如果我从这里开始，我能永远停留在这一组状态中吗？</p><p>And in the long run, you’re certain that you’re going to get out of the transient states, and get to some
            class
            of recurrent states, and get stuck forever. So, let’s see, in this diagram, if I start here, could I stay in
            this lump of states forever?</p>
        <p>只要我处于这种状态，我就会不断访问状态 1 和 2。每次我访问状态 2，我逃脱的概率都是正的。所以从长远来看，如果我待在这里，我会无数次访问状态 2，我会有无数次逃脱的机会。但如果你有无限次逃脱的机会，最终你就会逃脱。</p><p>Well as long as I’m staying in this type of states, I would keep visiting states 1 and 2 Each time that I
            visit
            state 2, there’s going to be positive probability that I escape. So in the long run, if I were to stay here,
            I
            would visit state 2 an infinite number of times, and I would get infinite chances to escape. But if you have
            infinite chances to escape, eventually you will escape.</p>
        <h2 id="unknown-340">未知</h2><h2>Unknown</h2>
        <p>因此，您确信从这里开始，您将以 1 的概率移动到这些状态，或者移动到这些状态。因此，从瞬时状态开始，您只会在瞬时状态下停留随机但有限的时间。之后，您最终会进入一类循环状态。当我说类别时，它们的意思是，在这张图片中，我将循环状态分为 2 个类别或类别。</p><p>So you are certain that with probability 1, starting from here, you’re going to move either to those states,
            or
            to those states. So starting from transient states, you only stay at the transient states for random but
            finite
            amount of time. And after that happens, you end up in a class of recurrent states. And when I say class,
            what
            they mean is that, in this picture, I divide the recurrent states into 2 classes, or categories.</p>
        <p>它们有什么特别之处？这些状态是循环的。这些状态是循环的。但两者之间没有沟通。如果你从这里开始，你就会被困在这里。如果你从这里开始，你就会被困在那里。在这种情况下，初始状态确实很重要，因为如果你从这里开始，你就会被困在这里。如果你从这里开始，你就会被困在那里。因此，根据初始状态，这将影响你的链的长期行为。
        </p><p>What’s special about them? These states are recurrent. These states are recurrent. But there’s no
            communication
            between the 2. If you start here, you’re stuck here. If you start here, you are stuck there. And this is a
            case
            where the initial state does matter, because if you start here, you get stuck here. You start here, you get
            stuck there. So depending on the initial state, that’s going to affect the long term behavior of your chain.
        </p>
        <p>所以现在你可以猜测的是，为了使初始状态不重要，我们不应该有多个循环类。我们应该只有 1 个。但我们下次会回到这一点。</p><p>So the guess you can make at this point is that, for the initial state to not matter, we should not have
            multiple
            recurrent classes. We should have only 1. But we’re going to get back to this point next time.</p>
        <h1 id="markov-chains-ii">17.马尔可夫链 II</h1><h1>17. Markov Chains II</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:25 <a href="https://youtube.com/watch?v=ZulMqrvP-Pk">https://youtube.com/watch?v=ZulMqrvP-Pk</a></p><p> 12 years ago (Nov 10, 2012) — 51:25 <a href="https://youtube.com/watch?v=ZulMqrvP-Pk">https://youtube.com/watch?v=ZulMqrvP-Pk</a></p>
        <h2 id="mit-opencourseware-1">麻省理工学院开放课程</h2><h2>MIT OpenCourseWare</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality, educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.</p>
        <h2 id="overview">概述</h2><h2>Overview</h2>
        <p>教授：好的。今天我们首先回顾一下上次讨论的内容，回顾一下马尔可夫链的定义。</p><p>PROFESSOR: All right. So today, we’re going to start by taking stock of what we discussed last time, review
            the
            definition of Markov chains.</p>
        <p>然后，在大部分讲座中，我们将集中讨论它们的稳定状态行为。也就是说，我们将研究马尔可夫链在运行了很长时间后会做什么。我们能说出不同状态的概率吗？因此，我想重复我上次说过的一句话：马尔可夫链是一种非常非常有用的模型。</p><p>And then most of the lecture, we’re going to concentrate on their steady state behavior. Meaning, we’re going
            to
            look at what does a Markov chain do if it has run for a long time. What can we say about the probabilities
            of
            the different states? So what I would like to repeat is a statement I made last time that Markov chains is a
            very, very useful class of models.</p>
        <p>只要你以正确的方式设置状态，现实世界中的几乎所有事物都可以用马尔可夫链近似建模。所以我们将看一些例子。你将在家庭作业和复习中要做的问题中看到更多例子。另一方面，我们不会深入探讨这些例子。相反，我们将开发一般方法。好的。</p><p>Pretty much anything in the real world can be approximately modeled by a Markov chain provided that you set
            your
            states in the proper way. So we’re going to see some examples. You’re going to see more examples in the
            problems
            you’re going to do in homework and recitation. On the other hand, we’re not going to go too deep into
            examples.
            Rather, we’re going to develop the general methodology. OK.</p>
        <h2 id="markov-models">马尔可夫模型</h2><h2>Markov Models</h2>
        <p>好的。马尔可夫模型非常通用。</p><p>All right. Markov models can be pretty general.</p>
        <p>它们可以在连续或离散的时间内运行。它们可以具有连续或离散的状态空间。在这节课中，我们将只讨论状态空间是离散的、时间是离散的情况，因为这是最简单的情况。而且，这也是你在讨论其他课程中更一般的情况之前建立直觉的地方。因此，状态是离散的和有限的。状态的数量是有限的。</p><p>They can run in continuous or discrete time. They can have continuous or discrete state spaces. In this
            class,
            we’re going to stick just to the case where the state space is discrete and time is discrete because this is
            the
            simplest case. And also, it’s the one where you build your intuition before going to more general cases
            perhaps
            in other classes. So the state is discrete and finite. There’s a finite number of states.</p>
        <p>在任何时间点，过程都处于这些状态之一。时间是离散的，因此在每个时间单位，有人吹口哨，然后状态就会跳跃。当它跳跃时，它可以落在同一个地方，也可以落在其他地方。过程的演变由转移概率描述。Pij 是在当前状态为 i 的情况下下一个状态为 j 的概率。</p><p>At any point in time, the process is sitting on one of those states. Time is discrete, so at each unit of
            time,
            somebody whistles and then the state jumps. And when it jumps, it can either land in the same place, or it
            can
            land somewhere else. And the evolution of the process is described by transition probabilities. Pij is the
            probability that the next state is j given that the current state is i.</p>
        <p>马尔可夫链最重要的特性，即马尔可夫链或马尔可夫过程的定义，是无论你如何到达状态 I，也无论当时是什么时间，这个概率 Pij 每次都是相同的。所以我们的模型是时间同质的，这基本上意味着这些转移概率每次都是相同的。</p><p>And the most important property that the Markov chain has, the definition of a Markov chain or Markov
            process, is
            that this probability, Pij, is the same every time that you land at state I no matter how you got there and
            also
            no matter what time it is. So the model we have is time homogeneous, which basically means that those
            transition
            probabilities are the same at every time.</p>
        <p>因此，从这个意义上说，该模型是时不变的。因此，我们感兴趣的是，从长远来看，该链条或流程将会发生什么。因此，我们感兴趣的是，从某个状态开始，经过 n 步之后，我们发现自己处于某个特定状态 j 的概率。幸运的是，我们可以递归计算这些概率。</p><p>So the model is time invariant in that sense. So we’re interested in what the chain or the process is going
            to do
            in the longer run. So we’re interested, let’s say, in the probability that starting at a certain state, n
            times
            steps later, we find ourselves at some particular state j. Fortunately, we can calculate those probabilities
            recursively.</p>
        <p>当然，在第一个时间 1 时，假设我们现在处于状态 i，那么 1 时间后处于状态 j 的概率，根据定义，这只是转移概率。因此，通过了解这些，我们可以开始递归，该递归告诉我们超过 n 步的转移概率。这个递归是一个公式。它始终是正确的。您可以复制它或记住它。</p><p>Of course, at the first time 1, the probability of being 1 time later at state j given that we are right now
            at
            state i, by definition, this is just the transition probabilities. So by knowing these, we can start a
            recursion
            that tells us the transition probabilities for more than n steps. This recursion, it’s a formula. It’s
            always
            true. You can copy it or memorize it.</p>
        <p>但是，这个公式背后有一个重要思想，你应该牢记在心。基本上就是分而治之的思想。它是全概率定律的一个应用。让我们先确定 i。你处于状态 j 的概率，你将其分解为可以到达状态 j 的不同方式的概率。这些不同方式是什么？</p><p>But there is a big idea behind that formula that you should keep in mind. And basically, the divide and
            conquer
            idea. It’s an application of the total probability law. So let’s fix i. The probability that you find
            yourself
            at state j, you break it up into the probabilities of the different ways that you can get to state j. What
            are
            those different ways?</p>
        <p>不同的方式是您在上一次可能处于的不同状态 k。因此，有一定概率，有这个概率，您上一次处于状态 k。然后，有概率 Pkj，您会转换到状态 j。因此，这是一个可能的情况，在 n 次转换后，您会进入状态 j。通过对所有 k 求和，我们考虑了所有可能的情况。</p><p>The different ways are the different states k at which you might find yourself the previous time. So with
            some
            probability, with this probability, you find yourself at state k the previous time. And then with
            probability
            Pkj, you make a transition to state j. So this is a possible scenario that takes you to state j after n
            transitions. And by summing over all the k’s, then we have considered all the possible scenarios.</p>
        <p>现在，在我们进入更严肃的话题之前，让我们先做一些热身，了解一下如何使用转移概率来计算更一般的概率，然后讨论一下马尔可夫链的一些结构特性，最后进入今天的正题，即稳态行为。有人给你这个链，我们的惯例是，这里没有显示的弧对应于 0 概率。</p><p>Now, before we move to the more serious stuff, let’s do a little bit of warm up to get a handle on how we use
            transition probabilities to calculate more general probabilities, then talk about some structural properties
            of
            Markov chains, and then eventually get to the main business of today, which is a steady state behavior. So
            somebody gives you this chain, and our convention is that those arcs that are not shown here corresponds to
            0
            probabilities.</p>
        <p>图中所示的每条弧线都有非零概率，有人会给我们这个概率。假设链从状态 1 开始。我们想计算它遵循这条特定路径的概率。也就是说，它先到状态 2，然后到状态 6，然后到状态 7。我们如何计算特定轨迹的概率？嗯，这就是概率。</p><p>And each one of the arcs that’s shown has a non zero probability, and somebody gives it to us. Suppose that
            the
            chain starts at state 1. We want to calculate the probability that it follows this particular path. That is,
            it
            goes to 2, then to 6, then to 7. How do we calculate the probability of a particular trajectory? Well, this
            is
            the probability.</p>
        <p>所以这是从 1 到 2，再到 6，再到 7 的轨迹的概率。所以这个轨迹的概率我们使用乘法规则。几件事发生的概率是第一件事发生的概率，也就是从 1 到 2 的转变。然后假设我们处于状态 2，我们乘以下一个事件发生的条件概率。</p><p>So it’s the probability of the trajectory from 1 that you go to 2, then to 6, then to 7. So the probability
            of
            this trajectory is we use the multiplication rule. The probability of several things happening is the
            probability that the first thing happens, which is a transition from 1 to 2. And then given that we are at
            state
            2, we multiply with a conditional probability that the next event happens.</p>
        <p>也就是说，假设我们目前处于状态 1，那么 X2 等于 6。条件概率就是 P26。请注意，无论我们如何到达状态 2，这个条件概率都适用。这是马尔可夫假设。所以我们不关心我们是通过什么方式到达的。</p><p>That is, that X2 is equal to 6 given that right now, we are at state 1. And that conditional probability is
            just
            P26. And notice that this conditional probability applies no matter how we got to state 2. This is the
            Markov
            assumption. So we don’t care about the fact that we came in a particular way.</p>
        <p>假设我们来到这里，这个概率 P26，即下一次转换将带我们到 6。然后假设所有这些事情都发生了，所以假设现在我们处于状态 6，我们需要乘以条件概率，即下一次转换将带我们到状态 7。这只是 P67。因此，要找到遵循特定轨迹的概率，只需将特定轨迹上的转换概率相乘即可。</p><p>Given that we came in here, this probability P26, that the next transition takes us to 6. And then given that
            all
            that stuff happened, so given that right now, we are at state 6, we need to multiply with a conditional
            probability that the next transition takes us to state 7. And this is just the P67. So to find the
            probability
            of following a specific trajectory, you just multiply the transition probabilities along the particular
            trajectory.</p>
        <p>现在，如果你想计算其他东西，比如，假设他们从这个状态开始，那么 4 个时间步骤之后我发现自己处于状态 7 的概率。你如何计算这个概率？一种方法是使用 Rijs 的递归，我们知道它总是有效的。</p><p>Now, if you want to calculate something else, such as for example, the probability that 4 time steps later, I
            find myself at state 7 given that they started, let’s say, at this state. How do you calculate this
            probability?
            One way is to use the recursion for the Rijs that we know that it is always valid.</p>
        <p>但是对于简短而简单的例子，并且时间范围很短，也许你可以用蛮力的方式来做到这一点。蛮力的方式是什么？这是 4 个时间步骤之后我发现自己处于状态 7 的事件。此事件可以以各种方式发生。因此，我们可以评估所有不同的方式，并写下它们的概率。</p><p>But for short and simple examples, and with a small time horizon, perhaps you can do this in a brute force
            way.
            What would be the brute force way? This is the event that 4 time steps later, I find myself at state 7. This
            event can happen in various ways. So we can take stock of all the different ways, and write down their
            probabilities.</p>
        <p>所以从 2 开始。一种可能性是遵循这个轨迹，1 个转换，2 个转换，3 个转换，4 个转换。然后我会到达状态 7。这个轨迹的概率是多少？它是 P26 乘以 P67 乘以 P76 然后乘以 P67。所以这是一条特定轨迹的概率，它会在 4 个时间步骤后带你到达状态 7。但还有其他轨迹。那会是什么呢？</p><p>So starting from 2. One possibility is to follow this trajectory, 1 transition, 2 transitions, 3 transitions,
            4
            transitions. And that takes me to state 7. What’s the probability of this trajectory? It’s P26 times P67
            times
            P76 and then times P67. So this is a probability of a particular trajectory that takes you to state 7 after
            4
            time steps. But there’s other trajectories as well. What could be it?</p>
        <p>我可能从状态 2 开始，进入状态 6，停留在状态 6，再停留在状态 6。然后从状态 6 进入状态 7。所以肯定还有一个。另一个是什么？我想我可以走 1、2、6、7。好的。这是另一条轨迹。</p><p>I might start from state 2, go to state 6, stay at state 6, stay at state 6 once more. And then from state 6,
            go
            to state 7. And so there must be one more. What’s the other one? I guess I could go 1,2.6,7. OK. That’s the
            other trajectory.</p>
        <p>加上 P21 乘以 P12 乘以 P26 再乘以 P67。因此，转移概率，即我们处于状态 7 的总体概率，被分解为所有不同方式的概率之和，这些方式我可以在 4 步内到达状态 7。因此，我们始终可以做到这一点，而无需了解马尔可夫链或我们拥有的 Rij 的通用公式。这个过程有什么问题？</p><p>Plus P21 times P12 times P26 and times P67. So the transition probability, the overall probability of finding
            ourselves at state 7, is broken down as the sum of the probabilities of all the different ways that I can
            get to
            state 7 in exactly 4 steps. So we could always do that without knowing much about Markov chains or the
            general
            formula for the Rij’s that we had. What’s the trouble with this procedure?</p>
        <p>这个过程的问题在于，如果这个指数稍大一点，可能的轨迹数量就会变得相当大。如果这个 4 是 100，你问有多少条长度为 100 的不同轨迹可以带我从这里到那里，那么轨迹的数量将是巨大的。它会随着时间范围呈指数增长。这种计算是不可能的。</p><p>The trouble with this procedure is that the number of possible trajectories becomes quite large if this index
            is
            a little bigger. If this 4 was 100, and you ask how many different trajectories of length 100 are there to
            take
            me from here to there, that number of trajectories would be huge. It grows exponentially with the time
            horizon.
            And this kind of calculation would be impossible.</p>
        <p>基本方程，Rij 的递归基本上是一种组织计算的巧妙方法，这样你所做的计算量在时间范围内不是呈指数增长的。相反，它与时间范围呈线性关系。对于你在时间范围内需要的每个时间步骤，你只需不断重复相同的迭代即可。</p><p>The basic equation, the recursion that have for the Rij’s is basically a clever way of organizing this
            computation so that the amount of computation that you do is not exponential in the time horizon. Rather,
            it’s
            sort of linear with the time horizon. For each time step you need in the time horizon, you just keep
            repeating
            the same iteration over and over.</p>
        <h2 id="state-classification">状态分类</h2><h2>State Classification</h2>
        <p>好的。</p><p>OK.</p>
        <p>现在，我们上次简要讨论的另一件事是将马尔可夫链的不同状态分为两种类型。一般来说，马尔可夫链的状态是循环的，这意味着从循环状态可以到达其他地方。但从其他地方，总有某种方式可以返回。</p><p>Now, the other thing that we discussed last time, briefly, was a classification of the different states of
            the
            Markov chain in two different types. A Markov chain, in general, has states that are recurrent, which means
            that
            from a recurrent state, I can go somewhere else. But from that somewhere else, there’s always some way of
            coming
            back.</p>
        <p>所以如果你有这种形式的链条，无论你去哪里，无论你从哪里开始，你总能回到你开始的地方。这种状态被称为循环状态。</p><p>So if you have a chain of this form, no matter where you go, no matter where you start, you can always come
            back
            where you started. States of this kind are called recurrent.</p>
        <p>另一方面，如果你有几个这样的状态，这种类型的转换，那么这些状态就是瞬态的，因为从这些状态，有可能去到其他地方，而从那里你无法回到你开始的地方。马尔可夫链的一般结构基本上是瞬态的集合。你确定你最终会离开瞬态。</p><p>On the other hand, if you have a few states all this kind, a transition of this type, then these states are
            transient in the sense that from those states, it’s possible to go somewhere else from which place there’s
            no
            way to come back where you started. The general structure of a Markov chain is basically a collection of
            transient states. You’re certain that you are going to leave the transient states eventually.</p>
        <p>离开瞬时状态后，您将进入一个状态类，您会被困在其中。如果您进入这里，您就会被困住。如果您进入那里，您就会被困住。这是一个循环状态类。从任何状态，您都可以进入该类中的任何其他状态。这是另一个循环类。从这里的任何状态，您可以进入该类中的任何其他状态。</p><p>And after you leave the transient states, you enter into a class of states in which you are trapped. You are
            trapped if you get inside here. You are trapped if you get inside there. This is a recurrent class of
            states.
            From any state, you can get to any other state within this class. That’s another recurrent class. From any
            state
            inside here, you can get anywhere else inside that class.</p>
        <p>但是这两个类之间是没有交流的。如果你从这里开始，就没有办法到达那里。如果你有 2 个循环类，那么很明显，从长远来看，马尔可夫链的初始条件很重要。如果你从这里开始，从长远来看，你将被困在这里，这里也是如此。所以初始条件确实会产生影响。</p><p>But these 2 classes, you do not communicate. If you start here, there’s no way to get there. If you have 2
            recurrent classes, then it’s clear that the initial conditions of your Markov chain matter in the long run.
            If
            you start here, you will be stuck inside here for the long run and similarly about here. So the initial
            conditions do make a difference.</p>
        <p>另一方面，如果这个类不在这里，而你只有那个类，那么链会发生什么？假设你从这里开始。你四处走动。在某个时刻，你完成了那个转变。你被困在这里。在这里，你不断循环，因为随机性，你不断访问所有状态。</p><p>On the other hand, if this class was not here and you only had that class, what would happen to the chain?
            Let’s
            say you start here. You move around. At some point, you make that transition. You get stuck in here. And
            inside
            here, you keep circulating, because of the randomness, you keep visiting all states over and over.</p>
        <p>希望或可能，从长远来看，确切的时间或起点并不重要，但无论初始条件如何，处于该特定状态的概率都是相同的。因此，对于单个循环类，我们希望初始条件无关紧要。对于 2 个或更多循环类，初始条件肯定会很重要。</p><p>And hopefully or possibly, in the long run, it doesn’t matter exactly what time it is or where you started,
            but
            the probability of being at that particular state is the same no matter what the initial condition was. So
            with
            a single recurrent class, we hope that the initial conditions do not matter. With 2 or more recurrent
            classes,
            initial conditions will definitely matter.</p>
        <h2 id="periodicity">周期性</h2><h2>Periodicity</h2>
        <p>因此，我们有多少个循环类与链的长期行为以及初始条件的重要性程度有关。初始条件可能重要的另一种方式是，如果链具有周期性结构。定义周期性的方法有很多种。我发现最直观且数学符号最少的一种是以下方法。</p><p>So how many recurrent classes we have is something that has to do with the long term behavior of the chain
            and
            the extent to which initial conditions matter. Another way that initial conditions may matter is if a chain
            has
            a periodic structure. There are many ways of defining periodicity. The one that I find sort of the most
            intuitive and with the least amount of mathematical symbols is the following.</p>
        <p>如果可以将状态集中到多个簇（称为 d 簇或组）中，则链的状态空间被称为周期性的。转换图具有这样的属性：从一个簇，您总是可以转换到下一个簇。所以这里 d 等于 2。我们有两个状态空间子集。无论何时我们在这里，下次我们就会在那里。无论何时我们在这里，下次我们就会在那里。</p><p>The state space of a chain is said to be periodic if you can lump the states into a number of clusters called
            d
            clusters or groups. And the transition diagram has the property that from a cluster, you always make a
            transition into the next cluster. So here d is equal to 2. We have two subsets of the state space. Whenever
            we’re here, next time we’ll be there. Whenever we’re here, next time we will be there.</p>
        <p>所以这个链具有周期性结构。可能仍然有一些随机性。当我从这里跳到这里时，我跳到的状态可能是随机的，但我确定我会在这里。然后下一次，我会确定我在这里。</p><p>So this chain has a periodic structure. There may be still some randomness. When I jump from here to here,
            the
            state to which I jump may be random, but I’m sure that I’m going to be inside here. And then next time, I
            will
            be sure that I’m inside here.</p>
        <p>这将是图的结构，其中周期为 3。如果你从这个块开始，你知道下一次，你会处于这里的状态。下一次，你会处于这里的状态，依此类推。所以这些链肯定有一个周期结构。而且这种周期性会得到保持。如果我从这个块开始，在偶数时间，我肯定我在这里。</p><p>This would be a structure of a diagram in which we have a period of 3. If you start in this lump, you know
            that
            the next time, you would be in a state inside here. Next time, you’ll be in a state inside here, and so on.
            So
            these chains certainly have a periodic structure. And that periodicity gets maintained. If I start, let’s
            say,
            at this lump, at even times, I’m sure I’m here.</p>
        <p>在奇数时间，我确信我在这里。因此，确切时间对于确定不同状态的概率确实很重要。特别是，处于特定状态的概率不能转换为状态值。处于此处状态的概率在所有时间都是 0。一般来说，偶数时间会是某个正数。因此，它会变成 0 正数、零、正数、0 正数。</p><p>At odd times, I’m sure I am here. So the exact time does matter in determining the probabilities of the
            different
            states. And in particular, the probability of being at the particular state cannot convert to a state value.
            The
            probability of being at the state inside here is going to be 0 for all times. In general, it’s going to be
            some
            positive number for even times. So it goes 0 positive, zero, positive, 0 positive.</p>
        <p>不会稳定在任何东西上。因此，当我们具有周期性时，我们并不期望状态概率会收敛到某个值，而是期望它们振荡。</p><p>Doesn’t settle to anything. So when we have periodicity, we do not expect the states probabilities to
            converge to
            something, but rather, we expect them to oscillate.</p>
        <h2 id="is-it-periodic">是否是周期性的</h2><h2>Is it periodic</h2>
        <p>那么，我们如何判断马尔可夫链是否具有周期性呢？有系统的方法来判断，但通常对于我们在本课中看到的例子类型，我们只需目测链，然后判断它是否具有周期性。</p><p>Now, how can we tell whether a Markov chain is periodic or not? There are systematic ways of doing it, but
            usually with the types of examples we see in this class, we just eyeball the chain, and we tell whether it’s
            periodic or not.</p>
        <p>那么，下面的这个链是周期性的吗？有多少人认为它是周期性的？没有。只有一人。有多少人认为它不是周期性的？好的。不是周期性的？让我们看看。让我在这里画一些图。好的。它是周期性的吗？是的。从红色状态，你只能到达白色状态。从白色状态，你只能到达红色状态。</p><p>So is this chain down here, is it the periodic one or not? How many people think it’s periodic? No one. One.
            How
            many people think it’s not periodic? OK. Not periodic? Let’s see. Let me do some drawing here. OK. Is it
            periodic? It is. From a red state, you can only get to a white state. And from a white state, you can only
            get
            to a red state.</p>
        <p>因此，尽管从图片上看不出来，但这个链实际上具有这种结构。我们可以将各州分为红色州和白色州。从红色到白色总是从红色到白色，从白色到红色总是从白色到红色。因此，这说明有时目测并不那么容易。如果您有很多州，那么您可能会遇到一些困难。</p><p>So this chain, even though it’s not apparent from the picture, actually has this structure. We can group the
            states into red states and white states. And from reds, we always go to a white, and from a white, we always
            go
            to a red. So this tells you that sometimes eyeballing is not as easy. If you have lots and lots of states,
            you
            might have some trouble doing this exercise.</p>
        <p>另一方面，了解这一点非常有用。有时，很容易看出链不是周期性的。那是什么情况？假设你的链在某处有自转换。那么你自然就知道你的链不是周期性的。所以请记住，周期性的定义要求，如果你处于某一组状态中，下次你将处于不同的组中。但是如果你有自转换，则该属性不成立。</p><p>On the other hand, something very useful to know. Sometimes it’s extremely easy to tell that the chain is not
            periodic. What’s that case? Suppose that your chain has a self transition somewhere. Then automatically, you
            know that your chain is not periodic. So remember, the definition of periodicity requires that if you are in
            a
            certain group of states, next time, you will be in a different group. But if you have self transitions, that
            property is not true.</p>
        <p>如果你有一个可能的自我转换，那么在下一个时间步骤中，你有可能留在自己的组中。因此，只要你有自我转换，就意味着链不是周期性的。</p><p>If you have a possible self transition, it’s possible that you stay inside your own group for the next time
            step.
            So whenever you have a self transition, this implies that the chain is not periodic.</p>
        <h2 id="what-does-the-chain-do">链条起什么作用</h2><h2>What does the chain do</h2>
        <p>通常，这是我们大多数时候可以判断链不是周期性的的最简单、最容易的方法。
        </p><p>And usually that’s the simplest and easy way that we can tell most of the time that the chain is not
            periodic.
        </p>
        <p>现在，我们来谈谈今天的大话题，核心话题，即从长远来看，马尔可夫链会做什么。我们提出这个问题，也是我们上次通过查看示例激发的问题。这是我们上次示例中确实发生过的事情。因此，我们要问的是，这是否发生在每个马尔可夫链上。</p><p>So now, we come to the big topic of today, the central topic, which is the question about what does the chain
            do
            in the long run. The question we are asking and which we motivated last time by looking at an example. It’s
            something that did happen in our example of last time. So we’re asking whether this happens for every Markov
            chain.</p>
        <p>我们问的是，在某个时间 n 处于状态 j 的概率是否会稳定在稳定状态值。我们称之为 pi sub j。这些是问这个量在 n 趋于无穷大时是否有极限，以便我们可以讨论状态 j 的稳定状态概率。此外，我们问的是该状态的稳定状态概率是否不依赖于初始状态。</p><p>We’re asking the question whether the probability of being at state j at some time n settles to a steady
            state
            value. Let’s call it pi sub j. That these were asking whether this quantity has a limit as n goes to
            infinity,
            so that we can talk about the steady state probability of state j. And furthermore, we asked whether the
            steady
            state probability of that state does not depend on the initial state.</p>
        <p>换句话说，在链条运行了很长很长一段时间后，确切的时间是什么并不重要，链条从哪里开始也不重要。你可以告诉我状态为特定 j 的概率大约是稳定状态概率 pi sub j。确切的时间是什么并不重要，只要你告诉我已经过去了很多时间，这样 n 就是一个很大的数字。</p><p>In other words, after the chain runs for a long, long time, it doesn’t matter exactly what time it is, and it
            doesn’t matter where the chain started from. You can tell me the probability that the state is a particular
            j is
            approximately the steady state probability pi sub j. It doesn’t matter exactly what time it is as long as
            you
            tell me that a lot of time has elapsed so that n is a big number.</p>
        <p>所以这就是问题所在。我们已经看到了一些例子，我们知道情况并不总是如此。例如，正如我刚才讨论的那样，如果我们有 2 个循环类，那么从哪里开始确实很重要。如果我们从这里开始，处于状态 j 的概率 pi(j) 将为 0，但如果我们从那一团开始，那将是正数。</p><p>So this is the question. We have seen examples, and we understand that this is not going to be the case
            always.
            For example, as I just discussed, if we have 2 recurrent classes, where we start does matter. The
            probability
            pi(j) of being in that state j is going to be 0 if we start here, but it would be something positive if we
            were
            to start in that lump.</p>
        <p>因此，如果我们有多个循环类，那么初始状态确实很重要。但是，如果我们只有一类循环状态，并且您可以从每个循环状态转到任何其他循环状态，那么就不会出现这个问题。然后我们预计初始条件会被遗忘。所以这是我们需要的一个条件。然后我们需要的另一个条件是链不是周期性的。</p><p>So the initial state does matter if we have multiple recurrent classes. But if we have only a single class of
            recurrent states from each one of which you can get to any other one, then we don’t have that problem. Then
            we
            expect initial conditions to be forgotten. So that’s one condition that we need. And then the other
            condition
            that we need is that the chain is not periodic.</p>
        <p>如果链是周期性的，那么这些 Rij 就不会收敛。它们会不断振荡。如果我们没有周期性，那么我们就有希望获得所需的收敛。事实证明，这就是马尔可夫链的重大理论。稳态收敛定理。事实证明，只要满足这两个条件，rijs 就会收敛到稳态极限，我们称之为稳态概率。</p><p>If the chain is periodic, then these Rij’s do not converge. They keep oscillating. If we do not have
            periodicity,
            then there is hope that we will get the convergence that we need. It turns out this is the big theory of
            Markov
            chains. the steady state convergence theorem. It turns out that yes, the rijs do converge to a steady state
            limit, which we call a steady state probability as long as these two conditions are satisfied.</p>
        <p>我们不会证明这个定理。如果你真的感兴趣，章末的练习基本上会引导你证明这个结果，但在这个课堂上做这个可能有点太多了。这个定理背后的直观想法是什么？让我们看看。让我们直观地思考一下为什么初始状态并不重要。想象从不同的初始状态开始的两个链副本，状态随机移动。
        </p><p>We’re not going to prove this theorem. If you’re really interested, the end of chapter exercises basically
            walk
            you through a proof of this result, but it’s probably a little too much for doing it in this class. What is
            the
            intuitive idea behind this theorem? Let’s see. Let’s think intuitively as to why the initial state doesn’t
            matter. Think of two copies of the chain that starts at different initial states, and the state moves
            randomly.
        </p>
        <p>当状态从两个初始状态开始随机移动时，轨迹是随机的。只要在某个点有一个单一的循环类，并且在某个点没有周期性，这些状态，这两条轨迹就会发生碰撞。因为那里有足够的随机性。即使我们从不同的地方开始，状态也会是一样的。</p><p>As the state moves randomly starting from the two initial states a random trajectory. as long as you have a
            single recurrent class at some point, and you don’t have periodicity at some point, those states, those two
            trajectories, are going to collide. Just because there’s enough randomness there. Even though we started
            from
            different places, the state is going to be the same.</p>
        <p>当状态变得相同时，这些轨迹的未来在概率上是相同的，因为它们都是从相同的状态开始的。所以这意味着初始条件不再有任何影响。这就是初始状态被遗忘的原因的高级概念。即使你从不同的初始状态开始，在某个时候，你可能会发现自己处于与其他轨迹相同的状态。</p><p>After the state becomes the same, then the future of these trajectories, probabilistically, is the same
            because
            they both started at the same state. So this means that the initial conditions stopped having any influence.
            That’s sort of the high level idea of why the initial state gets forgotten. Even if you started at different
            initial states, at some time, you may find yourself to be in the same state as the other trajectory.</p>
        <p>一旦发生这种情况，你的初始条件就不会对未来产生任何影响。好的。</p><p>And once that happens, your initial conditions cannot have any effect into the future. All right.</p>
        <h2 id="steady-state-probabilities">稳态概率</h2><h2>Steady State Probabilities</h2>
        <p>那么让我们看看如何计算这些稳态概率。我们计算稳态概率的方法是采用这个递归，这对于最终步骤转移概率始终是正确的，并取两边的极限。这一边的极限是状态 j 的稳态概率，即 pi sub j。</p><p>So let’s see how we might calculate those steady state probabilities. The way we calculate the steady state
            probabilities is by taking this recursion, which is always true for the end step transition probabilities,
            and
            take the limit of both sides. The limit of this side is the steady state probability of state j, which is pi
            sub
            j.</p>
        <p>这一边的极限，我们把这个极限放在求和中。现在，当 n 趋于无穷大时，n 也趋于无穷大。所以这个 Rik 就是从状态 i 开始的状态 k 的稳态概率。现在我们从哪里开始并不重要。所以这只是状态 k 的稳态概率。所以这个项收敛到那个项，这给了我们一个满足稳态概率的方程。实际上，它不是一个方程。</p><p>The limit of this side, we put the limit inside the summation. Now, as n goes to infinity, n also goes to
            infinity. So this Rik is going to be the steady state probability of state k starting from state i. Now
            where we
            started doesn’t matter. So this is just the steady state probability of state k. So this term converges to
            that
            one, and this gives us an equation that’s satisfied by the steady state probabilities. Actually, it’s not
            one
            equation.</p>
        <p>我们为每个 j 得到一个方程。因此，如果我们有 10 个可能的状态，我们将得到 10 个线性方程组。未知数为 pi(1) 至 pi(10)。好的。10 个未知数，10 个方程。您可能认为我们在做生意。但实际上，这个方程组是奇异的。0 是该系统的一个可能解。如果您在任何地方都让 pi 等于零，则方程式得到满足。</p><p>We get one equation for each one of the j’s. So if we have 10 possible states, we’re going to get the system
            of
            10 linear equations. In the unknowns, pi(1) up to pi(10). OK. 10 unknowns, 10 equations. You might think
            that we
            are in business. But actually, this system of equations is singular. 0 is a possible solution of this
            system. If
            you plug pi equal to zero everywhere, the equations are satisfied.</p>
        <p>它没有唯一的解，所以也许我们需要一个条件来得到唯一可解的线性方程组。事实证明，这个方程组有一个唯一的解。如果你施加一个额外的条件，这很自然，pi(j) 是不同状态的概率，所以它们应该加起来为 1。所以你想把这个方程加到混合中。</p><p>It does not have a unique solution, so maybe we need one more condition to get the uniquely solvable system
            of
            linear equations. It turns out that this system of equations has a unique solution. If you impose an
            additional
            condition, which is pretty natural, the pi(j)’s are the probabilities of the different states, so they
            should
            add to 1. So you want this one equation to the mix.</p>
        <p>一旦你这样做了，这个方程组就会有一个唯一的解。所以我们可以通过求解这些线性方程来找到马尔可夫链的稳态概率，这在数字上很简单。</p><p>And once you do that, then this system of equations is going to have a unique solution. And so we can find
            the
            steady state probabilities of the Markov chain by just solving these linear equations, which is numerically
            straightforward.</p>
        <h2 id="balanced-equations">平衡方程</h2><h2>Balanced Equations</h2>
        <p>现在，这些方程非常重要。我的意思是，它们是马尔可夫链的中心点。它们有一个名字。它们被称为平衡方程。值得用不同的方式来解释它们。
        </p><p>Now, these equations are quite important. I mean, they’re the central point in the Markov chain. They have a
            name. They’re called the balance equations. And it’s worth interpreting them in a somewhat different way.
        </p>
        <p>因此，直观地讲，人们有时可以将概率视为频率。例如，如果我抛出一枚无偏硬币，正面的概率为 1/2，你也可以说，如果我继续抛硬币，从长远来看，有 1/2 的时间，我会看到正面。同样，让我们​​尝试解释这个 pi(j)，即稳态概率，即我处于状态 j 的长期概率。</p><p>So intuitively, one can sometimes think of probabilities as frequencies. For example, if I toss an unbiased
            coin,
            probability 1/2 of heads, you could also say that if I keep flipping that coin, in the long run, 1/2 of the
            time, I’m going to see heads. Similarly, let’s try an interpretation of this pi(j), the steady state
            probability, the long term probability of finding myself at state j.</p>
        <p>让我们试着将其解释为，如果我沿着马尔可夫链运行一条非常非常长的轨迹，我发现自己处于状态 j 的频率。因此，轨迹四处移动，访问状态。它以不同的频率访问不同的状态。让我们将您处于某个状态的概率视为与访问该状态的频率相同。事实证明这是一个正确的陈述。</p><p>Let’s try to interpret it as the frequency with which I find myself at state j if I run a very, very long
            trajectory over that Markov chain. So the trajectory moves around, visits states. It visits the different
            states
            with different frequencies. And let’s think of the probability that you are at a certain state as being sort
            of
            the same as the frequency of visiting that state. This turns out to be a correct statement.</p>
        <p>如果你更严谨，你就必须证明这一点。但这是一种有效的解释，它让我们对这些方程式的含义有了更多的直觉。所以让我们按如下方式思考。让我们专注于一个特定的状态 j，并考虑进入状态 j 的转换与离开状态 j 的转换，或者进入 j 的转换与从 j 开始的转换。因此从 j 开始的转换包括自我转换。好的。</p><p>If you were more rigorous, you would have to prove it. But it’s an interpretation which is valid and which
            gives
            us a lot of intuition about what these equation is saying. So let’s think as follows. Let’s focus on a
            particular state j, and think of transitions into the state j versus transitions out of the state j, or
            transitions into j versus transitions starting from j. So transition starting from that includes a self
            transition. Ok.</p>
        <p>那么，如果我们将 pi(j) 解释为频率，那么我们多久会获得一次转换呢？我们是这样考虑的。在 pi(1) 的时间内，我们将处于状态 1。每当我们处于状态 1 时，就会有一个概率 P1j 发生这种转换。</p><p>So how often do we get a transition, if we interpret the pi(j)’s as frequencies, how often do we get a
            transition
            into j? Here’s how we think about it. A fraction pi(1) of the time, we’re going to be at state 1. Whenever
            we
            are at state 1, there’s going to be a probability, P1j, that we make a transition of this kind.</p>
        <p>因此，在我们处于状态 1 的时间中，有一个频率 P1j，即下一个转换进入状态 j。那么在轨迹上发生的转换总数中，有多少转换是这种类型的？转换的这一部分是您处于状态 1 的时间比例乘以您恰好进入下一个状态 j 的时间比例。</p><p>So out of the times that we’re at state 1, there’s a frequency, P1j with which the next transition is into j.
            So
            out of the overall number of transitions that happen at the trajectory, what fraction of those transitions
            is
            exactly of that kind? That fraction of transitions is the fraction of time that you find yourself at 1 times
            the
            fraction with which out of one you happen to visit next state j.</p>
        <p>因此，我们将这个数字解释为此类转换的频率。在任何给定时间，我们的链可以进行不同类型的转换，一般形式的转换从某个 k，I 到某个 l。因此，我们尝试进行一些计算。每种特定类型的转换发生的频率是多少？这就是该特定类型的转换发生的频率。现在，转换到状态 j 的总频率是多少？
        </p><p>So we interpreted this number as the frequency of transitions of this kind. At any given time, our chain can
            do
            transitions of different kinds, transitions of the general form from some k, I go to some l. So we try to do
            some accounting. How often does a transition of each particular kind happen? And this is the frequency with
            which transitions of that particular kind happens. Now, what’s the total frequency of transitions into state
            j?
        </p>
        <p>状态 j 的转换可以通过从 1 到 j、从 2 到 j 或从状态 m 到 j 的转换来实现。因此，要找到我们观察到的转换到 j 的总频率，就是这个特定的总和。现在，当且仅当最后一次转换是进入状态 j 时，您才处于状态 j。</p><p>Transitions into state j can happen by having a transition from 1 to j, from 2 to j, or from state m to j. So
            to
            find the total frequency with which we would observe transitions into j is going to be this particular sum.
            Now,
            you are at state j if and only if the last transition was into state j.</p>
        <p>因此，您处于状态 j 的频率就是转换到状态 j 的频率。因此，这个等式准确地表达了这一陈述。处于状态 j 的概率是上次转换到状态 j 的概率之和。或者就频率而言，您处于状态 j 的频率是所有可能将您带入状态 j 的转换类型的频率之和。</p><p>So the frequency with which you are at j is the frequency with which transitions into j happen. So this
            equation
            expresses exactly that statement. The probability of being at state j is the sum of the probabilities that
            the
            last transition was into state j. Or in terms of frequencies, the frequency with which you find yourself at
            state j is the sum of the frequencies of all the possible transition types that take you inside state j.</p>
        <p>所以这是一个很有用的直觉，稍后我们将看到一个例子，它为我们提供了分析马尔可夫链的捷径。但在继续之前，让我们重新回顾一下上次的例子。让我们写下这个例子的平衡方程。所以我发现自己处于状态 1 的稳定状态概率是我上次处于状态 1 并进行自我转换的概率。</p><p>So that’s a useful intuition to have, and we’re going to see an example a little later that it gives us short
            cuts into analyzing Markov chains. But before we move, let’s revisit the example from last time. And let us
            write down the balance equations for this example. So the steady state probability that I find myself at
            state 1
            is the probability that the previous time I was at state 1 and I made a self transition.</p>
        <p>因此，我上次来这里并进行这种转变的概率，加上我上次来这里并进行那种转变的概率。</p><p>So the probability that I was here last time and I made a transition of this kind, plus the probability that
            the
            last time I was here and I made a transition of that kind.</p>
        <p>所以加上 pi(2) 乘以 0.2。同样，对于其他状态，我处于状态 2 的稳定状态的概率是我上次处于状态 1 时转换到状态 2 的概率加上上次处于状态 2 时转换到状态 1 的概率。现在，这是两个方程和两个未知数，pi(1) 和 pi(2)。</p><p>So plus pi(2) times 0.2. And similarly, for the other states, the steady state probably that I find myself at
            state 2 is the probability that last time I was at state 1 and I made a transition into state 2, plus the
            probability that the last time I was at state 2 and I made the transition into state 1. Now, these are two
            equations and two unknowns, pi(1) and pi(2).</p>
        <p>但你会注意到，这两个方程告诉你的是同一件事。它们告诉你 0.5pi(1) 等于 0.2pi(2)。如果你移动项的位置，这两个方程中的任何一个都会告诉你这个确切的结果。所以这两个方程实际上不是两个方程。它们只是一个方程。</p><p>But you notice that both of these equations tell you the same thing. They tell you that 0.5pi(1) equals
            0.2pi(2).
            Either of these equations tell you exactly this if you move terms around. So these two equations are not
            really
            two equations. It’s just one equation.</p>
        <p>它们是线性相关方程，为了解决这个问题，我们需要一个附加条件，即 pi(1) + pi(2) 等于 1。现在，我们有了两个方程组，你可以解它。解完后，你会发现 pi(1) 是 2/7，而 pi(2) 是 5/7。所以这些是两种不同状态的稳态概率。</p><p>They are linearly dependent equations, and in order to solve the problem, we need the additional condition
            that
            pi(1) + pi(2) is equal to 1. Now, we have our system of two equations, which you can solve. And once you
            solve
            it, you find that pi(1) is 2/7 and pi(2) is 5/7. So these are the steady state probabilities of the two
            different states.</p>
        <p>如果我们在某个状态（比如说状态 1）启动这个链，让它运行很长很长时间，链就会进入稳定状态。这是什么意思呢？这并不意味着状态本身进入稳定状态。状态将永远不停地跳来跳去。它会不时地访问这两个状态。所以跳跃永远不会停止。</p><p>If we start this chain, at some state, let’s say state 1, and we let it run for a long, long time, the chain
            settles into steady state. What does that mean? It does not mean that the state itself enters steady state.
            The
            state will keep jumping around forever and ever. It will keep visiting both states once in a while. So the
            jumping never ceases.</p>
        <p>进入稳定状态的因素是处于状态 1 的概率。因此，在时间 1 万亿时处于状态 1 的概率约为 2/7。在时间 2 万亿时处于状态 1 的概率同样约为 2/7。因此，处于该状态的概率稳定为一个稳定值。这就是稳定状态收敛的含义。它是概率的收敛，而不是过程本身的收敛。</p><p>The thing that gets into steady state is the probability of finding yourself at state 1. So the probability
            that
            you find yourself at state 1 at time one trillion is approximately 2/7. The probability you find yourself at
            state 1 at time two trillions is again, approximately 2/7. So the probability of being in that state settles
            into a steady value. That’s what the steady state convergence means. It’s convergence of probabilities, not
            convergence of the process itself.</p>
        <p>再次强调，本例中发生的两个主要事情，更一般地讲，当我们只有一个类并且没有周期性时，初始状态并不重要。这里有足够的随机性，所以无论你从哪里开始，随机性都会抹去你从哪里开始的任何记忆。而且在这个例子中，显然我们没有周期性，因为我们有自弧。</p><p>And again, the two main things that are happening in this example, and more generally, when we have a single
            class and no periodicity, is that the initial state does not matter. There’s enough randomness here so that
            no
            matter where you start, the randomness kind of washes out any memory of where you started. And also in this
            example, clearly, we do not have periodicity because we have self arcs.</p>
        <p>这尤其暗示了具体时间并不重要。</p><p>And this, in particular, implies that the exact time does not matter.</p>
        <h2 id="birthdeath-processes">出生死亡过程</h2><h2>BirthDeath Processes</h2>
        <p>所以现在，我们将用剩下的时间来研究一类特殊的链，这种链比较容易处理，但它仍然是一个重要的链。那么从这里可以得到什么教训呢？这是一个有两个状态的简单示例，我们可以通过求解一个简单的二乘二方程组来找到稳定状态概率。</p><p>So now, we’re going to spend the rest of our time by looking into a special class of chains that’s a little
            easier to deal with, but still, it’s an important class. So what’s the moral from here? This was a simple
            example with two states, and we could find the steady state probabilities by solving a simple system of two
            by
            two equations.</p>
        <p>如果你有一个包含 100 个状态的链，那么计算机解决一个 100 x 100 方程组是没有问题的。但你肯定不能手工完成，而且通常你无法得到任何闭式公式，所以你不一定能得到很多见解。所以人们会寻找特殊的结构或模型，它们可能会给你更多的见解，或者可能会引导你得到闭式公式。</p><p>If you have a chain with 100 states, it’s no problem for a computer to solve a system of 100 by 100
            equations.
            But you can certainly not do it by hand, and usually, you cannot get any closed form formulas, so you do not
            necessarily get a lot of insight. So one looks for special structures or models that maybe give you a little
            more insight or maybe lead you to closed form formulas.</p>
        <p>马尔可夫链的一个有趣子类是生死过程类，所有这些好事都会发生在这个子类中。那么生死过程是什么呢？它是一个马尔可夫链，其图表基本上是这样的。马尔可夫链的状态从 0 开始，一直到某个有限整数 m。</p><p>And an interesting subclass of Markov chains in which all of these nice things do happen, is the class of
            birth/death processes. So what’s a birth/death process? It’s a Markov chain who’s diagram looks basically
            like
            this. So the states of the Markov chain start from 0 and go up to some finite integer m.</p>
        <p>这个链的特别之处在于，如果你处于某个状态，下次你可以增加 1，也可以减少 1，或者你可以保持原样。所以这就像在任何给定时间跟踪一些人口。一个人出生，或者一个人死亡，或者什么都没有发生。再说一次，我们这里不考虑双胞胎。</p><p>What’s special about this chain is that if you are at a certain state, next time you can either go up by 1,
            you
            can go down by 1, or you can stay in place. So it’s like keeping track of some population at any given time.
            One
            person gets born, or one person dies, or nothing happens. Again, we’re not accounting for twins here.</p>
        <p>因此，我们给出了这个结构，并给出了转换概率，即与不同类型的转换相关的概率。因此，我们使用 P 表示向上转换，Q 表示向下转换。这种链的一个例子是我们上次讨论的超市柜台模型。</p><p>So we’re given this structure, and we are given the transition probabilities, the probabilities associated
            with
            transitions of the different types. So we use P’s for the upward transitions, Q’s for the downward
            transitions.
            An example of a chain of this kind was the supermarket counter model that we discussed last time.</p>
        <p>也就是说，如果有顾客到达，状态就会增加 1。或者顾客结束服务，状态就会减少 1，或者什么事都不会发生，你留在原地，等等。在超市模型中，这里的这些 P 都被视为相等，因为我们假设到达率在每个时间段都是恒定的。</p><p>That is, a customer arrives, so this increments the state by 1. Or a customer finishes service, in which
            case,
            the state gets decremented by 1, or nothing happens in which you stay in place, and so on. In the
            supermarket
            model, these P’s inside here were all taken to be equal because we assume that the arrival rate was sort of
            constant at each time slot.</p>
        <p>但是，您可以稍微概括一下，假设这些转换概率 P1 在这里，P2 在那里，等等，可能因状态而异。所以一般来说，从状态 i 开始，下一个转换向上的概率为 Pi。下一个转换向下的概率为 Qi。因此，从该状态开始，下一个转换向下的概率将是 Q_(i+1)。
        </p><p>But you can generalize a little bit by assuming that these transition probabilities P1 here, P2 there, and so
            on
            may be different from state to state. So in general, from state i, there’s going to be a transition
            probability
            Pi that the next transition is upwards. And there’s going to be a probability Qi that the next transition is
            downwards. And so from that state, the probability that the next transition is downwards is going to be
            Q_(i+1).
        </p>
        <p>这就是我们的链条结构。正如我所说，这是超市柜台上发生的事情的粗略模型，但它也是多种服务系统的良好模型。同样，你在某处有一个带有缓冲区的服务器。作业进入缓冲区。因此缓冲区不断增加。服务器处理作业，因此缓冲区不断减少。</p><p>So this is the structure of our chain. As I said, it’s a crude model of what happens at the supermarket
            counter
            but it’s also a good model for lots of types of service systems. Again, you have a server somewhere that has
            a
            buffer. Jobs come into the buffer. So the buffer builds up. The server processes jobs, so the buffer keeps
            going
            down.</p>
        <p>链的状态就是缓冲区内的任务数量。或者，您可以考虑某个城市外的活跃电话。</p><p>And the state of the chain would be the number of jobs that you have inside your buffer. Or you could be
            thinking
            about active phone calls out of a certain city.</p>
        <p>每次拨打电话时，活动电话数就会增加 1。每次电话停止通话时，计数就会减少 1。因此，对于这种类型的过程，将出现具有这种结构的模型。它们确实出现在许多模型中。或者您可以考虑特定人群中患有疾病的人数。</p><p>Each time that the phone call is placed, the number of active phone calls goes up by 1. Each time that the
            phone
            call stops happening, is terminated, then the count goes down by 1. So it’s for processes of this kind that
            a
            model with this structure is going to show up. And they do show up in many, many models. Or you can think
            about
            the number of people in a certain population that have a disease.</p>
        <p>因此，每多一个人患上流感，病例数就会增加。每多一个人痊愈，病例数就会减少。在这种流行病模型中，这些概率肯定取决于当前状态。如果已经有很多人患上流感，那么另一个人感染的概率就会相当高。</p><p>So 1 more person gets the flu, the count goes up. 1 more person gets healed, the count goes down. And these
            probabilities in such an epidemic model would certainly depend on the current state. If lots of people
            already
            have the flu, the probability that another person catches it would be pretty high.</p>
        <p>而如果没有人患流感，那么发生有人患流感的转变的概率就会非常小。因此，转变率，即新患病人数的发生率肯定取决于已经患病的人数。这促使出现 P 值（向上转变概率）取决于链状态的情况。那么我们如何研究这个链呢？
        </p><p>Whereas, if no one has the flu, then the probability that you get a transition where someone catches the flu,
            that probability would be pretty small. So the transition rates, the incidence of new people who have the
            disease definitely depends on how many people already have the disease. And that motivates cases where those
            P’s, the upward transition probabilities, depend on the state of the chain. So how do we study this chain?
        </p>
        <p>您可以坐下来，用圆周率写出 n 个线性方程组。这样，就可以找到这个链的稳定状态概率。但这有点难。它比实际需要做的工作要多。有一种非常巧妙的捷径适用于出生/死亡过程。它基于我们刚才讨论的频率解释。</p><p>You can sit down and write the system of n linear equations in the pi’s. And this way, find the steady state
            probabilities of this chain. But this is a little harder. It’s more work than one actually needs to do.
            There’s
            a very clever shortcut that applies to birth/death processes. And it’s based on the frequency interpretation
            that we discussed a little while ago.</p>
        <p>让我们在这个链的中间某处画一条线，并更详细地关注这个部分和那个部分之间的关​​系。想象一下这个链沿着这个方向、那个方向继续延伸。但我们只关注两个相邻的状态，并看看这个特定的切口。这个链会做什么？假设它从这里开始。它会四处移动。在某个时刻，它会过渡到另一侧。</p><p>Let’s put a line somewhere in the middle of this chain, and focus on the relation between this part and that
            part
            in more detail. So think of the chain continuing in this direction, that direction. But let’s just focus on
            2
            adjacent states, and look at this particular cut. What is the chain going to do? Let’s say it starts here.
            It’s
            going to move around. At some point, it makes a transition to the other side.</p>
        <p>这是从 I 到 i+1 的转换。它在另一侧停留一段时间。它到达这里，最终将转换到这一侧。然后它继续移动，依此类推。现在，这里必须遵守一定的平衡。通过这条线向上转换的次数不能与向下转换的次数相差太大。因为我们通过这条线，所以下次我们将通过那条线。</p><p>And that’s a transition from I to i+1. It stays on the other side for some time. It gets here, and
            eventually,
            it’s going to make a transition to this side. Then it keeps moving and so on. Now, there’s a certain balance
            that must be obeyed here. The number of upward transitions through this line cannot be very different from
            the
            number of downward transitions. Because we cross this way, then next time, we’ll cross that way.</p>
        <p>那么下次，我们再从这边穿过。我们再从那边穿过。因此，这种转变发生的频率必须与那种转变发生的长期频率相同。你不可能上涨 100 次，而只下跌 50 次。如果你上涨了 100 次，那就意味着你下跌了 99 次、100 次或 101 次，但没有什么太大的区别。</p><p>Then next time, we’ll cross this way. We’ll cross that way. So the frequency with which transitions of this
            kind
            occur has to be the same as the long term frequency that transitions of that kind occur. You cannot go up
            100
            times and go down only 50 times. If you have gone up 100 times, it means that you have gone down 99, or 100,
            or
            101, but nothing much more different than that.</p>
        <p>因此，观察到这种类型的转换的频率是多少。也就是说，在大量的转换中，这种类型的转换占多少比例？这个比例必须与碰巧是这种类型的转换的比例相同。这些比例是多少？我们之前讨论过这个问题。</p><p>So the frequency with which transitions of this kind get observed. That is, out of a large number of
            transitions,
            what fraction of transitions are of these kind? That fraction has to be the same as the fraction of
            transitions
            that happened to be of that kind. What are these fractions? We discussed that before.</p>
        <p>观察到这种转换的时间比例就是我们恰好处于该状态的时间比例。在我们处于该状态的时间中，恰好是向上转换的转换比例。所以这就是观察到这种转换的频率。用同样的论点，这就是观察到那种转换的频率。</p><p>The fraction of times at which transitions of this kind are observed is the fraction of time that we happen
            to be
            at that state. And out of the times that we are in that state, the fraction of transitions that happen to be
            upward transitions. So this is the frequency with which transitions of this kind are observed. And with the
            same
            argument, this is the frequency with which transitions of that kind are observed.</p>
        <p>由于这两个频率相同，这两个数字也一定相同，我们得到了一个将 Pi 与 P_(i+1) 联系起来的方程。这个方程的形式很好，因为它给出了一个递归。如果我们知道 pi(i)，那么我们就可以立即计算出 pi(i+1)。所以这是一个几乎很容易求解的方程组。但是我们如何开始呢？</p><p>Since these two frequencies are the same, these two numbers must be the same, and we get an equation that
            relates
            the Pi to P_(i+1). This has a nice form because it gives us a recursion. If we knew pi(i), we could then
            immediately calculate pi(i+1). So it’s a system of equations that’s very easy to solve almost. But how do we
            get
            started?</p>
        <p>如果我知道 pi(0)，我可以通过 pi(1) 找到，然后使用这个递归找到 pi(2)、pi(3) 等等。但我们不知道 pi(0)。这是另一个未知数。这是一个未知数，我们实际上需要使用额外的规范化条件，即 pi 的总和为 1。使用这个规范化条件后，我们就可以找到所有的 pi。</p><p>If I knew pi(0), I could find by pi(1) and then use this recursion to find pi(2), pi(3), and so on. But we
            don’t
            know pi(0). It’s one more unknown. It’s an unknown, and we need to actually use the extra normalization
            condition that the sum of the pi’s is 1. And after we use that normalization condition, then we can find all
            of
            the pi’s.</p>
        <h2 id="special-case">特例</h2><h2>Special Case</h2>
        <p>因此，您基本上将 pi(0) 固定为一个符号，用符号方式求解该方程，然后所有内容都以 pi(0) 的形式表示。然后使用该归一化条件找到 pi(0)，就大功告成了。让我们在一个特定的特殊情况下说明此过程的细节。因此，在我们的特殊情况下，我们现在将通过假设所有向上的 P 都是相同的，并且所有向下的 Q 都是相同的来简化事情。</p><p>So you basically fix pi(0) as a symbol, solve this equation symbolically, and everything gets expressed in
            terms
            of pi(0). And then use that normalization condition to find pi(0), and you’re done. Let’s illustrate the
            details
            of this procedure on a particular special case. So in our special case, we’re going to simplify things now
            by
            assuming that all those upward P’s are the same, and all of those downward Q’s are the same.</p>
        <p>因此，在每个时间点，如果你处于中间位置，则上升的概率为 P，下降的概率为 Q。这个 rho，即 P/Q 的比率，是上升频率与下降频率的比值。如果是服务系统，你可以将其视为系统负载程度的衡量标准。</p><p>So at each point in time, if you’re sitting somewhere in the middle, you have probability P of moving up and
            probability Q of moving down. This rho, the ratio of P/Q is frequency of going up versus frequency of going
            down. If it’s a service system, you can think of it as a measure of how loaded the system is.</p>
        <p>如果 P 等于 Q，则意味着如果你处于这种状态，你向左或向右移动的可能性是相等的，因此系统是平衡的。状态没有朝这个方向或那个方向移动的趋势。
        </p><p>If P is equal to Q, it’s means that if you’re at this state, you’re equally likely to move left or right, so
            the
            system is kind of balanced. The state doesn’t have a tendency to move in this direction or in that
            direction.
        </p>
        <p>如果 rho 大于 1，使得 P 大于 Q，则意味着每当我处于中间某个状态时，我更有可能向右移动而不是向左移动，这意味着我的状态当然是随机的，但它有朝那个方向移动的趋势。</p><p>If rho is bigger than 1 so that P is bigger than Q, it means that whenever I’m at some state in the middle,
            I’m
            more likely to move right rather than move left, which means that my state, of course it’s random, but it
            has a
            tendency to move in that direction.</p>
        <p>如果你把它看作排队的顾客数量，这意味着你的系统有负载过重和排队的趋势。因此，rho 大于 1 表示负载过重，排队会越积越多。rho 小于 1 表示系统中的队列有减少的趋势。现在，让我们写下方程式。我们有这个递归 P_(i+1) 是 Pi 乘以 Pi 除以 Qi。</p><p>And if you think of this as a number of customers in queue, it means your system has the tendency to become
            loaded and to build up a queue. So rho being bigger than 1 corresponds to a heavy load, where queues build
            up.
            Rho less than 1 corresponds to the system where queues have the tendency to drain down. Now, let’s write
            down
            the equations. We have this recursion P_(i+1) is Pi times Pi over Qi.</p>
        <p>在我们这里，P 和 Q 不依赖于特定索引，所以我们得到了这个关系。而这个 Q 上的 P 只是负载因子 rho。一旦你看了这个等式，你就会清楚地意识到 pi(1) 是 rho 乘以 pi(0)。pi(2) 将是 所以我们会详细讲解。所以 pi(1) 是 pi(0) 乘以 rho。pi(2) 是 pi(1) 乘以 rho，也就是 pi(0) 乘以 rho 的平方。</p><p>In our case here, the P’s and the Q’s do not depend on the particular index, so we get this relation. And
            this P
            over Q is just the load factor rho. Once you look at this equation, clearly you realize that by pi(1) is rho
            times pi(0). pi(2) is going to be So we’ll do it in detail. So pi(1) is pi(0) times rho. pi(2) is pi(1)
            times
            rho, which is pi(0) times rho squared.</p>
        <p>然后你继续进行这个计算。你会发现你可以用 pi(0) 来表示每个 pi(i)，然后你得到这个 rho^i 因子。然后你使用我们得到的最后一个方程</p><p>And then you continue doing this calculation. And you find that you can express every pi(i) in terms of pi(0)
            and
            you get this factor of rho^i. And then you use the last equation that we have</p>
        <p>概率之和必须等于 1。该等式告诉我们，pi(0) rho 与 I 之和除以 1，从 0 到 m 所有 i 等于 1。因此，pi(0) 等于 1 除以（I 从 0 到 m 的所有 i 与 rho 与 I 之和）。</p><p>That the sum of the probabilities has to be equal to 1. And that equation is going to tell us that the sum
            over
            all i’s from 0 to m of pi(0) rho to the I is equal to 1. And therefore, pi(0) is 1 over (the sum over the
            rho to
            the I for I going from 0 to m).</p>
        <p>现在我们找到了 pi(0)，代入这个表达式，我们就得到了所有不同状态的稳态概率。让我们来看一些特殊情况。假设 rho 等于 1。如果 rho 等于 1，那么 pi(i) 等于 pi(0)。这意味着所有稳态概率都相等。这意味着从长远来看，每个状态出现的可能性都相等。所以这是一个例子。</p><p>So now we found pi(0), and by plugging in this expression, we have the steady state probabilities of all of
            the
            different states. Let’s look at some special cases of this. Suppose that rho is equal to 1. If rho is equal
            to
            1, then pi(i) is equal to pi(0). It means that all the steady state probabilities are equal. It’s means that
            every state is equally likely in the long run. So this is an example.</p>
        <p>这被称为对称随机游走。这是对醉酒者进行建模的一种非常流行的模型。因此，你可以从任意时间点的状态开始。要么你原地不动，要么你有相同的概率向左或向右走。两个方向都没有偏差。你可能会认为，在这样的过程中，你会倾向于卡在一端或另一端附近。</p><p>It’s called a symmetric random walk. It’s a very popular model for modeling people who are drunk. So you
            start at
            a state at any point in time. Either you stay in place, or you have an equal probability of going left or
            going
            right. There’s no bias in either direction. You might think that in such a process, you will tend to kind of
            get
            stuck near one end or the other end.</p>
        <p>嗯，目前还不清楚会发生什么。事实证明，在这样的模型中，从长远来看，醉酒者处于上述任何一种状态的可能性都是相等的。</p><p>Well, it’s not really clear what to expect. It turns out that in such a model, in the long run, the drunk
            person
            is equally likely to be at any one of those states.</p>
        <p>如果 rho 等于 1，则所有 i 的稳定状态概率都相同。因此，如果您在随机时间出现并询问我的状态在哪里，您将被告知它出现在这些地方的概率是相等的。所以让我们记下来。如果 rho 等于 1，则意味着所有 pi(i) 都是 1/(M+1) M+1，因为这就是我们模型中的状态数。</p><p>The steady state probability is the same for all i’s if rho is equal to 1. And so if you show up at a random
            time, and you ask where is my state, you will be told it’s equally likely to be at any one of those places.
            So
            let’s make that note. If rho equal to 1, implies that all the pi(i)’s are 1/(M+1) M+1 because that’s how
            many
            states we have in our model.</p>
        <p>现在，让我们看一个不同的案例。假设 M 是一个巨大的数字。那么本质上，我们的超市拥有非常大的空间，有足够的空间来容纳他们的顾客。但假设系统处于稳定状态。P 小于 Q，这意味着顾客得到服务的速度往往比他们到达的速度要快。这条链条中的趋势往往朝着这个方向发展。</p><p>Now, let’s look at a different case. Suppose that M is a huge number. So essentially, our supermarket has a
            very
            large space, a lot of space to store their customers. But suppose that the system is on the stable side. P
            is
            less than Q, which means that there’s a tendency for customers to be served faster than they arrive. The
            drift
            in this chain, it tends to be in that direction.</p>
        <p>因此，当 rho 小于 1 时（就是这种情况），当 M 趋向无穷大时，这个无穷和就是一个几何级数的和。并且您（希望）认识到这个级数趋向于 1/(1 rho)。并且因为它在分母中，所以 pi(0) 最终等于 1 rho。</p><p>So when rho is less than 1, which is this case, and when M is going to infinity, this infinite sum is the sum
            of
            a geometric series. And you recognize it (hopefully) this series is going to 1/(1 rho). And because it’s in
            the
            denominator, pi(0) ends up being 1 rho.</p>
        <p>因此，通过取 M 趋于无穷大的极限，在这种情况下，当 rho 小于 1 时，该级数收敛，我们得到这个公式。所以我们得到了 pi(i) 的闭式公式。具体来说，pi(i) 是 (1 rho)(rho 到 i)。所以这些 pi(i) 本质上是一个概率分布。它们告诉我们，如果我们在第 10 亿次出现，我们会问，我的状态在哪里？</p><p>So by taking the limit as M goes to infinity, in this case, and when rho is less than 1 so that this series
            is
            convergent, we get this formula. So we get the closed form formula for the pi(i)’s. In particular, pi(i) is
            (1
            rho)(rho to the i). to So these pi(i)’s are essentially a probability distribution. They tell us if we show
            up
            at time 1 billion and we ask, where is my state?</p>
        <p>您将被告知状态为 0。您的系统为空的概率为 1 rho，减去或系统中只有一名客户，这种情况发生的概率为 (rho 1) 乘以 rho。它一直这样下去。它几乎是一个几何分布，只是它已经移动，因此它从 0 开始，而通常的几何分布从 1 开始。所以这是排队理论的一个简短介绍。</p><p>You will be told that the state is 0. Your system is empty with probability 1 rho, minus or there’s one
            customer
            in the system, and that happens with probability (rho 1) times rho. And it keeps going down this way. And
            it’s
            pretty much a geometric distribution except that it has shifted so that it starts at 0 whereas the usual
            geometric distribution starts at 1. So this is a mini introduction into queuing theory.</p>
        <p>这是你开始学习排队理论时遇到的第一个也是最简单的模型。这显然是排队现象的模型，例如超市柜台，其中 P 对应于到达，Q 对应于离开。当 M 非常大且 rho 小于 1 时，这个特定的排队系统有一个非常简单且漂亮的闭式解决方案。这就是它如此受欢迎的原因。</p><p>This is the first and simplest model that one encounters when you start studying queuing theory. This is
            clearly
            a model of a queueing phenomenon such as the supermarket counter with the P’s corresponding to arrivals, the
            Q’s
            corresponding to departures. And this particular queuing system when M is very, very large and rho is less
            than
            1, has a very simple and nice solution in closed form. And that’s why it’s very much liked.</p>
        <p>让我花两秒钟画出最后一幅图。这就是不同 i 的概率。它给出了 PMF。这个 PMF 有一个预期值。这个期望值，即系统中预期的客户数量，由这个公式给出。这个公式对任何试图分析这种系统的人来说都很有趣，它告诉你以下内容。</p><p>And let me just take two seconds to draw one last picture. So this is the probability of the different i’s.
            It
            gives you a PMF. This PMF has an expected value. And the expectation, the expected number of customers in
            the
            system, is given by this formula. And this formula, which is interesting to anyone who tries to analyze a
            system
            of this kind, tells you the following.</p>
        <p>只要 rho 小于 1，那么系统中预期的客户数量就是有限的。但是如果 rho 变得非常接近 1，那么如果您的负载系数是 0.99，您预计系统中在任何时候都会有大量客户。好的。好的。祝您周末愉快。我们下次继续。</p><p>That as long as a rho is less than 1, then the expected number of customers in the system is finite. But if
            rho
            becomes very close to 1 So if your load factor is something like .99, you expect to have a large number of
            customers in the system at any given time. OK. All right. Have a good weekend. We’ll continue next time.</p>
        <h1 id="markov-chains-iii">18.马尔可夫链 III</h1><h1>18. Markov Chains III</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:50 <a href="https://youtube.com/watch?v=HIMxdWDLEK8">https://youtube.com/watch?v=HIMxdWDLEK8</a></p><p> 12 years ago (Nov 10, 2012) — 51:50 <a href="https://youtube.com/watch?v=HIMxdWDLEK8">https://youtube.com/watch?v=HIMxdWDLEK8</a></p>
        <h2 id="intro-4">简介</h2><h2>Intro</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.</p>
        <h2 id="agenda">议程</h2><h2>Agenda</h2>
        <p>约翰·齐茨克利斯：我们要回顾一下上次讨论的内容。</p><p>JOHN TSITSIKLIS: So what we’re going to do is to review what we have discussed last time.</p>
        <p>然后我们将讨论马尔可夫链的经典应用，即分析如何确定电话系统的规模。最后，今天将讲两件新事情。我们将了解如何计算与马尔可夫链有关的某些有趣数量。</p><p>Then we’re going to talk about the classic application of Markov chains to analyze how do you dimension a
            phone
            system. And finally, there will be two new things today. We will see how we can calculate certain
            interesting
            quantities that have to do with Markov chains.</p>
        <h2 id="markov-chain">马尔可夫链</h2><h2>Markov Chain</h2>
        <p>那么让我们开始吧。我们已经得到了马尔可夫链，并且我们假设我们的链是相当不错的。</p><p>So let us start. We’ve got our Markov chain and let’s make the assumption that our chain is kind of nice.</p>
        <p>所谓“好”是指我们可能有一些瞬时状态。然后我们有一个单一的循环类，其中有循环状态。因此，这是一个单一的循环类，因为从该类中的任何状态都可以进入任何其他状态。因此，一旦进入这里，你就会循环并继续访问所有这些状态。这些状态看起来是瞬时状态。</p><p>And by nice we mean that we’ve got maybe some transient states. And then we’ve got a single recurrent class
            of
            recurrent states. So this is a single recurrent class in the sense that from any state in that class you can
            get
            to any other state. So once you’re in here you’re going to circulate and keep visiting all of those states.
            Those states appear transient.</p>
        <p>轨迹可能会在这里移动，但最终其中一个转变将发生，而你最终将陷入这个肿块。</p><p>The trajectory may move around here, but eventually one of these transitions will happen and you’re going to
            end
            up in this lump.</p>
        <h2 id="steady-state">稳定状态</h2><h2>Steady State</h2>
        <p>让我们假设单个循环类不是周期性的。这些是最佳的马尔可夫链。它们之所以最佳是因为它们具有以下属性，即当时间 n 很大时，您发现自己处于某个特定状态 j 的概率。</p><p>Let’s make the assumption that the single recurrent class is not periodic. These are the nicest kind of
            Markov
            chains. And they’re nicest because they have the following property, the probability that you find yourself
            at
            some particular state j at the time n when that time is very large.</p>
        <p>该概率稳定在一个稳定状态值，我们用 pi sub j 表示。该陈述包含两个部分。一部分是这个极限存在。因此，状态 j 的概率稳定在某个值，而且该概率不受 i 的影响。无论你从哪里开始，状态 j 的概率在长期内都是相同的。</p><p>That probability settles to a steady state value that we denote by pi sub j. And there are two parts in the
            statement. One part is that this limit exists. So the probability of state j settles to something, and
            furthermore that probability is not affected by i. It doesn’t matter where you started, no matter where you
            started, the probability of state j is going to be the same in the long run.</p>
        <p>也许更清晰的符号可能是这种形式。给定初始状态为 I，处于状态 j 的概率在极限中等于 pi(j)。现在，如果我不告诉你从哪里开始，而你查看处于状态 i 的无条件概率，你可以对初始状态取平均值，使用总期望定理，你将得到相同的答案极限 pi(j)。</p><p>Maybe a clearer notation could be of this form. The probability of being at state j given the initial state
            being
            I is equal to pi(j) in the limit. Now, if I don’t tell you where you started and you look at the
            unconditional
            probability of being at state i, you can average over the initial states, use the total expectation theorem
            and
            you’re going to get the same answer pi(j) in the limit.</p>
        <p>因此，这说明，在给定初始状态的情况下，极限条件下的条件概率与无条件概率相同。而这正是我们认为具有独立性的情况。因此，这个结果告诉我们，Xn 和 Xi 大致独立。当 n 趋于无穷大时，它们在极限条件下变得独立。这就是稳态定理告诉我们的。</p><p>So this tells you that to the conditional probability given the initial state in the limit is the same as the
            unconditional probability. And that’s a situation that we recognize as being one where we have independence.
            So
            what this result tells us is that Xn and Xi are approximately independent. They become independent in the
            limit
            as n goes to infinity. So that’s what the steady state theorem tells us.</p>
        <p>初始条件无关紧要，因此您在某个较大的时间 n 的状态与您的初始状态无关，不受其影响。了解初始状态不会告诉您有关您在时间 n 的状态的任何信息，因此不会告诉您当时的状态。抱歉，应该是 1，或者应该是 0，因此状态不受过程开始位置的影响。</p><p>The initial conditions don’t matter, so your state at some large time n has nothing to do, is not affected by
            what your initial state was. Knowing the initial state doesn’t tell you anything about your state at time n,
            therefore the states at the times. sorry that should be a 1, or it should be a 0 so the state is not
            affected by
            where the process started.</p>
        <p>因此，如果马尔可夫链要运行很长时间，而我们感兴趣的问题是状态在哪里，那么你的答案将是，我不知道，它是随机的。但它将是一个具有特定概率的特定 j。因此，稳态概率对我们来说很有趣，这提出了我们如何计算它们的问题。</p><p>So if the Markov chain is to operate for a long time and we’re interested in the question where is the state,
            then your answer would be, I don’t know, it’s random. But it’s going to be a particular j with this
            particular
            probability. So the steady state probabilities are interesting to us and that raises the question of how do
            we
            compute them.</p>
        <p>我们计算它们的方法是求解一个线性方程组，这些方程称为平衡方程，再加上一个额外的方程，即概率必须满足的归一化方程，因为概率之和必须始终为 1。我们上次谈到了这个方程的解释。从某种意义上说，这基本上是概率流的守恒。进来的必须出去。</p><p>The way we compute them is by solving a linear system of equations, which are called the balance equations,
            together with an extra equation, the normalization equation that has to be satisfied by probability, because
            probabilities must always add up to 1. We talked about the interpretation of this equation last time. It’s
            basically a conservation of probability flow in some sense. What comes in must get out.</p>
        <p>在特定时间发现自己处于状态 j 的概率是最后一次转换将我带入状态 j 的总概率。最后一次转换以各种方式将我带入状态 j。可能是我之前处于特定状态 j 时，我从 k 转换到了 j。因此，我们将这里的这个数字解释为这些特定类型 k 到 j 的转换发生的频率。</p><p>The probability of finding yourself at state j at a particular time is the total probability of the last
            transition taking me into state j. The last transition takes me into state j in various ways. It could be
            that
            the previous time I was at the particular state, j and I made a transition from k into j. So this number
            here,
            we interpret as the frequency with which transitions of these particular type k to j, occur.</p>
        <p>然后通过将所有 k 相加，我们考虑了所有类型的转换，这些转换会引导我们进入状态 j。因此，处于 j 的概率是进入 j 的概率的总和。如果我们有多个循环类会怎么样？如果我们把这张图片改成这样。所以这里我们得到了一个次级循环类。如果你在这里，你就不能去那里。如果你在这里，你就不能去那里。</p><p>And then by adding over all k’s we consider transitions of all types that lead us inside state j. So the
            probability of being at the j is the sum total of the probabilities of getting into j. What if we had
            multiple
            recurrent classes? So if we take this picture and change it to this. So here we got a secondary recurrent
            class.
            If you’re here, you cannot get there. If you are here, you cannot get there.</p>
        <p>从长远来看会发生什么？嗯，从长远来看，如果你从这里开始，你最终会经历一个转变，要么是这种类型的转变，你会在这里结束，要么是那种类型的转变，你会在那里结束。</p><p>What happens in the long run? Well, in the long run, if you start from here you’re going to make a transition
            eventually, either of this type and you would end up here, or you will make a transitional of that type and
            you
            will end up there.</p>
        <p>如果你最终到达这里，你的链的长期统计数据，即不同状态的概率，将是这个链单独考虑的稳定状态概率。所以你继续，只为这个链求解这个方程组，这些将是你的稳定状态概率。如果你碰巧到达这里。</p><p>If you end up here, the long term statistics of your chain, that is, the probabilities of the different
            states,
            will be the steady state probabilities of this chain regarded in isolation. So you go ahead and you solve
            this
            system of equations just for this chain, and these will be your steady state probabilities. If you happened
            to
            get in here.</p>
        <p>另一方面，如果你碰巧去了那里，考虑到那个事件，那么从长远来看，发生的事情只与这个链条本身的运行有关。所以你要找到那个子链内的稳态概率。所以你要分别针对这个链条和那个链条求解线性系统、稳态方程。</p><p>If, on the other hand, it happens that you went there, given that event, then what happens in the long run
            has to
            do with just this chain running by itself. So you find the steady state probabilities inside that sub chain.
            So
            you solve the linear system, the steady state equations, for this chain separately and for that chain
            separately.</p>
        <p>如果你碰巧从这里开始，那么这个子链的稳定状态概率将适用。当然，这提出了一个问题，如果我从这里开始，我怎么知道我会到达这里还是那里？嗯，你不知道，这是随机的。结果可能是你到达了这里，也可能是到达了那里。</p><p>If you happen to start inside here then the steady state probabilities for this sub chain are going to apply.
            Now
            of course this raises the question, if I start here, how do I know whether I’m going to get here or there?
            Well,
            you don’t know, it’s random. It may turn out that you get to here, it may turn out that you get there.</p>
        <p>因此，我们感兴趣的是计算你最终到达这里的概率与你最终到达那里的概率。这是我们今天讲座结束时要做的工作。因此，作为热身，为了了解如何解释这些稳定状态概率，让我们看一下我们熟悉的例子。这是一个 2 状态马尔可夫链。</p><p>So we will be interested in calculating the probabilities that eventually you end up here versus the
            probability
            that eventually you end up there. This is something that we’re going to do towards the end of today’s
            lecture.
            So, as a warm up, just to see how we interpret those steady state probabilities, let us look at our familiar
            example. This is a 2 state Markov chain.</p>
        <p>上次我们确实写下了该链的平衡方程，我们发现稳定状态概率分别为 2/7 和 5/7。所以让我们尝试计算一些数量。假设您从状态 1 开始，并且想要计算这个特定的概率。</p><p>Last time we did write down the balance equations for this chain and we found the steady state probabilities
            to
            be 2/7 and 5/7 respectively. So let us try to calculate some quantities. Suppose that you start at state 1,
            and
            you want to calculate to this particular probability.</p>
        <p>因此，由于我们假设从状态 1 开始，因此本质上我们在这里以初始状态等于 1 为条件。现在两件事发生的条件概率是第一件事发生的概率。但我们生活在一个我们说初始状态为 1 的世界。然后假设这件事发生了，第二件事发生的概率。</p><p>So since we’re assuming that we’re starting at state 1, essentially here we are conditioning on the initial
            state
            being equal to 1. Now the conditional probability of two things happening is the probability that the first
            thing happens. But we’re living in the world where we said that the initial state was 1. And then given that
            this thing happened, the probability that the second thing happens.</p>
        <p>但是，我们再说一遍，我们讨论的是初始状态为 1 的条件概率。那么这个量是多少呢？这个是从状态 1 到状态 1 的转换概率，所以是 P11。第二个概率呢？假设你从 1 开始，下一次从 1 开始，那么在 100 时你从 1 开始的概率是多少？</p><p>But again, we’re talking about conditional probabilities given that the initial state was 1. So what is this
            quantity? This one is the transition probability from state 1 to state 1, so it’s P11. How about the second
            probability? So given that you started at 1 and the next time you were at 1, what’s the probability that at
            the
            time 100 you are at 1?</p>
        <p>现在，由于马尔可夫特性，如果我告诉你此时你处于 1，你如何到达那里并不重要。所以这部分条件作用并不重要。</p><p>Now because of the Markov property, if I tell you that at this time you are at 1, it doesn’t matter how you
            get
            there. So this part of the conditioning doesn’t matter.</p>
        <p>而我们所得到的是从状态 1 到状态 1 的 99 步转换概率。因此，您到达 1 然后在 99 步之后再次回到状态 1 的概率是第一次转换将您带到 1 的概率乘以从 1 开始的接下来 99 个转换中，在 99 步之后您再次回到状态 1 的概率。现在，99 可能是一个很大的数字，因此我们对这个数量进行近似。</p><p>And what we have is the 99 step transition probability from state 1 to state 1. So the probability that you
            get
            to 1 and then 99 steps later you find yourself again at one is the probability that the first transition
            takes
            you to 1 times the probability that over the next 99 transitions starting from 1, after 99 steps you end up
            again at state 1. Now, 99 is possibly a big number, and so we approximate this quantity.</p>
        <p>我们使用状态 1 的稳定状态概率。这给了我们这个特定表达式的近似值。我们可以做同样的事情来计算相同类型的事物。所以你从状态 1 开始。100 步之后你再次处于状态 1 的概率是多少？所以这将是 P11。不是 P</p><p>We’re using the steady state probability of state 1. And that gives us an approximation for this particular
            expression. We can do the same thing to calculate something of the same kind. So you start at state 1.
            What’s
            the probability that 100 steps later you are again at state 1? So that’s going to be P11. not P</p>
        <p>R11。100 步转移概率，即从 1 开始到达 1，然后在时间 100 到达 1 之后，下一次您发现自己处于状态 2 的概率是多少？这将是概率 P12。近似地，由于 100 是一个很大的数字，这大约是 pi(1) 乘以 P12。好的。这就是我们可以使用稳态概率进行近似的方法。</p><p>R11. The 100 step transition probability that starting from 1 you get to 1, and then after you get to 1 at
            time
            100 what’s the probability that the next time you find yourself at state 2? This is going to be the
            probability
            P12. And approximately, since 100 is a large number, this is approximately pi(1) times P12. OK. So that’s
            how we
            can use steady state probabilities to make approximations.</p>
        <p>或者，例如，如果您继续做这种例子，您可以问在时间 100 时 X 为 1，并且在时间 200 时 X 等于 1 的概率是多少。那么这将是在 100 步内从 1 到 1 的转换概率，然后在接下来的 100 步中您又从 1 回到 1。这将大约是 pi(1) 乘以 pi(1)。</p><p>Or you could, for example, if you continue doing examples of this kind, you could ask for what’s the
            probability
            that X at time 100 is 1, and also X at time 200 is equal to 1. Then this is going to be the transition
            probability from 1 to 1 in 100 steps, and then over the next 100 steps from 1 you get again to 1. And this
            is
            going to be approximately pi(1) times pi(1).</p>
        <p>因此，当这里涉及的数字 n 很大时，我们用稳态概率来近似多步转移概率。现在我说 99 或 100 很大。我们如何知道它足够大，以至于限制已经生效，并且我们的近似值是好的？</p><p>So we approximate multi step transition probabilities by the steady state probabilities when the number n
            that’s
            involved in here is big. Now I said that’s 99 or 100 is big. How do we know that it’s big enough so that the
            limit has taken effect, and that our approximation is good?</p>
        <p>这和马尔可夫链的时间尺度有关，我所说的时间尺度是指初始状态被遗忘需要多长时间。需要多长时间才能有足够的随机性，以至于事物混合在一起，而不管你从哪里开始？所以如果你看看这个链，平均需要 5 次尝试才能完成这种转变。</p><p>This has something to do with the time scale of our Markov chain, and by time scale, I mean how long does it
            take
            for the initial states to be forgotten. How long does it take for there to be enough randomness so that
            things
            sort of mix and it doesn’t matter where you started? So if you look at this chain, it takes on the average,
            let’s say 5 tries to make a transition of this kind.</p>
        <p>平均需要 2 次尝试才能发生这种转变。因此，每 10 个时间步骤左右就会出现一点随机性。超过 100 次步骤就会出现很多随机性，因此您预计初始状态将被遗忘。这没关系。在 100 个时间步骤中发生了足够的混合和随机性。因此，这种近似是好的。</p><p>It takes on the average 2 tries for a transition of that kind to take place. So every 10 time steps or so
            there’s
            a little bit of randomness. Over 100 times steps there’s a lot of randomness, so you expect that the initial
            state will have been forgotten. It doesn’t matter. There’s enough mixing and randomness that happens over
            100
            time steps. And so this approximation is good.</p>
        <p>另一方面，如果数字不同，故事就会有所不同。假设这个数字是 0.999，那个数字是 0.998 之类的数字，那么这个数字就变成 0.002，那个数字就变成 0.001。假设数字是这种类型。忘记初始状态需要多长时间？如果我从这里开始，下次我回到那里的概率是千分之一。</p><p>On the other hand, if the numbers were different, the story would have been different. Suppose that this
            number
            is 0.999 and that number is something like 0.998, so that this number becomes 0.002, and that number becomes
            0.001. Suppose that the numbers were of this kind. How long does it take to forget the initial state? If I
            start
            here, there’s a probability of 1 in 1,000 that next time I’m going to be there.</p>
        <p>因此，平均而言，我需要大约一千次尝试才能离开该状态。因此，在大约一千个时间步骤中，我的初始状态确实很重要。如果我告诉你你从这里开始，你很确定，假设在接下来的 100 个时间步骤中，你仍将在这里。因此，初始状态有很大的影响。在这种情况下，我们说这个马尔可夫链的时间尺度要慢得多。</p><p>So on the average it’s going to take me about a thousand tries just to leave that state. So, over roughly a
            thousand time steps my initial state really does matter. If I tell you that you started here, you’re pretty
            certain that, let’s say over the next 100 time steps, you will still be here. So the initial state has a big
            effect. In this case we say that this Markov chain has a much slower time scale.</p>
        <p>混合需要更长的时间，忘记初始状态需要更长的时间，这意味着如果步骤数只有 99，我们就无法进行这种近似。在这里，我们可能需要 n 足够大，比如说，10,000 左右，然后我们才能开始使用近似值。</p><p>It takes a much longer time to mix, it takes a much longer time for the initial state to be forgotten, and
            this
            means that we cannot do this kind of approximation if the number of steps is just 99. Here we might need n
            to be
            as large as, let’s say, 10,000 or so before we can start using the approximation.</p>
        <p>因此，当使用这种近似时，需要了解状态移动的速度，并将其考虑在内。因此，有一整个子领域专门用于估计或计算不同马尔可夫链混合的速度，这就是何时可以应用这些稳态近似的问题。
        </p><p>So when one uses that approximation, one needs to have some sense of how quickly does the state move around
            and
            take that into account. So there’s a whole sub field that deals with estimating or figuring out how quickly
            different Markov chains mix, and that’s the question of when can you apply those steady state
            approximations.
        </p>
        <h2 id="erlang">Erlang</h2><h2>Erlang</h2>
        <p>现在让我们更接近现实世界。</p><p>So now let’s get a little closer to the real world.</p>
        <p>我们将要讨论一个著名的问题，它是由一位名叫 Erlang 的丹麦工程师提出、发起和解决的。我们在泊松过程中看到的 Erlang 分布就是以他的名字命名的。所以这是 100 多年前的事了，当时电话才刚刚出现。</p><p>We’re going to talk about a famous problem that was posed, started, and solved by a Danish engineer by the
            name
            of Erlang. This is the same person whose name is given to the Erlang distribution that we saw in the context
            of
            the Poisson processes. So this was more than 100 years ago, when phones had just started existing.</p>
        <p>他试图弄清楚建立电话系统需要什么，即要为一个社区设置多少条线路才能与外界通信。故事是这样的。你有一个村庄，这个村庄有一定数量的人口，你想设置电话线。</p><p>And he was trying to figure out what it would take to set up a phone system that how many lines should you
            set up
            for a community to be able to communicate to the outside world. So here’s the story. You’ve got a village,
            and
            that village has a certain population, and you want to set up phone lines.</p>
        <p>因此，您要设置多条电话线，假设该号码为 B，用于连接外部世界。您要怎么做呢？嗯，您希望 B 比较小。您不想设置太多线路，因为那样成本太高。</p><p>So you want to set up a number of phone lines, let’s say that number is B, to the outside world. And how do
            you
            want to do that? Well, you want B to be kind of small. You don’t want to set up too many wires because
            that’s
            expensive.</p>
        <p>另一方面，您需要有足够的线路，以便当有一定数量的人同时打电话时，他们都将获得一条线路并能够通话。因此，如果 B 是 10，并且有 12 个人想同时通话，那么其中 2 个人会收到忙音，而这不是我们想要的。</p><p>On the other hand, you want to have enough wires so that if a reasonable number of people place phone calls
            simultaneously, they will all get a line and they will be able to talk. So if B is 10 and 12 people want to
            talk
            at the same time, then 2 of these people would get a busy signal, and that’s not something that we like.</p>
        <p>我们希望 B 足够大，这样在合理条件下，几乎可以肯定没有人会收到忙音。那么我们如何对这种情况进行建模呢？</p><p>We would like B to be large enough so that there’s a substantial probability, that there’s almost certainty
            that,
            under reasonable conditions, no one is going to get a busy signal. So how do we go about modeling a
            situation
            like this?</p>
        <p>好吧，要建立一个模型，你需要两个部分，一个是描述如何发起电话呼叫，以及一旦开始电话呼叫，需要多长时间才能结束电话呼叫？所以我们将做出最简单的假设。我们假设电话呼叫源自泊松过程。也就是说，在那个群体中，人们实际上并不协调。</p><p>Well, to set up a model you need two pieces, one is to describe how do phone calls get initiated, and once a
            phone call gets started, how long does it take until the phone call is terminated? So we’re going to make
            the
            simplest assumptions possible. Let’s assume that phone calls originate as a Poisson process. That is, out of
            that population people do not really coordinate.</p>
        <p>在完全随机的时间，不同的人会决定接听电话。不同的人之间没有依赖关系，不同的时间没有什么特别的，不同的时间是独立的。因此，泊松模型是模拟这种情况的合理方法。它将是一个具有某个速率 lambda 的泊松过程。现在，在实践中很容易估计速率 lambda。</p><p>At completely random times, different people with decide to pick up the phone. There’s no dependencies
            between
            different people, there’s nothing special about different times, different times are independent. So a
            Poisson
            model is a reasonable way of modeling this situation. And it’s going to be a Poisson process with some rate
            lambda. Now, the rate lambda would be easy to estimate in practice.</p>
        <p>你观察几天内那个村庄发生的事情，然后你就可以算出人们打电话的频率。现在，关于电话本身，我们假设电话的持续时间是一个随机变量，该变量具有指数分布，具有某个参数 mu。所以 1/mu 是电话的平均持续时间。所以平均持续时间，同样，很容易估计。</p><p>You observe what happens in that village just over a couple of days, and you figure out what’s the rate at
            which
            people attempt to place phone calls. Now, about phone calls themselves, we’re going to make the assumption
            that
            the duration of a phone call is a random variable that has an exponential distribution with a certain
            parameter
            mu. So 1/mu is the mean duration of a phone call. So the mean duration,,again, is easy to estimate.</p>
        <p>你只要观察一下正在发生的事情，看看这些电话的平均通话时长。指数假设是一个好的假设吗？嗯，这意味着大多数电话通话时间会比较短，但会有一小部分电话通话时间会更长，然后还有一小部分电话通话时间会更长。所以这听起来很有道理。</p><p>You just observe what’s happening, see on the average how long these phone calls are. Is the exponential
            assumption a good assumption? Well, it’s means that most phone calls will be kind of short, but there’s
            going to
            be a fraction of phone calls that are going to be larger, and then a very small fraction that are going to
            be
            even larger. So it sounds plausible.</p>
        <p>这不太现实，也就是说，通话时间不足 15 秒的电话并不常见。所以要么什么都没发生，要么你不得不说几句话等等。此外，在人们使用拨号调制解调器连接互联网的时代，这种假设完全不成立，因为如果电话是免费的，人们会拨号，然后让他们的电话线保持几个小时的繁忙状态。</p><p>It’s not exactly realistic, that is, phone calls that last short of 15 seconds are not that common. So either
            nothing happens or you have to say a few sentences and so on. Also, back into the days when people used to
            connect to the internet using dial up modems, that assumption was completely destroyed, because people would
            dial up and then keep their phone line busy for a few hours, if the phone call was a free one.</p>
        <p>所以在那些时候，电话通话时长的指数假设完全被推翻了。但撇开这个细节不谈，这是一个合理的假设，可以开始解决这个问题。</p><p>So at those times the exponential assumption for the phone call duration was completely destroyed. But
            leaving
            that detail aside, it’s sort of a reasonable assumption to just get started with this problem.</p>
        <h2 id="markov-process-model">马尔可夫过程模型</h2><h2>Markov Process Model</h2>
        <p>好的，既然我们有了这些假设，让我们试着建立模型。我们将建立一个马尔可夫过程模型。</p><p>All right, so now that we have those assumptions, let’s try to come up with the model. And we’re going to set
            up
            a Markov process model.</p>
        <p>现在泊松过程在连续时间内运行，而作为指数随机变量的通话时长也是连续随机变量，因此我们似乎处于连续时间宇宙中。但我们只针对离散时间情况开始了马尔可夫链。我们要做什么？我们可以开发连续时间马尔可夫链理论，这是可能的。但我们不会在这堂课中这样做。</p><p>Now the Poisson process runs in continuous time, and call durations being exponential random variables also
            are
            continuous random variables, so it seems that we are in a continuous time universe. But we have only started
            Markov chains for the discrete time case. What are we going to do? We can either develop the theory of
            continuous time Markov chains, which is possible. But we are not going to do that in this class.</p>
        <p>或者我们可以将时间离散化，并使用离散时间模型。因此，我们将以熟悉的方式将时间离散化，就像我们开始泊松过程时所做的那样。我们将时间轴分成小的离散小槽，每个小槽都有一个持续时间增量。因此，这个增量应该是一个非常小的数字。那么系统的状态是什么？</p><p>Or we can discretize time and work with a discrete time model. So we’re going to discretize time in the
            familiar
            way, the way we did it when we started the Poisson process. We’re going to take the time axis and split it
            into
            little discrete mini slots, where every mini slot has a duration delta. So this delta is supposed to be a
            very
            small number. So what is the state of the system?</p>
        <p>所以，当你查看某个特定时间的系统情况时，我问你现在发生了什么，你会告诉我什么信息？嗯，你会告诉我，现在在这些大写字母 B 的线路中，有 10 条线路处于繁忙状态，或者有 12 条线路处于繁忙状态。这描述了系统的状态，告诉我此时正在发生什么。</p><p>So, you look at the situation in the system at some particular time and I ask you what is going on right now,
            what’s the information you would tell me? Well, you would tell me that right now out of these capital B
            lines,
            10 of them are busy, or 12 of them are busy. That describes the state of the system, that tells me what’s
            happening at this point.</p>
        <p>因此，我们通过从 0 到 B 的数字来设置状态基础。0 对应于所有电话线路都空闲、无人通话的状态。大写 B 对应于所有电话线路都忙的情况。然后，您得到了介于两者之间的状态。现在让我们看看转换概率。假设现在我们有 i 1 条线路忙。或者，让我看看这里。</p><p>So we set up our states base by being the numbers from 0 to B. 0 corresponds to a state in which all the
            phone
            lines are free, no one is talking. Capital B corresponds to a case where all the phone lines are busy. And
            then
            you’ve got states in between. And now let’s look at the transition probabilities. Suppose that right so now
            we
            have i 1 lines that are busy. Or maybe, let me look here.</p>
        <p>假设有 I 条线路处于忙状态。下次会发生什么？可能发生的情况是，新电话被拨打，在这种情况下我的状态上升 1，或者现有通话终止，在这种情况下我的状态下降 1，或者两者均不发生，在这种情况下我保持在同一状态。</p><p>Suppose that there’s I lines that are busy. What can happen the next time? What can happen is that the new
            phone
            call gets placed, in which case my state moves up by 1, or an existing call terminates, in which case my
            state
            goes down by 1, or none of the two happens, in which case I stay at the same state.</p>
        <p>嗯，也有可能电话挂断后又同时打来一个新电话。但是当你把时间段设得很小的时候，这个概率阶 delta 平方就会很小，所以我们忽略这一点。那么向上转换的概率是多少呢？这就是泊松过程在持续时间为 delta 的迷你时间段内记录到达的概率。</p><p>Well, it’s also possible that the phone call gets terminated and a new phone call gets placed sort of
            simultaneously. But when you take your time slots to be very, very small, this is going to have a negligible
            probability order of delta squared, so we ignore this. So what’s the probability of an upwards transition?
            That’s the probability that the Poisson process records an arrival during a mini slot of duration delta.</p>
        <p>根据泊松过程的定义，发生这种情况的概率就是 lambda delta。因此，这些向上转换中的每一个都具有相同的 lambda delta 概率。因此，在这张图中，到处都有 lambda delta。</p><p>By the definition of the Poisson process, the probability of this happening is just lambda delta. So each one
            of
            these upwards transitions has the same probability of lambda delta. So you’ve got lambda deltas everywhere
            in
            this diagram.</p>
        <h2 id="phone-call-terminations">终止通话</h2><h2>Phone Call Terminations</h2>
        <p>那么，电话呼叫终止情况如何？如果您有一个处于活动状态的呼叫，那么如果您在这里，电话呼叫终止的概率是多少？因此，电话呼叫的持续时间呈指数增长，参数为 mu。
        </p><p>How about, now, phone call terminations? If you had the single call that was active, so if you were here,
            what’s
            the probability that the phone call terminates? So the phone call has an exponential duration with parameter
            mu.
        </p>
        <p>我们之前讨论过，指数随机变量可以被认为是泊松过程中的首次到达时间。因此，在 delta 时间间隔内发生此事件的概率只是 mu 乘以 delta。因此，如果您现在正在拨打一个电话，则该通话终止的概率为 mu 乘以 delta。但假设我们有 1 个当前处于活动状态的电话。</p><p>And we discussed before that an exponential random variable can be thought of as the first arrival time in a
            Poisson process. So the probability that you get this event to happen over a delta time interval is just mu
            times delta. So if you have a single phone call that’s happening right now, with probability mu times delta,
            that call is going to terminate. But suppose that we have I phone calls that are currently active.</p>
        <p>每一个都有终止的概率，但总的来说，其中一个终止的概率是 I 乘以 mu delta。这是因为你从每个不同的电话中得到了 mu delta 贡献，即终止的概率。好的，现在这是一个近似计算，因为它忽略了两个电话同时终止的可能性。</p><p>Each one of them has a probability of mu delta, of terminating, but collectively the probability that one of
            them
            terminates becomes I times mu delta. So that’s because you get the mu delta contribution the probability of
            termination from each one of the different phone calls. OK, now this is an approximate calculation, because
            it
            ignores the possibility that two phone calls terminate at the same time.</p>
        <p>再次，思考为什么这是正确的速率，当您同时拨打 1 个电话并等待其中一个终止时，这就像让 1 个并行运行的分离泊松过程，并且您要求其中一个过程记录事件的概率。</p><p>Again, the way to think of why this is the correct rate, when you have I phone calls that are simultaneously
            running and waiting for one of them to terminate, this is like having I separate Poisson processes that are
            running in parallel, and you ask for the probability that one of those processes records an event.</p>
        <p>现在，当你把所有这些过程放在一起时，它就像一个泊松过程，总速率为 I 乘以 mu，因此 I 乘以 mu delta 是当时发生电话终止事件的总体概率。所以无论如何，这是向下转换的转换概率。现在我们得到了这个，我们可以分析这个链。</p><p>Now when you put all those process together, it’s like having a Poisson process with total rate I times mu,
            and
            so I times mu delta is the overall probability that something happens in terms of phone call terminations at
            those times. So in any case, this is the transition probability for downwards transitions. Now that we’ve
            got
            this, we can analyze this chain.</p>
        <p>这条链具有我们在上节课快结束时讨论过的生死形式。对于生死链，很容易将其写出来以找到稳定状态概率。我们不是将平衡方程写成一般形式，而是通过查看此图中特定切口处发生的情况，从概率守恒或转换的角度来思考。</p><p>This chain has the birth death form that we discussed towards the end of last lecture. And for birth death
            chains, it’s easy to write it out to find the steady state probabilities. Instead of writing down the
            balance
            equations in the general form, we think in terms of a conservation of probabilities or of transitions by
            looking
            at what happens across a particular cut in this diagram.</p>
        <p>从这里到这里的链中转换的数量必须大约等于从这里到那里的转换的数量，因为任何出现的东西都必须先下降，然后上升，依此类推。因此，观察到这种转换的频率必须与这种转换的频率相同。这种转换发生的频率是多少？</p><p>Number of transitions in the chain that cross from here to here has to be approximately equal to the number
            of
            transitions from here to there because whatever comes up must come down and then come up and so on. So the
            frequency with which transitions of this kind are observed has to be the same as the frequency of
            transitions of
            this kind. What’s the frequency of how often the transitions of this kind happen?</p>
        <p>我说的频率是指相当一部分迷你时隙涉及这种转换？好吧，要发生这种转换，我们需要处于状态 i 1，这种情况发生的频率是这个样子。然后是这种转换的概率 lambda delta。因此，观察到这种转换的转换频率是 lambda delta 乘以 pi(i 1)。</p><p>And by frequency I mean quite percentage of the mini slots involve a transition of this kind? Well, for a
            transition of that kind to happen we need to be at states i 1, which happens this much of the time. And then
            the
            probability lambda delta that the transition is of this kind. So the frequency of transitions of with which
            this
            kind of transition is observed is lambda delta times pi(i 1).</p>
        <h2 id="fraction-of-time-steps">时间步长的分数</h2><h2>Fraction of Time Steps</h2>
        <p>这是从特定状态到特定状态的转换所占的时间步长比例。这必须与观察到此类转换的频率相同，并且该频率将是 I mu delta 乘以 pi(i)，然后我们取消 delta，剩下这个等式。因此，这个等式用 pi(i 1) 来表示 pi(i)。</p><p>This is the fraction of time steps at which a transition from specifically this state to specifically that
            state
            are observed. This has to be the same as the frequency with which transitions of that kind are observed, and
            that frequency is going to be I mu delta times pi(i), and then we cancel the deltas, and we are left with
            this
            equation here. So this equation expresses pi(i) in terms of pi(i 1).</p>
        <p>因此，如果我们知道 pi(0)，我们可以使用该方程来确定 pi(1)。一旦我们知道 pi(1)，我们就可以使用该方程来确定 pi(2)，依此类推，继续下去。由此得出的一般公式，我不会做代数运算，这是一个简单的替换，你会发现 pi(i)，即状态 I 的稳态概率由这个表达式给出，它涉及我们开始的 pi(0)。那么 pi(0) 是什么？</p><p>So if we knew pi(0) we can use that equation to determine pi(1). Once we know pi(1), we can use that equation
            to
            determine pi(2), and so on, you keep going. And the general formula that comes out of this, I will not do
            the
            algebra, it’s a straightforward substitution, you find that pi(i), the steady state probability of state I
            is
            given by this expression, which involves the pi(0) from which we started. Now what is pi(0)?</p>
        <p>嗯，我们还不知道，但是我们可以用归一化方程找到它。pi(i) 的总和必须等于 1。所以所有这些数字的总和必须等于 1。而实现这一点的唯一方法是将 pi(0) 设置为等于该特定数字。</p><p>Well, we don’t know yet, but we can find it by using the normalization equation. The sum of pi(i) has to be
            equal
            to 1. So the sum of all of those numbers has to be equal to 1. And the only way that this can happen is by
            setting pi(0) to be equal to that particular number.</p>
        <p>所以如果我告诉你大写字母 B 的值，你就可以建立这个马尔可夫链，你可以计算 pi(0)，然后你可以计算 pi(i)，这样你就知道这个链的稳定状态概率是多少，所以你就可以回答这个问题了。如果我在某个随机时间进入，我找到这里或那里的状态的可能性有多大？</p><p>So if I tell you the value of capital B, you can set up this Markov chain, you can calculate pi(0), and then
            you
            can calculate pi(i), and so you know what fraction, you know the steady state probabilities of this chain,
            so
            you can answer the question. If I drop in at a random time, how likely is it that I’m going to find the
            states
            to be here, or the states to be there?</p>
        <p>因此，稳定状态概率是概率，但我们也将其解释为频率。因此，一旦我找到 pi(i)，它还会告诉我状态等于 i 的时间比例。您可以针对每个可能的 i 回答这个问题。现在，我们为什么要做这个练习？我们感兴趣的是系统繁忙的概率。因此，如果一个人打了一个新的电话，它就会从天而降。</p><p>So the steady state probabilities are probabilities, but we also interpret them as frequencies. So once I
            find
            pi(i), it also tells me what fraction of the time is the state equal to i. And you can answer that question
            for
            every possible i. Now, why did we do this exercise? We’re interested in the probability of the system is
            busy.
            So if a person, a new phone call gets placed, it just drops out of the sky.</p>
        <p>根据泊松过程，新来电将发现系统处于随机状态。该随机状态在稳定状态下用概率 pi(i) 描述。您发现系统忙碌的概率就是您进入该状态时恰好是特定数字 B 的概率。因此，我代入 b 是忙碌的概率。</p><p>According to that Poisson process, that new phone call is going to find the system at a random state. That
            random
            state is described in steady state by the probabilities pi(i)’s. And the probability that you find the
            system to
            be busy is the probability that when you drop in the state happens to be that particular number B. So I sub
            b is
            the probability of being busy.</p>
        <p>这是在精心设计的系统中您希望它很小的概率。因此，您会问，给定我的 lambda 和 mu，我的设计问题是确定大写 B 的电话线数量，以使这个数字很小，我们能做到吗？我们能通过进行简单的计算找出 B 的合适值吗？</p><p>And this is the probability that you would like to be small in a well engineered system. So you ask the
            question,
            how should, given my lambda and mu, my design question is to determine capital B the number of phone lines
            so
            that this number is small. Could we have done, could we figure out a good value for B by doing a back of the
            envelope calculation?</p>
        <p>假设 lambda 为 30，mu 为 1/3。所以我想，让我们把这些速率设为每分钟的呼叫次数。这个 mu 再次表示为每分钟的速率。再次重申，mu 的单位是每分钟的呼叫次数。因此，由于我们的时间单位是分钟，因此通话的平均时长为 1/mu 分钟。因此，典型的通话或平均通话持续时间为 3 分钟。</p><p>Let’s suppose that lambda is 30 and that mu is 1/3. So I guess that’s, let us these rates to be calls per
            minute.
            And this mu, again, is a rate per minute. Again, the units of mu are going to be calls per minute. So since
            our
            time unit is minutes, the mean duration of calls is 1/mu minutes. So a typical call, or on the average a
            call
            lasts for 3 minutes.</p>
        <p>因此，每分钟会有 30 个电话。平均每个电话持续 3 分钟。因此，平均而言，如果 B 是无限的，则每个电话都会接通。平均有多少个电话处于活动状态？因此，每分钟会有 30 个电话。如果一个电话持续恰好 1 分钟，那么在任何时候，您都会有 30 个电话处于活动状态。现在，一个电话平均持续 3 分钟。</p><p>So you get 30 calls per minute. Each call lasts for 3 minutes on the average. So on the average, if B was
            infinite, every call goes through. How many calls would be active on the average? So you get 30 per minute.
            If a
            call lasted exactly 1 minute, then at any time you would have 30 calls being active. Now a call lasts on the
            average for 3 minutes.</p>
        <p>因此，每分钟您都会产生 90 分钟的通话时间。因此，从平均值的角度考虑，您会预期在任何时候都会有大约 90 个通话处于活动状态。</p><p>So during each minute you generate 90 minutes of talking time. So by thinking in terms of averages you would
            expect that at any time there would be about 90 calls that are active.</p>
        <p>如果平均有 90 个电话处于活动状态，您可能会说，好吧，我将把大写字母 B 设置为 90。但这不是很好，因为如果平均要拨打的电话数量是 90，那么有时候会是 85，有时候会是 95。为确保电话能够接通，您可能希望将大写字母 B 设置为比 90 稍大的数字。比 90 大多少？</p><p>And if 90 calls are active on the average, you could say OK, I’m going to set up my capital B to be 90. But
            that’s not very good, because if the average number of phone calls that want to happen is if the average
            number
            is 90, sometimes you’re going to have 85, sometimes you will have 95. And to be sure that the phone calls
            will
            go through you probably want to choose your capital B to be a number a little larger than 90. How much
            larger
            than 90?</p>
        <p>嗯，这个问题可以用数字来回答。所以你要按照下面的步骤来做。我尝试了不同的大写 B 值。对于任何给定的大写 B 值，我都会进行数值计算，找出系统繁忙的概率，然后我会问，B 的值是多少，使得繁忙的概率大约为 1%。</p><p>Well, this is a question that you can answer numerically. So you go through the following procedure. I tried
            different values of capital B. For any given value of capital B, I do this numerical calculation, I find the
            probability that the system is busy, and then I ask what’s the value of B that makes my probability of being
            busy to be, let’s say, roughly 1%.</p>
        <p>如果您使用他们提供的参数进行计算，您会发现 B 大约为 106。因此，根据他们提供的参数，平均有 90 个电话处于活动状态，您实际上需要一些余量来防止波动，如果突然有更多的人想要通话，并且您想很好地保证来电者遇到繁忙系统的概率非常小，那么您将需要大约 106 条电话线。</p><p>And if you do that calculation with the parameters that they gave you, you find that B would be something
            like
            106. So with the parameters they gave where you have, on the average, 90 phone calls being active, you
            actually
            need some margin to protect against the fluctuation, if suddenly by chance more people want to talk, and if
            you
            want to have a good guarantee that an incoming person will have a very small probability of finding a busy
            system, then you will need about 106 phone lines.</p>
        <p>这就是 Erlang 很久以前进行的计算和争论。有趣的是，Erlang 在发明马尔可夫链之前就进行了这项计算。因此，马尔可夫的工作以及对马尔可夫链的研究开始于 Erlang 发明之后的 10 到 15 年。因此，显然他没有将其称为马尔可夫链。但这是他可以从第一原理进行研究的东西。所以这是一件非常有用的事情。
        </p><p>So that’s the calculation and the argument that the Erlang went through a long time ago. It’s actually
            interesting that Erlang did this calculation before Markov chains were invented. So Markov’s work, and the
            beginning of work on Markov chains, happens about 10 15 years after Erlang. So obviously he didn’t call that
            a
            Markov chain. But it was something that he could study from first principles. So this is a pretty useful
            thing.
        </p>
        <p>至少在过去，从该模型得出的这些概率都会被很好地列在手册中，每个像样的电话公司工程师都会随身携带。所以这是最实用的。它是马尔可夫链在现实世界中的标准应用之一。</p><p>These probabilities that come out of that model, at least in the old days, they would all be very well
            tabulated
            in handbooks that every decent phone company engineer would sort of have with them. So this is about as
            practical as it gets. It’s one of the sort of standard real world applications of Markov chains.</p>
        <h2 id="new-skills">新技能</h2><h2>New Skills</h2>
        <p>现在，为了结束我们的主题，我们将考虑一些新技能，看看我们如何计算与马尔可夫链有关的几个额外的有趣数量。所以我们在这里要处理的问题是我在谈论这幅图时暗示的问题。你从一个瞬态开始，最终会到达这里或那里。</p><p>So now to close our subjects, we’re going to consider a couple of new skills and see how we can calculate the
            few
            additional interesting quantities that have to do with the Markov chain. So the problem we’re going to deal
            with
            here is the one I hinted that when I was talking about this picture. You start at a transient state, you’re
            going to eventually end up here or there.</p>
        <p>我们想要找到两种选择之一发生或另一种发生的概率。所以在这幅图中，我们有一类状态是瞬态的。这些状态是瞬态的，因为你将在这些状态之间移动，但你可以进行转换，然后进入一个你之后无法逃脱的状态。你会在这里结束还是在那里结束？你不知道。</p><p>We want to find the probabilities of one option of the two happening or the other happening. So in this
            picture
            we have a class of states that’s are transient. These are transient because you’re going to move around
            those
            states, but there’s a transition that you can make, and you go to a state from which you cannot escape
            afterwards. Are you going to end up here or are you going to end up there? You don’t know.</p>
        <p>这是随机的。让我们试着计算一下你最终处于状态 4 的概率。现在，你最终处于状态 4 的概率取决于你从哪里开始。因为如果你从这里开始，你可能有更多机会到达 4，因为你立即就得到了这个机会，而如果你从这里开始，你更有可能以这种方式逃脱，因为到达那里需要时间。</p><p>It’s random. Let’s try to calculate the probability that you end up at state 4. Now, the probability that you
            end
            up at state 4 will depend on where you start. Because if you start here, you probably have more chances of
            getting to 4 because you get that chance immediately, whereas if you start here there’s more chances that
            you’re
            going to escape that way because it kind of takes you time to get there.</p>
        <p>您更有可能立即退出。因此，退出并最终进入状态 4 的概率将取决于初始状态。这就是为什么当我们谈论这些吸收概率时，我们会包含一个指标 I，它告诉我们初始状态是什么。我们想要找到这个吸收概率，即我们最终进入不同初始状态的概率。现在对于某些初始状态，这个问题很容易回答。</p><p>It’s more likely that you exit right away. So the probability of exiting and ending up at state 4 will depend
            on
            the initial state. That’s why when we talk about these absorption probability we include an index I that
            tells
            us what the initial state is. And we want to find this absorption probability, the probability that we end
            up
            here for the different initial states. Now for some initial states this is very easy to answer.</p>
        <p>如果你从状态 4 开始，那么你最终到达链的这一部分的概率是多少？概率是 1。你肯定会到达那里，那是你开始的地方。如果你从状态 5 开始，那么你最终到达状态 4 的概率是多少？概率是 0，没有办法到达那里。现在，如果你从状态 2 这样的状态开始呢？</p><p>If you start at state 4, what’s the probability that eventually you end up in this part of the chain? It’s 1.
            You’re certain to be there, that’s where you started. If you start at state 5, what’s the probability that
            you
            end up eventually at state 4? It’s probability 0, there’s no way to get there. Now, how about if you start
            at a
            state like state 2?</p>
        <p>如果你从状态 2 开始，那么可能会发生几种不同的事情。要么你立即到达状态 4，这种情况发生的概率为 0.2，要么你到达状态 1，这种情况发生的概率为 0.6。所以如果你到达状态 4，你就完成了。我们就到了。如果你到达状态 1，然后呢？从状态 1 开始有两种可能性。</p><p>If you start at state 2 then there’s a few different things that can happen. Either you end up at state 4
            right
            away and this happens with probability 0.2, or you end up at state 1, and this happens with probability 0.6.
            So
            if you end up at state 4, you are done. We are there. If you end up at state 1, then what? Starting from
            state 1
            there’s two possibilities.</p>
        <p>要么你最终会处于状态 4，要么你最终会处于状态 5。这种情况发生的概率是多少？我们不知道它是多少，但我们将其定义为 a1。这是概率 a1 是假设初始状态为 1，你最终会处于状态 4 的概率。所以这个概率是 a1。所以我们感兴趣的事件可以以两种方式发生。</p><p>Either eventually you’re going to end up at state 4, or eventually you’re going to end up at state 5. What’s
            the
            probability of this happening? We don’t know what it is, but it’s what we defined to be a1. This is the
            probability a1 is the probability that eventually you settle in state 4 given that the initial state was 1.
            So
            this probability is a1. So our event of interest can happen in two ways.</p>
        <p>我要么直接去那里，要么以 0.6 的概率去这里。如果我去那里，最终我会到达状态 4，发生这种情况的概率为 a1。因此，最终到达状态 4 的总概率将是此事件发生的不同方式的概率之和。</p><p>Either I go there directly, or I go here with probability 0.6. And given that I go there, eventually I end up
            at
            state 4, which happens with probability a1. So the total probability of ending up at state 4 is going to be
            the
            sum of the probabilities of the different ways that this event can happen.</p>
        <p>因此，在这种情况下，我们的方程将是，即 a2，将是 0.2（这是直接到达那里的概率），加上概率 0.8，我最终会到达状态 1，然后从状态 1，我将以概率 a1 到达状态 4。因此，这是我们得到的一个特定方程，用于说明如果我们从这个状态开始会发生什么。我们可以从任何其他状态开始进行类似的论证。</p><p>So our equation, in this case, is going to be, that’s a2, is going to be 0.2 (that’s the probability of going
            there directly) plus with probability 0.8 I end up at state 1, and then from state 1 I will end up at state
            4
            with probability a1. So this is one particular equation that we’ve got for what happens if we start from
            this
            state. We can do a similar argument starting from any other state.</p>
        <p>从状态 I 开始，最终到达状态 4 的概率是，我们考虑下一步去哪里的不同可能情况，也就是我的状态 j，概率为 Pij。</p><p>Starting from state I the probability that eventually I end up at state 4 is, we consider the different
            possible
            scenarios of where do I go next, which is my state j, with probability Pij.</p>
        <p>下次我转到 j 时，假设我从 j 开始，这是我最终到达状态 4 的概率。所以我们这里的这个方程只是我们为初始状态为 2 的特定情况写下的符号的抽象版本。所以你为这里的每个状态写下一个这种类型的方程。</p><p>Next time I go to j, and given that I started at j, this is the probability that I end up at state 4. So this
            equation that we have here is just an abstract version in symbols of what we wrote down for the particular
            case
            where the initial state was 2. So you write down an equation of this type for every state inside here.</p>
        <p>您将得到一个单独的方程式，用于计算 a1、a2 和 a3。这​​将是一个包含 3 个未知数的 3 个方程组，即瞬态中的 a。因此，您可以求解这个 3x3 方程组。</p><p>You’ll have a separate equation for a1, a2, and a3. And that’s going to be a system of 3 equations with 3
            unknowns, the a’s inside the transient states. So you can solve that 3 by 3 system of equations.</p>
        <p>幸运的是，它有一个唯一的解决方案，所以一旦你解决了它，你就找到了吸收的概率和最终在状态 4 被吸收的概率。现在，在我们这里的图片中，这是一个单一状态，而那个也是一个单一状态。如果我们的循环或捕获集由多个状态组成，情况会如何变化？好吧，我们有多个状态并不重要。</p><p>Fortunately, it turns out to have a unique solution, and so once you solve it you have found the
            probabilities of
            absorption and the probability that eventually you get absorbed at state 4. Now, in the picture that we had
            here, this was a single state, and that one was a single state. How do things change if our recurrent, or
            trapping sets consist of multiple states? Well, it doesn’t really matter that we have multiple states.</p>
        <p>重要的是，这是一个块状物，一旦我们到达那里，我们就会陷入其中。所以，如果图片是这样的，0.1 和 0.2，这基本上意味着，每当你处于这种状态时，有 0.3 的概率会最终进入那个块状物并被困在里面。</p><p>All that matters is that this is one lump and once we get there we are stuck in there. So if the picture was,
            let’s say, like this, 0.1 and 0.2, that basically means that whenever you are in that state there’s a total
            probability of 0.3 of ending in that lump and getting stuck inside that lump.</p>
        <p>因此，你可以将这张图片进行修改，使其总概率为 0.3，即最终在该块内的某个位置。同样，你将该块视为一个实体，然后从任何状态记录下我在这里最终进入该实体的总概率。</p><p>So you would take that picture and change it and make it instead a total probability of 0.3, of ending
            somewhere
            inside that lump. And similarly, you take this lump and you view it as just one entity, and from any state
            you
            record the total probability that given that I’m here I end up in that entity.</p>
        <p>所以基本上，如果你唯一关心的是你最终会陷入这个困境的概率，你可以用一个单一的状态来代替这个困境，把它看作一个单一的状态，然后用这个公式来计算概率。好的，现在我们知道了这条链会到达哪里。至少我们在概率上知道。
        </p><p>So basically, if the only thing you care is the probability that you’re going to end up in this lump, you can
            replace that lump with a single state, view it as a single state, and calculate probabilities using this
            formula. All right, so now we know where the chain is going to get to. At least we know probabilistically.
        </p>
        <p>我们知道它到达这里的概率是多少，这也告诉我们它最终到达那里的概率是多少。另一个问题是，我们要花多长时间才能到达这个状态或那个状态？我们可以将这种情况称为事件吸收，这意味着状态进入了某个循环类别，它无法从中脱离出来。好的。</p><p>We know with what probability it is going to go here, and that also tells us the probability that eventually
            it’s
            going to get there. Other question, how long is it going to take until we get to either this state or that
            state? We can call that event absorption, meaning that the state got somewhere into a recurrent class from
            which
            it could not get out. Okay.</p>
        <p>让我们在只有 1 个吸收状态的情况下处理这个问题。因此，我们的马尔可夫链比上一张幻灯片中的马尔可夫链稍微简单一些。我们有瞬时状态，有循环状态，一旦进入循环状态，您就会停留在那里。因此，我们确信无论从哪里开始，我们都会到达这里。这需要多长时间？</p><p>Let’s deal with that question for the case where we have only 1 absorbing state. So here our Markov chain is
            a
            little simpler than the one in the previous slide. We’ve got our transient states, we’ve got our recurrent
            state, and once you get into the recurrent state you just stay there. So here we’re certain that no matter
            where
            we start we’re going to end up here. How long is it going to take?</p>
        <p>嗯，我们不知道。这是一个随机变量。这个随机变量的期望值，我们称之为 mu。但到达那里需要多长时间肯定取决于我们从哪里开始。所以让我们再次在我们的符号中放入这个索引 I，它表示我们从哪里开始。现在参数将与我们之前使用的参数类型相同。</p><p>Well, we don’t know. It’s a random variable. The expected value of that random variable, let’s call it mu.
            But
            how long it takes to get there certainly depends on where we start. So let’s put in our notation again this
            index I that indicates where we started from. And now the argument is going to be of the same type as the
            one we
            used before.</p>
        <p>我们可以再次以树的形式思考，考虑所有可能的选择。假设你从状态 1 开始。从状态 1 开始，预计到你最终放弃状态的时间是 mu1。现在，从状态 1 开始，有哪些可能性？</p><p>We can think in terms of a tree once more, that considers all the possible options. So suppose that you start
            at
            state 1. Starting from state 1, the expected time until you end up dropping states is mu1. Now, starting
            from
            state 1, what are the possibilities?</p>
        <p>你进行第一次转换，第一次转换会带你进入状态 2 或状态 3。进入状态 2 的概率为 0.6，进入状态 3 的概率为 0.4。从状态 2 开始，最终你会进入状态 4。这需要多长时间？我们不知道，这是一个随机变量。
        </p><p>You make your first transition, and that first transition is going to take you either to state 2 or to state
            3.
            It takes you to state 2 with probability 0.6, it takes you to state 3 with probability 0.4. Starting from
            state
            2, eventually you’re going to get to state 4. How long does it take? We don’t know, it’s a random variable.
        </p>
        <p>但发生这种情况的预期时间是 mu2。从状态 2 开始，需要多长时间才能到达状态 4。同样，从状态 3 开始，您需要按照平均 mu3 时间步长才能到达状态 4。那么我到达状态 4 所需的时间预期值是多少？</p><p>But the expected time until this happens is mu2. Starting from state 2, how long does it take you to get to
            state
            4. And similarly starting from state 3, it’s going to take you on the average mu3 time steps until you get
            to
            state 4. So what’s the expected value of the time until I end at state 4?</p>
        <p>因此，以 0.6 的概率，我将最终处于状态 2，从那里开始，预期时间为 mu2，以 0.4 的概率，我将最终处于状态 3，从那里开始，我将花费很多时间。所以这是第一次转换后预期需要的时间。但我们也为第一次转换花费了 1 个时间步。</p><p>So with probability 0.6, I’m going to end up at state 2 and from there on it’s going to be expected time mu2,
            and
            with probability 0.4 I’m going to end up at state 3, and from there it’s going to take me so much time. So
            this
            is the expected time it’s going to take me after the first transition. But we also spent 1 time step for the
            first transition.</p>
        <p>到达那里的总时间是第一次转换的时间，即 1，加上从下一个状态开始的预期时间。这里的表达式是从下一个状态开始的预期时间，但我们还需要考虑第一次转换，所以我们加 1。这就是我们的 mu1。所以我们再次有一个将不同的 mu 联系在一起的线性方程。</p><p>The total time to get there is the time of the first transition, which is 1, plus the expected time starting
            from
            the next state. This expression here is the expected time starting from the next state, but we also need to
            account for the first transition, so we add 1. And this is going to be our mu1. So once more we have a
            linear
            equation that ties together the different mu’s.</p>
        <p>在这种情况下，从状态 4 开始的方程当然会很简单，从该状态开始，第一次到达该状态所需的预期步数当然是 0，因为你已经到达该状态了。所以对于该状态，这没问题，对于所有其他状态，你会得到这种形式的方程。现在我们将为每个状态建立一个方程。</p><p>And the equation starting from state 4 in this case, of course is going to be simple, starting from that
            state
            the expected number of steps it takes you to get there for the first time is of course, 0 because you’re
            already
            there. So for that state this is fine, and for all the other states you get an equation of this form. Now
            we’re
            going to have an equation for every state.</p>
        <p>这是一个线性方程组，我们可以再次求解它们，这给出了我们的链条在这种吸收状态下被吸收的预期时间。很高兴知道这个方程组总是有一个唯一的解。好的，所以这是预期的吸收时间。对于这个场景的吸收状态，我们的情况是这样的。</p><p>It’s a system of linear equations, once more we can solve them, and this gives us the expected times until
            our
            chain gets absorbed in this absorbing state. And it’s nice to know that this system of equations always has
            a
            unique solution. OK so this was the expected time to absorption. For this case where we had this scene
            absorbing
            state.</p>
        <p>假设我们有瞬时状态，并且有多个循环类或多个吸收状态。假设你有这样的图。我们想计算到达这里或那里的预期时间。到达吸收状态的预期时间。有什么诀窍吗？</p><p>Suppose that we have our transient states and that we have multiple recurrent classes, or multiple absorbing
            states. Suppose you’ve got the picture like this. And we want to calculate the expected time until we get
            here
            or there. Expected time until we get to an absorbing state. What’s the trick?</p>
        <p>好吧，我们可以把这两种状态放在一起，把它们看作一种糟糕的状态，我们感兴趣的是到达那里需要多长时间。所以把它们作为一个状态，并相应地合并所有这些概率。所以从这里开始，我下一次进入这个状态并被吸收的概率将是这个概率加上那个概率。</p><p>Well, we can lump both of these states together and think of them as just one bad state, one place for which
            we’re interested in how long it takes us to get there. So lump them as one state, and accordingly kind of
            merge
            all of those probabilities. So starting from here, my probability that the next I end up in this lump and
            they
            get absorbed is going to be this probability plus that probability.</p>
        <p>所以我们要改变这种状况。把它想象成一个大状态。然后把这两个概率加在一起，得到一个单一的概率，也就是下次我发现自己处于某个吸收状态时从这里开始的概率。所以一旦你知道如何处理这种情况，你也可以找到有多个吸收状态的情况下的预期吸收时间。</p><p>So we would change that picture. Think of this as being just one big state. And sort of add those two
            probabilities together to come up with a single probability, which is the probability that starting from
            here
            next time I find myself at some absorbing state. So once you know how to deal with a situation like this,
            you
            can also find expected times to absorption for the case where you’ve got multiple absorbing states.</p>
        <p>您只需将所有这些多个吸收状态集中为一个状态即可。</p><p>You just lump all of those multiple absorbing states into a single one.</p>
        <h2 id="related-questions">相关问题</h2><h2>Related Questions</h2>
        <p>最后，还有一种相关的量值得关注。这个问题与上一张幻灯片中的问题几乎相同，只是这里我们没有任何吸收状态。相反，我们有一个单一的循环状态类。你从某个状态 i 开始。你有一个特殊状态，即状态 s。</p><p>Finally, there’s a kind of related quantity that’s of interest. The question is almost the same as in the
            previous slide, except that here we do not have any absorbing states. Rather, we have a single recurrent
            class
            of states. You start at some state i. You have a special state, that is state s.</p>
        <p>你会问，我需要多长时间才能第一次到达 s？这是一类单一的循环状态。所以你知道状态在这里不断循环，并不断访问所有可能的状态。所以最终会访问这个状态。这需要多长时间？好的。</p><p>And you ask the question, how long is it going to take me until I get to s for the first time? It’s a single
            recurrent class of states. So you know that the state keeps circulating here and it keeps visiting all of
            the
            possible states. So eventually this state will be visited. How long does it take for this to happen? Ok.</p>
        <p>因此，我们感兴趣的是，这个过程需要多长时间，需要多长时间才能第一次到达 s。我们并不关心之后会发生什么。所以我们不妨改变这幅图，从 s 中删除转换，让它们自行转换。答案会改变吗？不会。我们唯一改变的是到达 s 之后发生的事情。</p><p>So we’re interested in how long it takes for this to happen, how long it takes until we get to s for the
            first
            time. And we don’t care about what happens afterwards. So we might as well change this picture and remove
            the
            transitions out of s and to make them self transitions. Is the answer going to change? No.&nbsp;The only thing
            that
            we changed was what happens after you get to s.</p>
        <p>但到达 s 之后会发生什么并不重要。我们要处理的问题是到达 s 需要多长时间。因此，本质上，在我们完成这种转换之后，问题与之前相同，即最终到达这种状态需要多长时间。现在在这张新图中，这种状态是一种吸收状态。或者你可以从第一原理来思考。</p><p>But what happens after you get to s doesn’t matter. The question we’re dealing with is how long does it take
            us
            to get to s. So essentially, it’s after we do this transformation it’s the same question as before, what’s
            the
            time it takes until eventually we hit this state. And it’s now in this new picture, this state is an
            absorbing
            state. Or you can just think from first principles.</p>
        <p>从状态本身 s 开始，您需要 0 个时间步骤才能到达 s。从其他任何地方开始，您都需要一次转换，然后在第一次转换之后，您会发现自己处于状态 j，概率为 Pij，从那时起，您将花费预期时间 Tj 直到到达该终端状态 s。因此，一旦这些方程式再次具有唯一解，您就可以解决它们并找到答案。</p><p>Starting from the state itself, s, it takes you 0 time steps until you get to s. Starting from anywhere else,
            you
            need one transition and then after the first transition you find yourself at state j with probability Pij
            and
            from then on you are going to take expected time Tj until you get to that terminal state s. So once more
            these
            equations have a unique solution, you can solve them and find the answer.</p>
        <p>最后，还有一个相关问题，即 s 的平均重复时间。在这个问题中，你从 s 开始，链将随机移动，你问下次回到 s 需要多长时间。所以请注意区别。这里我们讨论的是时间 0 之后的第一次，而这里只是任何地方的第一次。</p><p>And finally, there’s a related question, which is the mean recurrence time of s. In that question you start
            at s,
            the chain will move randomly, and you ask how long is it going to take until I come back to s for the next
            time.
            So notice the difference. Here we’re talking the first time after time 0, whereas here it’s just the first
            time
            anywhere.</p>
        <p>因此，如果您从 s 开始，Ts* 就不为 0。您想要进行至少一次转换，这就是回到 s 所需的时间。那么，我需要多长时间才能回到 s？</p><p>So here if you start from s, Ts* is not 0. You want to do at least one transition and that’s how long it’s
            going
            to take me until it gets back to s. Well, how long does it take me until I get back to s?</p>
        <p>我进行第一次转换，然后在第一次转换之后，我计算从下一个状态到回到 s 需要多长时间的预期时间。所以我写下的所有这些方程式看起来都差不多。但它们是不同的。所以你可以记住所有这些方程式，或者更好的方法是只了解基本概念。</p><p>I do my first transition, and then after my first transition I calculate the expected time from the next
            state
            how long it’s going to take me until I come back to s. So all of these equations that I wrote down, they all
            kind of look the same. But they are different. So you can either memorize all of these equations, or instead
            what’s better is to just to get the basic idea.</p>
        <p>也就是说，要计算概率或期望值，你需要使用总概率或总期望定理，并条件化第一个转换，然后从那里开始。所以明天你将在复习中练习一下这些技能，当然这也在你的问题集中。</p><p>That is, to calculate probabilities or expected values you use the total probability or total expectation
            theorem
            and conditional the first transition and take it from there. So you’re going to get a little bit of practice
            with these skills in recitation tomorrow, and of course it’s in your problem set as well.</p>
        <h1 id="weak-law-of-large-numbers">19.弱大数定律</h1><h1>19. Weak Law of Large Numbers</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 50:13 <a href="https://youtube.com/watch?v=3eiio3Tw7UQ">https://youtube.com/watch?v=3eiio3Tw7UQ</a></p><p> 12 years ago (Nov 10, 2012) — 50:13 <a href="https://youtube.com/watch?v=3eiio3Tw7UQ">https://youtube.com/watch?v=3eiio3Tw7UQ</a></p>
        <h2 id="unknown-341">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。约翰·齐西克利斯：我们今天要开始一个新单元。所以我们将讨论极限定理。为了介绍这个主题，让我们考虑以下情况。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. JOHN TSITSIKLIS: We’re going to start
            today a new unit. so we will be talking about limit theorems. So just to introduce the topic, let’s think of
            the
            following situation.</p>
        <p>南极有一群企鹅。如果你随机挑选一只企鹅并测量它们的身高，它们的身高期望值将是企鹅群中不同企鹅身高的平均值。所以假设当你挑选一只企鹅时，每只企鹅的概率都是相同的。那么期望值就是那里所有企鹅的平均值。</p><p>There’s a population of penguins down at the South Pole. And if you were to pick a penguin at random and
            measure
            their height, the expected value of their height would be the average of the heights of the different
            penguins
            in the population. So suppose when you pick one, every penguin is equally likely. Then the expected value is
            just the average of all the penguins out there.</p>
        <p>所以你的老板要求你找出期望值是多少。一种方法是去测量每一只企鹅。这可能有点耗时。所以，你也可以随机挑选几只企鹅，比如说 n 只。然后你测量每只企鹅的身高。
        </p><p>So your boss asks you to find out what that the expected value is. One way would be to go and measure each
            and
            every penguin. That might be a little time consuming. So alternatively, what you can do is to go and pick
            penguins at random, pick a few of them, let’s say a number n of them. So you measure the height of each one.
        </p>
        <h2 id="unknown-342">未知</h2><h2>Unknown</h2>
        <p>然后计算出你收集到的企鹅的平均身高。这就是你对期望值的估计。现在，我们称之为样本均值，也就是你所收集的样本内的平均值。这和期望值感觉上有点相似，期望值也是均值。但期望值是一种不同的均值。</p><p>And then you calculate the average of the heights of those penguins that you have collected. So this is your
            estimate of the expected value. Now, we called this the sample mean, which is the mean value, but within the
            sample that you have collected. This is something that’s sort of feels the same as the expected value, which
            is
            again, the mean. But the expected value’s a different kind of mean.</p>
        <p>期望值是整个总体的平均值，而样本平均值是你所测量的较小样本的平均值。期望值是一个数字。样本平均值是一个随机变量。它是一个随机变量，因为你收集的样本是随机的。现在，我们认为这是估计期望的合理方法。</p><p>The expected value is the mean over the entire population, whereas the sample mean is the average over the
            smaller sample that you have measured. The expected value is a number. The sample mean is a random variable.
            It’s a random variable because the sample you have collected is random. Now, we think that this is a
            reasonable
            way of estimating the expectation.</p>
        <p>因此，在极限情况下，当 n 趋于无穷大时，样本均值（即我们构建的估计值）可能会以某种方式接近预期值。这是什么意思？接近意味着什么？在什么意义上？这个陈述是正确的吗？这是我们在处理极限定理时要处理的那种陈述。</p><p>So in the limit as n goes to infinity, it’s plausible that the sample mean, the estimate that we are
            constructing, should somehow get close to the expected value. What does this mean? What does it mean to get
            close? In what sense? And is this statement true? This is the kind of statement that we deal with when
            dealing
            with limit theorems.</p>
        <h2 id="unknown-343">未知</h2><h2>Unknown</h2>
        <p>这就是极限定理的主题，如果你要处理大量的随机变量，并且可能要取平均值等等，会发生什么。那么我们为什么要为此烦恼呢？好吧，如果你从事抽样业务，那么知道这种估计预期值的特定方法实际上可以让你接近真实答案会令人放心。还有一个更高层次的原因，它更抽象和数学化。
        </p><p>That’s the subject of limit theorems, when what happens if you’re dealing with lots and lots of random
            variables,
            and perhaps take averages and so on. So why do we bother about this? Well, if you’re in the sampling
            business,
            it would be reassuring to know that this particular way of estimating the expected value actually gets you
            close
            to the true answer. There’s also a higher level reason, which is a little more abstract and mathematical.
        </p>
        <p>因此，如果你手头有一两个随机变量，概率问题就很容易处理。你可以写下它们的质量函数、关节密度函数等等。你可以在纸上或电脑上计算，得到答案。如果你要处理 100 个随机变量，并且试图得到任何事物的精确答案，那么概率问题在计算上就变得难以处理。</p><p>So probability problems are easy to deal with if you’re having in your hands one or two random variables. You
            can
            write down their mass functions, joints density functions, and so on. You can calculate on paper or on a
            computer, you can get the answers. Probability problems become computationally intractable if you’re
            dealing,
            let’s say, with 100 random variables and you’re trying to get the exact answers for anything.</p>
        <p>因此，原则上，我们拥有的相同公式仍然适用。但它们涉及对大量指标组合的求和。这让生活变得极其困难。但当你突破极限，面对大量变量的情况时，你就可以开始取极限。当你取极限时，奇妙的事情就会发生。</p><p>So in principle, the same formulas that we have, they still apply. But they involve summations over large
            ranges
            of combinations of indices. And that makes life extremely difficult. But when you push the envelope and you
            go
            to a situation where you’re dealing with a very, very large number of variables, then you can start taking
            limits. And when you take limits, wonderful things happen.</p>
        <h2 id="unknown-344">未知</h2><h2>Unknown</h2>
        <p>许多公式开始变得简单，通过考虑这些极限，你实际上可以得到有用的答案。这就是为什么研究极限定理是有用的重要原因。所以今天我们要做的，首先我们要从一个有用的、简单的工具开始，这个工具允许我们将概率与预期值联系起来。马尔可夫不等式是我们要写下的第一个不等式。</p><p>Many formulas start simplifying, and you can actually get useful answers by considering those limits. And
            that’s
            sort of the big reason why looking at limit theorems is a useful thing to do. So what we’re going to do
            today,
            first we’re going to start with a useful, simple tool that allows us to relates probabilities with expected
            values. The Markov inequality is the first inequality we’re going to write down.</p>
        <p>然后利用它，我们将得到切比雪夫不等式，一个相关的不等式。然后我们需要定义当我们谈论随机变量时我们所说的收敛是什么意思。这是一个概念，是对通常的数字序列极限收敛概念的概括。</p><p>And then using that, we’re going to get the Chebyshev’s inequality, a related inequality. Then we need to
            define
            what do we mean by convergence when we talk about random variables. It’s a notion that’s a generalization of
            the
            notion of the usual convergence of limits of a sequence of numbers.</p>
        <p>一旦我们掌握了收敛的概念，我们就会发现，样本均值确实会收敛到真实均值，收敛到 X 的期望值。这个命题被称为弱大数定律。之所以称之为弱大数定律，是因为还有一个强大数定律，强大数定律与样本均值收敛，只是数学内容略有不同。</p><p>And once we have our notion of convergence, we’re going to see that, indeed, the sample mean converges to the
            true mean, converges to the expected value of the X’s. And this statement is called the weak law of large
            numbers. The reason it’s called the weak law is because there’s also a strong law, which is a statement with
            the
            same flavor, but with a somewhat different mathematical content.</p>
        <h2 id="unknown-345">未知</h2><h2>Unknown</h2>
        <p>但这有点抽象，我们不会深入讨论。所以弱定律就是你要了解的全部内容。好的。现在我们开始离题。我们的第一个工具是所谓的马尔可夫不等式。让我们取一个始终为非负的随机变量。无论如何，它都不会得到负值。为了简单起见，我们假设它是一个离散随机变量。</p><p>But it’s a little more abstract, and we will not be getting into this. So the weak law is all that you’re
            going
            to get. All right. So now we start our digression. And our first tool will be the so called Markov
            inequality.
            So let’s take a random variable that’s always non negative. No matter what, it gets no negative values. To
            keep
            things simple, let’s assume it’s a discrete random variable.</p>
        <p>因此，期望值是随机变量可能取的所有值的总和。随机变量可能取的值根据其相应的概率加权。现在，这是所有 x 的总和。但 x 取非负值。PMF 也是非负的。因此，如果我对较少的事物求和，我将得到较小的值。</p><p>So the expected value is the sum over all possible values that a random variable can take. The values of the
            random variables that can take weighted according to their corresponding probabilities. Now, this is a sum
            over
            all x’s. But x takes non negative values. And the PMF is also non negative. So if I take a sum over fewer
            things, I’m going to get a smaller value.</p>
        <p>因此，当我把所有项相加时，所得的和小于或等于我只将大于某个常数的项相加时所得的和。现在，如果我将大于 a 的 x 相加，则出现的 x 总是大于或等于 a。所以我们得到了这个不等式。现在，a 是一个常数。我可以把它拉出求和。</p><p>So the sum when I add over everything is less than or equal to the sum that I will get if I only add those
            terms
            that are bigger than a certain constant. Now, if I’m adding over x’s that are bigger than a, the x that
            shows up
            there will always be larger than or equal to a. So we get this inequality. And now, a is a constant. I can
            pull
            it outside the summation.</p>
        <h2 id="unknown-346">未知</h2><h2>Unknown</h2>
        <p>然后我剩下所有大于 a 的 x 的概率。这只是大于 a 的概率。好的，这就是马尔可夫不等式。基本上告诉我们预期值大于或等于这个数字。它将预期值与概率联系起来。它告诉我们，如果预期值很小，那么 x 大的概率也会很小。</p><p>And then I’m left with the probabilities of all the x’s that are bigger than a. And that’s just the
            probability
            of being bigger than a. OK, so that’s the Markov inequality. Basically tells us that the expected value is
            larger than or equal to this number. It relates expected values to probabilities. It tells us that if the
            expected value is small, then the probability that x is big is also going to be small.</p>
        <p>因此，它将关于期望值小的陈述转换为关于概率小的陈述。好的。我们实际上需要的是同一陈述的一个略有不同的版本。我们要做的是将这个不等式应用于特殊类型的非负随机变量。您可以考虑将相同的计算应用于这种形式的随机变量，（X 减去 mu）平方，其中 mu 是 X 的期望值。</p><p>So it’s translates a statement about smallness of expected values to a statement about smallness of
            probabilities. OK. What we actually need is a somewhat different version of this same statement. And what
            we’re
            going to do is to apply this inequality to a non negative random variable of a special type. And you can
            think
            of applying this same calculation to a random variable of this form, (X minus mu) squared, where mu is the
            expected value of X.</p>
        <p>现在，这是一个非负随机变量。因此，按照我们之前推导的思路，这个随机变量的期望值，也就是方差，大于这个随机变量大于某个值的概率。让我用 a 的平方而不是 a 乘以 a 的平方。</p><p>Now, this is a non negative random variable. So, the expected value of this random variable, which is the
            variance, by following the same thinking as we had in that derivation up to there, is bigger than the
            probability that this random variable is bigger than some. let me use a squared instead of an a times the
            value
            a squared.</p>
        <h2 id="unknown-347">未知</h2><h2>Unknown</h2>
        <p>所以现在，这个概率当然等于 X 减 mu 的绝对值大于 a 乘以 a 的平方的概率。这一边等于 X 的方差。所以这将 X 的方差与我们的随机变量远离其均值的概率联系起来。如果方差小，则意味着远离均值的概率也很小。</p><p>So now of course, this probability is the same as the probability that the absolute value of X minus mu is
            bigger
            than a times a squared. And this side is equal to the variance of X. So this relates the variance of X to
            the
            probability that our random variable is far away from its mean. If the variance is small, then it means that
            the
            probability of being far away from the mean is also small.</p>
        <p>所以我通过将马尔可夫不等式应用于这个特定的非负随机变量来推导这个公式。或者只是为了强调这个信息，并增加你对这个不等式的信心，让我们再看一遍推导过程，在这里，我将从第一原理开始，但使用与此处证明中相同的想法。好的。为了多样化，现在让我们将 X 视为连续随机变量。</p><p>So I derived this by applying the Markov inequality to this particular non negative random variable. Or just
            to
            reinforce, perhaps, the message, and increase your confidence in this inequality, let’s just look at the
            derivation once more, where I’m going, here, to start from first principles, but use the same idea as the
            one
            that was used in the proof out here. Ok. So just for variety, now let’s think of X as being a continuous
            random
            variable.</p>
        <p>无论是离散的还是连续的，导出都是相同的。因此，根据定义，方差就是积分，是这个特定的积分。现在，如果我积分，而不是在整个范围内积分，我只对远离平均值的 x 进行积分，积分就会变得更小。所以 mu 是平均值。把 c 想象成一个大数字。</p><p>The derivation is the same whether it’s discrete or continuous. So by definition, the variance is the
            integral,
            is this particular integral. Now, the integral is going to become smaller if I integrate, instead of
            integrating
            over the full range, I only integrate over x’s that are far away from the mean. So mu is the mean. Think of
            c as
            some big number.</p>
        <h2 id="unknown-348">未知</h2><h2>Unknown</h2>
        <p>这些是远离平均值左侧的 x，从负无穷到 mu 减 c。这些是远离平均值正侧的 x。因此，通过对较少的东西进行积分，我得到的积分较小。现在，对于这个范围内的任何 x，这个距离 x 减 mu 至少为 c。因此，这个平方至少为 c 平方。</p><p>These are x’s that are far away from the mean to the left, from minus infinity to mu minus c.&nbsp;And these are
            the
            x’s that are far away from the mean on the positive side. So by integrating over fewer stuff, I’m getting a
            smaller integral. Now, for any x in this range, this distance, x minus mu, is at least c.&nbsp;So that squared is
            at
            least c squared.</p>
        <p>所以这个积分范围内的项至少是 c 平方。所以我可以把它取出来放在积分之外。剩下的就是密度的积分。另一边也是一样。那么，取出的因素就是这个项 c 平方。</p><p>So this term over this range of integration is at least c squared. So I can take it outside the integral. And
            I’m
            left just with the integral of the density. Same thing on the other side. And so what factors out is this
            term c
            squared.</p>
        <p>在里面，我们剩下在 mu 减 c 左边的概率，然后是在 mu 加 c 右边的概率，这与与平均值的距离的绝对值大于或等于 c 的概率相同。所以这就是我们在那里证明的不等式，只是在这里我使用了 c。在那里我使用了 a，但它是完全相同的。</p><p>And inside, we’re left with the probability of being to the left of mu minus c, and then the probability of
            being
            to the right of mu plus c, which is the same as the probability that the absolute value of the distance from
            the
            mean is larger than or equal to c.&nbsp;So that’s the same inequality that we proved there, except that here I’m
            using c.&nbsp;There I used a, but it’s exactly the same one.</p>
        <h2 id="unknown-349">未知</h2><h2>Unknown</h2>
        <p>如果你把这个项放到另一边，并把它写成这个形式，那么这个不等式可能更容易理解。它告诉我们什么？它告诉我们，如果 c 是一个大数，它告诉我们距离平均值超过 c 的概率将是一个小数。当 c 很大时，这个概率就很小。现在，这是直观的。</p><p>This inequality was maybe better to understand if you take that term and send it to the other side and write
            it
            this form. What does it tell us? It tells us that if c is a big number, it tells us that the probability of
            being more than c away from the mean is going to be a small number. When c is big, this is small. Now, this
            is
            intuitive.</p>
        <p>方差是分布扩散的度量，即分布范围有多广。它告诉我们，如果方差很小，分布范围就不是很大。从数学上讲，这可以理解为，当方差小时，距离较远的概率就会很小。而且，距离越远，也就是说，如果 c 是一个较大的数字，这个概率也会变小。</p><p>The variance is a measure of the spread of the distribution, how wide it is. It tells us that if the variance
            is
            small, the distribution is not very wide. And mathematically, this translates to this statement that when
            the
            variance is small, the probability of being far away is going to be small. And the further away you’re
            looking,
            that is, if c is a bigger number, that probability also becomes small.</p>
        <p>也许，思考这个不等式内容的更直观的方法是，用数字 k 代替 c，其中 k 是正数，sigma 是标准差。所以我们只需用 k sigma 代替 c 即可。这样就变成了 k sigma 平方。这些 sigma 平方相互抵消。我们得到 1/k 平方。现在，这是什么？这是距离平均值有 k 个标准差的事件。</p><p>Maybe an even more intuitive way to think about the content of this inequality is to, instead of c, use the
            number k, where k is positive and sigma is the standard deviation. So let’s just plug k sigma in the place
            of
            c.&nbsp;So this becomes k sigma squared. These sigma squared’s cancel. We’re left with 1 over k square. Now, what
            is
            this? This is the event that you are k standard deviations away from the mean.</p>
        <h2 id="unknown-350">未知</h2><h2>Unknown</h2>
        <p>例如，这个陈述告诉你，如果你看一次测验的成绩，班上有多少人与平均值相差 3 个标准差？这是有可能的，但人数不会很多。最多只有 1/9 的人与平均值相差 3 个标准差或更多。所以切比雪夫不等式真的很有用。</p><p>So for example, this statement here tells you that if you look at the test scores from a quiz, what fraction
            of
            the class are 3 standard deviations away from the mean? It’s possible, but it’s not going to be a lot of
            people.
            It’s going to be at most, 1/9 of the class that can be 3 standard deviations or more away from the mean. So
            the
            Chebyshev inequality is a really useful one.</p>
        <p>每当你想要将概率和期望值联系起来时，它就派上用场了。所以如果你知道你的期望值，或者特别是你的方差很小，这就能告诉你一些关于尾部概率的信息。所以这是我们第一次题外话的结束。我们已经掌握了这个不等式。我们的第二次题外话是讨论极限。</p><p>It comes in handy whenever you want to relate probabilities and expected values. So if you know that your
            expected values or, in particular, that your variance is small, this tells you something about tailed
            probabilities. So this is the end of our first digression. We have this inequality in our hands. Our second
            digression is talk about limits.</p>
        <p>我们最终想讨论随机变量的极限，但作为热身，我们将从序列的极限开始。所以给你一个数字序列，a1、a2、a3 等等。我们想定义一个序列收敛到一个数字的概念。你大概知道这是什么意思，但让我们再多讲一下。所以这是 a。随着 n 的增加，我们有数值序列。</p><p>We want to eventually talk about limits of random variables, but as a warm up, we’re going to start with
            limits
            of sequences. So you’re given a sequence of numbers, a1, a2, a3, and so on. And we want to define the notion
            that a sequence converges to a number. You sort of know what this means, but let’s just go through it some
            more.
            So here’s a. We have our sequence of values as n increases.</p>
        <h2 id="unknown-351">未知</h2><h2>Unknown</h2>
        <p>我们所说的序列收敛到 a 的意思是，当你查看这些值时，它们会越来越接近 a。所以这里的这个值是典型的 a sub n。它们越来越接近 a，并且会一直接近。所以让我们试着让它更精确。这意味着让我们确定接近的含义。</p><p>What do we mean by the sequence converging to a is that when you look at those values, they get closer and
            closer
            to a. So this value here is your typical a sub n.&nbsp;They get closer and closer to a, and they stay closer. So
            let’s try to make that more precise. What it means is let’s fix a sense of what it means to be close.</p>
        <p>让我看一下从 epsilon 到 a + epsilon 的区间。如果我的序列收敛到 a，这意味着随着 n 的增加，我得到的序列的值最终会停留在这个区间内。由于它们收敛到 a，这意味着它们最终会小于 a + epsilon 并大于 epsilon。
        </p><p>Let me look at an interval that goes from a epsilon to a + epsilon. Then if my sequence converges to a, this
            means that as n increases, eventually the values of the sequence that I get stay inside this band. Since
            they
            converge to a, this means that eventually they will be smaller than a + epsilon and bigger than a epsilon.
        </p>
        <p>因此，收敛意味着给定一个围绕数字 a 的正长度带，您得到的序列的值最终会进入并停留在该带内。这就是收敛含义的图形定义。现在让我们将其转化为数学陈述。</p><p>So convergence means that given a band of positive length around the number a, the values of the sequence
            that
            you get eventually get inside and stay inside that band. So that’s sort of the picture definition of what
            convergence means. So now let’s translate this into a mathematical statement.</p>
        <h2 id="unknown-352">未知</h2><h2>Unknown</h2>
        <p>给定一个正长度的带，无论该带有多宽或多窄，对于每个正 epsilon，序列最终都会进入该带内。最终是什么意思？存在一个时间，因此在该时间之后会发生某事。而发生的事情是，在该时间之后，我们就在该带内了。</p><p>Given a band of positive length, no matter how wide that band is or how narrow it is, so for every epsilon
            positive, eventually the sequence gets inside the band. What does eventually mean? There exists a time, so
            that
            after that time something happens. And the something that happens is that after that time, we are inside
            that
            band.</p>
        <p>所以这是一个正式的数学定义，它实际上翻译了我之前用冗长的方式讲述的内容，并以图片的形式展示。给定一个特定的波段，即使它很窄，最终，在一定时间 n0 之后，序列的值将停留在这个波段内。</p><p>So this is a formal mathematical definition, which actually translates what I was telling in the wordy way
            before, and showing in terms of the picture. Given a certain band, even if it’s narrow, eventually, after a
            certain time n0, the values of the sequence are going to stay inside this band.</p>
        <p>现在，如果我将 epsilon 取得很小，那么最终我会进入频带内，但可能要等待更长时间才能让值进入频带内，这一点仍然是正确的。好吧，这就是确定性序列收敛到某个值的含义。现在，随机变量呢？随机变量序列收敛到一个数字意味着什么？</p><p>Now, if I were to take epsilon to be very small, this thing would still be true that eventually I’m going to
            get
            inside of the band, except that I may have to wait longer for the values to get inside here. All right,
            that’s
            what it means for a deterministic sequence to converge to something. Now, how about random variables. What
            does
            it mean for a sequence of random variables to converge to a number?</p>
        <h2 id="unknown-353">未知</h2><h2>Unknown</h2>
        <p>我们只是要稍微扭曲一下这个词的定义。对于数字，我们说数字最终会进入那个区间。但是如果我们有具有特定分布的随机变量而不是数字，那么这里我们处理的不是 a_n，而是具有这种分布的随机变量，我们希望这个分布进入这个区间，所以它会集中在这个区间内。</p><p>We’re just going to twist a little bit of the word definition. For numbers, we said that eventually the
            numbers
            get inside that band. But if instead of numbers we have random variables with a certain distribution, so
            here
            instead of a_n we’re dealing with a random variable that has a distribution, let’s say, of this kind, what
            we
            want is that this distribution gets inside this band, so it gets concentrated inside here.</p>
        <p>分布进入这个区间意味着什么？我的意思是，随机变量有一个分布。它可能有一些尾部，所以可能不是整个分布都集中在区间内。但我们希望这个分布越来越多地集中在这个区间内。这样，从某种意义上说，落在区间外的概率收敛到 0 变得越来越小。</p><p>What does it means that the distribution gets inside this band? I mean a random variable has a distribution.
            It
            may have some tails, so maybe not the entire distribution gets concentrated inside of the band. But we want
            that
            more and more of this distribution is concentrated in this band. So that in a sense that the probability of
            falling outside the band converges to 0 becomes smaller and smaller.</p>
        <p>换句话说，如果以下条件成立，那么随机变量序列或概率分布序列（它们将是相同的）将收敛到特定数字 a。如果我考虑围绕 a 的一个小范围，那么我的随机变量落在这个范围之外的概率（即该曲线下的面积）随着 n 趋向无穷大而变得越来越小。</p><p>So in words, we’re going to say that the sequence random variables or a sequence of probability
            distributions,
            that would be the same, converges to a particular number a if the following is true. If I consider a small
            band
            around a, then the probability that my random variable falls outside this band, which is the area under this
            curve, this probability becomes smaller and smaller as n goes to infinity.</p>
        <h2 id="unknown-354">未知</h2><h2>Unknown</h2>
        <p>在这个区间之外的概率收敛到 0。所以这是直观的想法。所以一开始，我们的分布可能无处不在。随着 n 的增加，分布开始集中在区间内。当 a 更大时，我们的分布就更加集中在区间内，因此这些区间外的概率变得越来越小。所以相应的数学表述如下。我在 a 周围固定一个区间，a +/ epsilon。</p><p>The probability of being outside this band converges to 0. So that’s the intuitive idea. So in the beginning,
            maybe our distribution is sitting everywhere. As n increases, the distribution starts to get concentrating
            inside the band. When a is even bigger, our distribution is even more inside that band, so that these
            outside
            probabilities become smaller and smaller. So the corresponding mathematical statement is the following. I
            fix a
            band around a, a +/ epsilon.</p>
        <p>给定该波段，落在该波段之外的概率，该概率收敛到 0。或者换句话说，该概率的极限等于 0。如果要将其转化为完整的数学表述，则必须写下以下混乱的内容。对于每个 epsilon 正数，该表述的极限为 0。某事物的极限为 0 是什么意思？</p><p>Given that band, the probability of falling outside this band, this probability converges to 0. Or another
            way to
            say it is that the limit of this probability is equal to 0. If you were to translate this into a complete
            mathematical statement, you would have to write down the following messy thing. For every epsilon positive
            that’s this statement the limit is 0. What does it mean that the limit of something is 0?</p>
        <p>我们翻回上一张幻灯片。为什么？因为概率是一个数字。所以这里我们讨论的是收敛到 0 的数字序列。数字序列收敛到 0 意味着什么？这意味着对于任何 epsilon prime 正数，都存在某个 n0，使得对于每个大于 n0 的 n，以下情况是正确的，即这个概率小于或等于 epsilon prime。</p><p>We flip back to the previous slide. Why? Because a probability is a number. So here we’re talking about a
            sequence of numbers convergent to 0. What does it mean for a sequence of numbers to converge to 0? It means
            that
            for any epsilon prime positive, there exists some n0 such that for every n bigger than n0 the following is
            true
            that this probability is less than or equal to epsilon prime.</p>
        <h2 id="unknown-355">未知</h2><h2>Unknown</h2>
        <p>所以这个数学表述有点难以解析。对于该波段的每个大小，然后你都要定义数字序列的极限收敛到 0 意味着什么。但用文字描述这一点要容易得多，基本上，用这幅图来思考。随着 n 的增加，落在这些波段之外的概率变得越来越小。</p><p>So the mathematical statement is a little hard to parse. For every size of that band, and then you take the
            definition of what it means for the limit of a sequence of numbers to converge to 0. But it’s a lot easier
            to
            describe this in words and, basically, think in terms of this picture. That as n increases, the probability
            of
            falling outside those bands just become smaller and smaller.</p>
        <p>因此，我们的分布集中在特定数字 a 周围的任意窄带中。好的。让我们看一个例子。假设一个随机变量 Yn 具有这种特定类型的离散分布。它会收敛到某个值吗？好吧，这个随机变量的概率分布集中在 0 处，它位于 0 处的概率越来越大。如果我将一个带固定在 0 附近
        </p><p>So the statement is that our distribution gets concentrated in arbitrarily narrow little bands around that
            particular number a. OK. So let’s look at an example. Suppose a random variable Yn has a discrete
            distribution
            of this particular type. Does it converge to something? Well, the probability distribution of this random
            variable gets concentrated at 0 there’s more and more probability of being at 0. If I fix a band around 0
        </p>
        <p>因此，如果我将区间从负 epsilon 移到 epsilon，并观察该区间。超出该区间的概率为 1/n。当 n 趋于无穷大时，该概率趋于 0。因此，在这种情况下，我们确实存在收敛。并且 Yn 的概率收敛到数字 0。因此，这只是捕捉了从该图中显而易见的事实，即当 n 趋于无穷大时，我们的概率分布越来越多地集中在 0 附近。</p><p>So if I take the band from minus epsilon to epsilon and look at that band. the probability of falling outside
            this band is 1/n.&nbsp;As n goes to infinity, that probability goes to 0. So in this case, we do have
            convergence.
            And Yn converges in probability to the number 0. So this just captures the facts obvious from this picture,
            that
            more and more of our probability distribution gets concentrated around 0, as n goes to infinity.</p>
        <h2 id="unknown-356">未知</h2><h2>Unknown</h2>
        <p>现在，需要注意的一件有趣的事情是，即使 Yn 收敛到 0，如果你写下 Yn 的预期值，它会是什么？它将是这个值的概率的 n 倍，即 1/n。&nbsp;</p><p>Now, an interesting thing to notice is the following, that even though Yn converges to 0, if you were to
            write
            down the expected value for Yn, what would it be? It’s going to be n times the probability of this value,
            which
            is 1/n.&nbsp;</p>
        <p>所以期望值是 1。如果你看一下 Yn 平方的期望值，它就是 0。乘以这个概率，然后乘以 n 平方乘以这个概率，等于 n。这实际上趋向于无穷大。</p><p>So the expected value turns out to be 1. And if you were to look at the expected value of Yn squared, this
            would
            be 0. times this probability, and then n squared times this probability, which is equal to n.&nbsp;And this
            actually
            goes to infinity.</p>
        <p>因此，我们可能遇到了这种奇怪的情况，即一个随机变量趋于 0，但该随机变量的期望值不会趋于 0。并且该随机变量的二阶矩实际上趋于无穷大。因此，这告诉我们，概率的收敛可以告诉你一些信息，但并不能告诉你全部信息。随机变量收敛到 0 并不意味着期望值或方差等的收敛。</p><p>So we have this, perhaps, strange situation where a random variable goes to 0, but the expected value of this
            random variable does not go to 0. And the second moment of that random variable actually goes to infinity.
            So
            this tells us that convergence in probability tells you something, but it doesn’t tell you the whole story.
            Convergence to 0 of a random variable doesn’t imply anything about convergence of expected values or of
            variances and so on.</p>
        <h2 id="unknown-357">未知</h2><h2>Unknown</h2>
        <p>原因是概率收敛告诉你这里的尾部概率非常小。但它没有告诉你这个尾部有多大。如本例所示，尾部概率很小，但尾部作用距离很远，因此它对预期值或预期值平方的贡献不成比例。好的。现在我们已经掌握了回到样本均值并研究其属性所需的一切。</p><p>So the reason is that convergence in probability tells you that this tail probability here is very small. But
            it
            doesn’t tell you how far does this tail go. As in this example, the tail probability is small, but that tail
            acts far away, so it gives a disproportionate contribution to the expected value or the expected value
            squared.
            OK. So now we’ve got everything that we need to go back to the sample mean and study its properties.</p>
        <p>可悲的是，我们有一系列随机变量。它们是独立的。它们具有相同的分布。我们假设它们具有有限的均值和有限的方差。我们正在查看样本均值。现在原则上，您可以计算样本均值的概率分布，因为我们知道如何找到独立随机变量和的分布。您可以反复使用卷积公式。</p><p>So the sad thing is that we have a sequence of random variables. They’re independent. They have the same
            distribution. And we assume that they have a finite mean and a finite variance. We’re looking at the sample
            mean. Now in principle, you can calculate the probability distribution of the sample mean, because we know
            how
            to find the distributions of sums of independent random variables. You use the convolution formula over and
            over.</p>
        <p>但这相当复杂，所以我们不看这个。我们只看期望值、方差和样本均值远离真实均值的概率。那么这个随机变量的期望值是多少？一组随机变量的期望值是期望值的总和。然后我们在分母中得到这个因数 n。</p><p>But this is pretty complicated, so let’s not look at that. Let’s just look at expected values, variances, and
            the
            probabilities that the sample mean is far away from the true mean. So what is the expected value of this
            random
            variable? The expected value of a sum of random variables is the sum of the expected values. And then we
            have
            this factor of n in the denominator.</p>
        <h2 id="unknown-358">未知</h2><h2>Unknown</h2>
        <p>这些期望值中的每一个都是 mu，所以我们得到 mu。因此，样本均值，即期望中的 Mn 的平均值与我们总体中的真实均值相同。现在，这是一个很好的概念点，当你写下这个表达式时，会涉及两种平均值。我们知道期望是某种平均值。样本均值也是我们观察到的值的平均值。</p><p>Each one of these expected values is mu, so we get mu. So the sample mean, the average value of this Mn in
            expectation is the same as the true mean inside our population. Now here, this is a fine conceptual point,
            there’s two kinds of averages involved when you write down this expression. We understand that expectations
            are
            some kind of average. The sample mean is also an average over the values that we have observed.</p>
        <p>但这是两种不同的平均值。样本均值是我们在一次探险中收集到的企鹅身高的平均值。期望值可以这样理解，我的概率实验是一次南极探险。这里的期望值是指大量探险的平均值。所以我的探险是一个随机实验，我收集随机样本，并记录 Mn。
        </p><p>But it’s two different kinds of averages. The sample mean is the average of the heights of the penguins that
            we
            collected over a single expedition. The expected value is to be thought of as follows, my probabilistic
            experiment is one expedition to the South Pole. Expected value here means thinking on the average over a
            huge
            number of expeditions. So my expedition is a random experiment, I collect random samples, and they record
            Mn.
        </p>
        <p>一次探险的平均结果就是我们进行无数次探险，然后对每次探险的平均值进行平均后的结果。所以这个 Mn 是一次探险的平均值。这个期望值是想象中的无限次探险的平均值。当然，另一件要时刻牢记的事情是，期望值给你的是数字，而样本平均值实际上是一个随机变量。</p><p>The average result of an expedition is what we would get if we were to carry out a zillion expeditions and
            average the averages that we get at each particular expedition. So this Mn is the average during a single
            expedition. This expectation is the average over an imagined infinite sequence of expeditions. And of
            course,
            the other thing to always keep in mind is that expectations give you numbers, whereas the sample mean is
            actually a random variable.</p>
        <h2 id="unknown-359">未知</h2><h2>Unknown</h2>
        <p>好的。那么这个随机变量，它有多随机？它的方差有多大？所以随机变量总和的方差是方差的总和。但由于我们除以 n，当你计算方差时，这会带来一个 n 平方的因子。所以方差是 n 的 sigma 平方。具体来说，样本均值的方差变得越来越小。</p><p>All right. So this random variable, how random is it? How big is its variance? So the variance of a sum of
            random
            variables is the sum of the variances. But since we’re dividing by n, when you calculate variances this
            brings
            in a factor of n squared. So the variance is sigma squared over n.&nbsp;And in particular, the variance of the
            sample
            mean becomes smaller and smaller.</p>
        <p>这意味着，当你估计企鹅的平均身高时，如果你取一个大样本，那么你的估计就不会太随机。如果你有一个大样本量，你的估计中的随机性就会变小。样本量大会消除实验中的随机性。现在让我们应用切比雪夫不等式来说明样本均值的尾部概率。</p><p>It means that when you estimate that average height of penguins, if you take a large sample, then your
            estimate
            is not going to be too random. The randomness in your estimates become small if you have a large sample
            size.
            Having a large sample size kind of removes the randomness from your experiment. Now let’s apply the
            Chebyshev
            inequality to say something about tail probabilities for the sample mean.</p>
        <p>您与真实平均值相差超过 epsilon 的概率小于或等于该数量的方差除以该数字的平方。所以这只是将切比雪夫不等式翻译成我们在这里得到的特定上下文。我们找到了方差。它是 sigma 平方除以 n。所以我们最终得到了这个表达式。那么这个表达式有什么作用呢？</p><p>The probability that you are more than epsilon away from the true mean is less than or equal to the variance
            of
            this quantity divided by this number squared. So that’s just the translation of the Chebyshev inequality to
            the
            particular context we’ve got here. We found the variance. It’s sigma squared over n.&nbsp;So we end up with this
            expression. So what does this expression do?</p>
        <h2 id="unknown-360">未知</h2><h2>Unknown</h2>
        <p>对于任何给定的 epsilon，如果我固定 epsilon，那么这个小于 sigma 平方除以 n epsilon 平方的概率，在 n 趋于无穷大时收敛到 0。这只是概率收敛的定义。如果发生这种情况，即偏离平均值超过 epsilon 的概率，该概率趋于 0，无论我如何选择 epsilon，这都是正确的，那么根据定义，我们有概率收敛。
        </p><p>For any given epsilon, if I fix epsilon, then this probability, which is less than sigma squared over n
            epsilon
            squared, converges to 0 as n goes to infinity. And this is just the definition of convergence in
            probability. If
            this happens, that the probability of being more than epsilon away from the mean, that probability goes to
            0,
            and this is true no matter how I choose my epsilon, then by definition we have convergence in probability.
        </p>
        <p>因此，我们证明了样本均值在概率上收敛于真实均值。这就是弱大数定律告诉我们的。因此，在某种模糊意义上，它告诉我们，当你对样本中的许多测量值取平均值时，样本均值就是真实均值的良好估计，因为随着样本量的增加，样本均值会趋近于真实均值。</p><p>So we have proved that the sample mean converges in probability to the true mean. And this is what the weak
            law
            of large numbers tells us. So in some vague sense, it tells us that the sample means, when you take the
            average
            of many, many measurements in your sample, then the sample mean is a good estimate of the true mean in the
            sense
            that it approaches the true mean as your sample size increases.</p>
        <p>它接近真实平均值，但当然是在非常具体的意义上，在概率上，根据我们使用的收敛概念。既然我们在谈论抽样，让我们​​来看一个例子，这是构建民意调查的人面临的典型情况。所以你对人口的某些属性感兴趣。那么，有多少比例的人口更喜欢可口可乐而不是百事可乐？所以有一个数字 f，就是人口的比例。</p><p>It approaches the true mean, but of course in a very specific sense, in probability, according to this notion
            of
            convergence that we have used. So since we’re talking about sampling, let’s go over an example, which is the
            typical situation faced by someone who’s constructing a poll. So you’re interested in some property of the
            population. So what fraction of the population prefers Coke to Pepsi? So there’s a number f, which is that
            fraction of the population.</p>
        <h2 id="unknown-361">未知</h2><h2>Unknown</h2>
        <p>所以这是一个确切的数字。所以在 1 亿人口中，有 2000 万人喜欢可口可乐，那么 f 就是 0.2。我们想知道这个比例是多少。我们不可能问每个人。我们要做的就是随机抽取一些人群，询问他们的偏好。所以第 i 个人要么说喜欢可口可乐，要么说不喜欢。
        </p><p>And so this is an exact number. So out of a population of 100 million, 20 million prefer Coke, then f would
            be
            0.2. We want to find out what that fraction is. We cannot ask everyone. What we’re going to do is to take a
            random sample of people and ask them for their preferences. So the ith person either says yes for Coke or
            no.
        </p>
        <p>我们通过每次得到肯定答案时添加 1 来记录。然后我们计算这些 x 的平均值。这个平均值是多少？它是我们得到的 1 的数量除以 n。所以这是一个分数，但仅根据我们拥有的样本计算得出。因此，您可以将其视为基于我们拥有的样本的估计值 f_hat。</p><p>And we record that by putting a 1 each time that we get a yes answer. And then we form the average of these
            x’s.
            What is this average? It’s the number of 1’s that we got divided by n.&nbsp;So this is a fraction, but calculated
            only on the basis of the sample that we have. So you can think of this as being an estimate, f_hat, based on
            the
            sample that we have.</p>
        <p>现在，尽管我们在这里使用了小写字母，但这个 f_hat 当然是一个随机变量。f 是一个数字。这是总体中的真实分数。f_hat 是我们使用特定样本得到的估计值。好的。所以你的老板告诉你，我需要知道 f 是什么，但要去做一些抽样。你会如何回应？</p><p>Now, even though we used the lower case letter here, this f_hat is, of course, a random variable. f is a
            number.
            This is the true fraction in the overall population. f_hat is the estimate that we get by using our
            particular
            sample. Ok. So your boss told you, I need to know what f is, but go and do some sampling. What are you going
            to
            respond?</p>
        <h2 id="unknown-362">未知</h2><h2>Unknown</h2>
        <p>除非我询问所有人，否则我不可能确切地知道 f。对吧？没有办法。好吧，老板告诉你，好吧，那我就知道 f 在准确度之内。我想要你的答案，那就是你的答案，它接近正确答案的 1% 以内。所以如果真正的 f 是 0.4，你的答案应该在 0.39 和 0.41 之间。我想要一个非常准确的答案。</p><p>Unless I ask everyone in the whole population, there’s no way for me to know f exactly. Right? There’s no
            way.
            OK, so the boss tells you, well OK, then that’ll me f within an accuracy. I want an answer from you, that’s
            your
            answer, which is close to the correct answer within 1% point. So if the true f is 0.4, your answer should be
            somewhere between 0.39 and 0.41. I want a really accurate answer.</p>
        <p>你要说什么？好吧，没有人能保证我的答案会在 1% 以内。也许我运气不好，只是抽样了错误的人群，结果我的答案是错误的。所以我不能给你一个硬性保证，这个不等式会得到满足。但也许，我可以给你一个保证，这个不等式会得到满足，这个准确度要求会得到满足，而且很有信心。</p><p>What are you going to say? Well, there’s no guarantee that my answer will be within 1%. Maybe I’m unlucky and
            I
            just happen to sample the wrong set of people and my answer comes out to be wrong. So I cannot give you a
            hard
            guarantee that this inequality will be satisfied. But perhaps, I can give you a guarantee that this
            inequality
            will be satisfied, this accuracy requirement will be satisfied, with high confidence.</p>
        <p>也就是说，出错的概率会更小，我不太可能使用坏样本。但撇开运气不佳的较小概率不谈，我的答案将准确，符合你的准确度要求。因此，这两个数字是设计民意调查时通常的规格。所以这个数字是我们想要的准确度。这是期望的准确度。</p><p>That is, there’s going to be a smaller probability that things go wrong, that I’m unlikely and I use a bad
            sample. But leaving aside that smaller probability of being unlucky, my answer will be accurate within the
            accuracy requirement that you have. So these two numbers are the usual specs that one has when designing
            polls.
            So this number is the accuracy that we want. It’s the desired accuracy.</p>
        <h2 id="unknown-363">未知</h2><h2>Unknown</h2>
        <p>这个数字与我们想要的置信度有关。所以 1 减去这个数字，我们可以称之为我们想要从样本中获得的置信度。所以这实际上是 1 减去置信度。所以现在你的工作是确定 n 有多大，你应该使用多大的样本，以满足老板给你的规格。在这个阶段你所知道的只是切比雪夫不等式。</p><p>And this number has to do with the confidence that we want. So 1 minus that number, we could call it the
            confidence that we want out of our sample. So this is really 1 minus confidence. So now your job is to
            figure
            out how large an n, how large a sample should you be using, in order to satisfy the specs that your boss
            gave
            you. All you know at this stage is the Chebyshev inequality.</p>
        <p>所以你只需试着使用它。根据切比雪夫不等式，得到与真实答案相差 0.01 以上的答案的概率是这个随机变量的方差除以这个数字的平方。正如我们之前所讨论的，方差是 x 的方差除以 n。所以我们得到了这个表达式。
        </p><p>So you just try to use it. The probability of getting an answer that’s more than 0.01 away from the true
            answer
            is, by Chebyshev’s inequality, the variance of this random variable divided by this number squared. The
            variance, as we argued a little earlier, is the variance of the x’s divided by n.&nbsp;So we get this expression.
        </p>
        <p>所以我们希望这个数字小于或等于 0.05。好吧，这里我们遇到了一点困难。方差，(sigma_x) 平方，它是什么？(Sigma_x) 平方，如果你还记得伯努利随机变量的方差，就是这个量。但我们不知道它。f 是我们首先要估计的。所以方差是未知的，所以我不能在这里代入一个数字。</p><p>So we would like this number to be less than or equal to 0.05. OK, here we hit a little bit off a difficulty.
            The
            variance, (sigma_x) squared, what is it? (Sigma_x) squared is, if you remember the variance of a Bernoulli
            random variable, is this quantity. But we don’t know it. f is what we’re trying to estimate in the first
            place.
            So the variance is not known, so I cannot plug in a number inside here.</p>
        <h2 id="unknown-364">未知</h2><h2>Unknown</h2>
        <p>我能做的是保守一点，使用方差的上限。这个数字能有多大？好吧，你可以绘制 f 乘以 (1 f)。这是一条抛物线。</p><p>What I can do is to be conservative and use an upper bound of the variance. How large can this number get?
            Well,
            you can plot f times (1 f). It’s a parabola.</p>
        <p>它的根在 0 和 1 处。因此，根据对称性，最大值将在 1/2 处，而当 f 为 1/2 时，方差将变为 1/4。因此，我不知道 (sigma_x) 平方，但我将使用 (sigma_x) 平方的最坏情况值，即 4。现在我知道这是一个始终正确的不等式。</p><p>It has a root at 0 and at 1. So the maximum value is going to be, by symmetry, at 1/2 and when f is 1/2, then
            this variance becomes 1/4. So I don’t know (sigma_x) squared, but I’m going to use the worst case value for
            (sigma_x) squared, which is 4. And this is now an inequality that I know to be always true.</p>
        <p>我有我的规格，我的规格告诉我我希望这个数字小于 0.05。根据我所知道的，我能做的最好的事情就是说，好吧，我要取这个数字并使其小于 0.05。如果我选​​择 n 使其小于 0.05，那么我确信这个概率也小于 0.05。这个不等式成立需要什么条件？</p><p>I’ve got my specs, and my specs tell me that I want this number to be less than 0.05. And given what I know,
            the
            best thing I can do is to say, OK, I’m going to take this number and make it less than 0.05. If I choose my
            n so
            that this is less than 0.05, then I’m certain that this probability is also less than 0.05. What does it
            take
            for this inequality to be true?</p>
        <h2 id="unknown-365">未知</h2><h2>Unknown</h2>
        <p>您可以在这里求解 n，您会发现要满足这个不等式，n 应该大于或等于 50,000。所以您可以让 n 等于 50,000。因此，切比雪夫不等式告诉我们，如果您取 n 等于 50,000，那么根据切比雪夫不等式，我们保证满足给定的规格。好的。现在，50,000 是一个很大的样本量。对吗？</p><p>You can solve for n here, and you find that to satisfy this inequality, n should be larger than or equal to
            50,000. So you can just let n be equal to 50,000. So the Chebyshev inequality tells us that if you take n
            equal
            to 50,000, then by the Chebyshev inequality, we’re guaranteed to satisfy the specs that we were given. Ok.
            Now,
            50,000 is a bit of a large sample size. Right?</p>
        <p>如果你在报纸上看到任何报道说选民有这样或那样的想法，那么这些是根据 1,200 名左右的可能选民样本得出的。所以你通常会在这些关于民意调查的新闻中看到的数字，通常涉及的样本量约为 1,000 左右。你永远不会看到 50,000 的样本量。这太多了。那么我们可以在哪里偷工减料呢？</p><p>If you read anything in the newspapers where they say so much of the voters think this and that, this was
            determined on the basis of a sample of 1,200 likely voters or so. So the numbers that you will typically see
            in
            these news items about polling, they usually involve sample sizes about the 1,000 or so. You will never see
            a
            sample size of 50,000. That’s too much. So where can we cut some corners?</p>
        <p>好吧，我们基本上可以在三个地方偷工减料。这个要求有点太严格了。报纸报道通常会告诉你，我们的准确度是 +/- 3%，而不是 1%。而且因为这个数字是以平方的形式出现的，所以把它变成 3% 而不是 1，可以节省 10 倍。然后，5% 的置信度，我想这通常没问题。</p><p>Well, we can cut corners basically in three places. This requirement is a little too tight. Newspaper stories
            will usually tell you, we have an accuracy of +/ 3% points, instead of 1% point. And because this number
            comes
            up as a square, by making it 3% points instead of 1, saves you a factor of 10. Then, the five percent
            confidence, I guess that’s usually OK.</p>
        <h2 id="unknown-366">未知</h2><h2>Unknown</h2>
        <p>如果我们使用 10 这个因子，那么我们得到的样本量就是 10,000。这又有点太大了。那么我们可以在哪里修复这个问题呢？好吧，事实证明，我们在这里使用的这个不等式，切比雪夫不等式，只是一个不等式。它不是那么严格。它不是很准确。也许有更好的方法来计算或估计这个数量，比这个更小。</p><p>If we use that factor of 10, then we make our sample that we gain from here, then we get a sample size of
            10,000.
            And that’s, again, a little too big. So where can we fix things? Well, it turns out that this inequality
            that
            we’re using here, Chebyshev’s inequality, is just an inequality. It’s not that tight. It’s not very
            accurate.
            Maybe there’s a better way of calculating or estimating this quantity, which is smaller than this.</p>
        <p>通过使用更精确的不等式或更精确的界限，我们可以说服自己，我们可以用更小的样本量来解决。这种更精确的不等式来自差分极限定理，这是我们要考虑的下一个极限定理。我们今天将开始讨论，但下周将继续讨论。</p><p>And using a more accurate inequality or a more accurate bound, then we can convince ourselves that we can
            settle
            with a smaller sample size. This more accurate kind of inequality comes out of a difference limit theorem,
            which
            is the next limit theorem we’re going to consider. We’re going to start the discussion today, but we’re
            going to
            continue with it next week.</p>
        <p>在我告诉你另一个极限定理的具体含义之前，让我先给你介绍一下这里涉及的内容。我们处理的是独立同分布随机变量的和。每个 X 都有自己的分布。假设 X 的分布是这样的。这是 X 的密度。如果我把很多 X 加在一起，我会期望什么样的分布？</p><p>Before I tell you exactly what that other limit theorem says, let me give you the big picture of what’s
            involved
            here. We’re dealing with sums of i.i.d random variables. Each X has a distribution of its own. So suppose
            that X
            has a distribution which is something like this. This is the density of X. If I add lots of X’s together,
            what
            kind of distribution do I expect?</p>
        <h2 id="unknown-367">未知</h2><h2>Unknown</h2>
        <p>平均值将是单个 X 的平均值的 n 倍。因此，如果这是 mu，我将得到一个平均值 n 倍 mu。但我的方差也会增加。当我添加随机变量时，我会添加方差。因此，由于方差增加，我们将得到一个相当宽的分布。因此，这是 X1 加上一直到 Xn 的密度。</p><p>The mean is going to be n times the mean of an individual X. So if this is mu, I’m going to get a mean of n
            times
            mu. But my variance will also increase. When I add the random variables, I’m adding the variances. So since
            the
            variance increases, we’re going to get a distribution that’s pretty wide. So this is the density of X1 plus
            all
            the way to Xn.</p>
        <p>因此，随着 n 的增加，我的分布会发生变化，因为平均值为正。所以我不断添加东西。而且，我的分布变得越来越宽。方差增加。好吧，我们开始了一个不同的缩放。当我们研究弱大数定律时，我们开始了这个数量的缩放版本。在弱大数定律中，我们取这个随机变量并将其除以 n。&nbsp;</p><p>So as n increases, my distribution shifts, because the mean is positive. So I keep adding things. And also,
            my
            distribution becomes wider and wider. The variance increases. Well, we started a different scaling. We
            started a
            scaled version of this quantity when we looked at the weak law of large numbers. In the weak law of large
            numbers, we take this random variable and divide it by n.&nbsp;</p>
        <p>弱定律告诉我们，我们将得到一个高度集中在真实均值（即 mu）周围的分布。所以这里的密度是 X1 加 Xn 除以 n。因为我除以 n，所以均值变成了原始均值，即 mu。但弱大数定律告诉我们，这个随机变量的分布非常集中在均值周围。</p><p>And what the weak law tells us is that we’re going to get a distribution that’s very highly concentrated
            around
            the true mean, which is mu. So this here would be the density of X1 plus Xn divided by n.&nbsp;Because I’ve
            divided
            by n, the mean has become the original mean, which is mu. But the weak law of large numbers tells us that
            the
            distribution of this random variable is very concentrated around the mean.</p>
        <h2 id="unknown-368">未知</h2><h2>Unknown</h2>
        <p>因此，我们得到了一种非常狭窄的分布。在极限情况下，这种分布会集中于 mu 之上。因此，它是一种退化分布。因此，这是两个极端，总和没有缩放，缩放时除以 n。在这种极端情况下，我们得到了完全平坦分布的简单情况。在这种缩放中，我们得到了集中在单个点周围的分布。
        </p><p>So we get a distribution that’s very narrow in this kind. In the limit, this distribution becomes one that’s
            just
            concentrated on top of mu. So it’s sort of a degenerate distribution. So these are two extremes, no scaling
            for
            the sum, a scaling where we divide by n.&nbsp;In this extreme, we get the trivial case of a distribution that
            flattens out completely. In this scaling, we get a distribution that gets concentrated around a single
            point.
        </p>
        <p>再次，我们来看看一些中间缩放，它们会让事情变得更有趣。如果我们通过将总和除以 n 的平方根而不是除以 n 来进行缩放，事情就会变得有趣。这有什么影响？当我们通过除以 n 的平方根进行缩放时，Sn 对 n 平方根的方差将是 Sn 对总和除以 n 的方差。这就是方差的表现方式。</p><p>Again, we look at some intermediate scaling that makes things more interesting. Things do become interesting
            if
            we scale by dividing the sum by square root of n instead of dividing by n.&nbsp;What effect does this have? When
            we
            scale by dividing by square root of n, the variance of Sn over square root of n is going to be the variance
            of
            Sn over sum divided by n.&nbsp;That’s how variances behave.</p>
        <p>Sn 的方差是 n 西格玛平方，除以 n，也就是西格玛平方，这意味着当我们以这种特定方式缩放时，随着 n 的变化，方差不会改变。因此，我们的分布的宽度将保持恒定。分布的形状会发生变化，但不会像这里的情况一样变窄。它不会变宽，而是保持相同的宽度。</p><p>The variance of Sn is n sigma squared, divide by n, which is sigma squared, which means that when we scale in
            this particular way, as n changes, the variance doesn’t change. So the width of our distribution will be
            sort of
            constant. The distribution changes shape, but it doesn’t become narrower as was the case here. It doesn’t
            become
            wider, kind of keeps the same width.</p>
        <h2 id="unknown-369">未知</h2><h2>Unknown</h2>
        <p>因此，在极限情况下，这个分布可能呈现出一种有趣的形状。事实也确实如此。让我们做之前做过的事情。我们看看总和，然后把总和除以某个类似于 n 的平方根的数。因此 Sn 的方差是 n 的平方西格玛。西格玛 Sn 的方差是它的平方根。就是这个数字。</p><p>So perhaps in the limit, this distribution is going to take an interesting shape. And that’s indeed the case.
            So
            let’s do what we did before. So we’re looking at the sum, and we want to divide the sum by something that
            goes
            like square root of n.&nbsp;So the variance of Sn is n sigma squared. The variance of the sigma Sn is the square
            root
            of that. It’s this number.</p>
        <p>因此，实际上，我们按平方根 n 的顺序缩放。现在，我在这里做另一件事。如果我的随机变量具有正均值，那么这个量将具有正的且不断增长的均值。它将向右移动。为什么会这样？Sn 的均值与 n 成比例。当我除以平方根 n 时，这意味着均值按 n 的平方根缩放。&nbsp;</p><p>So effectively, we’re scaling by order of square root n.&nbsp;Now, I’m doing another thing here. If my random
            variable
            has a positive mean, then this quantity is going to have a mean that’s positive and growing. It’s going to
            be
            shifting to the right. Why is that? Sn has a mean that’s proportional to n.&nbsp;When I divide by square root n,
            then
            it means that the mean scales like square root of n.&nbsp;</p>
        <p>因此，在进行此除法后，我的分布仍会继续变化。我想保持分布不变，因此我减去了 Sn 的平均值。因此，我们在这里做的是一种标准技术或转换，即取一个随机变量，然后对其进行所谓的标准化。我删除该随机变量的平均值，然后除以标准差。这会产生一个具有 0 平均值和单位方差的随机变量。</p><p>So my distribution would still keep shifting after I do this division. I want to keep my distribution in
            place,
            so I subtract out the mean of Sn. So what we’re doing here is a standard technique or transformation where
            you
            take a random variable and you so called standardize it. I remove the mean of that random variable and I
            divide
            by the standard deviation. This results in a random variable that has 0 mean and unit variance.</p>
        <h2 id="unknown-370">未知</h2><h2>Unknown</h2>
        <p>Zn 测量的是以下内容：Zn 告诉我 I 距离平均值有多少个标准差。Sn 减去（X 的期望值的 n 倍）告诉我 Sn 距离 Sn 的平均值有多少个标准差。通过除以 Sn 的标准差，可以知道 I 距离平均值有多少个标准差。因此，我们将研究这个随机变量，它只是一个变换 Zn。</p><p>What Zn measures is the following, Zn tells me how many standard deviations am I away from the mean. Sn minus
            (n
            times expected value of X) tells me how much is Sn away from the mean value of Sn. And by dividing by the
            standard deviation of Sn this tells me how many standard deviations away from the mean am I. So we’re going
            to
            look at this random variable, which is just a transformation Zn.</p>
        <p>它是 Sn 的线性变换。我们将把这个随机变量与标准正态随机变量进行比较。标准正态就是你所熟悉的随机变量，由通常的公式给出，我们有它的表格。这个 Zn 的均值为 0，方差为 1。因此，在这方面，它具有与标准正态相同的统计数据。Zn 的分布可以是任何值</p><p>It’s a linear transformation of Sn. S And we’re going to compare this random variable to a standard normal
            random
            variable. So a standard normal is the random variable that you are familiar with, given by the usual
            formula,
            and for which we have tables for it. This Zn has 0 mean and unit variance. So in that respect, it has the
            same
            statistics as the standard normal. The distribution of Zn could be anything</p>
        <p>可能相当混乱。但有一个令人惊叹的定理，称为中心极限定理，它告诉我们 Zn 的分布在以下意义上接近标准正态分布，即您可以计算出这种类型的概率，您可以计算出 Zn 的极限与从标准正态表中获得的 Z 的概率相同。</p><p>Can be pretty messy. But there is this amazing theorem called the central limit theorem that tells us that
            the
            distribution of Zn approaches the distribution of the standard normal in the following sense, that
            probability
            is that you can calculate of this type that you can calculate for Zn is the limit becomes the same as the
            probabilities that you would get from the standard normal tables for Z.</p>
        <h2 id="unknown-371">未知</h2><h2>Unknown</h2>
        <p>这是关于累积分布函数的陈述。这个量作为 c 的函数，是随机变量 Zn 的累积分布函数。这是标准正态分布的累积分布函数。中心极限定理告诉我们，经过适当标准化后，多个随机变量之和的累积分布函数接近标准正态分布的累积分布函数。</p><p>It’s a statement about the cumulative distribution functions. This quantity, as a function of c, is the
            cumulative distribution function of the random variable Zn. This is the cumulative distribution function of
            the
            standard normal. The central limit theorem tells us that the cumulative distribution function of the sum of
            a
            number of random variables, after they’re appropriately standardized, approaches the cumulative distribution
            function over the standard normal distribution.</p>
        <p>具体来说，这告诉我们，当 n 很大时，我们可以通过计算 Z 的概率来计算 Zn 的概率。这将是一个很好的近似值。Z 的概率很容易计算，因为它们已经很好地制成表格。所以我们得到了一个非常好的捷径来计算 Zn 的概率。现在，你感兴趣的不是 Zn。你感兴趣的是 Sn。Sn 在这里反转这个关系</p><p>In particular, this tells us that we can calculate probabilities for Zn when n is large by calculating
            instead
            probabilities for Z. And that’s going to be a good approximation. Probabilities for Z are easy to calculate
            because they’re well tabulated. So we get a very nice shortcut for calculating probabilities for Zn. Now,
            it’s
            not Zn that you’re interested in. What you’re interested in is Sn. And Sn inverting this relation here</p>
        <p>Sn 是平方根 n sigma Zn 加上 n X 的期望值。好的。现在，如果你能计算出 Zn 的概率，哪怕是近似的，那么你肯定也能计算出 Sn 的概率，因为一个是另一个的线性函数。下次我们会讲一点。你也会得到一些背诵练习。</p><p>Sn is square root n sigma Zn plus n expected value of X. All right. Now, if you can calculate probabilities
            for
            Zn, even approximately, then you can certainly calculate probabilities for Sn, because one is a linear
            function
            of the other. And we’re going to do a little bit of that next time. You’re going to get, also, some practice
            in
            recitation.</p>
        <h2 id="unknown-372">未知</h2><h2>Unknown</h2>
        <p>在更模糊的层面上，你可以将中心极限定理描述为以下内容：当 n 很大时，你可以假装 Zn 是标准正态随机变量，并像 Zn 是标准正态一样进行计算。现在，假装 Zn 是正态的就等于假装 Sn 是正态的，因为 Sn 是 Zn 的线性函数。我们知道正态随机变量的线性函数是正态的。</p><p>At a more vague level, you could describe the central limit theorem as saying the following, when n is large,
            you
            can pretend that Zn is a standard normal random variable and do the calculations as if Zn was standard
            normal.
            Now, pretending that Zn is normal is the same as pretending that Sn is normal, because Sn is a linear
            function
            of Zn. And we know that linear functions of normal random variables are normal.</p>
        <p>因此，中心极限定理本质上告诉我们，我们可以假装 Sn 是一个正态随机变量，并像对待正态随机变量一样进行计算。但从数学上讲，中心极限定理并没有讨论 Sn 的分布，因为 Sn 的分布在极限中退化，只是一个非常平坦和长的东西。因此严格从数学上讲，它是关于 Zn 的累积分布的陈述。实际上，使用它的方式就是假装 Sn 是正态的。很好。享受感恩节假期。</p><p>So the central limit theorem essentially tells us that we can pretend that Sn is a normal random variable and
            do
            the calculations just as if it were a normal random variable. Mathematically speaking though, the central
            limit
            theorem does not talk about the distribution of Sn, because the distribution of Sn becomes degenerate in the
            limit, just a very flat and long thing. So strictly speaking mathematically, it’s a statement about
            cumulative
            distributions of Zn’s. Practically, the way you use it is by just pretending that Sn is normal. Very good.
            Enjoy
            the Thanksgiving Holiday.</p>
        <h1 id="central-limit-theorem">20.中心极限定理</h1><h1>20. Central Limit Theorem</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:23 <a href="https://youtube.com/watch?v=Tx7zzD4aeiA">https://youtube.com/watch?v=Tx7zzD4aeiA</a></p><p> 12 years ago (Nov 10, 2012) — 51:23 <a href="https://youtube.com/watch?v=Tx7zzD4aeiA">https://youtube.com/watch?v=Tx7zzD4aeiA</a></p>
        <h2 id="unknown-373">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：我们今天将结束对极限定理的讨论。我要提醒大家什么是中心极限定理，我们上次简要介绍过它。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: We’re going to finish
            today
            our discussion of limit theorems. I’m going to remind you what the central limit theorem is, which we
            introduced
            briefly last time.</p>
        <p>我们将讨论它到底说了什么以及它的含义。然后我们将应用到几个例子，主要是二项分布。好的，情况是，我们正在处理大量独立的、相同分布的随机变量。我们想看看它们的总和，并谈谈总和的分布。</p><p>We’re going to discuss what exactly it says and its implications. And then we’re going to apply to a couple
            of
            examples, mostly on the binomial distribution. OK, so the situation is that we are dealing with a large
            number
            of independent, identically distributed random variables. And we want to look at the sum of them and say
            something about the distribution of the sum.</p>
        <p>我们可能想说总和近似地服从正态随机变量的分布，尽管从形式上看这并不完全正确。当 n 趋于无穷大时，总和的分布变得非常分散，并且不会收敛到极限分布。为了得到一个有趣的极限，我们首先需要取总和并将其标准化。</p><p>We might want to say that the sum is distributed approximately as a normal random variable, although,
            formally,
            this is not quite right. As n goes to infinity, the distribution of the sum becomes very spread out, and it
            doesn’t converge to a limiting distribution. In order to get an interesting limit, we need first to take the
            sum
            and standardize it.</p>
        <h2 id="unknown-374">未知</h2><h2>Unknown</h2>
        <p>标准化的意思是减去平均值，然后除以标准差。现在，平均值当然是每个 X 的期望值的 n 倍。标准差是方差的平方根。方差是 sigma 平方的 n 倍，其中 sigma 是 X 的方差，所以这就是标准差。</p><p>By standardizing it, what we mean is to subtract the mean and then divide by the standard deviation. Now, the
            mean is, of course, n times the expected value of each one of the X’s. And the standard deviation is the
            square
            root of the variance. The variance is n times sigma squared, where sigma is the variance of the X’s so
            that’s
            the standard deviation.</p>
        <p>完成此操作后，我们得到一个随机变量，其均值为 0，其中心化，方差等于 1。因此，无论 n 有多大，方差都保持不变。因此，Zn 的分布会随着 n 不断变化，但变化幅度不会太大。它会保持原位。</p><p>And after we do this, we obtain a random variable that has 0 mean its centered and the variance is equal to
            1.
            And so the variance stays the same, no matter how large n is going to be. So the distribution of Zn keeps
            changing with n, but it cannot change too much. It stays in place.</p>
        <p>均值为 0，宽度也大致保持不变，因为方差为 1。令人惊讶的是，随着 n 的增长，Zn 的分布会稳定在某种渐近形状中。这就是标准正态随机变量的形状。因此，标准正态意味着它具有 0 均值和单位方差。</p><p>The mean is 0, and the width remains also roughly the same because the variance is 1. The surprising thing is
            that, as n grows, that distribution of Zn kind of settles in a certain asymptotic shape. And that’s the
            shape of
            a standard normal random variable. So standard normal means that it has 0 mean and unit variance.</p>
        <h2 id="unknown-375">未知</h2><h2>Unknown</h2>
        <p>更准确地说，中心极限定理告诉我们的是 Zn 的累积分布函数与其与标准正态分布的累积分布函数之间的关系。因此，对于任何给定数字 c，Zn 小于或等于 c 的概率在极限中与标准正态分布小于或等于 c 的概率相同。&nbsp;</p><p>More precisely, what the central limit theorem tells us is a relation between the cumulative distribution
            function of Zn and its relation to the cumulative distribution function of the standard normal. So for any
            given
            number, c, the probability that Zn is less than or equal to c, in the limit, becomes the same as the
            probability
            that the standard normal becomes less than or equal to c.&nbsp;</p>
        <p>当然，这很有用，因为这些概率可以从正态表中获得，而如果要精确计算，Zn 的分布可能是一个非常复杂的表达式。因此，关于中心极限定理有一些评论。首先，它具有普遍性，这非常令人惊奇。X 的分布是什么并不重要。它可以是任何分布，只要它具有有限的均值和有限的方差。</p><p>And of course, this is useful because these probabilities are available from the normal tables, whereas the
            distribution of Zn might be a very complicated expression if you were to calculate it exactly. So some
            comments
            about the central limit theorem. First thing is that it’s quite amazing that it’s universal. It doesn’t
            matter
            what the distribution of the X’s is. It can be any distribution whatsoever, as long as it has finite mean
            and
            finite variance.</p>
        <p>当您使用中心极限定理进行近似时，您唯一需要知道的关于 X 分布的信息就是均值和方差。您需要它们来标准化 Sn。我的意思是，要减去均值并除以标准差，您需要知道均值和方差。
        </p><p>And when you go and do your approximations using the central limit theorem, the only thing that you need to
            know
            about the distribution of the X’s are the mean and the variance. You need those in order to standardize Sn.
            I
            mean to subtract the mean and divide by the standard deviation you need to know the mean and the variance.
        </p>
        <h2 id="unknown-376">未知</h2><h2>Unknown</h2>
        <p>但这些是你应用它所需要知道的唯一事情。此外，这是一种非常精确的计算捷径。因此，原则上，你可以通过多次卷积 X 的分布来计算 Zn 的分布。但这很繁琐，如果你尝试用分析方法进行计算，它可能是一个非常复杂的表达式。</p><p>But these are the only things that you need to know in order to apply it. In addition, it’s a very accurate
            computational shortcut. So the distribution of this Zn’s, in principle, you can calculate it by convolution
            of
            the distribution of the X’s with itself many, many times. But this is tedious, and if you try to do it
            analytically, it might be a very complicated expression.</p>
        <p>而只需求助于标准正态随机变量的标准正态表，事情就可以非常快速地完成。因此，如果您不想得到概率问题的确切答案，这是一个很好的计算捷径。现在，从更哲学的层面上讲，它证明了我们为什么真正对正态随机变量感兴趣。</p><p>Whereas by just appealing to the standard normal table for the standard normal random variable, things are
            done
            in a very quick way. So it’s a nice computational shortcut if you don’t want to get an exact answer to a
            probability problem. Now, at a more philosophical level, it justifies why we are really interested in normal
            random variables.</p>
        <p>每当你遇到一个嘈杂的现象时，你观察到的噪声是通过添加许多彼此独立的小随机片段而产生的，你将要观察到的整体效应可以用一个正态随机变量来描述。</p><p>Whenever you have a phenomenon which is noisy, and the noise that you observe is created by adding the lots
            of
            little pieces of randomness that are independent of each other, the overall effect that you’re going to
            observe
            can be described by a normal random variable.</p>
        <h2 id="unknown-377">未知</h2><h2>Unknown</h2>
        <p>举一个 100 年前的经典例子，假设你有一种液体，液体中悬浮着一小粒灰尘或其他物质。这个小颗粒会完全随机地被分子撞击，所以你会看到这个颗粒在液体中随机移动。</p><p>So in a classic example that goes 100 years back or so, suppose that you have a fluid, and inside that fluid,
            there’s a little particle of dust or whatever that’s suspended in there. That little particle gets hit by
            molecules completely at random and so what you’re going to see is that particle kind of moving randomly
            inside
            that liquid.</p>
        <p>现在，对于这种随机运动，如果你问，一秒钟后，我的粒子在 x 轴上沿 x 方向的位移是多少。这种位移可以用正态随机变量非常很好地建模。原因是该粒子的位置是由撞击该粒子的大量分子随机撞击的累积效应决定的。</p><p>Now that random motion, if you ask, after one second, how much is my particle displaced, let’s say, in the x
            axis
            along the x direction. That displacement is very, very well modeled by a normal random variable. And the
            reason
            is that the position of that particle is decided by the cumulative effect of lots of random hits by
            molecules
            that hit that particle.</p>
        <p>这是一种著名的物理模型，被称为布朗运动。有些人用这个模型来描述金融市场的变动。有人认为，价格变动与市场中许多不同参与者做出的许多小决定和许多小事件有关。</p><p>So that’s a sort of celebrated physical model that goes under the name of Brownian motion. And it’s the same
            model that some people use to describe the movement in the financial markets. The argument might go that the
            movement of prices has to do with lots of little decisions and lots of little events by many, many different
            actors that are involved in the market.</p>
        <h2 id="unknown-378">未知</h2><h2>Unknown</h2>
        <p>因此，股票价格的分布可能可以用正态随机变量很好地描述。至少直到最近人们还想相信这一点。现在的证据表明，这些分布实际上更偏向重尾分布，因为极端事件发生的可能性比正态随机变量似乎表明的要大一些。</p><p>So the distribution of stock prices might be well described by normal random variables. At least that’s what
            people wanted to believe until somewhat recently. Now, the evidence is that, actually, these distributions
            are a
            little more heavy tailed in the sense that extreme events are a little more likely to occur that what normal
            random variables would seem to indicate.</p>
        <p>但是，作为第一个模型，再次重申，至少作为一个起始模型，包含正态随机变量是一个合理的论点。所以这是事情的哲学方面。在更准确的数学方面，重要的是要准确地理解中心极限定理是一种什么样的陈述。它是关于这些标准化随机变量的 CDF 收敛到正态分布的 CDF 的陈述。
        </p><p>But as a first model, again, it could be a plausible argument to have, at least as a starting model, one that
            involves normal random variables. So this is the philosophical side of things. On the more accurate,
            mathematical side, it’s important to appreciate exactly quite kind of statement the central limit theorem
            is.
            It’s a statement about the convergence of the CDF of these standardized random variables to the CDF of a
            normal.
        </p>
        <p>因此，这是关于 CDF 收敛的陈述。这不是关于 PMF 收敛或 PDF 收敛的陈述。现在，如果做出额外的数学假设，中心极限定理的变体会讨论 PDF 和 PMF。但一般来说，情况不一定如此。我将用以下图表来说明这一点。我这里有一个图表，但您的幻灯片中没有。但为了说明这一点，请考虑两个不同的离散分布。</p><p>So it’s a statement about convergence of CDFs. It’s not a statement about convergence of PMFs, or convergence
            of
            PDFs. Now, if one makes additional mathematical assumptions, there are variations of the central limit
            theorem
            that talk about PDFs and PMFs. But in general, that’s not necessarily the case. And I’m going to illustrate
            this
            with. I have a plot here which is not in your slides. But just to make the point, consider two different
            discrete distributions.</p>
        <h2 id="unknown-379">未知</h2><h2>Unknown</h2>
        <p>这个离散分布取值为 1、4.7。这个离散分布可以取值为 1、2.4、6 和 7。所以这个分布的周期为 3，这个分布的值范围更有趣一些。这两个分布中的数字经过调整，因此它们具有相同的均值和方差。</p><p>This discrete distribution takes values 1,4.7. This discrete distribution can take values 1,2.4,6, and 7. So
            this
            one has sort of a periodicity of 3, this one, the range of values is a little more interesting. The numbers
            in
            these two distributions are cooked up so that they have the same mean and the same variance.</p>
        <p>现在，我要做的是取随机变量的八个独立副本，并绘制八个随机变量之和的 PMF。现在，如果我绘制这 8 个随机变量之和的 PMF，我会得到与此图中的这些项目相对应的图。</p><p>Now, what I’m going to do is to take eight independent copies of the random variable and plot the PMF of the
            sum
            of eight random variables. Now, if I plot the PMF of the sum of 8 of these, I get the plot, which
            corresponds to
            these bullets in this diagram.</p>
        <p>如果我根据此分布取 8 个随机变量，并将它们相加，计算它们的 PMF，我得到的 PMF 就是此处用 X 表示的 PMF。至少当您目测时，这两个 PMF 看起来确实不同。
        </p><p>If I take 8 random variables, according to this distribution, and add them up and compute their PMF, the PMF
            I
            get is the one denoted here by the X’s. The two PMFs look really different, at least, when you eyeball them.
        </p>
        <h2 id="unknown-380">未知</h2><h2>Unknown</h2>
        <p>另一方面，如果你绘制它们的 CDF，那么如果你将它们与正常的 CDF（即连续曲线）进行比较，CDF 当然会逐步上升，因为我们研究的是离散随机变量。但它非常接近正常的 CDF。如果我们取 n 而不是 8，取 16，那么巧合度会更好。</p><p>On the other hand, if you were to plot the CDFs of them, then the CDFs, if you compare them with the normal
            CDF,
            which is this continuous curve, the CDF, of course, it goes up in steps because we’re looking at discrete
            random
            variables. But it’s very close to the normal CDF. And if we, instead of n equal to 8, we were to take 16,
            then
            the coincidence would be even better.</p>
        <p>因此，就 CDF 而言，当我们将其中 8 个或 16 个相加时，我们得到的非常接近正常的 CDF。如果我取其中 8 个或 16 个，我们得到的图基本上是相同的。因此，CDF 本质上是相互叠加的，尽管两个 PMF 看起来完全不同。因此，这是为了理解，从形式上讲，我们只有关于 CDF 的陈述，而不是关于 PMF 的陈述。</p><p>So in terms of CDFs, when we add 8 or 16 of these, we get very close to the normal CDF. We would get
            essentially
            the same picture if I were to take 8 or 16 of these. So the CDFs sit, essentially, on top of each other,
            although the two PMFs look quite different. So this is to appreciate that, formally speaking, we only have a
            statement about CDFs, not about PMFs.</p>
        <p>现在在实践中，如何使用中心极限定理？好吧，它告诉我们，我们可以通过将 Zn 视为标准正态随机变量来计算概率。现在 Zn 是 Sn 的线性函数。相反，Sn 是 Zn 的线性函数。正态的线性函数是正态的。所以如果我假装 Zn 是正态的，这本质上与我们假装 Sn 是正态的是一样的。</p><p>Now in practice, how do you use the central limit theorem? Well, it tells us that we can calculate
            probabilities
            by treating Zn as if it were a standard normal random variable. Now Zn is a linear function of Sn.
            Conversely,
            Sn is a linear function of Zn. Linear functions of normals are normal. So if I pretend that Zn is normal,
            it’s
            essentially the same as if we pretend that Sn is normal.</p>
        <h2 id="unknown-381">未知</h2><h2>Unknown</h2>
        <p>因此，我们可以计算与 Sn 有关的概率，就好像 Sn 是正态的一样。现在，中心极限定理并没有告诉我们 Sn 近似正态。正式陈述是关于 Zn 的，但实际上，当你使用结果时，你可以假装 Sn 是正态的。最后，它是一个极限定理，所以它告诉我们当 n 趋于无穷大时会发生什么。</p><p>And so we can calculate probabilities that have to do with Sn as if Sn were normal. Now, the central limit
            theorem does not tell us that Sn is approximately normal. The formal statement is about Zn, but, practically
            speaking, when you use the result, you can just pretend that Sn is normal. Finally, it’s a limit theorem, so
            it
            tells us about what happens when n goes to infinity.</p>
        <p>如果我们要在实践中使用它，那么 n 当然不会是无穷大。也许 n 等于 15。当 n 是一个很小的数字（小到 15）时，我们可以使用极限定理吗？事实证明，这是一个非常好的近似值。即使对于非常小的 n 值，它也能给我们非常准确的答案。</p><p>If we are to use it in practice, of course, n is not going to be infinity. Maybe n is equal to 15. Can we use
            a
            limit theorem when n is a small number, as small as 15? Well, it turns out that it’s a very good
            approximation.
            Even for quite small values of n, it gives us very accurate answers.</p>
        <p>因此，在实践中，n 大于 15 或 20 左右时，我们能得到非常好的结果。没有好的定理能给我们提供硬性保证，因为近似的质量确实取决于 X 的分布细节。</p><p>So n over the order of 15, or 20, or so give us very good results in practice. There are no good theorems
            that
            will give us hard guarantees because the quality of the approximation does depend on the details of the
            distribution of the X’s.</p>
        <h2 id="unknown-382">未知</h2><h2>Unknown</h2>
        <p>如果 X 的分布一开始看起来有点像正态分布，那么对于较小的 n 值，您会看到，总和基本上呈正态分布。如果 X 的分布与正态分布有很大不同，则需要较大的 n 值才能使中心极限定理生效。让我们用几个代表性的图来说明这一点。</p><p>If the X’s have a distribution that, from the outset, looks a little bit like the normal, then for small
            values
            of n, you are going to see, essentially, a normal distribution for the sum. If the distribution of the X’s
            is
            very different from the normal, it’s going to take a larger value of n for the central limit theorem to take
            effect. So let’s illustrates this with a few representative plots.</p>
        <p>因此，我们先从 1 到 8 的离散均匀分布开始。让我们将 2 个随机变量相加，2 个随机变量与此 PMF 相加，然后找到总和的 PMF。这是 2 个离散均匀分布的卷积，我相信您之前已经看过这个练习。当您将其与其自身进行卷积时，您会得到一个三角形。因此，这是两个离散均匀分布之和的 PMF。现在让我们继续。</p><p>So here, we’re starting with a discrete uniform distribution that goes from 1 to 8. Let’s add 2 of these
            random
            variables, 2 random variables with this PMF, and find the PMF of the sum. This is a convolution of 2
            discrete
            uniforms, and I believe you have seen this exercise before. When you convolve this with itself, you get a
            triangle. So this is the PMF for the sum of two discrete uniforms. Now let’s continue.</p>
        <p>让我们将其与自身进行卷积。这将给出 4 个离散均匀分布之和的 PMF。我们得到这个，它开始看起来像一个正态分布。如果我们将 n 设置为 32，那么它看起来本质上就像一个正态分布。这是一个很好的近似值。所以这是具有此均匀分布的 32 个离散随机变量之和的 PMF。如果我们从不对称的 PMF 开始。</p><p>Let’s convolve this with itself. These was going to give us the PMF of a sum of 4 discrete uniforms. And we
            get
            this, which starts looking like a normal. If we go to n equal to 32, then it looks, essentially, exactly
            like a
            normal. And it’s an excellent approximation. So this is the PMF of the sum of 32 discrete random variables
            with
            this uniform distribution. If we start with a PMF which is not symmetric.</p>
        <h2 id="unknown-383">未知</h2><h2>Unknown</h2>
        <p>这个是围绕均值对称的。但是如果我们从非对称的 PMF 开始，那么这是一个截断的几何 PMF，当我添加 8 个 PMF 时，结果就不那么好了。也就是说，如果我将其与自身卷积 8 次，我会得到这个 PMF，它可能与正常的 PMF 有点相似。</p><p>This one is symmetric around the mean. But if we start with a PMF which is non symmetric, so this is, here,
            is a
            truncated geometric PMF, then things do not work out as nicely when I add 8 of these. That is, if I convolve
            this with itself 8 times, I get this PMF, which maybe resembles a little bit to the normal one.</p>
        <p>但如果你关注这里和那里的细节，你就能真正看出它与正常情况不同。在这里它有点急剧上升。在这里它下降得慢一点。所以这里存在不对称，这是我们开始时分布不对称的结果。如果我们去 16，它看起来会好一点，但你仍然可以看到这个尾部和那个尾部之间的不对称。</p><p>But you can really tell that it’s different from the normal if you focus at the details here and there. Here
            it
            sort of rises sharply. Here it tails off a bit slower. So there’s an asymmetry here that’s present, and
            which is
            a consequence of the asymmetry of the distribution we started with. If we go to 16, it looks a little
            better,
            but still you can see the asymmetry between this tail and that tail.</p>
        <p>如果达到 32，仍然会存在一些不对称，但至少现在它开始看起来像正态分布。因此，这些图的寓意是，在获得真正好的近似值之前，您需要的 n 值可能会略有不同。但对于 20 到 30 左右范围内的 n 值，通常您会期望获得相当好的近似值。</p><p>If you get to 32 there’s still a little bit of asymmetry, but at least now it starts looking like a normal
            distribution. So the moral from these plots is that it might vary, a little bit, what kind of values of n
            you
            need before you get the really good approximation. But for values of n in the range 20 to 30 or so, usually
            you
            expect to get a pretty good approximation.</p>
        <h2 id="unknown-384">未知</h2><h2>Unknown</h2>
        <p>至少这些图表的视觉检查告诉我们这一点。既然我们知道我们手中有一个很好的近似值，那就使用它吧。让我们通过重新回顾上次的一个例子来使用它。这是民意调查问题。我们感兴趣的是具有某种习惯的人口比例。我们试图找出 f 是多少。</p><p>At least that’s what the visual inspection of these graphs tells us. So now that we know that we have a good
            approximation in our hands, let’s use it. Let’s use it by revisiting an example from last time. This is the
            polling problem. We’re interested in the fraction of population that has a certain habit been. And we try to
            find what f is.</p>
        <p>我们的做法是随机调查人们，记录他们的回答，不管他们是否有这个习惯。所以对于每个人，我们得到伯努利随机变量。概率为 f，一个人会回答 1，或者是，所以这是概率 f。剩下的概率是 1 f，这个人会回答否。</p><p>And the way we do it is by polling people at random and recording the answers that they give, whether they
            have
            the habit or not. So for each person, we get the Bernoulli random variable. With probability f, a person is
            going to respond 1, or yes, so this is with probability f.&nbsp;And with the remaining probability 1 f, the
            person
            responds no.</p>
        <p>我们记录这个数字，即回答“是”的人数除以总人数。这就是我们询问的人口比例。这是我们样本中回答“是”的人数。正如我们上次讨论的那样，您可能要从民意调查的一些规格开始。规格有两个参数。您想要的准确度和您希望获得的确实获得所需准确度的信心。
        </p><p>We record this number, which is how many people answered yes, divided by the total number of people. That’s
            the
            fraction of the population that we asked. This is the fraction inside our sample that answered yes. And as
            we
            discussed last time, you might start with some specs for the poll. And the specs have two parameters. the
            accuracy that you want and the confidence that you want to have that you did really obtain the desired
            accuracy.
        </p>
        <h2 id="unknown-385">未知</h2><h2>Unknown</h2>
        <p>因此，这里的规格是，我们希望我们的估计值与真实答案的误差在 1% 点以内的概率为 95%。因此，感兴趣的事件是这样的。这是民意调查的结果减去与真实答案的距离小于或大于 1%。我们感兴趣的是计算或近似这个特定的概率。因此，我们想使用中心极限定理来做到这一点。</p><p>So the specs here is that we want, probability 95% that our estimate is within 1% point from the true answer.
            So
            the event of interest is this. That’s the result of the poll minus distance from the true answer is less or
            bigger than 1% point. And we’re interested in calculating or approximating this particular probability. So
            we
            want to do it using the central limit theorem.</p>
        <p>安排这种计算机制的一种方法是，采取感兴趣的事件，并通过从不等式的两边减去和除以数字来对其进行处理，以便将其带入标准化随机变量 Zn 的图像，然后应用中心极限定理。</p><p>And one way of arranging the mechanics of this calculation is to take the event of interest and massage it by
            subtracting and dividing things from both sides of this inequality so that you bring him to the picture the
            standardized random variable, the Zn, and then apply the central limit theorem.</p>
        <p>所以，让我把感兴趣的事件完整地写下来，Mn 是这个量，所以我把它放在这里，减去 f，等于 nf 除以 n。所以这和那个事件是一样的。我们要计算这个事件的概率。这与我们应用中心极限定理的形式并不完全一样。要应用中心极限定理，我们需要在这里得到 sigma 平方根 n。&nbsp;</p><p>So the event of interest, let me write it in full, Mn is this quantity, so I’m putting it here, minus f,
            which is
            the same as nf divided by n.&nbsp;So this is the same as that event. We’re going to calculate the probability of
            this. This is not exactly in the form in which we apply the central limit theorem. To apply the central
            limit
            theorem, we need, down here, to have sigma square root n.&nbsp;</p>
        <h2 id="unknown-386">未知</h2><h2>Unknown</h2>
        <p>那么我怎么才能把 sigma 平方根 n 放在这里呢？我可以用这个不等式的两边除以 sigma。然后我可以从这里取一个平方根 n 的因子并将其发送到另一边。所以这个事件和那个事件是相同的。当且仅当那个事件发生时，这个事件才会发生。所以在这里计算这个事件的概率和计算这个事件发生的概率是一样的。</p><p>So how can I put sigma square root n here? I can divide both sides of this inequality by sigma. And then I
            can
            take a factor of square root n from here and send it to the other side. So this event is the same as that
            event.
            This will happen if and only if that will happen. So calculating the probability of this event here is the
            same
            as calculating the probability that this events happens.</p>
        <p>现在我们可以开始讨论这个问题了，因为我们在这里得到的随机变量是 Zn，或者说 Zn 的绝对值，我们正在讨论 Zn，或者说 Zn 的绝对值大于某个数字的概率。</p><p>And now we are in business because the random variable that we got in here is Zn, or the absolute value of
            Zn,
            and we’re talking about the probability that Zn, absolute value of Zn, is bigger than a certain number.</p>
        <p>由于 Zn 可以用标准正态随机变量来近似，所以我们的近似值不是要求 Zn 大于这个数字，而是要求 Z，即 Z 的绝对值大于这个数字。这就是我们要计算的概率。现在 Z 是一个标准正态随机变量。这里有一个小困难，上次我们也遇到过。</p><p>Since Zn is to be approximated by a standard normal random variable, our approximation is going to be,
            instead of
            asking for Zn being bigger than this number, we will ask for Z, absolute value of Z, being bigger than this
            number. So this is the probability that we want to calculate. And now Z is a standard normal random
            variable.
            There’s a small difficulty, the one that we also encountered last time.</p>
        <h2 id="unknown-387">未知</h2><h2>Unknown</h2>
        <p>困难在于 Xi 的标准差 sigma 是未知的。sigma 等于 f 倍。在这个例子中，sigma 是 f 倍 (1 f)，而我们唯一知道的关于 sigma 的事情是它将是一个小于 1/2 的数字。好的，所以我们必须在这里使用不等式。</p><p>And the difficulty is that the standard deviation, sigma, of the Xi’s is not known. Sigma is equal to f
            times.
            sigma, in this example, is f times (1 f), and the only thing that we know about sigma is that it’s going to
            be a
            number less than 1/2. OK, so we’re going to have to use an inequality here.</p>
        <p>我们将使用保守的 sigma 值，即等于 1/2 的 sigma 值，并用它代替精确的 sigma 值。这给了我们一个这样的不等式。让我们来确认一下为什么这个不等式是这样的。我们在轴上得到了两个数字。一个数字是 0.01 平方根 n 除以 sigma。另一个数字是 0.02 平方根 n。&nbsp;</p><p>We’re going to use a conservative value of sigma, the value of sigma equal to 1/2 and use that instead of the
            exact value of sigma. And this gives us an inequality going this way. Let’s just make sure why the
            inequality
            goes this way. We got, on our axis, two numbers. One number is 0.01 square root n divided by sigma. And the
            other number is 0.02 square root of n.&nbsp;</p>
        <p>我的主张是这些数字以这种特定方式相互关联。为什么会这样？Sigma 小于 2。所以 1/sigma 大于 2。因此，由于 1/sigma 大于 2，这意味着这个数字位于那个数字的右侧。因此，我们得到 Z 大于这个数字的概率。落在那里的概率小于落入这个区间的概率。</p><p>And my claim is that the numbers are related to each other in this particular way. Why is this? Sigma is less
            than 2. So 1/sigma is bigger than 2. So since 1/sigma is bigger than 2 this means that this numbers sits to
            the
            right of that number. So here we have the probability that Z is bigger than this number. The probability of
            falling out there is less than the probability of falling in this interval.</p>
        <h2 id="unknown-388">未知</h2><h2>Unknown</h2>
        <p>这就是最后一个不等式的意思。这个概率小于那个概率。这是我们感兴趣的概率，但由于我们不知道 sigma，我们取保守值，并在此区间的概率方面使用上限。现在我们开始做正题了。我们可以开始使用我们的常规表来计算感兴趣的概率。</p><p>So that’s what that last inequality is saying. this probability is smaller than that probability. This is the
            probability that we’re interested in, but since we don’t know sigma, we take the conservative value, and we
            use
            an upper bound in terms of the probability of this interval here. And now we are in business. We can start
            using
            our normal tables to calculate probabilities of interest.</p>
        <p>例如，假设 n 为 10,000。计算将如何进行？我们要计算 Z 的绝对值大于 0.2 乘以 1000 的概率，也就是 Z 的绝对值大于或等于 2 的概率。这里我们来做一些技巧，以保持状态。
        </p><p>So for example, let’s say that’s we take n to be 10,000. How is the calculation going to go? We want to
            calculate
            the probability that the absolute value of Z is bigger than 0.2 times 1000, which is the probability that
            the
            absolute value of Z is larger than or equal to 2. And here let’s do some mechanics, just to stay in shape.
        </p>
        <p>由于正态分布围绕均值对称，所以绝对值大于或等于 2 的概率将是 Z 大于或等于 2 的概率的两倍。我们可以使用 Z 的累积分布函数来计算这个概率吗？嗯，累积分布函数几乎给出了小于某个值的概率，而不是大于某个值的概率。</p><p>The probability that you’re larger than or equal to 2 in absolute value, since the normal is symmetric around
            the
            mean, this is going to be twice the probability that Z is larger than or equal to 2. Can we use the
            cumulative
            distribution function of Z to calculate this? Well, almost the cumulative gives us probabilities of being
            less
            than something, not bigger than something.</p>
        <h2 id="unknown-389">未知</h2><h2>Unknown</h2>
        <p>因此，我们还需要再一步，将其写为 1 减去 Z 小于或等于 2 的概率。现在，您可以从常规表中读出这个概率。常规表会告诉您这个概率是 0.9772。您确实会得到答案。</p><p>So we need one more step and write this as 1 minus the probability that Z is less than or equal to 2. And
            this
            probability, now, you can read off from the normal tables. And the normal tables will tell you that this
            probability is 0.9772. And you do get an answer.</p>
        <p>答案是 0.0456。好吧，我们尝试了 10,000。我们发现误差的概率是 4.5%，所以我们做得比我们设定的要好。所以这告诉我们也许我们还有一些回旋余地。也许我们可以使用较小的样本量，而仍然不需要设定。让我们试着找出我们可以突破极限的程度。我们可以将 n 取多小？</p><p>And the answer is 0.0456. OK, so we tried 10,000. And we find that our probably of error is 4.5%, so we’re
            doing
            better than the spec that we had. So this tells us that maybe we have some leeway. Maybe we can use a
            smaller
            sample size and still stay without our specs. Let’s try to find how much we can push the envelope. How much
            smaller can we take n?</p>
        <p>要回答这个问题，我们需要进行这种计算，本质上就是倒着算。我们将这个数字固定为 0.05，然后从这里倒着算。我这里犯了错误吗？10,000。所以我这里少了一个 0。啊，但是我取了平方根，所以是 100。0.02 是从哪里来的？啊，从这里来的。好的，好的。0.02 乘以 100，得到 2。好的，好的。很好，好的。</p><p>To answer that question, we need to do this kind of calculation, essentially, going backwards. We’re going to
            fix
            this number to be 0.05 and work backwards here to find. did I do a mistake here? 10,000. So I’m missing a 0
            here. Ah, but I’m taking the square root, so it’s 100. Where did the 0.02 come in from? Ah, from here. OK,
            all
            right. 0.02 times 100, that gives us 2. OK, all right. Very good, OK.</p>
        <h2 id="unknown-390">未知</h2><h2>Unknown</h2>
        <p>所以我们现在必须反向进行计算，弄清楚如果这是 0.05，我们在这里需要什么样的数字，然后在这里，从中我们就能知道我们需要什么 n 值。好的，所以我们想要找到 n，使得 Z 大于 0.02 平方根 n 的概率是 0.05。好的，所以 Z 是一个标准正态随机变量。</p><p>So we’ll have to do this calculation now backwards, figure out if this is 0.05, what kind of number we’re
            going
            to need here and then here, and from this we will be able to tell what value of n do we need. OK, so we want
            to
            find n such that the probability that Z is bigger than 0.02 square root n is 0.05. OK, so Z is a standard
            normal
            random variable.</p>
        <p>我们想要的是超出这个范围的概率。我们想要的是这两个尾部加在一起的概率。这两个尾部加在一起的概率应该是 0.05。这意味着这个尾部本身的概率应该是 0.025。这意味着这个概率应该是 0.975。现在，如果这个概率是 0.975，那么这个数字应该是多少？</p><p>And we want the probability that we are outside this range. We want the probability of those two tails
            together.
            Those two tails together should have probability of 0.05. This means that this tail, by itself, should have
            probability 0.025. And this means that this probability should be 0.975. Now, if this probability is to be
            0.975, what should that number be?</p>
        <p>你去查看标准表，找到与该数字对应的条目。我实际上带了一张标准表。0.975 就在这里。它告诉你与之对应的数字是 1.96。所以这告诉我们这个数字应该等于 1.96。现在，你从这里开始计算。
        </p><p>You go to the normal tables, and you find which is the entry that corresponds to that number. I actually
            brought
            a normal table with me. And 0.975 is down here. And it tells you that to the number that corresponds to it
            is
            1.96. So this tells us that this number should be equal to 1.96. And now, from here, you do the
            calculations.
        </p>
        <h2 id="unknown-391">未知</h2><h2>Unknown</h2>
        <p>你会发现 n 是 9604。因此，如果样本量为 10,000，我们得到的误差概率为 4.5%。如果样本量稍微小一些，为 9,600，我们可以得到误差概率为 0.05，这正是我们的预期。因此，这基本上就是你使用中心极限定理的两种方式。要么你给定 n，然后尝试计算概率。</p><p>And you find that n is 9604. So with a sample of 10,000, we got probability of error 4.5%. With a slightly
            smaller sample size of 9,600, we can get the probability of a mistake to be 0.05, which was exactly our
            spec. So
            these are essentially the two ways that you’re going to be using the central limit theorem. Either you’re
            given
            n and you try to calculate probabilities.</p>
        <p>或者你得到了概率，你想逆向求出 n 本身。所以在这个例子中，我们处理的随机变量当然是二项式随机变量。Xi 是伯努利分布，所以 Xi 的总和是二项式的。所以中心极限定理当然适用于二项分布。更准确地说，它当然适用于二项式随机变量的标准化版本。</p><p>Or you’re given the probabilities, and you want to work backwards to find n itself. So in this example, the
            random variable that we dealt with was, of course, a binomial random variable. The Xi’s were Bernoulli, so
            the
            sum of the Xi’s were binomial. So the central limit theorem certainly applies to the binomial distribution.
            To
            be more precise, of course, it applies to the standardized version of the binomial random variable.</p>
        <p>因此，这基本上就是我们在上一个示例中所做的。我们固定了数字 p，即我们实验成功的概率。p 对应于上一个示例中的 f。假设每个 Xi 都是伯努利随机变量，并且假设这些随机变量是独立的。当我们将它们相加时，我们得到一个具有二项分布的随机变量。</p><p>So here’s what we did, essentially, in the previous example. We fixed the number p, which is the probability
            of
            success in our experiments. p corresponds to f in the previous example. Let every Xi a Bernoulli random
            variable
            and are standing assumption is that these random variables are independent. When we add them, we get a
            random
            variable that has a binomial distribution.</p>
        <h2 id="unknown-392">未知</h2><h2>Unknown</h2>
        <p>我们知道二项式的均值和方差，所以我们取 Sn，减去均值，也就是除以标准差。中心极限定理告诉我们，这个随机变量的累积分布函数在极限中是一个标准正态随机变量。让我们再举一个计算的例子。让我们取 n 为。让我们选择一些特定的数字来处理。</p><p>We know the mean and the variance of the binomial, so we take Sn, we subtract the mean, which is this, divide
            by
            the standard deviation. The central limit theorem tells us that the cumulative distribution function of this
            random variable is a standard normal random variable in the limit. So let’s do one more example of a
            calculation. Let’s take n to be. let’s choose some specific numbers to work with.</p>
        <p>因此，在这个例子中，首先要做的是找到 Sn 的期望值，即 n 乘以 p。它是 18。然后我们需要记下标准差。Sn 的方差是方差的总和。它是 np 乘以 (1 p)。
        </p><p>So in this example, first thing to do is to find the expected value of Sn, which is n times p.&nbsp;It’s 18. Then
            we
            need to write down the standard deviation. The variance of Sn is the sum of the variances. It’s np times (1
            p).
        </p>
        <p>在这个特定例子中，p 乘以 (1 p) 是 1/4，n 是 36，所以是 9。这告诉我们这个 n 的标准差等于 3。所以我们要做的是取感兴趣的事件，即 Sn 小于 21，并以涉及标准化随机变量的方式重写它。因此，要做到这一点，我们需要减去平均值。</p><p>And in this particular example, p times (1 p) is 1/4, n is 36, so this is 9. And that tells us that the
            standard
            deviation of this n is equal to 3. So what we’re going to do is to take the event of interest, which is Sn
            less
            than 21, and rewrite it in a way that involves the standardized random variable. So to do that, we need to
            subtract the mean.</p>
        <h2 id="unknown-393">未知</h2><h2>Unknown</h2>
        <p>因此我们将其写为 Sn 3 应该小于或等于 21 3。这是同一个事件。然后除以标准差 3，我们得到这个结果。因此事件本身。</p><p>So we write this as Sn 3 should be less than or equal to 21 3. This is the same event. And then divide by the
            standard deviation, which is 3, and we end up with this. So the event itself of.</p>
        <p>听众：是的，应该减去 18，这样可以得到一个更好的数字，也就是 1。因此，感兴趣的事件，即 Sn 小于 21，与标准正态随机变量小于或等于 1 的事件相同。再一次，您可以在正态表中查找这一点。</p><p>AUDIENCE: Should subtract, 18, yes, which gives me a much nicer number out here, which is 1. So the event of
            interest, that Sn is less than 21, is the same as the event that a standard normal random variable is less
            than
            or equal to 1. And once more, you can look this up at the normal tables.</p>
        <p>你会发现你得到的答案是 0.43。现在将我们通过中心极限定理得到的答案与精确答案进行比较是很有趣的。精确答案涉及精确二项分布。我们这里得到的是 Sn 等于 k ​​的二项式概率。Sn 等于 k ​​由以下公式给出。</p><p>And you find that the answer that you get is 0.43. Now it’s interesting to compare this answer that we got
            through the central limit theorem with the exact answer. The exact answer involves the exact binomial
            distribution. What we have here is the binomial probability that, Sn is equal to k. Sn being equal to k is
            given
            by this formula.</p>
        <h2 id="unknown-394">未知</h2><h2>Unknown</h2>
        <p>然后，我们将 k 的所有值从 0 到 21 相加，编写两行代码来计算这个和，然后我们得到了精确的答案，即 0.8785。因此，两者之间存在相当好的一致性，尽管你不会认为这一定是极好的一致性。我们能做得更好吗？好的。事实证明我们可以。这就是想法。</p><p>And we add, over all values for k going from 0 up to 21, we write a two lines code to calculate this sum, and
            we
            get the exact answer, which is 0.8785. So there’s a pretty good agreements between the two, although you
            wouldn’t call that’s necessarily excellent agreement. Can we do a little better than that? OK. It turns out
            that
            we can. And here’s the idea.</p>
        <p>因此，我们的随机变量 Sn 的均值为 18。它具有二项分布。它由 PMF 描述，其形状大致如此，并且不断延续。使用中心极限定理基本上就是假装 Sn 是正态的，具有正确的均值和方差。因此，假设 Zn 具有 0 均值单位方差，我们用具有 0 均值单位方差的 Z 来近似它。</p><p>So our random variable Sn has a mean of 18. It has a binomial distribution. It’s described by a PMF that has
            a
            shape roughly like this and which keeps going on. Using the central limit theorem is basically pretending
            that
            Sn is normal with the right mean and variance. So pretending that Zn has 0 mean unit variance, we
            approximate it
            with Z, that has 0 mean unit variance.</p>
        <p>如果你假设 Sn 是正态的，那么你可以用具有正确均值和正确方差的正态来近似它。因此它仍然以 18 为中心。并且它的方差与二项式 PMF 相同。因此，使用中心极限定理本质上意味着我们保持均值和方差不变，但我们假设我们的分布是正态的。</p><p>If you were to pretend that Sn is normal, you would approximate it with a normal that has the correct mean
            and
            correct variance. So it would still be centered at 18. And it would have the same variance as the binomial
            PMF.
            So using the central limit theorem essentially means that we keep the mean and the variance what they are
            but we
            pretend that our distribution is normal.</p>
        <h2 id="unknown-395">未知</h2><h2>Unknown</h2>
        <p>我们想计算 Sn 小于或等于 21 的概率。我假设我的随机变量是正态的，所以我在这里画一条线，然后计算正态曲线下面积到 21 的面积。这就是我们所做的。现在，一个聪明的人说，Sn 是一个离散随机变量。</p><p>We want to calculate the probability that Sn is less than or equal to 21. I pretend that my random variable
            is
            normal, so I draw a line here and I calculate the area under the normal curve going up to 21. That’s
            essentially
            what we did. Now, a smart person comes around and says, Sn is a discrete random variable.</p>
        <p>因此，Sn 小于或等于 21 的事件与 Sn 严格小于 22 的事件相同，因为两者之间的任何情况都不可能发生。因此，我将使用中心极限定理近似，再次假设 Sn 是正常的，并在假设 Sn 是正常的条件下找到此事件的概率。</p><p>So the event that Sn is less than or equal to 21 is the same as Sn being strictly less than 22 because
            nothing in
            between can happen. So I’m going to use the central limit theorem approximation by pretending again that Sn
            is
            normal and finding the probability of this event while pretending that Sn is normal.</p>
        <p>因此，这个人会在这里（22）画一条线，然后计算到 22 为止的正态曲线下的面积。谁是对的？哪一个更好？其实都不是，但如果我们分摊差异，我们可以做得比两者都更好。</p><p>So what this person would do would be to draw a line here, at 22, and calculate the area under the normal
            curve
            all the way to 22. Who is right? Which one is better? Well neither, but we can do better than both if we
            sort of
            split the difference.</p>
        <h2 id="unknown-396">未知</h2><h2>Unknown</h2>
        <p>因此，用 Sn 表示同一事件的另一种方法是将其写为 Sn 小于 21.5。就离散随机变量 Sn 而言，这三个都是完全相同的事件。但是当你进行连续近似时，它们会给出不同的概率。</p><p>So another way of writing the same event for Sn is to write it as Sn being less than 21.5. In terms of the
            discrete random variable Sn, all three of these are exactly the same event. But when you do the continuous
            approximation, they give you different probabilities.</p>
        <p>问题在于你是否要将正态曲线下的面积积分到这里、到中点，还是到 22。事实证明，积分到中点可以得到更好的数值结果。所以我们取 21 和 1/2，并将正态曲线下的面积积分到这里。让我们进行这个计算，看看会得到什么。我们要在这里改变什么？</p><p>It’s a matter of whether you integrate the area under the normal curve up to here, up to the midway point, or
            up
            to 22. It turns out that integrating up to the midpoint is what gives us the better numerical results. So we
            take here 21 and 1/2, and we integrate the area under the normal curve up to here. So let’s do this
            calculation
            and see what we get. What would we change here?</p>
        <p>我们现在将写成 21.5，而不是 21。这个 18 变成了，不，那个 18 保持不变。但是这个 21 变成了 21.5。所以这个变成了 1 + 0.5 乘以 3。这是 117。所以我们现在查看正态分布表，并求出 Z 小于 1.17 的概率。因此，这里用标准正态分布小于 1.17 的概率来近似。正态分布表会告诉我们这个结果是 0.879。回到上一张幻灯片，这次我们用这个改进的近似值得到的结果是 0.879。这是对正确数字的非常好的近似。</p><p>Instead of 21, we would now write 21 and 1/2. This 18 becomes, no, that 18 stays what it is. But this 21
            becomes
            21 and 1/2. And so this one becomes 1 + 0.5 by 3. This is 117. So we now look up into the normal tables and
            ask
            for the probability that Z is less than 1.17. So this here gets approximated by the probability that the
            standard normal is less than 1.17. And the normal tables will tell us this is 0.879. Going back to the
            previous
            slide, what we got this time with this improved approximation is 0.879. This is a really good approximation
            of
            the correct number.</p>
        <h2 id="unknown-397">未知</h2><h2>Unknown</h2>
        <p>这是我们用 21 得到的结果。这是我们用 21 和 1/2 得到的结果。这是一个非常准确的近似值。这个数值示例的寓意是，进行 1 和 1/2 校正确实可以得到更好的近似值。事实上，我们甚至可以使用这个 1/2 的想法来计算单个概率。
        </p><p>This is what we got using the 21. This is what we get using the 21 and 1/2. And it’s an approximation that’s
            sort
            of right on. a very good one. The moral from this numerical example is that doing this 1 and 1/2 correction
            does
            give us better approximations. In fact, we can use this 1/2 idea to even calculate individual probabilities.
        </p>
        <p>因此假设您想要近似计算 Sn 等于 19 的概率。如果您假设 Sn 是正常的并计算这个概率，那么正态随机变量等于 19 的概率为 0。所以您得不到一个有趣的答案。</p><p>So suppose you want to approximate the probability that Sn equal to 19. If you were to pretend that Sn is
            normal
            and calculate this probability, the probability that the normal random variable is equal to 19 is 0. So you
            don’t get an interesting answer.</p>
        <p>通过将这个事件 19 写成与落在 18 和 1/2 和 19 和 1/2 之间的事件相同，并使用正态近似来计算这个概率，您可以得到一个更有趣的答案。就我们之前的图而言，这对应于以下内容。我们感兴趣的是 Sn 等于 19 的概率。所以我们对这个条的高度感兴趣。</p><p>You get a more interesting answer by writing this event, 19 as being the same as the event of falling between
            18
            and 1/2 and 19 and 1/2 and using the normal approximation to calculate this probability. In terms of our
            previous picture, this corresponds to the following. We are interested in the probability that Sn is equal
            to
            19. So we’re interested in the height of this bar.</p>
        <h2 id="unknown-398">未知</h2><h2>Unknown</h2>
        <p>我们将考虑从这里到这里的正态曲线下的面积，并将该面积用作该特定条形高度的近似值。所以我们基本上是在做，我们取正态曲线下的概率，该概率被分配到连续的值上，并将其归因于不同的离散值。</p><p>We’re going to consider the area under the normal curve going from here to here, and use this area as an
            approximation for the height of that particular bar. So what we’re basically doing is, we take the
            probability
            under the normal curve that’s assigned over a continuum of values and attributed it to different discrete
            values.</p>
        <p>中点以上的值归因于 19。中点以下的值归因于 18。所以这个绿色区域是我们对 19 处 PMF 值的近似值。因此，同样地，如果您想近似此时的 PMF 值，您可以取这个区间，并对这个区间内正态曲线下的面积进行积分。</p><p>Whatever is above the midpoint gets attributed to 19. Whatever is below that midpoint gets attributed to 18.
            So
            this is green area is our approximation of the value of the PMF at 19. So similarly, if you wanted to
            approximate the value of the PMF at this point, you would take this interval and integrate the area under
            the
            normal curve over that interval.</p>
        <p>事实证明，这可以很好地近似二项式的 PMF。实际上，当这项业务开始时，这就是中心极限定理首次被证明的背景。所以这项业务可以追溯到几百年前。</p><p>It turns out that this gives a very good approximation of the PMF of the binomial. And actually, this was the
            context in which the central limit theorem was proved in the first place, when this business started. So
            this
            business goes back a few hundred years.</p>
        <h2 id="unknown-399">未知</h2><h2>Unknown</h2>
        <p>中心极限定理首先通过考虑 p 等于 1/2 时二项式随机变量的 PMF 得到认可。人们进行了代数运算，发现 PMF 的精确表达式与从正态分布中得到的表达式非常接近。然后，证明扩展到 p 的更一般值的二项式。&nbsp;</p><p>And the central limit theorem was first approved by considering the PMF of a binomial random variable when p
            is
            equal to 1/2. People did the algebra, and they found out that the exact expression for the PMF is quite well
            approximated by that expression hat you would get from a normal distribution. Then the proof was extended to
            binomials for more general values of p.&nbsp;</p>
        <p>因此，我们在这里将其作为一般中心极限定理的改进来讨论，但是，从历史上看，这种改进是整个业务的开始。</p><p>So here we talk about this as a refinement of the general central limit theorem, but, historically, that
            refinement was where the whole business got started in the first place.</p>
        <p>好吧，让我们来看看 Sn 等于 19 的概率的近似机制。正好是 19。正如我们所说，我们将把这个事件写成一个涵盖从 18.5 到 19.5 单位长度间隔的事件。这是感兴趣的事件。第一步是调整感兴趣的事件，使其涉及我们的 Zn 随机变量。所以从所有边中减去 18。</p><p>All right, so let’s go through the mechanics of approximating the probability that Sn is equal to 19. exactly
            19.
            As we said, we’re going to write this event as an event that covers an interval of unit length from 18 and
            1/2
            to 19 and 1/2. This is the event of interest. First step is to massage the event of interest so that it
            involves
            our Zn random variable. So subtract 18 from all sides.</p>
        <h2 id="unknown-400">未知</h2><h2>Unknown</h2>
        <p>除以所有边的标准差 3。这就是事件的等效表示。这是我们标准化的随机变量 Zn。这些只是这些数字。为了进行近似，我们想要找到这个事件的概率，但 Zn 近似为正态分布，因此我们在这里代入 Z，即标准正态分布。因此，我们想要找到标准正态分布落入这个区间的概率。</p><p>Divide by the standard deviation of 3 from all sides. That’s the equivalent representation of the event. This
            is
            our standardized random variable Zn. These are just these numbers. And to do an approximation, we want to
            find
            the probability of this event, but Zn is approximately normal, so we plug in here the Z, which is the
            standard
            normal. So we want to find the probability that the standard normal falls inside this interval.</p>
        <p>您可以使用 CDF 找到这些，因为这是小于这个但不小于那个的概率。所以它是两个累积概率之间的差值。然后，查找常规表格。找到这些数量的两个数字，最后，得到二项式 PMF 单个条目的数值答案。事实证明，这是一个相当不错的近似值。</p><p>You find these using CDFs because this is the probability that you’re less than this but not less than that.
            So
            it’s a difference between two cumulative probabilities. Then, you look up your normal tables. You find two
            numbers for these quantities, and, finally, you get a numerical answer for an individual entry of the PMF of
            the
            binomial. This is a pretty good approximation, it turns out.</p>
        <p>如果您使用精确公式进行计算，您将得到非常接近的结果。第三位数字有误差。这非常好。所以我想我们在这里讨论二项式时所做的与我之前所说的略有矛盾。中心极限定理是关于累积分布函数的陈述。一般来说，它不会告诉您如何近似 PMF 本身。</p><p>If you were to do the calculations using the exact formula, you would get something which is pretty close. an
            error in the third digit. this is pretty good. So I guess what we did here with our discussion of the
            binomial
            slightly contradicts what I said before. that the central limit theorem is a statement about cumulative
            distribution functions. In general, it doesn’t tell you what to do to approximate PMFs themselves.</p>
        <h2 id="unknown-401">未知</h2><h2>Unknown</h2>
        <p>总体而言确实如此。另一方面，对于二项分布的特殊情况，中心极限定理近似，加上 1/2 校正，即使对于单个 PMF 来说也是一个非常好的近似。好吧，我们在力学上花了不少时间。所以今天让我们花最后几分钟思考一下，看看一个小谜题。谜题如下。</p><p>And that’s indeed the case in general. One the other hand, for the special case of a binomial distribution,
            the
            central limit theorem approximation, with this 1/2 correction, is a very good approximation even for the
            individual PMF. All right, so we spent quite a bit of time on mechanics. So let’s spend the last few minutes
            today thinking a bit and look at a small puzzle. So the puzzle is the following.</p>
        <p>考虑在单位间隔内运行的泊松过程。到达率等于 1。所以这是单位间隔。令 X 为到达次数。这是泊松分布，平均值为 1。现在，让我把这个间隔分成 n 个小部分。所以每个部分的长度为 1/n。令 Xi 为第 I 个小间隔内的到达次数。</p><p>Consider Poisson process that runs over a unit interval. And where the arrival rate is equal to 1. So this is
            the
            unit interval. And let X be the number of arrivals. And this is Poisson, with mean 1. Now, let me take this
            interval and divide it into n little pieces. So each piece has length 1/n.&nbsp;And let Xi be the number of
            arrivals
            during the Ith little interval.</p>
        <p>好的，我们对随机变量 Xi 了解多少？它们本身是泊松分布的。它是在一小段时间间隔内到达的次数。我们还知道，当 n 很大时，间隔的长度很短，这些 Xi 近似于伯努利分布，平均值为 1/n。我想我们是否将它们建模为伯努利分布并不重要。重要的是 Xi 是独立的。为什么它们是独立的？</p><p>OK, what do we know about the random variables Xi? Is they are themselves Poisson. It’s a number of arrivals
            during a small interval. We also know that when n is big, so the length of the interval is small, these Xi’s
            are
            approximately Bernoulli, with mean 1/n.&nbsp;Guess it doesn’t matter whether we model them as Bernoulli or not.
            What
            matters is that the Xi’s are independent. Why are they independent?</p>
        <h2 id="unknown-402">未知</h2><h2>Unknown</h2>
        <p>因为在泊松过程中，这些联合间隔是相互独立的。所以 Xi 是独立的。它们也具有相同的分布。并且我们有 X，即到达的总人数，是 Xn 的总和。因此，中心极限定理告诉我们，当我们有大量这些随机变量时，独立、同分布的随机变量的总和大致表现得像一个正态随机变量。</p><p>Because, in a Poisson process, these joint intervals are independent of each other. So the Xi’s are
            independent.
            And they also have the same distribution. And we have that X, the total number of arrivals, is the sum over
            the
            Xn’s. So the central limit theorem tells us that, approximately, the sum of independent, identically
            distributed
            random variables, when we have lots of these random variables, behaves like a normal random variable.</p>
        <p>因此，通过将 X 分解为 iid 随机变量的和，并使用越来越大的 n 值，通过取极限，应该可以得出 X 服从正态分布。另一方面，我们知道 X 服从泊松分布。所以这里的论证一定有问题。在这种情况下，我们真的可以使用中心极限定理吗？</p><p>So by using this decomposition of X into a sum of i.i.d random variables, and by using values of n that are
            bigger and bigger, by taking the limit, it should follow that X has a normal distribution. On the other
            hand, we
            know that X has a Poisson distribution. So something must be wrong in this argument here. Can we really use
            the
            central limit theorem in this situation?</p>
        <p>那么，中心极限定理需要什么呢？我们需要有独立的、相同分布的随机变量。我们这里就有了。我们希望它们具有有限的均值和有限的方差。我们这里也有，均值方差是有限的。另一个从未明确提出但本质上存在的假设是什么？或者换句话说，这里使用中心极限定理的论证有什么缺陷？有什么想法吗？</p><p>So what do we need for the central limit theorem? We need to have independent, identically distributed random
            variables. We have it here. We want them to have a finite mean and finite variance. We also have it here,
            means
            variances are finite. What is another assumption that was never made explicit, but essentially was there? Or
            in
            other words, what is the flaw in this argument that uses the central limit theorem here? Any thoughts?</p>
        <h2 id="unknown-403">未知</h2><h2>Unknown</h2>
        <p>因此，在中心极限定理中，我们说，考虑。固定一个概率分布，让 Xi 按照该概率分布分布，并添加越来越大的 Xi 数量。但潜在的、未说明的假设是，我们固定了 Xi 的分布。当我们让 n 增加时，每个 Xi 的统计数据不会改变。而在这里，我在捉弄你。</p><p>So in the central limit theorem, we said, consider. fix a probability distribution, and let the Xi’s be
            distributed according to that probability distribution, and add a larger and larger number or Xi’s. But the
            underlying, unstated assumption is that we fix the distribution of the Xi’s. As we let n increase, the
            statistics of each Xi do not change. Whereas here, I’m playing a trick on you.</p>
        <p>随着我取越来越多的随机变量，我实际上正在改变这些随机变量。当我取更大的 n 时，Xi 是具有不同均值和不同方差的随机变量。所以我添加了更多这样的变量，但同时，在这个例子中，我正在改变它们的分布。这不符合中心极限定理的设定。在中心极限定理中，你首先要固定 X 的分布。</p><p>As I’m taking more and more random variables, I’m actually changing what those random variables are. When I
            take
            a larger n, the Xi’s are random variables with a different mean and different variance. So I’m adding more
            of
            these, but at the same time, in this example, I’m changing their distributions. That’s something that
            doesn’t
            fit the setting of the central limit theorem. In the central limit theorem, you first fix the distribution
            of
            the X’s.</p>
        <p>你先让它固定，然后根据特定的固定分布考虑增加更多。这就是问题所在。这就是中心极限定理不适用于这种情况的原因。我们很幸运它不适用，因为否则，我们将面临一个巨大的矛盾，破坏概率论。好的，但现在这仍然给我们留下了一点困境。假设，在这里，我们本质上是在添加独立的伯努利随机变量。</p><p>You keep it fixed, and then you consider adding more and more according to that particular fixed
            distribution. So
            that’s the catch. That’s why the central limit theorem does not apply to this situation. And we’re lucky
            that it
            doesn’t apply because, otherwise, we would have a huge contradiction destroying probability theory. OK, but
            now
            that’s still leaves us with a little bit of a dilemma. Suppose that, here, essentially we’re adding
            independent
            Bernoulli random variables.</p>
        <h2 id="unknown-404">未知</h2><h2>Unknown</h2>
        <p>因此，问题在于中心极限定理与 n 趋于无穷时的渐近线有关。如果我们考虑一个二项式，有人给我们关于该二项式参数的具体数字，那么我们使用哪种近似值可能并不明显。具体来说，我们对二项式有两种不同的近似值。</p><p>So the issue is that the central limit theorem has to do with asymptotics as n goes to infinity. And if we
            consider a binomial, and somebody gives us specific numbers about the parameters of that binomial, it might
            not
            necessarily be obvious what kind of approximation do we use. In particular, we do have two different
            approximations for the binomial.</p>
        <p>如果我们固定 p，那么二项式就是来自固定分布的伯努利函数之和，我们会考虑越来越多的伯努利函数。当我们将它们相加时，中心极限定理告诉我们，我们得到的是正态分布。还有另一种极限，它具有此示例的风格，其中我们仍然处理二项式，即 n 个伯努利函数之和。我们让该和，即伯努利函数的数量趋于无穷大。</p><p>If we fix p, then the binomial is the sum of Bernoulli’s that come from a fixed distribution, we consider
            more
            and more of these. When we add them, the central limit theorem tells us that we get the normal distribution.
            There’s another sort of limit, which has the flavor of this example, in which we still deal with a binomial,
            sum
            of n Bernoulli’s. We let that sum, the number of the Bernoulli’s go to infinity.</p>
        <p>但是每个伯努利的成功概率都趋近于 0，我们这样做的目的是使 np（期望的成功次数）保持有限。这是我们第一次定义泊松过程时处理的情况。我们有很多时间段，但在每个时间段内，到达的概率都很小。</p><p>But each Bernoulli has a probability of success that goes to 0, and we do this in a way so that np, the
            expected
            number of successes, stays finite. This is the situation that we dealt with when we first defined our
            Poisson
            process. We have a very, very large number so lots, of time slots, but during each time slot, there’s a tiny
            probability of obtaining an arrival.</p>
        <h2 id="unknown-405">未知</h2><h2>Unknown</h2>
        <p>在这种设置下，在离散时间中，我们有一个二项分布，或伯努利过程，但当我们取极限时，我们得到泊松过程和泊松近似。因此，这是二项式的两个同样有效的近似值。但它们在不同的渐近状态下有效。在一种状态下，我们固定 p，让 n 趋于无穷大。在另一种状态下，我们让 n 和 p 同时变化。</p><p>Under that setting, in discrete time, we have a binomial distribution, or Bernoulli process, but when we take
            the
            limit, we obtain the Poisson process and the Poisson approximation. So these are two equally valid
            approximations of the binomial. But they’re valid in different asymptotic regimes. In one regime, we fixed
            p,
            let n go to infinity. In the other regime, we let both n and p change simultaneously.</p>
        <p>现在，在现实生活中，你永远不会处理限制情况。你处理的是实际数字。所以如果有人告诉你数字是这样的，那么你应该说这就是符合泊松描述的情况。大量的插槽，每个插槽的成功概率都很小。</p><p>Now, in real life, you’re never dealing with the limiting situations. You’re dealing with actual numbers. So
            if
            somebody tells you that the numbers are like this, then you should probably say that this is the situation
            that
            fits the Poisson description. large number of slots with each slot having a tiny probability of success.</p>
        <p>另一方面，如果 p 是这样的，而 n 是 500，那么你期望得到成功次数的分布。它的平均值为 50，并且有相当大的分布。事实证明，在这种情况下，正态近似会更好。根据经验，如果 n 乘以 p 大于 10 或 20，你可以开始使用正态近似。</p><p>On the other hand, if p is something like this, and n is 500, then you expect to get the distribution for the
            number of successes. It’s going to have a mean of 50 and to have a fair amount of spread around there. It
            turns
            out that the normal approximation would be better in this context. As a rule of thumb, if n times p is
            bigger
            than 10 or 20, you can start using the normal approximation.</p>
        <h2 id="unknown-406">未知</h2><h2>Unknown</h2>
        <p>如果 n 乘以 p 是一个小数，那么你最好使用泊松近似。但是没有关于如何做到这一点的硬定理或规则。好的，所以从下次开始我们将再次切换基础。我们将把我们在这门课上学到的所有内容放在一起，开始解决推理问题。</p><p>If n times p is a small number, then you prefer to use the Poisson approximation. But there’s no hard
            theorems or
            rules about how to go about this. OK, so from next time we’re going to switch base again. And we’re going to
            put
            together everything we learned in this class to start solving inference problems.</p>
        <h1 id="bayesian-statistical-inference-i">21. 贝叶斯统计推断 I</h1><h1>21. Bayesian Statistical Inference I</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 48:50 <a href="https://youtube.com/watch?v=1jDBM9UM9xk">https://youtube.com/watch?v=1jDBM9UM9xk</a></p><p> 12 years ago (Nov 10, 2012) — 48:50 <a href="https://youtube.com/watch?v=1jDBM9UM9xk">https://youtube.com/watch?v=1jDBM9UM9xk</a></p>
        <h2 id="unknown-407">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：它涉及真实现象。所以我们有真实发生的事情。所以这可能是我们试图模拟的银行到达过程。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: It involves real phenomena
            out
            there. So we have real stuff that happens. So it might be an arrival process to a bank that we’re trying to
            model.</p>
        <p>这是现实，但这是我们迄今为止一直在做的事情。我们一直在研究概率现象的模型。我们需要以某种方式将两者联系起来。将它们联系起来的方式是，我们观察现实世界，这为我们提供数据。然后，基于这些数据，我们试图提出一个模型来说明到底发生了什么。</p><p>This is a reality, but this is what we have been doing so far. We have been playing with models of
            probabilistic
            phenomena. And somehow we need to tie the two together. The way these are tied is that we observe the real
            world
            and this gives us data. And then based on these data, we try to come up with a model of what exactly is
            going
            on.</p>
        <p>例如，对于到达过程，您可能会问相关模型，我的到达过程是泊松分布还是其他分布？如果是泊松分布，那么到达过程的速率是多少？</p><p>For example, for an arrival process, you might ask the model in question, is my arrival process Poisson or is
            it
            something different? If it is Poisson, what is the rate of the arrival process?</p>
        <h2 id="unknown-408">未知</h2><h2>Unknown</h2>
        <p>一旦你想出了模型，并确定了模型的参数，那么你就可以用它来预测现实，或者找出某些隐藏的东西，某些隐藏的现实方面，你无法直接观察到，但你可以尝试推断它们是什么。这就是模型的用处所在。现在这个领域当然非常有用。它几乎无处不在。</p><p>Once you come up with your model and you come up with the parameters of the model, then you can use it to
            make
            predictions about reality or to figure out certain hidden things, certain hidden aspects of reality, that
            you do
            not observe directly, but you try to infer what they are. So that’s where the usefulness of the model comes
            in.
            Now this field is of course tremendously useful. And it shows up pretty much everywhere.</p>
        <p>所以我们在过去的几节课中讨论了民意调查的例子。这当然是一个实际的应用。你抽样，并根据你拥有的样本，尝试对给定人群的偏好做出一些推断。比如在医学领域，你想尝试某种药物是否会产生影响。</p><p>So we talked about the polling examples in the last couple of lectures. This is, of course, a real
            application.
            You sample and on the basis of the sample that you have, you try to make some inferences about, let’s say,
            the
            preferences in a given population. Let’s say in the medical field, you want to try whether a certain drug
            makes
            a difference or not.</p>
        <p>因此，人们会进行医学试验，得到一些结果，然后你需要从数据中以某种方式理解它们并做出决定。新药有用还是没用？我们如何系统地解决这类问题？一个更吸引人、更近期的话题是著名的 Netflix 竞赛，Netflix 会为你提供一个巨大的电影和人物列表。</p><p>So people would do medical trials, get some results, and then from the data somehow you need to make sense of
            them and make a decision. Is the new drug useful or is it not? How do we go systematically about the
            question of
            this type? A sexier, more recent topic, there’s this famous Netflix competition where Netflix gives you a
            huge
            table of movies and people.</p>
        <h2 id="unknown-409">未知</h2><h2>Unknown</h2>
        <p>人们已经对电影进行了评分，但并不是每个人都看过其中的所有电影。您有一些评分。例如，这个人给那部电影打了 4 分。所以您得到的表格部分填满了。Netflix 要求您向人们提出建议。这意味着尝试猜测。这个人会有多喜欢这部特定的电影？</p><p>And people have rated the movies, but not everyone has watched all of the movies in there. You have some of
            the
            ratings. For example, this person gave a 4 to that particular movie. So you get the table that’s partially
            filled. And the Netflix asks you to make recommendations to people. So this means trying to guess. This
            person
            here, how much would they like this particular movie?</p>
        <p>你可以开始想，也许这个人给出的评分和另一个人有些相似。如果另一个人也看过这部电影，那么另一个人的评分可能与此有关。但当然，事情要复杂得多。这是一场激烈的竞争，人们一直在使用统计学中所有沉重、枯燥的机器，试图想出好的推荐系统。
        </p><p>And you can start thinking, well, maybe this person has given somewhat similar ratings with another person.
            And
            if that other person has also seen that movie, maybe the rating of that other person is relevant. But of
            course
            it’s a lot more complicated than that. And this has been a serious competition where people have been using
            every heavy, wet machinery that there is in statistics, trying to come up with good recommendation systems.
        </p>
        <p>然后，其他人当然会尝试分析财务数据。有人会给你数值序列，比如说 SMP 指数。你看到这样的数据，就可以提出问题。如何使用我们工具包中的任何模型来对这些数据进行建模？如何预测之后会发生什么，等等？</p><p>Then the other people, of course, are trying to analyze financial data. Somebody gives you the sequence of
            the
            values, let’s say of the SMP index. You look at something like this and you can ask questions. How do I
            model
            these data using any of the models that we have in our bag of tools? How can I make predictions about what’s
            going to happen afterwards, and so on?</p>
        <h2 id="unknown-410">未知</h2><h2>Unknown</h2>
        <p>在工程方面，任何有噪声的地方都会产生推断。从某种意义上说，信号处理只是一个推断问题。你观察嘈杂的信号，并试图弄清楚那里到底发生了什么，或者发送了什么样的信号。也许这个领域的起源可以追溯到几百年前，当时人们会观察、对天空中行星的位置进行天文观测。
        </p><p>On the engineering side, anywhere where you have noise inference comes in. Signal processing, in some sense,
            is
            just an inference problem. You observe signals that are noisy and you try to figure out exactly what’s
            happening
            out there or what kind of signal has been sent. Maybe the beginning of the field could be traced a few
            hundred
            years ago where people would observe, make astronomical observations of the position of the planets in the
            sky.
        </p>
        <p>他们可能认为行星的轨道是椭圆形的。如果是彗星，那可能是抛物线、双曲线，不知道具体是什么。但他们应该有一个模型。当然，天文测量结果并不完全准确。他们会尝试找到符合这些数据的曲线。</p><p>They would have some beliefs that perhaps the orbits of planets is an ellipse. Or if it’s a comet, maybe it’s
            a
            parabola, hyperbola, don’t know what it is. But they would have a model of that. But, of course,
            astronomical
            measurements would not be perfectly exact. And they would try to find the curve that fits these data.</p>
        <p>如何根据嘈杂的数据选择这条特定曲线，并尝试以某种原则性的方式做到这一点？好的，所以这类问题。显然，应用无处不在。但这与我们迄今为止所做的概念上有何关联？到目前为止，我们一直在实践的推理领域和概率领域之间的关系是什么？</p><p>How do you go about choosing this particular curve on the base of noisy data and try to do it in a somewhat
            principled way? OK, so questions of this type. clearly the applications are all over the place. But how is
            this
            related conceptually with what we have been doing so far? What’s the relation between the field of inference
            and
            the field of probability as we have been practicing until now?</p>
        <h2 id="unknown-411">未知</h2><h2>Unknown</h2>
        <p>嗯，从数学上讲，根据我们目前所做的，接下来几节课的内容可能只是课堂上的练习或家庭作业问题。这意味着你不会学到任何有关概率论的新事实。我们要做的一切只是对你已经知道的事情的简单应用。所以从某种意义上说，统计和推理只是概率的应用练习。</p><p>Well, mathematically speaking, what’s going to happen in the next few lectures could be just exercises or
            homework problems in the class in based on what we have done so far. That means you’re not going to get any
            new
            facts about probability theory. Everything we’re going to do will be simple applications of things that you
            already do know. So in some sense, statistics and inference is just an applied exercise in probability.</p>
        <p>但实际上，事情并没有那么简单。如果你遇到一个概率问题，那么就会有一个正确的答案。就会有一个正确的解决方案。而且这个正确的解决方案是唯一的。不存在歧义。概率论有明确的规则。这些是公理。你会得到一些关于概率分布的信息。你被要求计算某些其他的东西。不存在歧义。答案总是唯一的。</p><p>But actually, things are not that simple in the following sense. If you get a probability problem, there’s a
            correct answer. There’s a correct solution. And that correct solution is unique. There’s no ambiguity. The
            theory of probability has clearly defined rules. These are the axioms. You’re given some information about
            probability distributions. You’re asked to calculate certain other things. There’s no ambiguity. Answers are
            always unique.</p>
        <p>在统计问题中，问题不再有唯一的答案。如果我给你数据，并问你估算行星运动的最佳方法是什么，理性的人可以想出不同的方法。理性的人会试图争辩说我的方法具有这些理想的特性，但其他人可能会说，这是另一种具有某些理想特性的方法。最好的方法是什么并不清楚。</p><p>In statistical questions, it’s no longer the case that the question has a unique answer. If I give you data
            and I
            ask you what’s the best way of estimating the motion of that planet, reasonable people can come up with
            different methods. And reasonable people will try to argue that’s my method has these desirable properties
            but
            somebody else may say, here’s another method that has certain desirable properties. And it’s not clear what
            the
            best method is.</p>
        <h2 id="unknown-412">未知</h2><h2>Unknown</h2>
        <p>因此，最好对问题有所了解，至少要知道人们试图考虑的一般方法是什么，如何解决这些问题。所以我们将会看到很多不同的推理方法。我们不会告诉你哪一种比另一种更好。但重要的是要了解这些不同方法之间的概念是什么。</p><p>So it’s good to have some understanding of what the issues are and to know at least what is the general class
            of
            methods that one tries to consider, how does one go about such problems. So we’re going to see lots and lots
            of
            different inference methods. We’re not going to tell you that one is better than the other. But it’s
            important
            to understand what are the concepts between those different methods.</p>
        <p>最后，统计数据可能会被严重滥用。也就是说，人们可能会想出一些你认为合理但实际上并不合理的方法。下次我会举一些例子，并进一步讨论这个问题。</p><p>And finally, statistics can be misused really badly. That is, one can come up with methods that you think are
            sound, but in fact they’re not quite that. I will bring some examples next time and talk a little more about
            this.</p>
        <p>所以，他们想说，你有一些数据，你想从中得出一些推论，许多人会做的是去维基百科，找到他们认为适用于这种情况的统计测试，插入数字，然后呈现结果。他们得到的结论真的合理吗？还是他们误用了统计方法？事实上，太多人确实误用了统计数据，人们得到的结论往往是错误的。</p><p>So, they want to say, you have some data, you want to make some inference from them, what many people will do
            is
            to go to Wikipedia, find a statistical test that they think it applies to that situation, plug in numbers,
            and
            present results. Are the conclusions that they get really justified or are they misusing statistical
            methods?
            Well, too many people actually do misuse statistics and conclusions that people get are often false.</p>
        <h2 id="unknown-413">未知</h2><h2>Unknown</h2>
        <p>因此，除了能够复制统计测试并使用它们之外，重要的是要了解不同方法之间的假设是什么，以及它们具有什么样的保证（如果有的话）。</p><p>So it’s important to, besides just being able to copy statistical tests and use them, to understand what are
            the
            assumptions between the different methods and what kind of guarantees they have, if any.</p>
        <p>好的，我们将在本学期剩下的几节课中快速浏览一下推理领域，并尽量从高层次上强调主要的概念、技能和技巧。让我们从一些概括和一般性陈述开始。第一个陈述是，统计或推理问题以非常不同的形式出现。</p><p>All right, so we’ll try to do a quick tour through the field of inference in this lecture and the next few
            lectures that we have left this semester and try to highlight at the very high level the main concept
            skills,
            and techniques that come in. Let’s start with some generalities and some general statements. One first
            statement
            is that statistics or inference problems come up in very different guises.</p>
        <p>它们看起来可能形式迥然不同。但从根本上讲，基本问题总是大同小异。让我们来看这个例子。有一个未知信号正在被发送。它通过某种介质发送，而该介质只是接收信号并将其放大一定倍数。所以你可以想象有人在喊叫。那里有空气。</p><p>And they may look as if they are of very different forms. Although, at some fundamental level, the basic
            issues
            turn out to be always pretty much the same. So let’s look at this example. There’s an unknown signal that’s
            being sent. It’s sent through some medium, and that medium just takes the signal and amplifies it by a
            certain
            number. So you can think of somebody shouting. There’s the air out there.</p>
        <h2 id="unknown-414">未知</h2><h2>Unknown</h2>
        <p>你喊的声音会在空气中衰减，直到到达接收器。然后接收器会观察到这个声音，但会伴随一些随机噪音。这里我指的是 S。S 是正在发送的信号。你观察到的是 X。你观察到 X，那么我们在这里会遇到什么样的推理问题呢？在某些情况下，你想建立一个你正在处理的物理现象的模型。</p><p>What you shouted will be attenuated through the air until it gets to a receiver. And that receiver then
            observes
            this, but together with some random noise. Here I meant S. S is the signal that’s being sent. And what you
            observe is an X. You observe X, so what kind of inference problems could we have here? In some cases, you
            want
            to build a model of the physical phenomenon that you’re dealing with.</p>
        <p>例如，你不知道信号的衰减，你试图根据你观察到的情况找出这个数字。所以在工程系统中，这样做的方式是，你设计一个特定的信号，你知道它是什么，你喊出一个特定的词，然后接收者听到。根据他们得到的信号的强度，他们试图猜测 A。</p><p>So for example, you don’t know the attenuation of your signal and you try to find out what this number is
            based
            on the observations that you have. So the way this is done in engineering systems is that you design a
            certain
            signal, you know what it is, you shout a particular word, and then the receiver listens. And based on the
            intensity of the signal that they get, they try to make a guess about A.</p>
        <p>所以你不知道 A，但知道 S。通过观察 X，你可以获得一些关于 A 的信息。所以在这种情况下，你试图建立一个信号传播介质的模型。所以有时人们会把这种问题称为系统识别。在这张图附带的推理问题的另一个版本中，你已经完成了建模。你知道你的 A。</p><p>So you don’t know A, but you know S. And by observing X, you get some information about what A is. So in this
            case, you’re trying to build a model of the medium through which your signal is propagating. So sometimes
            one
            would call problems of this kind, let’s say, system identification. In a different version of an inference
            problem that comes with this picture, you’ve done your modeling. You know your A.</p>
        <h2 id="unknown-415">未知</h2><h2>Unknown</h2>
        <p>您知道信号通过的介质，但这是一个通信系统。这个人试图向那个人传达一些东西。因此，您发送信号 S，但那个人收到的是噪声版的 S。因此，那个人试图根据 X 重建 S。因此，在这两种情况下，X 和未知量之间都存在线性关系。在一种情况下，A 是未知量，而我们知道 S。</p><p>You know the medium through which the signal is going, but it’s a communication system. This person is trying
            to
            communicate something to that person. So you send the signal S, but that person receives a noisy version of
            S.
            So that person tries to reconstruct S based on X. So in both cases, we have a linear relation between X and
            the
            unknown quantity. In one version, A is the unknown and we know S.</p>
        <p>在另一个版本中，A 是已知的，因此我们尝试推断 S。从数学上讲，您可以看到，在这两种情况下，这本质上是同一类问题。尽管您尝试解决的实际问题类型略有不同。因此，我们不会区分模型构建类型的问题与您尝试估计某些未知信号的模型等。</p><p>In the other version, A is known, and so we try to infer S. Mathematically, you can see that this is
            essentially
            the same kind of problem in both cases. Although, the kind of practical problem that you’re trying to solve
            is a
            little different. So we will not be making any distinctions between problems of the model building type as
            opposed to models where you try to estimate some unknown signal and so on.</p>
        <p>因为从概念上讲，解决这两类问题所使用的工具本质上是相同的。好的，接下来是一个非常有用的推理问题分类。您要估计的未知量可以是离散量，取值较少。所以这可能是离散问题，例如我们很久以前在这门课上遇到的飞机雷达问题。所以有两种可能性。</p><p>Because conceptually, the tools that one uses for both types of problems are essentially the same. OK, next a
            very useful classification of inference problems. the unknown quantity that you’re trying to estimate could
            be
            either a discrete one that takes a small number of values. So this could be discrete problems, such as the
            airplane radar problem we encountered back a long time ago in this class. So there’s two possibilities.</p>
        <h2 id="unknown-416">未知</h2><h2>Unknown</h2>
        <p>飞机在那里，或者飞机不在那里。你试图在这两个选项之间做出决定。或者你可能遇到其他问题，比如说有四个可能的选择。你不知道哪一个是正确的，但你得到数据并试图找出哪一个是正确的。在这类问题中，通常你想根据数据做出决定。</p><p>An airplane is out there or an airplane is not out there. And you’re trying to make a decision between these
            two
            options. Or you can have other problems would you have, let’s say, four possible options. You don’t know
            which
            one is true, but you get data and you try to figure out which one is true. In problems of these kind,
            usually
            you want to make a decision based on your data.</p>
        <p>你感兴趣的是做出正确决定的概率。你希望这个概率尽可能高。估算问题有点不同。这里你有一些未知的连续量。你试图对这个量做出一个很好的猜测。你希望你的猜测尽可能接近真实量。所以民意调查问题就是这种类型。</p><p>And you’re interested in the probability of making a correct decision. You would like that probability to be
            as
            high as possible. Estimation problems are a little different. Here you have some continuous quantity that’s
            not
            known. And you try to make a good guess of that quantity. And you would like your guess to be as close as
            possible to the true quantity. So the polling problem was of this type.</p>
        <p>有一部分未知的人口具有某种财产。你试图尽可能准确地估计 f。因此，这里的区别在于，通常这里的未知量采用离散值集。这里未知量采用连续值集。这里我们感兴趣的是误差概率。这里我们感兴趣的是误差的大小。</p><p>There was an unknown fraction f of the population that had some property. And you try to estimate f as
            accurately
            as you can. So the distinction here is that usually here the unknown quantity takes on discrete set of
            values.
            Here the unknown quantity takes a continuous set of values. Here we’re interested in the probability of
            error.
            Here we’re interested in the size of the error.</p>
        <h2 id="unknown-417">未知</h2><h2>Unknown</h2>
        <p>广义上讲，大多数推理问题要么属于这一类，要么属于那一类。不过，如果你想让生活变得复杂，你也可以思考或构建同时存在这两个方面的问题。好吧，最后，既然我们处于分类模式，那么在处理推理问题时，存在一个非常重要的二分法。这里有两个根本不同的哲学观点，即我们如何对未知量进行建模？</p><p>Broadly speaking, most inference problems fall either in this category or in that category. Although, if you
            want
            to complicate life, you can also think or construct problems where both of these aspects are simultaneously
            present. OK, finally since we’re in classification mode, there is a very big, important dichotomy into how
            one
            goes about inference problems. And here there’s two fundamentally different philosophical points of view,
            which
            is how do we model the quantity that is unknown?</p>
        <p>一种方法是说，存在一个具有确定值的量。他们只是碰巧不知道。但这是一个数字。它没有任何随机性。所以想象一下尝试估计某个物理量。你正在进行测量，尝试估计电子的质量，这是一种通用物理常数。它没有任何随机性。它是一个固定的数字。你得到数据，因为你有一些测量仪器。</p><p>In one approach, you say there’s a certain quantity that has a definite value. It just happens that they
            don’t
            know it. But it’s a number. There’s nothing random about it. So think of trying to estimate some physical
            quantity. You’re making measurements, you try to estimate the mass of an electron, which is a sort of
            universal
            physical constant. There’s nothing random about it. It’s a fixed number. You get data, because you have some
            measuring apparatus.</p>
        <p>测量仪器，取决于你得到的结果，会受到电子真实质量的影响，但也有一些噪音。你从测量仪器中取出数据，然后试着估算出这个量 theta。所以这绝对是一张合法的图片，但这张图中重要的是这个 theta 是用小写字母写的。</p><p>And that measuring apparatus, depending on what that results that you get are affected by the true mass of
            the
            electron, but there’s also some noise. You take the data out of your measuring apparatus and you try to come
            up
            with some estimate of that quantity theta. So this is definitely a legitimate picture, but the important
            thing
            in this picture is that this theta is written as lowercase.</p>
        <h2 id="unknown-418">未知</h2><h2>Unknown</h2>
        <p>这就是说它是一个实数，而不是一个随机变量。还有一种不同的哲学方法，即，好吧，任何我不知道的东西，我都应该把它建模为一个随机变量。是的，我知道。电子的质量并不是真正随机的。它是一个常数。但我不知道它是什么。我可能对它有什么模糊的感觉，也许是因为其他人做过实验。</p><p>And that’s to make the point that it’s a real number, not a random variable. There’s a different
            philosophical
            approach which says, well, anything that I don’t know I should model it as a random variable. Yes, I know.
            The
            mass of the electron is not really random. It’s a constant. But I don’t know what it is. I have some vague
            sense, perhaps, what it is perhaps because of the experiments that some other people carried out.</p>
        <p>因此，我可能对 Theta 的可能值有一个先验分布。这个先验分布并不意味着性质是随机的，而更像是我对这个常数恰好在哪里的主观信念的主观描述。因此，即使它不是真正随机的，我也会在实验开始前对我的初始信念进行建模。就先验分布而言，我将其视为随机变量。</p><p>So perhaps I have a prior distribution on the possible values of Theta. And that prior distribution doesn’t
            mean
            that the nature is random, but it’s more of a subjective description of my subjective beliefs of where do I
            think this constant number happens to be. So even though it’s not truly random, I model my initial beliefs
            before the experiment starts. In terms of a prior distribution, I view it as a random variable.</p>
        <p>然后我通过一些测量仪器观察另一个相关的随机变量。然后我再次使用它来创建一个估计值。所以从哲学上讲，这两幅图彼此非常不同。在这里，我们将未知量视为未知数。在这里，我们将它们视为随机变量。当我们将它们视为随机变量时，我们几乎已经知道我们应该做什么。我们应该只使用贝叶斯规则。</p><p>Then I observe another related random variable through some measuring apparatus. And then I use this again to
            create an estimate. So these two pictures philosophically are very different from each other. Here we treat
            the
            unknown quantities as unknown numbers. Here we treat them as random variables. When we treat them as a
            random
            variables, then we know pretty much already what we should be doing. We should just use the Bayes rule.</p>
        <h2 id="unknown-419">未知</h2><h2>Unknown</h2>
        <p>根据 X，找到 Theta 的条件分布。这就是我们在本讲和下一讲中主要要做的事情。现在在这两种情况下，你最终得到的是一个估计值。但实际上，这个估计值是什么类型的对象？在这两种情况下，它都是一个随机变量。为什么？即使在这个 theta 是常数的情况下，我的数据也是随机的。我进行数据处理。</p><p>Based on X, find the conditional distribution of Theta. And that’s what we will be doing mostly over this
            lecture
            and the next lecture. Now in both cases, what you end up getting at the end is an estimate. But actually,
            that
            estimate is what kind of object is it? It’s a random variable in both cases. Why? Even in this case where
            theta
            was a constant, my data are random. I do my data processing.</p>
        <p>所以我计算了数据的函数，数据是随机变量。所以在这里我们输出的是随机变量的函数。所以这里的量也是随机的。它受到噪音和我一直在做的实验的影响。这就是为什么这些估计量将用大写的 Thetas 表示。我们将使用帽子。帽子，通常在估计中，表示对某事物的估计。</p><p>So I calculate a function of the data, the data are random variables. So out here we output something which
            is a
            function of a random variable. So this quantity here will be also random. It’s affected by the noise and the
            experiment that I have been doing. That’s why these estimators will be denoted by uppercase Thetas. And we
            will
            be using hats. Hat, usually in estimation, means an estimate of something.</p>
        <p>好的，这就是大局。我们将从贝叶斯版本开始。然后在最后几节课中，我们将讨论非贝叶斯版本或经典版本。顺便说一句，我应该说，统计学家们已经激烈争论了 100 年，关于统计学的正确方法是采用经典方法还是贝叶斯方法。双方之间一直存在着反复的争论。
        </p><p>All right, so this is the big picture. We’re going to start with the Bayesian version. And then the last few
            lectures we’re going to talk about the non Bayesian version or the classical one. By the way, I should say
            that
            statisticians have been debating fiercely for 100 years whether the right way to approach statistics is to
            go
            the classical way or the Bayesian way. And there have been tides going back and forth between the two sides.
        </p>
        <h2 id="unknown-420">未知</h2><h2>Unknown</h2>
        <p>如今，贝叶斯方法由于各种原因而变得越来越流行。我们稍后会再讨论这个问题。好吧，在贝叶斯估计中，我们掌握的是贝叶斯规则。如果你有贝叶斯规则，那么剩下的事情就不多了。我们有不同形式的贝叶斯规则，这取决于我们处理的是离散数据，以及要估计的离散量，还是连续数据，等等。</p><p>These days, Bayesian methods tend to become a little more popular for various reasons. We’re going to come
            back
            to this later. All right, so in Bayesian estimation, what we got in our hands is Bayes rule. And if you have
            Bayes rule, there’s not a lot that’s left to do. We have different forms of the Bayes rule, depending on
            whether
            we’re dealing with discrete data, And discrete quantities to estimate, or continuous data, and so on.</p>
        <p>在假设检验问题中，未知量 Theta 是离散的。因此，在这两种情况下，我们都有一个 Theta 的 P。我们获得数据，即 X。根据我们观察到的 X，我们可以计算出给定数据的 Theta 的后验分布。那么，要使用贝叶斯推理，我们从哪里开始呢？我们从一些先验开始。这些是我们对 Theta 可能是什么的初步信念。</p><p>In the hypothesis testing problem, the unknown quantity Theta is discrete. So in both cases here, we have a P
            of
            Theta. We obtain data, the X’s. And on the basis of the X that we observe, we can calculate the posterior
            distribution of Theta, given the data. So to use Bayesian inference, what do we start with? We start with
            some
            priors. These are our initial beliefs about what Theta that might be.</p>
        <p>那是在我们进行实验之前。我们有一个实验仪器的模型。实验仪器的模型告诉我们，如果这个 Theta 是真的，我就会看到这种类型的 X。如果另一个 Theta 是真的，我就会看到 X 在其他地方。这就是我的仪器的模型。</p><p>That’s before we do the experiment. We have a model of the experimental aparatus. And the model of the
            experimental apparatus tells us if this Theta is true, I’m going to see X’s of that kind. If that other
            Theta is
            true, I’m going to see X’s that they are somewhere else. That models my apparatus.</p>
        <h2 id="unknown-421">未知</h2><h2>Unknown</h2>
        <p>基于这些知识，一旦我观察到这两个函数，我们已经看到，如果你知道这两个函数，你也可以计算出这里的分母。所以所有这些函数都是可用的，所以你可以计算，你也可以找到这个函数的公式。一旦你观察到数据，那个X，你就在这里代入那些X的数值。</p><p>And based on that knowledge, once I observe I have these two functions in my hands, we have already seen that
            if
            you know those two functions, you can also calculate the denominator here. So all of these functions are
            available, so you can compute, you can find a formula for this function as well. And as soon as you observe
            the
            data, that X’s, you plug in here the numerical value of those X’s.</p>
        <p>然后你得到了一个 Theta 函数。这是给定你所看到的数据后验分布。所以你已经做过很多这类练习了。所以我们就不多说了。对于连续数据的情况，有一个类似的公式。</p><p>And you get a function of Theta. And this is the posterior distribution of Theta, given the data that you
            have
            seen. So you’ve already done a fair number of exercises of these kind. So we not say more about this. And
            there’s a similar formula as you know for the case where we have continuous data.</p>
        <p>如果 X 是连续随机变量，那么公式是相同的，只是 X 由密度描述，而不是由概率质量函数描述。好的，现在如果 Theta 是连续的，那么我们要处理的是估计问题。但故事再次相同。您将使用贝叶斯规则根据您观察到的数据得出 Theta 的后验密度。</p><p>If the X’s are continuous random variable, then the formula is the same, except that X’s are described by
            densities instead of being described by a probability mass functions. OK, now if Theta is continuous, then
            we’re
            dealing with estimation problems. But the story is once more the same. You’re going to use the Bayes rule to
            come up with the posterior density of Theta, given the data that you have observed.</p>
        <h2 id="unknown-422">未知</h2><h2>Unknown</h2>
        <p>现在，为了举例，我们回到这张图片。假设有东西在空中飞行，也许这只是一个靠近地球的空中物体。由于重力，它所遵循的轨迹将是一条抛物线。所以这是抛物线的一般方程。Zt 是物体在时间 t 的位置。</p><p>Now just for the sake of the example, let’s come back to this picture here. Suppose that something is flying
            in
            the air, and maybe this is just an object in the air close to the Earth. So because of gravity, the
            trajectory
            that it’s going to follow it’s going to be a parabola. So this is the general equation of a parabola. Zt is
            the
            position of my objects at time t.</p>
        <p>但我不知道具体是哪条抛物线。所以抛物线的参数是未知量。我能做的就是去测量物体在不同时间的位置。但不幸的是，我的测量结果很嘈杂。我想要做的是模拟物体的运动。所以我猜在图中，轴 t 会朝这个方向，Z 会朝这个方向。</p><p>But I don’t know exactly which parabola it is. So the parameters of the parabola are unknown quantities. What
            I
            can do is to go and measure the position of my objects at different times. But unfortunately, my
            measurements
            are noisy. What I want to do is to model the motion of my object. So I guess in the picture, the axis would
            be t
            going this way and Z going this way.</p>
        <p>根据他们得到的数据，这些就是我的 X。我想计算出 Theta。也就是说，我想计算出这个抛物线的精确方程。现在如果有人给你 Theta 的概率分布，这些就是你的先验。所以这是给定的。我们需要给定 Theta 的 X 的条件分布。好吧，给定这个方程的 Theta，我们有 Z 的条件分布。</p><p>And on the basis of the data that they get, these are my X’s. I want to figure out the Thetas. That is, I
            want to
            figure out the exact equation of this parabola. Now if somebody gives you probability distributions for
            Theta,
            these would be your priors. So this is given. We need the conditional distribution of the X’s given the
            Thetas.
            Well, we have the conditional distribution of Z, given the Thetas from this equation.</p>
        <h2 id="unknown-423">未知</h2><h2>Unknown</h2>
        <p>然后通过运用这个方程，你还可以发现当 Theta 取特定值时 X 的分布情况。所以你确实得到了你可能需要的所有密度。你可以应用贝叶斯规则。最后，你的最终结果将是给定你观察到的 X 的 Theta 分布公式。除了一种计算，或者让事情变得更有趣。</p><p>And then by playing with this equation, you can also find how is X distributed if Theta takes a particular
            value.
            So you do have all of the densities that you might need. And you can apply the Bayes rule. And at the end,
            your
            end result would be a formula for the distribution of Theta, given to the X that you have observed. except
            for
            one sort of computation, or to make things more interesting.</p>
        <p>这些 X 和 Theta 并非我们这里所见的单个随机变量，通常这些 X 和 Theta 将是多维随机变量或将对应多个变量。因此，这里的小 Theta 实际上代表 Theta0、Theta1 和 Theta2 的三元组。这里的 X 代表我们观察到的整个 X 序列。</p><p>Instead of these X’s and Theta’s being single random variables that we have here, typically those X’s and
            Theta’s
            will be multi dimensional random variables or will correspond to multiple ones. So this little Theta here
            actually stands for a triplet of Theta0, Theta1, and Theta2. And that X here stands here for the entire
            sequence
            of X’s that we have observed.</p>
        <p>因此，实际上，在完成推理之后，您最终要得到的对象是一个函数，您插入数据的值，然后得到 Theta 函数，该函数会告诉您不同 Theta 三元组的相对可能性。所以，我想说的是，这并不比您迄今为止处理过的问题更难，除了可能在有趣的推理问题中通常会出现的复杂性。</p><p>So in reality, the object that you’re going to get at to the end after inference is done is a function that
            you
            plug in the values of the data and you get the function of the Theta’s that tells you the relative
            likelihoods
            of different Theta triplets. So what I’m saying is that this is no harder than the problems that you have
            dealt
            with so far, except perhaps for the complication that’s usually in interesting inference problems.</p>
        <h2 id="unknown-424">未知</h2><h2>Unknown</h2>
        <p>您的 Theta 和 X 通常是随机变量的向量，而不是单个随机变量。现在，如果您要在离散数据的情况下进行估计，情况也没有什么不同。我们仍然有相同类型的贝叶斯规则，只是密度被 PMF 取代。如果 X 是离散的，您可以在此处放置 P，而不是放置 f。&nbsp;</p><p>Your Theta’s and X’s are often the vectors of random variables instead of individual random variables. Now if
            you
            are to do estimation in a case where you have discrete data, again the situation is no different. We still
            have
            a Bayes rule of the same kind, except that densities gets replaced by PMF’s. If X is discrete, you put a P
            here
            instead of putting an f.&nbsp;</p>
        <p>因此，离散数据的估计问题与投票问题类似。你有一枚硬币。它有一个未知参数 Theta。这是获得正面的概率。你多次抛硬币。你能告诉我 Theta 的真实值吗？这时，古典统计学家会说，好的，我要使用一个估计量，最合理的估计量就是这样。</p><p>So an example of an estimation problem with discrete data is similar to the polling problem. You have a coin.
            It
            has an unknown parameter Theta. This is the probability of obtaining heads. You flip the coin many times.
            What
            can you tell me about the true value of Theta? A classical statistician, at this point, would say, OK, I’m
            going
            to use an estimator, the most reasonable one, which is this.</p>
        <p>他们在 n 次试验中得到了多少次正面？除以总试验次数。这是我对硬币偏差的估计。然后，古典统计学家会从这里继续，并尝试证明一些属性并证明这个估计是正确的。例如，我们有弱大数定律，它告诉我们这个特定的估计在概率上收敛到真实参数。</p><p>How many heads did they obtain in n trials? Divide by the total number of trials. This is my estimate of the
            bias
            of my coin. And then the classical statistician would continue from here and try to prove some properties
            and
            argue that this estimate is a good one. For example, we have the weak law of large numbers that tells us
            that
            this particular estimate converges in probability to the true parameter.</p>
        <h2 id="unknown-425">未知</h2><h2>Unknown</h2>
        <p>这是一种很有用的保证。而传统统计学家基本上会以这种方式结束这个话题。贝叶斯主义者会做什么不同的事情？贝叶斯主义者会从假设 Theta 的先验分布开始。他们不会将 Theta 视为未知常数，而是会说 Theta 会随机说话，或者假装它会随机说话并假设 Theta 的分布。</p><p>This is a kind of guarantee that’s useful to have. And the classical statistician would pretty much close the
            subject in this way. What would the Bayesian person do differently? The Bayesian person would start by
            assuming
            a prior distribution of Theta. Instead of treating Theta as an unknown constant, they would say that Theta
            would
            speak randomly or pretend that it would speak randomly and assume a distribution on Theta.</p>
        <p>例如，如果你不知道他们还需要什么，你可能会假设硬币偏差的任何值都与硬币偏差的任何其他值一样可能。这样，概率分布就是均匀的。</p><p>So for example, if you don’t know they need anything more, you might assume that any value for the bias of
            the
            coin is as likely as any other value of the bias of the coin. And this way so the probability distribution
            that’s uniform.</p>
        <p>或者，如果你对制造硬币的制造过程更有信心，你可能会选择以 1/2 为中心的先验分布，并且以 1/2 为中心。这将是一个先验分布，你会说，好吧，我相信制造商试图让我的硬币公平。但他们经常会犯一些错误，所以我相信它大约是 1/2，但不完全是。
        </p><p>Or if you have a little more faith in the manufacturing processes that’s created that coin, you might choose
            your
            prior to be a distribution that’s centered around 1/2 and sits fairly narrowly centered around 1/2. That
            would
            be a prior distribution in which you say, well, I believe that the manufacturer tried to make my coin to be
            fair. But they often makes some mistakes, so it’s going to be, I believe, it’s approximately 1/2 but not
            quite.
        </p>
        <h2 id="unknown-426">未知</h2><h2>Unknown</h2>
        <p>因此，根据您的信念，您将为 Theta 分布选择一个适当的先验。然后，您将使用贝叶斯规则根据您观察到的数据找到不同 Theta 值的概率。因此，无论您使用哪个版本的贝叶斯规则，贝叶斯规则的最终产品都将是这种类型的图或那种类型的图。</p><p>So depending on your beliefs, you would choose an appropriate prior for the distribution of Theta. And then
            you
            would use the Bayes rule to find the probabilities of different values of Theta, based on the data that you
            have
            observed. So no matter which version of the Bayes rule that you use, the end product of the Bayes rule is
            going
            to be either a plot of this kind or a plot of that kind.</p>
        <p>那么我在这里绘制的是什么？这个轴是 Theta 轴。这些是我们试图估计的未知量的可能值。在连续情况下，theta 是一个连续随机变量。我获取数据。在观察数据后，我绘制了后验概率分布。我在这里绘制了 Theta 的概率密度。所以这是该密度的图。</p><p>So what am I plotting here? This axis is the Theta axis. These are the possible values of the unknown
            quantity
            that we’re trying to estimate. In the continuous case, theta is a continuous random variable. I obtain my
            data.
            And I plot for the posterior probability distribution after observing my data. And I’m plotting here the
            probability density for Theta. So this is a plot of that density.</p>
        <p>在离散情况下，theta 可以取有限多个值或一组离散值。对于每个值，我都会告诉你，根据我观察到的数据，该值是正确的可能性有多大。一般来说，在完成所有推理工作后，你要向老板汇报的要么是这种类型的图，要么是那种类型的图。</p><p>In the discrete case, theta can take finitely many values or a discrete set of values. And for each one of
            those
            values, I’m telling you how likely is that the value to be the correct one, given the data that I have
            observed.
            And in general, what you would go back to your boss and report after you’ve done all your inference work
            would
            be either a plot of this kinds or of that kind.</p>
        <h2 id="unknown-427">未知</h2><h2>Unknown</h2>
        <p>于是你去找老板，他问你 Theta 的值是多少？你说，好吧，我只有有限的数据。我不知道它是多少。可能是这个，可能性很大。可能性很大。好吧，我们在这里输入一些数字。Theta 是这个值的概率是 0.3。</p><p>So you go to your boss who asks you, what is the value of Theta? And you say, well, I only have limited data.
            That I don’t know what it is. It could be this, with so much probability. There’s probability. OK, let’s
            throw
            in some numbers here. There’s probability 0.3 that Theta is this value.</p>
        <p>有 0.2 的概率 Theta 是这个值，有 0.1 的概率是这个值，有 0.1 的概率是这个值，有 0.2 的概率是那个值，以此类推。好吧，现在老板们通常想要简单的答案。他们会说，好吧，你说得太多了。你认为 Theta 是多少？现在你被迫做出决定。如果情况就是这样，你必须做出决定，你会怎么做？</p><p>There’s probability 0.2 that Theta is this value, 0.1 that it’s this one, 0.1 that it’s this one, 0.2 that
            it’s
            that one, and so on. OK, now bosses often want simple answers. They say, OK, you’re talking too much. What
            do
            you think Theta is? And now you’re forced to make a decision. If that was the situation and you have to make
            a
            decision, how would you make it?</p>
        <p>好吧，我要做出一个最有可能正确的决定。如果我做出这个决定，会发生什么？Theta 是这个值的概率为 0.2，这意味着如果我做出这个猜测，他们犯错的概率可能是 0.8。如果我做出这个决定，这个决定正确的概率可能是 0.3。</p><p>Well, I’m going to make a decision that’s most likely to be correct. If I make this decision, what’s going to
            happen? Theta is this value with probability 0.2, which means there’s probably 0.8 that they make an error
            if I
            make that guess. If I make that decision, this decision has probably 0.3 of being the correct one.</p>
        <h2 id="unknown-428">未知</h2><h2>Unknown</h2>
        <p>所以我的误差可能是 0.7。所以如果你想最大化做出正确决定的概率，或者如果你想最小化做出错误决定的概率，你要选择报告概率最高的 Theta 值。所以在这种情况下，我会选择报告这个特定的值，即根据我观察到的情况，最有可能的 Theta 值。</p><p>So I have probably of error 0.7. So if you want to just maximize the probability of giving the correct
            decision,
            or if you want to minimize the probability of making an incorrect decision, what you’re going to choose to
            report is that value of Theta for which the probability is highest. So in this case, I would choose to
            report
            this particular value, the most likely value of Theta, given what I have observed.</p>
        <p>这个值被称为最大后验概率估计。在我们的例子中就是这个。因此，选择后验 PMF 中概率最高的点。这是合理的做法。如果你想要最小化错误推理的概率，这是最佳做法。如果人们需要报告一个答案，如果他们需要报告一个决定，他们通常会这样做。</p><p>And that value is called them maximum a posteriori probability estimate. It’s going to be this one in our
            case.
            So picking the point in the posterior PMF that has the highest probability. That’s the reasonable thing to
            do.
            This is the optimal thing to do if you want to minimize the probability of an incorrect inference. And
            that’s
            what people do usually if they need to report a single answer, if they need to report a single decision.</p>
        <p>在估计上下文中呢？如果这就是您对 Theta 的了解，那么 Theta 可能就在这里，但也有很大的概率它就在这里。您会给老板的答案是什么？一种选择是使用相同的理念并说，好的，我要找到这个后验密度最高的 Theta。所以我会选择这里的这个点并报告这个特定的 Theta。</p><p>How about in the estimation context? If that’s what you know about Theta, Theta could be around here, but
            there’s
            also some sharp probability that it is around here. What’s the single answer that you would give to your
            boss?
            One option is to use the same philosophy and say, OK, I’m going to find the Theta at which this posterior
            density is highest. So I would pick this point here and report this particular Theta.</p>
        <h2 id="unknown-429">未知</h2><h2>Unknown</h2>
        <p>所以这就是我的 Theta，再次强调，Theta MAP，具有最高后验概率的 Theta，因为它对应于密度的峰值。但在这种情况下，最大后验概率的 Theta 是最有可能为真的 Theta。在连续情况下，你不能真正说这是 Theta 最可能的值。</p><p>So this would be my Theta, again, Theta MAP, the Theta that has the highest a posteriori probability, just
            because it corresponds to the peak of the density. But in this context, the maximum a posteriori probability
            theta was the one that was most likely to be true. In the continuous case, you cannot really say that this
            is
            the most likely value of Theta.</p>
        <p>在连续设置中，任何 Theta 值的概率都是零，所以当我们谈论密度时。所以它不是最有可能的。它是密度最高的值，也就是该邻域的概率最高。因此，在连续情况下选择这个特定估计值的理由远不如我们这里的理由那么令人信服。所以在这种情况下，理性的人可能会选择不同的数量来报告。
        </p><p>In a continuous setting, any value of Theta has zero probability, so when we talk about densities. So it’s
            not
            the most likely. It’s the one for which the density, so the probabilities of that neighborhoods, are
            highest. So
            the rationale for picking this particular estimate in the continuous case is much less compelling than the
            rationale that we had in here. So in this case, reasonable people might choose different quantities to
            report.
        </p>
        <p>最流行的方法是报告条件期望。所以我不知道 Theta 到底是什么。根据我拥有的数据，Theta 具有这种分布。让我只报告该分布的平均值。让我报告这个数字的重心。在这个图中，重心可能位于此处附近。这将是您可能选择报告的另一个估计值。
        </p><p>And the very popular one would be to report instead the conditional expectation. So I don’t know quite what
            Theta
            is. Given the data that I have, Theta has this distribution. Let me just report the average over that
            distribution. Let me report to the center of gravity of this figure. And in this figure, the center of
            gravity
            would probably be somewhere around here. And that would be a different estimate that you might choose to
            report.
        </p>
        <h2 id="unknown-430">未知</h2><h2>Unknown</h2>
        <p>所以重心就在这附近。这是给定数据的 Theta 的条件期望。因此，从某种意义上说，这是两种相当合理的选择向老板报告内容的方式。有些人可能会选择报告这个。有些人可能会选择报告那个。</p><p>So center of gravity is something around here. And this is a conditional expectation of Theta, given the data
            that you have. So these are two, in some sense, fairly reasonable ways of choosing what to report to your
            boss.
            Some people might choose to report this. Some people might choose to report that.</p>
        <p>并且先验地，如果没有令人信服的理由说明为什么一个比另一个更可取，除非你为游戏设定一些规则，并且你更准确地描述你的目标是什么。但无论你报告哪一个，一个答案，一个点估计，都不能真正告诉你整个故事。这个后验分布图传达的信息比你可能报告的任何单个数字都要多得多。</p><p>And a priori, if there’s no compelling reason why one would be preferable than other one, unless you set some
            rules for the game and you describe a little more precisely what your objectives are. But no matter which
            one
            you report, a single answer, a point estimate, doesn’t really tell you the whole story. There’s a lot more
            information conveyed by this posterior distribution plot than any single number that you might report.</p>
        <p>因此，一般来说，您可能希望说服您的老板，值得他们花时间查看整个图，因为该图涵盖了所有可能性。它告诉您的老板我们很可能处于该范围内，但也有明显的变化，我们的 Theta 恰好位于该范围内。</p><p>So in general, you may wish to convince your boss that’s it’s worth their time to look at the entire plot,
            because that plot sort of covers all the possibilities. It tells your boss most likely we’re in that range,
            but
            there’s also a distinct change that our Theta happens to lie in that range.</p>
        <h2 id="unknown-431">未知</h2><h2>Unknown</h2>
        <p>好吧，现在让我们试着区分这两者，看看在什么情况下这个估计值可能更好。在什么方面更好？我们需要一些规则。所以我们要引入一些规则。作为热身，我们将处理在没有任何信息（先验分布除外）的情况下进行估计的问题。</p><p>All right, now let us try to perhaps differentiate between these two and see under what circumstances this
            one
            might be the better estimate to perform. Better with respect to what? We need some rules. So we’re going to
            throw in some rules. As a warm up, we’re going to deal with the problem of making an estimation if you had
            no
            information at all, except for a prior distribution.</p>
        <p>所以这是为接下来的内容做准备，接下来的内容是考虑一些信息的估计。所以我们有一个 Theta。由于你的主观信念或他人的模型，你认为 Theta 均匀分布在 4 和 10 之间。你想得出一个点估计。让我们试着寻找一个估计值。在本例中，称之为 c。</p><p>So this is a warm up for what’s coming next, which would be estimation that takes into account some
            information.
            So we have a Theta. And because of your subjective beliefs or models by others, you believe that Theta is
            uniformly distributed between, let’s say, 4 and 10. You want to come up with a point estimate. Let’s try to
            look
            for an estimate. Call it c, in this case.</p>
        <p>我想选择一个数字来估计 Theta 的值。我会对我犯的错误的大小感兴趣。我真的不喜欢大错误，所以我会关注他们犯的错误的平方。所以我选择 c. Theta，它有一个我不知道的随机值。</p><p>I want to pick a number with which to estimate the value of Theta. I will be interested in the size of the
            error
            that I make. And I really dislike large errors, so I’m going to focus on the square of the error that they
            make.
            So I pick c.&nbsp;Theta that has a random value that I don’t know.</p>
        <h2 id="unknown-432">未知</h2><h2>Unknown</h2>
        <p>但无论它是什么，一旦它为人所知，它就会产生它与我猜测的之间的平方误差。我感兴趣的是取平均值，其中平均值是针对 Theta 的所有可能值和未知值取的。因此，问题是该问题的最小二乘公式，我们试图最小化最小二乘误差。</p><p>But whatever it is, once it becomes known, it results into a squared error between what it is and what I
            guessed
            that it was. And I’m interested in making a small air on the average, where the average is taken with
            respect to
            all the possible and unknown values of Theta. So the problem, this is a least squares formulation of the
            problem, where we try to minimize the least squares errors.</p>
        <p>如何找到最优 c？好吧，我们取这个表达式并展开它。使用期望的线性，它是。平方减去 2c 预期 Theta 加上 c 平方。这就是我们想要最小化的量，相对于 c。&nbsp;</p><p>How do you find the optimal c? Well, we take that expression and expand it. And it is, using linearity of
            expectations. square minus 2c expected Theta plus c squared. that’s the quantity that we want to minimize,
            with
            respect to c.&nbsp;</p>
        <p>为了实现最小化，对 c 求导，并将其设置为 0。这样，微分结果为，从这里减去 2 的 Theta 期望值，加上 2c 等于 0。通过求解这个方程得到的答案是，c 是 Theta 的期望值。因此，当您进行这种优化时，您会发现最佳估计值（您应该报告的内容）是 Theta 的期望值。</p><p>To do the minimization, take the derivative with respect to c and set it to 0. So that differentiation gives
            us
            from here minus 2 expected value of Theta plus 2c is equal to 0. And the answer that you get by solving this
            equation is that c is the expected value of Theta. So when you do this optimization, you find that the
            optimal
            estimate, the things you should be reporting, is the expected value of Theta.</p>
        <h2 id="unknown-433">未知</h2><h2>Unknown</h2>
        <p>因此，在这个特定示例中，您可以选择将估计值 c 设为这些值的中间值，即 7。好吧，如果您的老板问您，您的估计有多准确？您的误差会有多大？您可以报告的是您所犯的估计误差的平均大小。我们选择将 Theta 的预期值设为我们的估计值。</p><p>So in this particular example, you would choose your estimate c to be just the middle of these values, which
            would be 7. OK, and in case your boss asks you, how good is your estimate? How big is your error going to
            be?
            What you could report is the average size of the estimation error that you are making. We picked our
            estimates
            to be the expected value of Theta.</p>
        <p>因此，对于我选择进行估算的这种特定方法，这是我得到的均方误差。这是一个熟悉的量。它只是分布的方差。因此，如果您对均方误差感兴趣，则期望是估计数量的最佳方法。而由此产生的均方误差就是方差本身。如果我们现在也有数据，这个故事将如何改变？</p><p>So for this particular way that I’m choosing to do my estimation, this is the mean squared error that I get.
            And
            this is a familiar quantity. It’s just the variance of the distribution. So the expectation is that best way
            to
            estimate a quantity, if you’re interested in the mean squared error. And the resulting mean squared error is
            the
            variance itself. How will this story change if we now have data as well?</p>
        <p>现在有了数据，我们就可以计算后验分布或条件分布。因此，我们进入了一个新世界，在这里，我们不再使用 Theta 的原始分布，即先验分布，而是使用 Theta 分布的条件，并给出我们观察到的数据。
        </p><p>Now having data means that we can compute posterior distributions or conditional distributions. So we get
            transported into a new universe where instead the working with the original distribution of Theta, the prior
            distribution, now we work with the condition of distribution of Theta, given the data that we have observed.
        </p>
        <h2 id="unknown-434">未知</h2><h2>Unknown</h2>
        <p>现在回想一下我们的老口号：条件模型和条件概率与普通概率没有什么不同，只是我们现在生活在一个考虑到新信息的新宇宙中。因此，如果您使用这种理念，并且要求您最小化平方误差，但现在您生活在一个 X 已固定为某个东西的新宇宙中，最佳解决方案是什么？</p><p>Now remember our old slogan that conditional models and conditional probabilities are no different than
            ordinary
            probabilities, except that we live now in a new universe where the new information has been taken into
            account.
            So if you use that philosophy and you’re asked to minimize the squared error but now that you live in a new
            universe where X has been fixed to something, what would the optimal solution be?</p>
        <p>它又会是 theta 的期望，但哪个期望呢？它是适用于我们现在生活的新条件宇宙的期望。因此，根据我们之前所做的，通过相同的计算，我们会发现最佳估计是 Theta 的 X 的期望值，但最佳估计会考虑到我们拥有的信息。</p><p>It would again be the expectation of theta, but which expectation? It’s the expectation which applies in the
            new
            conditional universe in which we live right now. So because of what we did before, by the same calculation,
            we
            would find that the optimal estimates is the expected value of X of Theta, but the optimal estimate that
            takes
            into account the information that we have.</p>
        <p>因此，结论是，一旦你获得了数据，如果你想要最小化均方误差，你应该根据你拥有的数据报告这个未知量的条件估计。因此，这里的图像是 Theta 是未知的。你有产生测量值的仪器。因此，这会创建一个 X。你取一个 X，这里有一个进行计算的盒子。</p><p>So the conclusion, once you get your data, if you want to minimize the mean squared error, you should just
            report
            the conditional estimation of this unknown quantity based on the data that you have. So the picture here is
            that
            Theta is unknown. You have your apparatus that creates measurements. So this creates an X. You take an X,
            and
            here you have a box that does calculations.</p>
        <h2 id="unknown-435">未知</h2><h2>Unknown</h2>
        <p>它会进行计算，并根据您观察到的特定数据得出 Theta 的条件期望。到目前为止，我们在本课程中所做的工作在某种程度上是开发与此特定计算相关的计算工具和技能。如何计算 Theta 的后验密度以及如何计算期望、条件期望。所以原则上，我们知道如何做到这一点。</p><p>It does calculations and it spits out the conditional expectation of Theta, given the particular data that
            you
            have observed. And what we have done in this class so far is, to some extent, developing the computational
            tools
            and skills to do with this particular calculation. how to calculate the posterior density for Theta and how
            to
            calculate expectations, conditional expectations. So in principle, we know how to do this.</p>
        <p>原则上，我们可以对计算机进行编程，使其获取数据并得出条件期望。与我们想法不同的人可能会设计一台计算机，以不同的方式执行操作并得出其他估计值。所以我们进行了讨论，并决定对计算机进行编程以计算条件期望。其他人提出了一些关于如何估计随机变量的疯狂想法。</p><p>In principle, we can program a computer to take the data and to spit out condition expectations. Somebody who
            doesn’t think like us might instead design a calculating machine that does something differently and
            produces
            some other estimate. So we went through this argument and we decided to program our computer to calculate
            conditional expectations. Somebody else came up with some other crazy idea for how to estimate the random
            variable.</p>
        <p>他们想出了一些函数 g，并对其进行了编程，他们设计了一台机器，通过输出 X 的某个 g 来估计 Theta。这可能是一个替代估计器。哪一个更好？好吧，我们说服自己，在我们已经固定了数据特定值的宇宙中，这是最佳的。所以我们到目前为止所证明的就是这种关系。</p><p>They came up with some function g and the programmed it, and they designed a machine that estimates Theta’s
            by
            outputting a certain g of X. That could be an alternative estimator. Which one is better? Well, we convinced
            ourselves that this is the optimal one in a universe where we have fixed the particular value of the data.
            So
            what we have proved so far is a relation of this kind.</p>
        <h2 id="unknown-436">未知</h2><h2>Unknown</h2>
        <p>在这个条件宇宙中，我得到的均方误差（我是使用这个估计量的人）小于或等于这个人得到的均方误差，这个人就是使用这个估计量的人。对于任何特定的数据值，我都会比其他人做得更好。现在数据本身是随机的。如果我对数据的所有可能值取平均值，我应该还是会做得更好。</p><p>In this conditional universe, the mean squared error that I get. I’m the one who’s using this estimator. is
            less
            than or equal than the mean squared error that this person will get, the person who uses that estimator. For
            any
            particular value of the data, I’m going to do better than the other person. Now the data themselves are
            random.
            If I average over all possible values of the data, I should still be better off.</p>
        <p>如果我对任何可能的值 X 都比较满意，那么我对所有可能值的 X 的平均值也应该比较满意。因此，让我们根据 X 的概率分布对这个数量的两边取平均值。如果您想正式地做到这一点，您可以将这个数字之间的不等式写成随机变量之间的不等式。
        </p><p>If I’m better off for any possible value X, then I should be better off on the average over all possible
            values
            of X. So let us average both sides of this quantity with respect to the probability distribution of X. If
            you
            want to do it formally, you can write this inequality between numbers as an inequality between random
            variables.
        </p>
        <p>它表明，无论随机变量是什么，这个量都比那个量好。取两边的期望值，你会得到总体期望值之间的不等式。最后一个不等式告诉我，使用这个估计器并根据这个机器进行估计的人的均方估计误差将小于或等于另一个人产生的估计误差。</p><p>And it tells that no matter what that random variable turns out to be, this quantity is better than that
            quantity. Take expectations of both sides, and you get this inequality between expectations overall. And
            this
            last inequality tells me that the person who’s using this estimator who produces estimates according to this
            machine will have a mean squared estimation error that’s less than or equal to the estimation error that’s
            produced by the other person.</p>
        <h2 id="unknown-437">未知</h2><h2>Unknown</h2>
        <p>简而言之，条件期望估计量是最优估计量。它是终极估计机器。这就是您解决估计问题并报告单一值的方法。如果您被迫报告单一值，并且对估计误差感兴趣。好吧，虽然我们当然可以在一两个月前告诉您这个故事，但这实际上是关于理解条件期望具有非常好的属性。</p><p>In a few words, the conditional expectation estimator is the optimal estimator. It’s the ultimate estimating
            machine. That’s how you should solve estimation problems and report a single value. If you’re forced to
            report a
            single value and if you’re interested in estimation errors. OK, while we could have told you that story, of
            course, a month or two ago, this is really about interpretation about realizing that conditional
            expectations
            have a very nice property.</p>
        <p>但除此之外，任何进入这个行业的概率技能都只是能够计算条件期望的概率技能，而你已经知道如何去做了。所以结论是，所有最佳贝叶斯估计都只是意味着计算和报告条件期望。好吧，如果世界这么简单，那么统计学家就找不到工作了，如果生活这么简单的话。所以现实生活并不是那么简单。有很多复杂的事情。</p><p>But other than that, any probabilistic skills that come into this business are just the probabilistic skills
            of
            being able to calculate conditional expectations, which you already know how to do. So conclusion, all of
            optimal Bayesian estimation just means calculating and reporting conditional expectations. Well, if the
            world
            were that simple, then statisticians wouldn’t be able to find jobs if life is that simple. So real life is
            not
            that simple. There are complications.</p>
        <p>这也许会让他们的生活更有趣一些。好吧，一个复杂之处在于，我们要处理的是向量，而不是单个随机变量。我在这里使用符号，就好像 X 是一个随机变量。在现实生活中，你会得到几个数据。我们的故事会改变吗？不会，同样的论点。考虑到你观察到的所有数据，你仍然应该报告 Theta 的条件期望。</p><p>And that perhaps makes their life a little more interesting. OK, one complication is that we would deal with
            the
            vectors instead of just single random variables. I use the notation here as if X was a single random
            variable.
            In real life, you get several data. Does our story change? Not really, same argument. given all the data
            that
            you have observed, you should still report the conditional expectation of Theta.</p>
        <h2 id="unknown-438">未知</h2><h2>Unknown</h2>
        <p>但是，为了报告这个条件期望，需要做哪些工作呢？一个问题是，你需要为 Theta 制定一个合理的先验分布。你该怎么做呢？在给定的应用中，这有点像判断，你会使用什么先验。并且有一种避免做出愚蠢选择的技巧。</p><p>But what kind of work does it take in order to report this conditional expectation? One issue is that you
            need to
            cook up a plausible prior distribution for Theta. How do you do that? In a given application,this is a bit
            of a
            judgment call, what prior would you be working with. And there’s a certain skill there of not making silly
            choices.</p>
        <p>一个更实际、更实际的问题是，这个公式非常简洁、简洁，你可以用最少的墨水写出来。但其背后可能隐藏着大量的计算。因此，进行任何涉及多个随机变量的计算实际上都涉及计算多维积分。而多维积分很难计算。因此，在这里实际实现这台计算机可能并不容易，计算起来可能很复杂。</p><p>A more pragmatic, practical issue is that this is a formula that’s extremely nice and compact and simple that
            you
            can write with minimal ink. But the behind it there could be hidden a huge amount of calculation. So doing
            any
            sort of calculations that involve multiple random variables really involves calculating multi dimensional
            integrals. And the multi dimensional integrals are hard to compute. So implementing actually this
            calculating
            machine here may not be easy, might be complicated computationally.</p>
        <p>它在无法获得直觉方面也很复杂。所以也许你可能想要一个更简单的版本，一个更简单的替代公式，更容易使用，更容易计算。下次我们将讨论一种这样的更简单的替代方案。所以，再次总结一下，从高层次上讲，贝叶斯估计非常非常简单，前提是你已经掌握了本课程迄今为止所学的所有内容。</p><p>It’s also complicated in terms of not being able to derive intuition about it. So perhaps you might want to
            have
            a simpler version, a simpler alternative to this formula that’s easier to work with and easier to calculate.
            We
            will be talking about one such simpler alternative next time. So again, to conclude, at the high level,
            Bayesian
            estimation is very, very simple, given that you have mastered everything that has happened in this course so
            far.</p>
        <h2 id="unknown-439">未知</h2><h2>Unknown</h2>
        <p>存在一些实际问题，熟悉这些概念和问题也是有益的，一般来说，您更愿意报告完整的后验分布。但是，如果您被迫报告点估计，那么有许多合理的方法可以做到这一点。也许最合理的方法是只报告条件期望本身。</p><p>There are certain practical issues and it’s also good to be familiar with the concepts and the issues that in
            general, you would prefer to report that complete posterior distribution. But if you’re forced to report a
            point
            estimate, then there’s a number of reasonable ways to do it. And perhaps the most reasonable one is to just
            the
            report the conditional expectation itself.</p>
        <h1 id="bayesian-statistical-inference-ii">22.贝叶斯统计推断 II</h1><h1>22. Bayesian Statistical Inference II</h1>
        <p><img src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2MBERISGBUYLxoaL2NCOEJjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAACAwEBAQAAAAAAAAAAAAAAAQIEBQMGB//EAEQQAAIBAwEEBgcGAwcEAgMAAAABAgMEESEFEjFBBhMiUWFxFDIzcoGRsSM0QlJiwRUkNRZDU3OCkqEHJYPRRFRjorL/xAAYAQEBAQEBAAAAAAAAAAAAAAAAAQIDBP/EAB0RAQEBAAIDAQEAAAAAAAAAAAABEQIhAxIxcUH/2gAMAwEAAhEDEQA/APFWfAv0+JQs+BfpkqNCweLyk/CSNj8fwMWzeLmh5tG1ntLyDUU7nTacX+lG7bPNIwbp/wDcIP8ASjasn9lr3ilXEZe0dtUdn11Sqwk21lNGmuB5PpTJK7hmOri0mRF/+1Fp+SfyOkek1o16skePQFwez/tLZ/qJLpJZvnI8Wo5YbpR7ePSSy5yfyJx6S2K/G/keGwG6MHvY9JrDnUfyJrpLs16Oq0/I+f7osDB799KtmQluutL/AGskuleyspdbLX9DPn+PEdOUFNOosx8CYPryTjjuaydI1GuZzjOnKEV1kE1FaN+A96mv72n/ALjI7xqd5NST4FbeguFWn8x060HPd3o55YYFkYlkAGAhmoAAAoAAAA89tr+oy9xfuehPO7b/AKjL3I/uQUEa+xX9lceSMbJq7E9nc+6jTNcrR/e3+qP1MnpA96ym/wBS+pp2r0u/ej9TK2/9xmvGP1IMnZlBXd5Rt291VJYz3Gptuhb069lRoUup3t6L3nq1nCbMWjN0pxnTeJxeU0Wbq4nf3MZ1G9+WjbZRYnYXNrVSknFyqdVF54sL7Ztzs6pCVxFOm5LVPKfgWtr7Qp1+qowbzZzioSz6/DLOu2NyU760pOpUuKlwpwhyikk3gfo4WF3Tlc3No3u29zLNN/klyZbvNnT2hUdePZrqDjUh+tf+0eeWV3ppm9bXF5VtXOFOUqySSl+bHBjOukYr3k8PRriiSb3cFrau49oVHDCyk5Y5SxqVorslikTU3nBDmPKAlvPDI73gLeBsB5DJHILUDrF6DWW8EEngnGLyBI6qcqfapzcZd6ZzcdBAaEdr3vU9X6RJrvfEpc288dSKwAE0PJz4cxpeKA6RlKOd2TWVh4MPpN93o+8/obKeEYvSV5t6PvP6EoybPgX4FCzL8DK1atni4o+/+xtp9peRg0nivS982XKO9FT4S0yFh/y3pDlcN5xiGHzNW3cU3GK0wmebvqSp3VOGrSXM3Nm602UaK4HlulyxXt5eDPURPN9L1pbS8WiQebAAKJ0/XRM5w9dEmwhjFkChkHxJZIPiACfqsbE+DCvd28nOG9J5bS+iK87q5h1mLdy3ZYjrxR2tvZL3Y/QzKtOtv3WI1vWTjh/Qg2ZSkqLmo9rdzunTZtSrOdKVan1clNaZOUW40ItRbkocB7PuJ11CpOlKk99dmQHqrnPo1XDw9x4fdoeE2XfbZv6dzKO05x6iG81LHD5HuLiadGrH9D+h896OTUf4gpbutJpZ82SD1fRC9ub2xrTuq0q0o1MJy7sI3zy3QJ/9uuf81fRHqQAAAoAAAA83tz+pS/y4/uekPN7c/qUv8tfuQZxq7EeKdz7qMg1tiv7O591Gma4Wz++ecfqZm3tbGr5x+po2z1u/9P1M3bv3Cr5x+oGBTeqOucvTRnCD1ydXxAm/Vfey5e3vW7Rd1QlKEnBLPjjDKJJ+zfmUNSwseJ1hd1443as1hY4lfPgW9nW9K5rVI1puEIUpVHJcsAcs83qdY+rqjpVsatGn1i3Z0vzxeUalK5pfwWFSdvCo6VTq5cm01lAYrI+Ro1o2FejUnSlKjVis7ktU/JlK2pTuK8KNJZlN4SA5ZDw5mhX2dGkpqlcQq1aazOMeX/szt572QHFks4DKcc4SY+q4Yec6gdItNPvJqWhxxuslHiB2i88RVMLgKOmrIymlxAENnOU3wWhHfffkDtnvIocXJx0i35ISkuDWoHRNmN0k9hR95m5St61aLlSozlFc0jD6SpqjSTWGpPR+Qqsqz4F+mULPgX4GCuucVKb/AFo0rr2WnJmXN+q/1I1Lj2LCq9xWdarSb0cVjzN7ZnqPyPOP2kfM9Hs31fgUaMTA6VqDo0d54xJm+ihtfZv8RoxgpbrTzkkHhlxHFOTwkb76L1VwqxZB9GriPCaKMWMXGUd5YWeJ2dKnl/bwNJ9Hbr86Yv7N3T5xAzuqp/48CXV08e3p/Mv/ANmrpc4jXRq7fOI0Z/VU8e3p/Mr1MRm0mmu9Gz/Ze8fOI10Wve+JUYq5ZHCKnNRlJRT0b7jcXRS+a9aB0h0RvZPWcERW3SsbqVvQnb7soTpReX5HeGz75/4a+Jp2NF29hbUJPMqVNQfmWUiWjIjsy9f4qXzOtLZd4qkXKVPCafE1Yo6wJohVob0ZqPGSfHxPJ0+h1xGWevjHPcezAmjK6P7Jlsi3q0pTU9+e9p5GsIZQAAFgAACgPN7d/qT/AMtfuekPNbe/qf8A419WQZnM1NjvFO59wyuZqbHfYufcNM1wtvWu/wDT9TP25/T678vqXrd9u6+H1KG2/wCm1/h9SDztJ9tIsLDSKqljCSWVzLMNYlElxJSTxoRJJ9llC3NOKNKzt3HZ97USblUjGhBd7by/oZi4mpOtOlsSylTluzdxOba71ogODtrilBxnCpCHNPKRfpxx0fqv89xFL4IoVdoXdx7a4lPzL9eSpbGsKS41ZTqv6IDOkkFKFTrE6SlvLg48SaW/JRSy28I2oU69LrLPZrgqtJLrZvG9KXcgMizVa2vYzVCc8N78ccU+IXt1Sq5pULaNCC+LZcrbR2laJRrKUKylnfcfWXcQv50r+z9Np01CtCajWiuDzwYGWo5jurjxNC12dcVrWdyko04R0cvxeRn5aenHkbtRVKu1bKxg/s7ZR31yzjMmwMfO89VgnGL4JE40+vu5QpLO/Uaj8zXuatLZ6lQtaMZTgt2dZ6vL7gM6zq21KpOV3SdWCjpFc2drq0tri0d5YSe7D2lKXGBG0hRuIStqixVl7Ofj3EdlTdptKMavqVH1VSL7mBQUk+KTDEYy0XwZ0urd21zVov8Au5tCmszyuYGvs6s42uaVzb0kn2o1EslKnQjcbRhR62M3Wq6yiU5RyOjUlb14VYetTkpIC/dbTuPTJQoVHSo0ZuMKcdFhd5jdMKtvc29C4p9irKfbh8OJt3dn6TUldWcXOnV7TiuMXzR5/pPs+4trKhVrw3FKbSTevDuIsYdmaEDPszQgZKlV4fFGpWzKlupZ0yZdX1DRqtqnFp8gsVG9Y+Z6PZb7PwPNvl5nodkvRLwKNaJIjFkjICSSIkkwHFanTCIIkmAyUSORpgdETOaZNMCQ1xEhoDrEmjkmdEyDoicTkmdEwOiZJHNSJpgSAQAMAAoAACgPNbf/AKkv8tfU9KeZ2/8A1P8A8S+oGWamxvVufcRlPiamxuFz7iNM1xtI5qXeeSX1KO2V/wBsr+S//o0LJNzvElyX1KO2f6ZceS+pB5XBcoxe6VIrLSL9NYikaC3R/hJMS9V6cgIRS+Jo7PvlawlSrUIVqMnlxlyfgZ2GPXvA1L+psupayq2yqU6/+G9V8yG1KrjO0oJ6UbaHzepQxlNF6VvPaW7VpSi6ygoyhnXTRYAVlUVK6oVaj7EZpt/EjtCFaz2hV7TTc3KM0+KYq1lc20ftqcoRfeWLbajpUlTr29O5hH1d/iviByd/e3sFQnKVbkljLHOKsrKtQm/t67jmP5UidbbdaUHC2oUrWL07C1+ZmNuUnKTbb4tgXdmwhV2jb056xlNI2Nq1vQPSeqoTjVu5STqz/L4GFYV421/b16ibhTnvNIu1Lme0qO0J1ZydSM1UopvhHOqX/BB02G1C+VR/3VOU/ikQ2bcKpddTW1Vx2X4SfBkdhz3r7qn/AHtKcF5tC2O+qr1bmqtLWk5pP83BFHXaSpW13ChQfboJKcu+RLayjKdK7hoq9NT/ANS4le9k7ijRvudRYq45SOld72wbGTesalSC8sgd9tpSv+s5VqcamSikscS3ft1bDZ1f/wDG6b+DKcPWim8ZeMgTwiDhk1Z7CuXBVKM6daL4brM2rTnRqOnUi4yXJgStp16U/wCXlOMnp2eLMnpbRu1RoVrpVMyljM/I9PCqtmbJp16cU7m5bUZP8KXM8t0qvLi4t6KrVpVFvZSb8ARh2hfplC0L8DC1Or7N+RpxSnbr3TMqezfkalprb0/dBFF8Pib2yOXkY1zT6ubXJvJsbIeWiq2USRFEjIY0IEBNAIZQxpkRoDpFk0c4k0yDoiSIIkgOiJo5RZ0TAmhkUxog6InFnNE4gdUBGJIBgIYAgAC6A8zt/wDqf/iX1PTHmekH9S/8S+pRlPiamxf/AJHuGXzNTYvG49w0yNmLNzd+6vqUNtRxsu48l9TT2NHN3eZ/KvqUNtr/ALfcacv3A8hHsyT7i5SkptYevcVmkxxWHpyAuNE3HegsPhxRCE8rU6KJQnFbmv5RKCwTSxrxCLec94AqfHXgQcWpb0W0/BnfTHiyO7qAVK1WokpznLzeTitHqWGnu6I4uLc8cwOc1iTInWsu2/DQ54ASWg4ylF5i2s6DwCQEqNWdCtCrTfag1JFu/cXVlWt3ilXWZRXJ9xTx4By8AOtKvKFGpS4wny7n3l29TpbG2bRfrS36rXmzO5lmjSuL6ooxe/KEcLL4IC3B9ZsGS50K298JIrU5KNSMmspPh3mls/Z9zTVehVpPcq0msrVZWqMuOiSaA2qu1bCNJ+j2koVPewY9Wo6tRzlxfiLiG6BduFK52bazp6+jpwmu7xPMdI/YUfeZv21xVtKu/Sa10afBrxMvpfdUbi2oOFtGlU39XF6PTuCvP2hfplC14F6mZK6T9R+Rp2OttT8jMl6rNLZz/lYASv4/YJ80y7sfjEqX33VvxRZ2O+1EK3USIomZANCGAxiDJQySIjAkiaOaJog6Jk0zkmNMDsiSZyTJJgdUySZzTHkg7Jkos4qRNJ6PAFmK0GU4Xc6k6kaNFzVN7recaknXuP8A6r/3ICzkeStQuutnOnOnKnUgk3F9z5nbgBMZFMkAHmtv/wBSX+UvqekPN7feNpx8aS+pYMnma2w45lX9wzMG10fit+t7ptC2LF+mXeOOF9Tpd2NOtb1qdduEKia3ks4eSvO8exndXE6MppuMcLlqzK2ltm9q30KVJqNKdONTDWiyiIr1+jkk80bqjPwejK09g7QprejQ313weTTo3FepHMo0n5aHeNaquNvUXjGRR5+VtcUXirRqR84koywuD+J6JXqi8SnOD/VElv0avrqjUX6lgaPPxlFokqaNuVjaVP7hR9yRz/hFDPYq1I+9qUZLg0EU2zVex6ufs60JL5EXsm8hr1O8u+LQ0UGtOWCEIqNTefBF2pQlBdujUT90ryiuDTWe8Co4OWX3kXAt9VoRdICruD3MHd02kR3ZY4AccZeEG74HXcakGM8QOe7oOCcXo2s9zJ4Gkt5AWbW7urZPqq00mmsN5RxUTpBrGBunnVAcnHDFqjq4NEHBgIxeknsaPvM2notUYvSR5pUfeFIybXgXIPBStuBaTMrXeU+yzS2a/wCVj5sx8mtsx/yq82CLN7raTOmx324kLr7rU8g2Q+1EK9IiSIokZDAAKBIkJDIGNCGihokiI0wJoZFMaYE0SRDJJMCaJZOeRuWFkghcV+qpyl3I87LpLdVd7daUYPdSzxNHaNXMHg8VCnNutOKXrPmWD0tj0guLacZuO/GTk5x3uLfP/gt0Om8K9TcjZtP3jx0aFyoNJa+ZWownKpiHrGsR7ip0irLaUqtOnGKnFQWXnxNDZW2a17cVKFRRxBbzaR4OVK43ksPR66npehkJem3qmtVTjz8xnQ9jCWUjqV6eiOqehyVJtJZb0PE9INqQq3znS9WC3N7vPQ7fuXb7NnuvEp9lHkbfZc9o18zk40lovEsak1Xp3bm991W13F2w2tK0rxqU3nGjTKV/setaXO7TjKUOTRCGzrnXEcI21617GW07faVuoV7beg2m0nxwXI3Wz5wUZUkklhJw4HibS6qWlVKbaceXee3taFveWsK1N6TWSMWYTtdl19VuLyeA/g9s1mlUnHylkVTZizpg4ysK0PUnNeTDKctk1orsXG94SRWqbKr4zKhCfkdlK+pLEakmvFZO9vd3e+o1KalnnjAGVKxlH+4qw91i6upT9WvOPvrJ6harUTpwlxhF+aBjzsatxHhKlU//AFOsbqotZWz84yya87G2qetSj8NDjPZVF+pKcPJgxTjtKC0bqw846E/SbWtpKVCb/UtTrLZtSK+zr595HCps6tjtUaNTySCYJ2VnWWtvHzpvBVqbHtX6kq1N+OpKdlKKzK3qxXfCT0ILMfVuKsPeWS6K8tjLHYu033Si0c57Hu46wVOaXdJF5Tr40rUanvaElOsvXt4v/LlkujGq2NzB5nbzXikcJU914nFx8z0HpSXrRuKfmtDpTuKdVqG/Tm+6cUNHmuqi3owdFp956ipawnH7S0pvxisHCWz7SS1pVafjF5Gjzyp4fA6KLRr/AMKofguZp904EHsmq87lajPw3sMuigt1rVEezkuy2ddw09HlL3dTjUtqlN4nSnHziBVlFN8DB6URUaFF/qf0PS9Uef6XRxb0Pff0IsefteBaSOVhHMZFhLBFLBp7M+7f6jObcnw4Glsv2El+oIt3Cza1PdI7I9aJ3xGdGpCTS3o4Tb5nTZmz503FzqU0lxe8grcSGx5pb2lTeXehvcx2ZZMhLAyJIoaQxIZADQhplElHPAe4+5lO9rVaVOMqM3CW9xRTV3cr/wCRU+YGzuvuY1F/lZiu9uVwrz+Y1fXXKvPPmMG3uy7mSSfczD/iF5LDlXm34E4311/jTGDYw+5nKtNxWDOV9c59sycK9atTlKvNSw8LTkMFe9lmLPJwUt6tJJvEnzPT3byeWzUi67Slu72rXAsE4Qudx9mW8VacanWYgnvLiWqc7hU3L7TPEq05VFUbhnefEqLTVzvp7svE9N0JcvTb/fyn1cePxPMutdJ8J4bzg9D0NnOV/eymmm4R0Yo9imdFJ4OMXodIy0OavN9M5z3LSEX603k4WtfqIxiuCLHSyOXay3uEmsGbTu6FNYm9RHXxtn0uFSK30c51YcsalFXNKaW7JajTi5Jxlkr0e0cds2kFQhWj62TT6ObTo2VnOnd1NyGcxbKe03mx3WjH2hUdK2p00uLyzUefyPotG/tLhZpXFOS94sJqSymmvA+Rqu+ZYobTuKHsa84e7LAcX1C4q07ejKtVeIQWWzlQv7avHNOrF/E8CukF7OhKhVqupTmsNS1Iw2jSeN+nr3rQmD6N11P8yJKcXwZ8/p7Qp5zGvUh4Zyi5Da90sKncxkv1IZV17YDylLpBdxjidONTxizvHpGlhzhUj5odj0hxr1urhmKUjKp9IaNWGaa35fI0rSMp0E6tPcb1wwOX8RS0lRn8CavbWa7WnhKJ2dCm+RB2tN8iW4IKhZVVlRpv/gi9mUJawlKPkyU7GnIh6FKHs5uPkyohLZ1RezuHjukjlOwuIrhSqfDBZjC6pPSbl72olcXUX2oRa8AKc6tzQhuVqdWlBeq6ayc1tCMV98i/CpBpmmr157VFpeY5XNrV7NRZ96IGVLaVOpQapOl12dHLgKnUuOLoU6n+XI03abPqLHV0X5Ef4Ta/hUo+UimKKuHD2lvWp+PH6FintKjnE63+6J1/hs4v7O4kl3NHOpZXS0Tp1V+pEMWs21xBeymn5Hjf+o1tQobNtXTowjJ1XqvI9DUsZrWdlCXuM8l08p7lnbNU6sPtHpN5XADzmyob1Ob8S1UpNPOCGwo5oVfM0J004tYKrNqRlSeq48DQ2V7Ka/UUatJ68WXdlerU8wLlzS6y3ks4wslewk54SyXZL7OXkyls54wBv0bfCz1kizQo9XntN57yNP1Ud1wIHzHkWdQAaZLJEaAY0IEBX2gvsF7xmurGKcHTy+O9n/g0doPFvlpvElwM2cqed2UZbz1TKD0iDfsWt5d/BklcU20+qaysNZ4eJDrLd/gn2uHgySnbuSluT10+ICVbRfYvjg6dbGOYyg22tH3HJVaaWtOejw8HCvdSpXKh1TlGS4ootRrJ7r6qSzpjuLlD2M/MpQq0m49ieHp8S7btOnUSzlPUgq19TAlKVJzVSE9G93HCSfeeiqrLKF1bRnq0F1jxupwjv5n1mGscsFe3lUpVesSeG9TRqWqS4ElbU3brE4uWdVzKar+kbtSMIzm6f5mtUbnRSpi+uZNylHcjFSaxniZsLRPGEei2NbRt6b01fEaa3KcsxJSlurJzpcCVRZgzCMPbzU1RnLhCR567s5VKuaW84y7uCPQbZ1sqmnDH1POwrVnLqoy3VzfeI68XejYTo2FScpNzziJxs4V3VW821nXL4FyntCFSi6Sg4yWiXiK3qb0mpLE1xNN5F64jJ2KlnLjy7zA2hOVS6kuCikseODbubiNChCVTL3nokYM25zlOXGTyxHPnjg4kXA74Fgrk44Fr3nfdE4+AHLekhqrJc38ybghdWBKN3Ui9JNHeG0qsca582VNxi3fADZstuVLWpvxpwfnE2afTWem/awfjvYPG7vcGqfED6HT6WWEox341It8cLOC/S21s6pFNXVOOeUnhny5SkiSqyXNkwfW6VelWjvUqsJrvTydMnyane1YerOS+Jeobdv6EcU7maXnn6g19LDHgeFodMLyEd2ahUffJf+jQt+mUGvt6Gv6H/wCwa9Q4xfJEZUKcuMTHt+lNhW9ffpeMln6F6ltewq43LmGvfoB1VrRbysMsJJLCPKwva1CvUVWh1kFJ4nCXFHaO2sPG9VprP4o8DHbXT0uoGFS25Tloq0ZvxWC3Dascb0kmv0saY0cnjf8AqXNfwm2WNXV/Y9LHalGTSalHxweW/wCpFxRqbKtlGScut0+RZTHk9g/d6vvI0pSRl7DeKFTzRoZRpHC5ytYPDOuy/WqryFUxKLQ9m6Vaq8gNN+o/IzrF4n8TQXAzrP2j8wPVUvUj5HWPA40X9nHyOyMiSAQyhoYgyBIYkwAr33sP9SM6UU2aN88WzeG8NcDIubl0ZpKCaazqWDrvLtPcWnDQeE3nGCpG8c2/s45XiOneuc1Dq0svHEqLaSxgOzl5iuGmhxr3Lt2koKWfEhG9lUlhUVlfqCrejSwi7ax3qU/gY8NoN1FF0ca44m7s/ElVjh5WM5ArVKHaOU6BpTjrwIOHgZGHWo4k4mLSVb0hpQl1jqYxjTGT11a1Up5wdIUFGOFFfIozreh9pquBtWkcIrqluzzjiXaCwiDvTWDrjJw6xQRVr3s/Vp6eJqeO0ctoUYvrKcuEkeRnSWZ5lLeg8JxPSV5OVOVRycsczyirzp1amHqpP5Grw9Wo6WyrdbpOLXe0aVCO68y4tmfRu+LlFHSV65tQprMm8Iw6aubVpVJU6FTGabyl4MzN03buUo2VvSWrjhvzIuwoXMpKMuqqf8M6ejle2Hui3S9c7OuLbWcHKP5o6oq4M2YjlgMHTA91EHLAsHXdFugc8BgnusMBHPdE4I6YDAHLcQnA7YFgDi4MW74HfAYA4YDMscTthCcUBzU5In18oriw3BbgHalf1IerNrzLcNs3MdHPeXcZm54C3cAbsduKS+1oUn/pOkb+wqYzSlTl3xlg89quY8yIPT07qlxpXk4vulqYnS+c52dvvVoVFvvgsPgVVNopbVm5U6eW+PMYOmxn9jU8y85Gdsh4pVPMuyeQpuWp02e/5ir5Ir5O2z3/ADVRfpA1Y8DOtdK0l4s0I8DOoaXE1+oD1Nv7GPkdkcLb2EPI7ogkMihkDAAKGiSIEkwOV2s28jz20/aRXJo9Fdfd5mFe0pVKsGs6LkslgzVuxjial4HWg/t4Y4ZJ9TcS0lFv4EqVtUjUi916PuKjttLGYZKMVGMUpOXn3mnfUZTlBpPQpujcPKcXjyAjS9rFp5WUeqsOM/JHmIW9WMk918e49Ps/jPyQosyjljUNCQ0YVynDIbh2VOc1mEW14B1NXPs5fIDjNRUHJ8EcI3TdPK0QbTlOlR3WmnLvMqnV7UXns5wzv4+P9GlKsmuOThPD1S4lZSfD8raOsXn4HfRXu9pWcXG0VT7WWmF3mHdWs4XEpYx3k7zZjpVpVYJ6y3k/Es29R3bca2FUS+Zz5TVZapTcsYeC7s6guuU56a4imWZqFOLlVe5GPPvM67jUupxnSTjGmtGuOTnx49tWvQ3EHOS00Qqa7ejKNleXTpqlcQ3ljCnz+Jcp5TWeDO8YatCbhiMtYnDaGw6V0nVtcQqYy48pHOdRqGjLNvduEoxb9Z4JZqvKSi4ScZLDi8NdwGntyj/MRuYR7NRYl7yMzB5+UxAAAQIeAABYDdQxgQcBbp0ADnusWDqIDlgWDrhA4gccBg6OIt1gQwGCeBYCI7qFuImAEOrM/asd2nT8zTSM7bPs6fmFc9lvEJ+Zccihs+WIyLTkQTcjts5/zUvdKjkTta/U3MZPg9GBuxKFL7xP3i/F8ChDS5qeYHpbR5t4eRZTKtl92iWkZEkx5IokUMAQEANCGATgqlNwlwfE4qxopYW9jzLA0yiv6DS75fMkrGl3y+Z2yNMDhDZ1GOcOWviS/h9P80/mWEx5Arfw+n+eR0t7SFvOcoyk3LvZ2Q8gMYgyQea6R3Ve3v0qVarFOmnuwm1qZi2leRi36Vc6cusZf6Tya2knFLPVR4/EyoSlOCxFZbSNQb9KdWdnbzrznOUte08tJlepF0pzi+HFF24ju0opcIpJHGut+EaiWccV3nq4zII0Zbzk+/Eiwnib8UU7WWrXhg7uXBlgKrXMp1qVFyUs4k+DTLNaLlGSjxa0MHaEJyoUau81KEdUZtXNjQdL0qa3nmMeR2jQ3NEcdlylKjF1F2jQXFliI29NKCbJylht9w1pA41ZYpyZRKVXKj4slTnio5Z0RXy3OKXFIG22qceLeAL1zHrNjb71xV3v2Mrq4vkbtzBQ2VVguEYpmKcfJ9HN0UyLoHYZzFV0ZIi6clyLosAUXF9wF5wT5EXSi+QFIZaduiDt2QVxnR0ZIi4SXICAwwwAQAMBBgBgR3UG6SHgCG4Zm21ilT941jL277Kl7wFGy4MtFWy4MtABCaJg9UwNfZ9brrZZ9aGjOS+9VPMp7Nr9TdJN9mWjLr0vKnmB6Kxf8tEtRZS2e/5ZeZbRB0yMgnklkB5GmRAgnkCKGBNPQMkVgYEkNCHkCSY0QGmUTyPJDI8kE8jRDJJMCje7JheXHXOWHuqODhLY9O3pupvrs64wa2Shteuqduotpb75mp9HHG/RxxKibUWlyKi2lOinTp7snx1fEk7pSW/FNfmj3Hrl6Ees6u4TSxrgsp50M2vVTq9mSeMMuQk2spN6ZIO8qvUOM/w5wytd2qq0alSniUNXxON3Kbt6klLKeMCtnKnVqUt57udUcPa669Y628XCEW1jKyi7EoU212ZN9h4+BalPGMa5R3jk6OWjXiV7uSj1cO9klVS4lW4blXU28RRRYj+KXwRZsKWajqS4LgVqCVSlHHA0qcVThGHPmWDveP8Ak7hfoRgo2toy3bWt4qKMVHDyfRIBZGcwwEMBjEADAEAADSfFAMCDpRZF0FyOoZArSoEHQkuRcEQUurkuQnF9xewu4NyL5AUBlx0Ysg7fuArGVt72VL3jblQZj9IIONGln8wGdZcGWirZcGWgAaDAYAjNYaaL1nOVRuc9W+ZUxvLDNDZW6oLeWUA621rqxqdVTUMYz2kQ/tHfd1P5HDbXavU1w3UZ4G0uk14oY6ulnvF/aa9/JSMYANpdJ73/AA6RJdJ7zOtOkYYAb66U3K40KbJrpVW528PgzzoAek/tXP8A+svmSXSt41tn8GeZAYPUrpZT520/g0SXSyjztqnzR5QYHrV0sts60Kq+RJ9KbT/BrcPA8gTqfhx+VDEetXSq0/wqv/BL+1Vn/hVfkjxwxg9mulNi+KqL4E49J9n59af+08SAwe5XSfZ355fIzNr1obauKDtaj6mCal4NnmS9sq4VG53Zt7k1/wAlkF+nY0eujFSko9/Nmtb7Ng5b60T4JvXBmW9fchO4Ud6U5bkE+RYq3NaNJUqU92XFyPRKrSezrONXeVPhxb5hVqqdJ0qUUm1wS1Mmnf30KM41FGcV+J8Tna7Qi7xzlJxe7pjkxy5dNcfot5RdGVKpxzwfmduxK5rVI8N473Nv6XDtpRnyqQ5+ZWp0nbp0payXN8zlx7rfKCt2JxnyejOtB70IrubRzrLrqbio4eOPiK0nxysPOWjs5Ou5pLXUp3D7L7WTvXU5y3Ked58kWLDZzhGVe4qqOeyo4zgonYUpU6MZT+CLkITlPeb1M2vK7tpb8HGtT8Czs7alO6mqcluVM8GB12pU7Ch3yyzOLW0Z5unH8pVPPyvYaAEBgMaEBRIBABIBBkCQCABiACAGIYANCGAAgGAGH0o9hR979jdMLpP93o++/oBm7MScJlvcXcVdl+zn5l7ASufVoTpeJ1GRHKMHnhoWbOCpSaz2V3kFoNBSu6ca9VyXAreix/M/kW2LBU1V9Fj+Z/IfokPzv5FnGBYBqv6HD87+Qehw/O/kWcBghqv6FH/EfyD0KL/G/kWcc0SRTVT0Fcp/8C9Bl+b/AILi0HnXiDVP0GX5kHoMvzRLqAGqPoE/zIk7KcsdqOiwXkgSBqh6DUX4oh6DV74/M0MDS04A1neg1fD5idlVXcaWAwhprM9CrdxKNlXymomljAFNcMypbkZrG7r8RRvoxxvpyO1Smqiw/mcPQY49ZmpyNQutoSrx3ILcgQsLmdCu9ykqu8sbrWThdUurq7sXoKjKdGaqU3iS5jdaj0ltXpzinRn1T/JLgdq0oTW/UWGuJk09oKaXXU4t/mR3u7uDoNJ+ssE+Os5SxrxsqLekmkHotKlLSCk85z3lGG04xUU3rjBL+Jb1RR3owyvWZ3lc61t6O7vwpwjUS004nFXNOq91092T9ZPmZyurWpUSlWm58s8GRr7UUZOMqOJL8RrU13uE6E9+muw+MTnGlb1KsKkOxPKehCN3SrxeZ6+J0pQ3LadRLMnwwZtEK0+trzn3sgLVcRnCgQxDIAYhkDAQwAYgKGADIAAAAGIChjEADGIaADC6T/d6Pv8A7G6YXSf7vR9/9iChsn2c/MvlHZPsp+ZfSwEpYAYyIQDwGAAB7oJAIeMjHgCKQyWAwAsBgngAiBLA8DwFRwMlgMYAiNDaBIqGkGBgAAMApJDxgeB4CFgMEkh4QGdeQzUzg4QhnQv1cSkyvKO7qdZGnOGmjIzSzx4cCU+JGSyslCc3odFJOn2+eqODWUGHjGSIaeZ7zfM2HRTj9q1uySafcYbjh55MuUasqtnKEpNtNYz3FlV1dOk6jjCTklzNiyqRp0NfVawY9vUp2zUZr1uZbt6jnQlDdxFSyn3jZBc6+GWpRzjmSSoTTb0+OCnjuDLRyqat9RCXqzF6NP8AC4v4nCXYlhSz4ocaslwk/iQ106qouMGRejOkbma46k/SYS0lFfILrghnfNCfLHxHK3g12Zv4oGq4HX0afJp+TIypVIetBoKiCENAMBZABgAwAAAoBiGAGH0n9hR9/wDY3DD6T/d6Pv8A7AUdkezn5mgUNkezn5l9kQAAyBZGgSGAwAAgRJCQwGMQAMQwAeA0BBjUBgANAA8CY0A0gAa4AHmNIBgLgMARQDBCZBXvd2FGVXO7Jf8AJnRveU45R32vU9nT+LMw1LYq+qkJeqxb2HrzKQ95rma9jFt5SaFlFbM9MsTz3j2FmU47jTZzjWcJJx5HEZNFhV5PMm9eRs2st+3hLvWvmefTNfZNTepVIPlqiC9wEPmBEAAADzrpwDKFjQfDAD3lkaljg2RfEMhHVVZLHaOiumnw+RWAC719OXrJPzRJQoT5Y91lHzHvILq47aD9Wpu+8iLtZ/hakvBleM5J9mTOnXTXDDBqTpTj60WiB2p3kku1leTOsbmnL1lF+cQaqAW9yhPgseTE7aD9Wp80VdVgR3drVS03X5M5unOPGMl8AqJh9KPu9H3/ANjcRidKfu9D339AM/ZHsp+ZoMz9keyn5mijNCSGPABAmPIhgAAADAYgJAIMagPINiACSYZEPADyPzEkPzAegCyMIY34CyPOShjRFDRA8j5kRlDAQ0BhbQm53tTP4dCsdrv73V95nENGAAUGdEu4AAAGIAGXtl1HG4Uc6S0KKwdaFTqqsZdzyB6IBN51XB6gmyMmmN95EMkDDiCHjLKFh94NEtMCIENDEAgz3jxrqGEUCxyGmLAuZBNZeg914IJ45j3mVEk8cCSqyX4n8yGUwwB3jdST11O8bxvjn6lHA1lcANBV6U/WjF+awef6Xqn6LQcI47b4PPI0smL0nebej737BZVLY/sqnmaBn7H9lPzNHBGiTwMMBzCAYgAYyKY0wGgwIAJIOYkMAGAAA0IYDxgkiOR5ABiQZAedRiQZ8AGGRDKhoYkDyBJMedSAZAxb6DjeVG/xPKK5c2rLN0vCKKeQ0AACgAAAAAEwBonDDyn3AsSBRwwN+0lv2lN80sM7FDZNTNOpB64eTQJWURoGGAAAJcQEPIuAMgYIOKDBQmHLUA4EDWOYtGGB4KFoNBjJJQz4ARGs4JbgYYwRHqAwgRi9Jn/L0fff0NsxOkvsKPvfsCKex/ZVPMv51MzZk3CEscC8q0Xx0I27ZDJFNPg8jAeRiAAGIaAaABBEhoWRgAxLA8gAxAmAx/AjnIwHwDAgAkHMWQbAkgzqLIZAlkaIDWeYE0DZFDzqUY20l/OP3UVS5tT70n+lFQKAACgATLNG1da0qVY+tB6rwArgAABNakB5YGhsqWLvd/NFmuYmz5YvKT73g2xWQCADIb4jQgAYYwLILiAwFrxHxATDixoklkohgmo5JKKQyyBJY4EgA0oGRjKMuDTwSBo3U+QtxEhgQ3DD6TrFvR979j0CMLpV93oe/wDsSwYtlLdT8S3vN6Y0KNtwLSZlXeE2mtS2pZWSjB5ZZoZ1T5Eo6ggY8AAyKGAwIpkgGGRNgBLI9O8iAEvIWQABofkJAUMa4CyCAYCHgBghAETAim0xphTAWQzkDM2p7eLfOJSL+1V26T8GiiAAAABp7HfYrLGmhmGlsjhWfggKt9RVG4e6uzLVFc1NrwzRpzX4XgywAYhoo7Wst24pvukj0HNnm4PDz3Hooy3oxkuDWQiQyI2QMeSIEEt5Bpkisc0SSKJYGl3EcPOSSk1yLge6MW8u8kigGCGUAYAYHGjCdpeK6oRjPjvU58JGjTjbX+tq+qrcZUJ8fh3lUjKnGTT4NcGuKCYnKLjJxksNcULA9X6zcn3sYUjC6Vfd6Hv/ALG+YPSv7tQ99/QlGDbcCxzK9v6p2Mq70nhl6CxFYKFGLnUjCPGTwaVbdVRxh6sdEQJAJDegADECAMAuIwABhkMgGQbDIAGRieoAMeiEADyCAAJBkimGcASzqBFMYDyPJEeShhgESTRBR2nHNGEvyszTauodZb1I+GUYpQAMAA0dkPWqvBGcXtkyxWmubiBevYb1pUXhkwkehl2ouL5rB59rdk4vk8AA14iDBRNI3qDzb0+fZR59I2tnS3rRL8rwBa0HyEhpZIFvYGsvhwJxglx1JlwRUSQYGaQkh4HgAEG6hjAST7ySb5oAwA0xiwNAMZEYDQxEgAwelf3ah77+hvmB0s+7UPff0JRg2/qlu3outVUV8WAGWo0PQuodKtGeVKTSQAAXlMA86gBGQAwAWQAAGDAAAAAAHgYFCwNIYALmAwIEwAChoYAQA0AFDQMAAOJiVodXWnHuYABAAABlnZrxeQ8cjADWZjX9Pq7ueOEtUAAVxoYFD1NXZWXRqL9QwA0Iw7yaQAVDHgAAYDAoAwMADAYGABgYAADAAowGBgEGBoAAZg9K/u9D339AAlH/2Q==">12 年前 (2012 年 11 月 10 日) — 52:16 <a href="https://youtube.com/watch?v=XtNXQJkgkhI">https://youtube.com/watch?v=XtNXQJkgkhI</a></p><p> 12 years ago (Nov 10, 2012) — 52:16 <a href="https://youtube.com/watch?v=XtNXQJkgkhI">https://youtube.com/watch?v=XtNXQJkgkhI</a></p>
        <h2 id="unknown-440">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu 教授：今天我们将结束上次开始的贝叶斯推理讨论。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu PROFESSOR: So we’re going to finish
            today
            our discussion of Bayesian Inference, which we started last time.</p>
        <p>您可能已经看到，目前我们介绍的概念并不多，涉及计算概率的具体技能。相反，这更像是一种解释和框架的建立。因此，贝叶斯估计的框架是，有一些参数是未知的，但我们对它有一个先验分布。这些是关于这个变量可能是什么的信念，然后我们将获得一些测量值。</p><p>As you probably saw there’s not a huge lot of concepts that we’re introducing at this point in terms of
            specific
            skills of calculating probabilities. But, rather, it’s more of an interpretation and setting up the
            framework.
            So the framework in Bayesian estimation is that there is some parameter which is not known, but we have a
            prior
            distribution on it. These are beliefs about what this variable might be, and then we’ll obtain some
            measurements.</p>
        <p>测量结果会受到我们不知道的参数值的影响。而这种影响，即 X 受 Theta 影响的事实，可以通过引入条件概率分布来捕捉。X 的分布取决于 Theta。这是一个条件概率分布。所以我们有这两个密度的公式，即先验密度和条件密度。</p><p>And the measurements are affected by the value of that parameter that we don’t know. And this effect, the
            fact
            that X is affected by Theta, is captured by introducing a conditional probability distribution. the
            distribution
            of X depends on Theta. It’s a conditional probability distribution. So we have formulas for these two
            densities,
            the prior density and the conditional density.</p>
        <h2 id="unknown-441">未知</h2><h2>Unknown</h2>
        <p>有了这些，如果将它们相乘，我们还可以得到 X 和 Theta 的联合密度。所以我们知道了这一秒内需要知道的一切。现在我们观察随机变量 X。给定这个随机变量，我们能对 Theta 说什么？好吧，我们可以做的是，给定 X，我们总是可以计算出 theta 的条件分布。</p><p>And given that we have these, if we multiply them we can also get the joint density of X and Theta. So we
            have
            everything that’s there is to know in this second. And now we observe the random variable X. Given this
            random
            variable what can we say about Theta? Well, what we can do is we can always calculate the conditional
            distribution of theta given X.</p>
        <p>现在我们有了 X 的具体值，我们可以将其绘制为 Theta 的函数。好的。这是贝叶斯推理问题的完整答案。这个后验分布捕获了有关 Theta 的所有内容，这就是我们对 Theta 的了解。鉴于我们观察到的 X Theta 仍然是随机的，它仍然是未知的。它可能在这里、那里或那里，有几种概率。</p><p>And now that we have the specific value of X we can plot this as a function of Theta. OK. And this is the
            complete answer to a Bayesian Inference problem. This posterior distribution captures everything there is to
            say
            about Theta, that’s what we know about Theta. Given the X that we have observed Theta is still random, it’s
            still unknown. And it might be here, there, or there with several probabilities.</p>
        <p>另一方面，如果你想报告 Theta 的单个值，那么你需要做一些额外的工作。你从这里继续，对 X 进行一些数据处理。进行数据处理意味着你对数据应用某个函数，这个函数是你设计的。这就是所谓的估计器。一旦应用了该函数，它就会输出 Theta 的估计值，我们称之为 Theta hat。</p><p>On the other hand, if you want to report a single value for Theta then you do some extra work. You continue
            from
            here, and you do some data processing on X. Doing data processing means that you apply a certain function on
            the
            data, and this function is something that you design. It’s the so called estimator. And once that function
            is
            applied it outputs an estimate of Theta, which we call Theta hat.</p>
        <h2 id="unknown-442">未知</h2><h2>Unknown</h2>
        <p>这就是正在发生的事情的大致情况。现在要记住的一件事是，尽管我在这里写的是单个字母，但一般来说，Theta 或 X 可能是向量随机变量。所以想想看。它可能是一个集合 Theta1、Theta2、Theta3。也许我们得到了几个测量值，所以这个 X 实际上是向量 X1、X2，直到 Xn。好的，那么现在我们如何选择要报告的 Theta？</p><p>So this is sort of the big picture of what’s happening. Now one thing to keep in mind is that even though I’m
            writing single letters here, in general Theta or X could be vector random variables. So think of this. it
            could
            be a collection Theta1, Theta2, Theta3. And maybe we obtained several measurements, so this X is really a
            vector
            X1, X2, up to Xn. All right, so now how do we choose a Theta to report?</p>
        <p>有多种方法可以做到这一点。一种方法是查看后验分布并报告密度或 PMF 最高的 Theta 值。这称为最大后验估计。因此，我们选择后验最大的 Theta 值并报告它。另一种方法是尝试在均方误差方面达到最优。那么这是什么呢？</p><p>There are various ways of doing it. One is to look at the posterior distribution and report the value of
            Theta,
            at which the density or the PMF is highest. This is called the maximum a posteriori estimate. So we pick a
            value
            of theta for which the posteriori is maximum, and we report it. An alternative way is to try to be optimal
            with
            respects to a mean squared error. So what is this?</p>
        <p>如果我们有一个特定的估计量 g，那么这就是它将产生的估计值。这是 Theta 的真实值，所以这是我们的估计误差。我们查看估计误差的平方，然后查看平均值。我们希望这个平方估计误差尽可能小。我们如何设计我们的估计量 g 以使该误差尽可能小？</p><p>If we have a specific estimator, g, this is the estimate it’s going to produce. This is the true value of
            Theta,
            so this is our estimation error. We look at the square of the estimation error, and look at the average
            value.
            We would like this squared estimation error to be as small as possible. How can we design our estimator g to
            make that error as small as possible?</p>
        <h2 id="unknown-443">未知</h2><h2>Unknown</h2>
        <p>答案是，在给定 X 的情况下，生成 Theta 的条件期望作为估计值。因此，如果您的目标是使均方误差尽可能小，那么条件期望就是您可以生成的最佳估计值。因此，此处的陈述说明了在我们的实验中可能发生的所有 Theta 和所有 X 的平均值。</p><p>It turns out that the answer is to produce, as an estimate, the conditional expectation of Theta given X. So
            the
            conditional expectation is the best estimate that you could produce if your objective is to keep the mean
            squared error as small as possible. So this statement here is a statement of what happens on the average
            over
            all Theta’s and all X’s that may happen in our experiment.</p>
        <p>条件期望作为估计量具有更强的性质。它不仅在平均水平上是最优的，而且在你进行了特定观察的情况下也是最优的，无论你观察到什么。假设你观察到随机变量 X 的特定值。</p><p>The conditional expectation as an estimator has an even stronger property. Not only it’s optimal on the
            average,
            but it’s also optimal given that you have made a specific observation, no matter what you observe. Let’s say
            you
            observe the specific value for the random variable X.</p>
        <p>在此之后，如果您被要求生成一个最小化该均方误差的最佳估计 Theta 帽，则最佳估计将是给定您观察到的特定值的条件期望。这两个陈述几乎说的是同一件事，但这个陈述更强一些。这个陈述告诉您，无论发生什么特定的 X，条件期望都是最佳估计。</p><p>After that point if you’re asked to produce a best estimate Theta hat that minimizes this mean squared error,
            your best estimate would be the conditional expectation given the specific value that you have observed.
            These
            two statements say almost the same thing, but this one is a bit stronger. This one tells you no matter what
            specific X happens the conditional expectation is the best estimate.</p>
        <h2 id="unknown-444">未知</h2><h2>Unknown</h2>
        <p>这个告诉你，平均而言，在所有可能发生的 X 中，条件期望是最好的估计量。这实际上是这个的一个结果。如果条件期望对于任何特定的 X 都是最好的，那么即使 X 是随机的，并且你正在对所有可能的 X 求平均误差，它也是最佳的。</p><p>This one tells you on the average, over all X’s may happen, the conditional expectation is the best
            estimator.
            Now this is really a consequence of this. If the conditional expectation is best for any specific X, then
            it’s
            the best one even when X is left random and you are averaging your error over all possible X’s.</p>
        <p>好的，现在我们知道了生成估计值的最佳方法，让我们举一个简单的例子来看看事情是如何运作的。所以我们从一个未知的随机变量 Theta 开始，它在 4 到 10 之间均匀分布。然后我们有一个观察模型，它告诉我们，给定 Theta 的值，X 将是一个随机变量，范围在 Theta 1 和 Theta + 1 之间。所以把 X 看作 Theta 的噪声测量，加上一些噪声，介于 1 和 +1 之间。所以我们在这里使用的模型实际上是 X 等于 Theta 加上 U</p><p>OK so now that we know what is the optimal way of producing an estimate let’s do a simple example to see how
            things work out. So we have started with an unknown random variable, Theta, which is uniformly distributed
            between 4 and 10. And then we have an observation model that tells us that given the value of Theta, X is
            going
            to be a random variable that ranges between Theta 1, and Theta + 1. So think of X as a noisy measurement of
            Theta, plus some noise, which is between 1, and +1. So really the model that we are using here is that X is
            equal to Theta plus U</p>
        <p>其中 U 在 1、+1、1 和加 1 处均匀分布。因此，我们得到了 Theta 的真实值，但 X 可能是 Theta 1，也可能是一直到 Theta + 1。并且 X 在该区间内均匀分布。这等同于说 U 在该区间内均匀分布。因此，现在我们掌握了所需的所有信息，我们可以构造联合密度。</p><p>Where U is uniform on 1, and +1. one, and plus one. So we have the true value of Theta, but X could be Theta
            1,
            or it could be all the way up to Theta + 1. And the X is uniformly distributed on that interval. That’s the
            same
            as saying that U is uniformly distributed over this interval. So now we have all the information that we
            need,
            we can construct the joint density.</p>
        <h2 id="unknown-445">未知</h2><h2>Unknown</h2>
        <p>当然，联合密度是先验密度乘以条件密度。我们同时考虑这两个。这两个都是常数，所以联合密度也将是一个常数。1/6 乘以 1/2，这是 1/12。但它是一个常数，不是处处都是。只在可能的 x 和 theta 范围内。所以 theta 可以取 4 到 10 之间的任何值，所以这些是 theta 的值。</p><p>And the joint density is, of course, the prior density times the conditional density. We go both of these.
            Both
            of these are constants, so the joint density is also going to be a constant. 1/6 times 1/2, this is one over
            12.
            But it is a constant, not everywhere. Only on the range of possible x’s and thetas. So theta can take any
            value
            between four and ten, so these are the values of theta.</p>
        <p>对于任何给定的 theta 值，x 可以取从 theta 减一到 theta 加一的值。因此，如果您能想象，一条斜率为 1 的直线，x 可以取 theta 加一或减一的值。因此，这里的这个对象是可能的 x 和 theta 对的集合。因此，密度等于该集合的 1/12，其他地方为零。</p><p>And for any given value of theta x can take values from theta minus one, up to theta plus one. So here, if
            you
            can imagine, a line that goes with slope one, and then x can take that value of theta plus or minus one. So
            this
            object here, this is the set of possible x and theta pairs. So the density is equal to one over 12 over this
            set, and it’s zero everywhere else.</p>
        <p>因此，这里的外面密度为零，密度只适用于该点。好的，现在我们被要求根据 x 估计 theta。所以我们想要建立一个估计量，它将是一个从 x 到 theta 的函数。这就是我这样选择轴的原因。x 在这个轴上，theta 在那个轴上。因为我们要构建的估计量是 x 的函数。</p><p>So outside here the density is zero, the density only applies at that point. All right, so now we’re asked to
            estimate theta in terms of x. So we want to build an estimator which is going to be a function from the x’s
            to
            the thetas. That’s why I chose the axis this way. x to be on this axis, theta on that axis. Because the
            estimator we’re building is a function of x.</p>
        <h2 id="unknown-446">未知</h2><h2>Unknown</h2>
        <p>根据我们获得的观察结果，我们想要估计 theta。因此，我们知道，给定 x 的值，最佳估计量是条件期望。那么条件期望是什么？如果你固定 x 的一个特定值，比如说在这个范围内。这就是我们的 x，那么我们对 theta 了解多少？我们知道 theta 位于这个范围内。只能在这两个值之间采样 theta。</p><p>Based on the observation that we obtained, we want to estimate theta. So we know that the optimal estimator
            is
            the conditional expectation, given the value of x. So what is the conditional expectation? If you fix a
            particular value of x, let’s say in this range. So this is our x, then what do we know about theta? We know
            that
            theta lies in this range. Theta can only be sampled between those two values.</p>
        <p>那么 theta 具有什么样的分布？给定 x，theta 的条件分布是什么？还记得我们是如何从联合分布构建条件分布的吗？条件分布只是联合分布的一部分，应用于我们进行条件化的地方。因此联合分布是恒定的。因此，条件分布在此区间内也将是恒定密度。因此，theta 的后验分布在此区间内是均匀的。</p><p>And what kind of distribution does theta have? What is the conditional distribution of theta given x? Well,
            remember how we built conditional distributions from joint distributions? The conditional distribution is
            just a
            section of the joint distribution applied to the place where we’re conditioning. So the joint is constant.
            So
            the conditional is also going to be a constant density over this interval. So the posterior distribution of
            theta is uniform over this interval.</p>
        <p>因此，如果 θ 的后验在该间隔内是均匀的，则 θ 的预期值将是该间隔的交点。因此，如果您观察到 θ，您报告的估计值将是此处的这个特定点，即中点。即使您在此处某处获得 x，同样的论点也适用。给定这个 x，θ 可以取这两个值之间的值。</p><p>So if the posterior of theta is uniform over that interval, the expected value of theta is going to be the
            meet
            point of that interval. So the estimate which you report. if you observe that theta. is going to be this
            particular point here, it’s the midpoint. The same argument goes through even if you obtain an x somewhere
            here.
            Given this x, theta can take a value between these two values.</p>
        <h2 id="unknown-447">未知</h2><h2>Unknown</h2>
        <p>θ 在这个区间上将具有均匀分布，并且给定 x 的 θ 的条件期望将是该区间的中点。所以现在如果我们通过跟踪此图中的中点来绘制我们的估计量，您将获得一条曲线，该曲线的开始是这样的，然后斜率会改变。这样它就会跟踪中点，然后再次像这样。</p><p>Theta is going to have a uniform distribution over this interval, and the conditional expectation of theta
            given
            x is going to be the midpoint of that interval. So now if we plot our estimator by tracing midpoints in this
            diagram what you’re going to obtain is a curve that starts like this, then it changes slope. So that it
            keeps
            track of the midpoint, and then it goes like that again.</p>
        <p>因此，这里的蓝色曲线是 x 的 g，即在 x 等于小 x 的情况下，θ 的条件期望。因此，这是一条曲线，在我们的例子中，它由三段直线组成。但总体而言，它是非线性的。它不是贯穿该图的一条直线。一般情况下情况就是这样。x 的 g，我们的最优估计没有理由是 x 的线性函数。</p><p>So this blue curve here is our g of x, which is the conditional expectation of theta given that x is equal to
            little x. So it’s a curve, in our example it consists of three straight segments. But overall it’s non
            linear.
            It’s not a single line through this diagram. And that’s how things are in general. g of x, our optimal
            estimate
            has no reason to be a linear function of x.</p>
        <p>一般来说，这将是一些复杂的曲线。那么我们的估计有多好？我的意思是，你报告了你的 x，即基于 x 的 theta 估计值，你的老板问你预计会得到什么样的误差？在观察到 x 的特定值后，你可以向老板报告你认为的均方误差是多少。我们观察 x 的特定值。</p><p>In general it’s going to be some complicated curve. So how good is our estimate? I mean you reported your x,
            your
            estimate of theta based on x, and your boss asks you what kind of error do you expect to get? Having
            observed
            the particular value of x, what you can report to your boss is what you think is the mean squared error is
            going
            to be. We observe the particular value of x.</p>
        <h2 id="unknown-448">未知</h2><h2>Unknown</h2>
        <p>所以我们是在条件反射，我们生活在这个宇宙中。假设我们已经进行了这个观察，这是 theta 的真实值，这是我们得出的估计值，这是预期平方误差，假设我们已经进行了特定的观察。现在在这个条件宇宙中，这是给定 x 的 theta 的预期值。所以这是条件宇宙中这个随机变量的预期值。</p><p>So we’re conditioning, and we’re living in this universe. Given that we have made this observation, this is
            the
            true value of theta, this is the estimate that we have produced, this is the expected squared error, given
            that
            we have made the particular observation. Now in this conditional universe this is the expected value of
            theta
            given x. So this is the expected value of this random variable inside the conditional universe.</p>
        <p>因此，当你取一个随机变量的均方减去期望值时，这与该随机变量的方差是一样的。只不过它是条件宇宙内的方差。观察到 x 后，theta 仍然是一个随机变量。它根据后验分布进行分布。由于它是一个随机变量，所以它有一个方差。这个方差就是我们的均方误差。</p><p>So when you take the mean squared of a random variable minus the expected value, this is the same thing as
            the
            variance of that random variable. Except that it’s the variance inside the conditional universe. Having
            observed
            x, theta is still a random variable. It’s distributed according to the posterior distribution. Since it’s a
            random variable, it has a variance. And that variance is our mean squared error.</p>
        <p>因此，根据我们所做的观察，这就是 Theta 的后验分布的方差。好的，那么我们示例中的方差是多少？如果 X 恰好在这里，那么 Theta 在这个区间上是均匀的，并且这个区间的长度为 2。Theta 在长度为 2 的区间上均匀分布。这是 Theta 的后验分布。方差是多少？</p><p>So this is the variance of the posterior distribution of Theta given the observation that we have made. OK,
            so
            what is the variance in our example? If X happens to be here, then Theta is uniform over this interval, and
            this
            interval has length 2. Theta is uniformly distributed over an interval of length 2. This is the posterior
            distribution of Theta. What is the variance?</p>
        <h2 id="unknown-449">未知</h2><h2>Unknown</h2>
        <p>然后你记住均匀随机变量方差的公式，它是区间长度的平方除以 12，所以这是 1/3。所以，当这种情况适用时，均方误差 Theta 的方差将是 1/3。</p><p>Then you remember the formula for the variance of a uniform random variable, it is the length of the interval
            squared divided by 12, so this is 1/3. So the variance of Theta the mean squared error. is going to be 1/3
            whenever this kind of picture applies.</p>
        <p>当 X 介于 5 和 9 之间时，此图适用。如果 X 小于 5，则图略有不同，Theta 将在较小间隔内保持均匀。因此，Theta 的方差也将较小。因此，让我们开始绘制均方误差。在 5 到 9 之间，Theta 的方差是后验方差。</p><p>This picture applies when X is between 5 and 9. If X is less than 5, then the picture is a little different,
            and
            Theta is going to be uniform over a smaller interval. And so the variance of theta is going to be smaller as
            well. So let’s start plotting our mean squared error. Between 5 and 9 the variance of Theta the posterior
            variance.</p>
        <p>是 1/3。现在，当 X 落在这里时，Theta 均匀分布在一个较小的区间内。这个区间的大小在这个范围内呈线性变化。因此，当我们取该区间的平方时，我们得到了一个二次函数，表示我们从那个角落移动了多少。那么在那个角落，Theta 的方差是多少？
        </p><p>Is 1/3. Now when the X falls in here Theta is uniformly distributed over a smaller interval. The size of this
            interval changes linearly over that range. And so when we take the square size of that interval we get a
            quadratic function of how much we have moved from that corner. So at that corner what is the variance of
            Theta?
        </p>
        <h2 id="unknown-450">未知</h2><h2>Unknown</h2>
        <p>好吧，如果我观察到 X 等于 3，那么我肯定知道 Theta 等于 4。那么我就可以很好地知道 Theta 到底是多少了。</p><p>Well if I observe an X that’s equal to 3 then I know with certainty that Theta is equal to 4. Then I’m in
            very
            good shape, I know exactly what Theta is going to be.</p>
        <p>因此，在这种情况下，方差将为 0。如果我观察到一个比 Theta 稍大的 X，那么它是随机的，在一个小的间隔内取值，而 Theta 的方差将与该小间隔长度的平方成正比。因此，我们得到一条从这里开始呈二次上升的曲线。它向前上升了 1/3。在图片的另一端也是如此。</p><p>So the variance, in this case, is going to be 0. If I observe an X that’s a little larger than Theta is now
            random, takes values on a little interval, and the variance of Theta is going to be proportional to the
            square
            of the length of that little interval. So we get a curve that starts rising quadratically from here. It goes
            up
            forward 1/3. At the other end of the picture the same is true.</p>
        <p>如果您观察到 X 为 11，那么 Theta 只能等于 10。因此 Theta 中的误差等于 0，误差方差为 0。但是当我们获得略小于 11 的 X 时，均方误差再次呈二次方上升。所以我们最终得到了这样的图。该图告诉我们某些测量结果比其他测量结果更好。</p><p>If you observe an X which is 11 then Theta can only be equal to 10. And so the error in Theta is equal to 0,
            there’s 0 error variance. But as we obtain X’s that are slightly less than 11 then the mean squared error
            again
            rises quadratically. So we end up with a plot like this. What this plot tells us is that certain
            measurements
            are better than others.</p>
        <h2 id="unknown-451">未知</h2><h2>Unknown</h2>
        <p>如果你很幸运，看到 X 等于 3，那么你很幸运，因为你确切地知道 Theta 是多少。如果你看到 X 等于 6，那么你就不太可能了，因为它不能非常精确地告诉你 Theta。Theta 可能在该间隔的任何位置。因此，即使在你观察到 X 之后，Theta 的方差也是一个确定的数字，在我们的例子中是 1/3。</p><p>If you’re lucky, and you see X equal to 3 then you’re lucky, because you know Theta exactly what it is. If
            you
            see an X which is equal to 6 then you’re sort of unlikely, because it doesn’t tell you Theta with great
            precision. Theta could be anywhere on that interval. And so the variance of Theta even after you have
            observed X
            is a certain number, 1/3 in our case.</p>
        <p>因此，这个故事的寓意在于，误差方差或均方误差取决于您碰巧获得的特定观察结果。一些观察结果可能非常有用，一旦您看到一个特定的数字，您就知道 Theta 到底是什么。一些观察结果可能不那么有用。您观察到 X，但它仍然可能对 Theta 留下很多不确定性。因此，条件期望实际上是贝叶斯估计的基石。</p><p>So the moral to keep out of that story is that the error variance. or the mean squared error. depends on what
            particular observation you happen to obtain. Some observations may be very informative, and once you see a
            specific number than you know exactly what Theta is. Some observations might be less informative. You
            observe
            your X, but it could still leave a lot of uncertainty about Theta. So conditional expectations are really
            the
            cornerstone of Bayesian estimation.</p>
        <p>它们特别受欢迎，尤其是在工程领域。它们在信号处理、通信、控制理论等方面应用广泛。因此，值得我们尝试一下它们的理论特性，并了解一下其中涉及的一些微妙之处。实际上，我们在这里要做的事情并不涉及新的数学。但这将是练习操纵条件期望的好机会。
        </p><p>They’re particularly popular, especially in engineering contexts. There used a lot in signal processing,
            communications, control theory, so on. So that makes it worth playing a little bit with their theoretical
            properties, and get some appreciation of a few subtleties involved here. No new math in reality, in what
            we’re
            going to do here. But it’s going to be a good opportunity to practice manipulation of conditional
            expectations.
        </p>
        <h2 id="unknown-452">未知</h2><h2>Unknown</h2>
        <p>那么让我们看看我们得到的估计误差的期望值。Theta hat 是我们的估计量，是条件期望。Theta hat 减去 Theta 是什么样的误差？如果 Theta hat 大于 Theta，那么我们就犯了正误差。如果不是，如果它在另一边，我们就犯了负误差。</p><p>So let’s look at the expected value of the estimation error that we obtained. So Theta hat is our estimator,
            is
            the conditional expectation. Theta hat minus Theta is what kind of error do we have? If Theta hat, is bigger
            than Theta then we have made the positive error. If not, if it’s on the other side, we have made the
            negative
            error.</p>
        <p>然后，结果发现，平均而言，误差相互抵消。那么让我们进行计算。让我们计算给定 X 的误差预期值。现在根据定义，误差是给定 X 的 Theta hat 预期值减去 Theta。我们使用期望的线性将其分解为给定 X 的 Theta hat 预期值减去给定 X 的 Theta 预期值。现在怎么办？</p><p>Then it turns out that on the average the errors cancel each other out, on the average. So let’s do this
            calculation. Let’s calculate the expected value of the error given X. Now by definition the error is
            expected
            value of Theta hat minus Theta given X. We use linearity of expectations to break it up as expected value of
            Theta hat given X minus expected value of Theta given X. And now what?</p>
        <p>我们的估计是基于 X 的数据做出的。如果我告诉你 X，你就知道 Theta hat 是什么。请记住，条件期望是一个随机变量，它是你正在对其设定条件的随机变量的函数。如果你知道 X，那么你就知道给定 X 的条件期望，你就知道 Theta hat 是什么。所以 Theta hat 是 X 的函数。</p><p>Our estimate is made on the basis of the data of the X’s. If I tell you X then you know what Theta hat is.
            Remember that the conditional expectation is a random variable which is a function of the random variable,
            on
            which you’re conditioning on. If you know X then you know the conditional expectation given X, you know what
            Theta hat is going to be. So Theta hat is a function of X.</p>
        <h2 id="unknown-453">未知</h2><h2>Unknown</h2>
        <p>如果它是 X 的函数，那么一旦我告诉你 X，你就知道 Theta hat 是什么。所以这个条件期望就是 Theta hat 本身。这就是。根据定义。</p><p>If it’s a function of X then once I tell you X you know what Theta hat is going to be. So this conditional
            expectation is going to be Theta hat itself. Here this is. just by definition.</p>
        <p>θ 帽，因此我们得到等于 0 的结果。所以我们证明了，无论我观察到什么，并且假设我观察到了平均的一些东西，我的误差将为 0。这是一个涉及随机变量相等的陈述。请记住，条件期望是随机变量，因为它们取决于你正在设定的条件。0 是一种微不足道的随机变量。</p><p>Theta hat, and so we get equality to 0. So what we have proved is that no matter what I have observed, and
            given
            that I have observed something on the average my error is going to be 0. This is a statement involving
            equality
            of random variables. Remember that conditional expectations are random variables because they depend on the
            thing you’re conditioning on. 0 is sort of a trivial random variable.</p>
        <p>这说明这个随机变量与 0 随机变量完全相等。更具体地说，它说明无论你观察到的 X 值是什么，误差的条件期望都将为 0。这就引出了这里的这个陈述，即数字之间的不平等。</p><p>This tells you that this random variable is identically equal to the 0 random variable. More specifically it
            tells you that no matter what value for X you observe, the conditional expectation of the error is going to
            be
            0. And this takes us to this statement here, which is inequality between numbers.</p>
        <h2 id="unknown-454">未知</h2><h2>Unknown</h2>
        <p>不管你观察到大写 X 的具体值是什么，平均而言，你的误差将等于 0。所以这是这些陈述的一个不太抽象的版本。这是两个数字之间的不等式。对于 X 的每个值，它都是正确的，因此从这些随机变量等于该随机变量的角度来看，它是正确的。
        </p><p>No matter what specific value for capital X you have observed, your error, on the average, is going to be
            equal
            to 0. So this is a less abstract version of these statements. This is inequality between two numbers. It’s
            true
            for every value of X, so it’s true in terms of these random variables being equal to that random variable.
        </p>
        <p>因为请记住，根据我们的定义，这个随机变量是当大写 X 恰好等于小写 x 时取特定值的随机变量。这并不意味着您的误差为 0，它只意味着您的误差在某种意义上既可能落在正值一侧，也可能落在负值一侧。所以有时您的误差为正值，有时为负值。</p><p>Because remember according to our definition this random variable is the random variable that takes this
            specific
            value when capital X happens to be equal to little x. Now this doesn’t mean that your error is 0, it only
            means
            that your error is as likely, in some sense, to fall on the positive side, as to fall on the negative side.
            So
            sometimes your error will be positive, sometimes negative.</p>
        <p>平均而言，这些因素相互抵消，平均结果为 0。因此，这个属性有时被称为 Theta hat 无偏。因此，我们的估计值 Theta hat 不会偏高。也不会偏低。平均而言，它恰到好处。因此，让我们在这里再多玩一会儿。</p><p>And on the average these things cancel out and give you a 0 on the average. So this is a property that’s
            sometimes giving the name we say that Theta hat is unbiased. So Theta hat, our estimate, does not have a
            tendency to be on the high side. It does not have a tendency to be on the low side. On the average it’s just
            right. So let’s do a little more playing here.</p>
        <h2 id="unknown-455">未知</h2><h2>Unknown</h2>
        <p>让我们看看我们的误差与数据的任意函数有何关系。让我们在条件宇宙中执行此操作并查看这个量。在已知 X 的条件宇宙中，X 的 h 也是已知的。因此，你可以将其拉出期望值。在给定 X 值的条件宇宙中，这个量就变成了一个常数。它没有任何随机性。</p><p>Let’s see how our error is related to an arbitrary function of the data. Let’s do this in a conditional
            universe
            and look at this quantity. In a conditional universe where X is known then h of X is known. And so you can
            pull
            it outside the expectation. In the conditional universe where the value of X is given this quantity becomes
            just
            a constant. There’s nothing random about it.</p>
        <p>所以你可以把它取出来，也就是期望值，然后这样写。我们刚刚计算出这个量是 0。所以这个数字也为 0。现在做完这些，我们可以取两边的期望值。现在让我们使用迭代期望定律。</p><p>So you can pull it out, the expectation, and write things this way. And we have just calculated that this
            quantity is 0. So this number turns out to be 0 as well. Now having done this, we can take expectations of
            both
            sides. And now let’s use the law of iterated expectations.</p>
        <p>条件期望的期望给出了无条件期望，而无条件期望也将为 0。因此，我们在这里使用迭代期望定律。好的。好的，我们为什么要这样做？我们这样做是因为我想计算 Theta tilde 和 Theta hat 之间的协方差。Theta hat 是问一个问题，误差和估计之间是否存在系统关系？</p><p>Expectation of a conditional expectation gives us the unconditional expectation, and this is also going to be
            0.
            So here we use the law of iterated expectations. OK. OK, why are we doing this? We’re doing this because I
            would
            like to calculate the covariance between Theta tilde and Theta hat. Theta hat is, ask the question is there
            a
            systematic relation between the error and the estimate?</p>
        <h2 id="unknown-456">未知</h2><h2>Unknown</h2>
        <p>因此，为了计算协方差，我们使用了以下特性：我们可以通过计算乘积的期望值减去期望值的乘积来计算协方差。我们得到了什么？这是 0，因为我们刚刚证明了这一点。这是 0，因为我们之前证明了这一点。</p><p>So to calculate the covariance we use the property that we can calculate the covariances by calculating the
            expected value of the product minus the product of the expected values. And what do we get? This is 0,
            because
            of what we just proved. And this is 0, because of what we proved earlier.</p>
        <p>误差的期望值等于 0。因此，误差与任何 X 函数之间的协方差等于 0。让我们将其用于考虑的 X 函数是 Theta hat 本身的情况。Theta hat 是我们的估计值，它是 X 的函数。因此这个 0 结果仍然适用，我们得到这个协方差等于 0。好的，这就是我们证明的。</p><p>That the expected value of the error is equal to 0. So the covariance between the error and any function of X
            is
            equal to 0. Let’s use that to the case where the function of X we’re considering is Theta hat itself. Theta
            hat
            is our estimate, it’s a function of X. So this 0 result would still apply, and we get that this covariance
            is
            equal to 0. OK, so that’s what we proved.</p>
        <p>让我们看看，从这一切中可以得到什么教训？首先，你应该非常熟悉这种涉及条件期望的计算。我们使用的主要两个原则是，当你对一个随机变量进行条件计算时，该随机变量的任何函数都会变成一个常数，并且可以从条件期望中抽取出来。我们使用的另一个原则是迭代期望定律，所以这些是所涉及的技能。</p><p>Let’s see, what are the morals to take out of all this? First is you should be very comfortable with this
            type of
            calculation involving conditional expectations. The main two things that we’re using are that when you
            condition
            on a random variable any function of that random variable becomes a constant, and can be pulled out the
            conditional expectation. The other thing that we are using is the law of iterated expectations, so these are
            the
            skills involved.</p>
        <h2 id="unknown-457">未知</h2><h2>Unknown</h2>
        <p>现在回到实质上，为什么这个结果很有趣？这告诉我们误差与估计值不相关。在什么假设情况下这些情况不会发生？每当 Theta hat 为正时，我的误差往往为负。假设每当 Theta hat 很大时，你就会说哦，我的估计值太大了，也许真正的 Theta 偏低，所以我预计我的误差为负。</p><p>Now on the substance, why is this result interesting? This tells us that the error is uncorrelated with the
            estimate. What’s a hypothetical situation where these would not happen? Whenever Theta hat is positive my
            error
            tends to be negative. Suppose that whenever Theta hat is big then you say oh my estimate is too big, maybe
            the
            true Theta is on the lower side, so I expect my error to be negative.</p>
        <p>这种情况会违反这一条件。这一条件告诉你，无论 Theta 帽是什么，你都不希望你的错误在正侧或负侧。你的错误平均仍为 0。因此，如果你获得了非常高的估计值，你没有理由怀疑真正的 Theta 低于你的估计值。</p><p>That would be a situation that would violate this condition. This condition tells you that no matter what
            Theta
            hat is, you don’t expect your error to be on the positive side or on the negative side. Your error will
            still be
            0 on the average. So if you obtain a very high estimate this is no reason for you to suspect that the true
            Theta
            is lower than your estimate.</p>
        <p>如果您怀疑真实的 Theta 低于您的估计值，您应该更改您的 Theta 帽。如果您进行估计，并在获得该估计值后说我认为我的估计值太大，因此误差为负。如果您这样想，则意味着您的估计值不是最佳的，您的估计值应该被修正为更小的。</p><p>If you suspected that the true Theta was lower than your estimate you should have changed your Theta hat. If
            you
            make an estimate and after obtaining that estimate you say I think my estimate is too big, and so the error
            is
            negative. If you thought that way then that means that your estimate is not the optimal one, that your
            estimate
            should have been corrected to be smaller.</p>
        <h2 id="unknown-458">未知</h2><h2>Unknown</h2>
        <p>这意味着存在比您使用的更好的估计值，但我们在这里使用的估计值是均方误差的最佳值，没有办法改进它。这句话确实抓住了这一点。也就是说，知道 Theta 帽并不能为您提供有关误差的大量信息，因此您没有理由调整您的估计值。</p><p>And that would mean that there’s a better estimate than the one you used, but the estimate that we are using
            here
            is the optimal one in terms of mean squared error, there’s no way of improving it. And this is really
            captured
            in that statement. That is knowing Theta hat doesn’t give you a lot of information about the error, and
            gives
            you, therefore, no reason to adjust your estimate from what it was.</p>
        <p>最后，所有这些的结果。这是误差的定义。将 Theta 发送到这边，将 Theta 波浪线发送到那边，您将得到这个关系。真实参数由两个量组成。估计值和它们得到的带负号的误差。这两个量彼此不相关。它们的协方差为 0，因此，它的方差是这两个量的方差之和。
        </p><p>Finally, a consequence of all this. This is the definition of the error. Send Theta to this side, send Theta
            tilde to that side, you get this relation. The true parameter is composed of two quantities. The estimate,
            and
            the error that they got with a minus sign. These two quantities are uncorrelated with each other. Their
            covariance is 0, and therefore, the variance of this is the sum of the variances of these two quantities.
        </p>
        <p>那么，如何解释这个等式呢？我们试图估计的随机变量 theta 中存在一些固有的随机性。Theta hat 试图估计它，试图接近它。如果 Theta hat 总是接近 Theta，那么由于 Theta 是随机的，Theta hat 也必须非常随机，因此它具有不确定性。Theta hat 越不确定，它与 Theta 的移动就越一致。</p><p>So what’s an interpretation of this equality? There is some inherent randomness in the random variable theta
            that
            we’re trying to estimate. Theta hat tries to estimate it, tries to get close to it. And if Theta hat always
            stays close to Theta, since Theta is random Theta hat must also be quite random, so it has uncertainty in
            it.
            And the more uncertain Theta hat is the more it moves together with Theta.</p>
        <h2 id="unknown-459">未知</h2><h2>Unknown</h2>
        <p>因此，它从 Theta 中消除的不确定性越多。这就是 Theta 中剩余的不确定性。我们完成估算后剩下的不确定性。因此，理想情况下，为了获得较小的误差，我们希望这个量很小。这等同于说这个量应该很大。在理想情况下，Theta hat 与 Theta 相同。这是我们所能期望的最好结果。</p><p>So the more uncertainty it removes from Theta. And this is the remaining uncertainty in Theta. The
            uncertainty
            that’s left after we’ve done our estimation. So ideally, to have a small error we want this quantity to be
            small. Which is the same as saying that this quantity should be big. In the ideal case Theta hat is the same
            as
            Theta. That’s the best we could hope for.</p>
        <p>这相当于 0 误差，Theta 中的所有不确定性都被 Theta hat 中的不确定性所吸收。有趣的是，这里的这个关系只是我们过去某个时候见过的总方差定律的另一种变体。我将跳过该推导，但这是一个有趣的事实，它可以为您提供总方差定律的另一种解释。好的，现在让我们回到我们的例子。</p><p>That corresponds to 0 error, and all the uncertainly in Theta is absorbed by the uncertainty in Theta hat.
            Interestingly, this relation here is just another variation of the law of total variance that we have seen
            at
            some point in the past. I will skip that derivation, but it’s an interesting fact, and it can give you an
            alternative interpretation of the law of total variance. OK, so now let’s return to our example.</p>
        <p>在我们的例子中，我们得到了最优估计量，我们看到它是一条非线性曲线，就像这样。我稍微夸大了拐角，以表明它是非线性的。这是最优估计量。它是 X 的非线性函数，非线性通常意味着复杂。有时条件期望真的很难计算，因为每当你必须计算期望时，你都需要做一些积分。</p><p>In our example we obtained the optimal estimator, and we saw that it was a nonlinear curve, something like
            this.
            I’m exaggerating the corner of a little bit to show that it’s nonlinear. This is the optimal estimator. It’s
            a
            nonlinear function of X nonlinear generally means complicated. Sometimes the conditional expectation is
            really
            hard to compute, because whenever you have to compute expectations you need to do some integrals.</p>
        <h2 id="unknown-460">未知</h2><h2>Unknown</h2>
        <p>如果涉及许多随机变量，它可能对应于多维积分。我们不喜欢这样。我们能否想出一种更简单的方法来估计 Theta？想出一个点估计，它仍然具有一些不错的特性，它有一些很好的动机，但更简单。更简单是什么意思？也许是线性的。让我们给自己穿上紧身衣，将自己限制在这些形式的估计量上。</p><p>And if you have many random variables involved it might correspond to a multi dimensional integration. We
            don’t
            like this. Can we come up, maybe, with a simpler way of estimating Theta? Of coming up with a point estimate
            which still has some nice properties, it has some good motivation, but is simpler. What does simpler mean?
            Perhaps linear. Let’s put ourselves in a straitjacket and restrict ourselves to estimators that’s are of
            these
            forms.</p>
        <p>我的估计被限制为 X 的线性函数。所以我的估计量将是一条曲线，一条线性曲线。它可能是这个，可能是那个，也许它会是这样的。我想选择最佳的线性函数。这是什么意思？这意味着我以这种形式写下我的 Theta 帽。</p><p>My estimate is constrained to be a linear function of the X’s. So my estimator is going to be a curve, a
            linear
            curve. It could be this, it could be that, maybe it would want to be something like this. I want to choose
            the
            best possible linear function. What does that mean? It means that I write my Theta hat in this form.</p>
        <p>如果我固定了某个 a 和 b，那么我的估计量的函数形式就固定了，这就是相应的均方误差。这是真实参数与该参数估计值之间的误差，我们取它的平方。现在，最佳线性估计量被定义为这些均方误差在所有 a 和 b 选择中最小的估计量。</p><p>If I fix a certain a and b I have fixed the functional form of my estimator, and this is the corresponding
            mean
            squared error. That’s the error between the true parameter and the estimate of that parameter, we take the
            square of this. And now the optimal linear estimator is defined as one for which these mean squared error is
            smallest possible over all choices of a and b.</p>
        <h2 id="unknown-461">未知</h2><h2>Unknown</h2>
        <p>因此，我们希望最小化所有 a 和 b 的这个表达式。我们如何实现最小化？这是一个平方，你可以展开它。写下平方展开式中的所有项。因此，你将得到 Theta 平方的期望值项。你将得到另一个项。a 平方的 X 平方的期望值，另一个项是 b 平方，然后你将得到各种交叉项。</p><p>So we want to minimize this expression over all a’s and b’s. How do we do this minimization? Well this is a
            square, you can expand it. Write down all the terms in the expansion of the square. So you’re going to get
            the
            term expected value of Theta squared. You’re going to get another term. a squared expected value of X
            squared,
            another term which is b squared, and then you’re going to get to various cross terms.</p>
        <p>这里实际上是 a 和 b 的二次函数。因此，将我们要最小化的这个量视为 a 和 b 的某个函数 h，并且它恰好是二次函数。我们如何最小化二次函数？我们将该函数关于 a 和 b 的导数设置为 0，然后进行代数运算。</p><p>What you have here is really a quadratic function of a and b. So think of this quantity that we’re minimizing
            as
            some function h of a and b, and it happens to be quadratic. How do we minimize a quadratic function? We set
            the
            derivative of this function with respect to a and b to 0, and then do the algebra.</p>
        <p>经过代数运算后，你会发现 a 的最佳选择是 1，所以这是 X 旁边的系数。这是最佳 a。最佳 b 对应于常数项。因此，这个项和这个乘以那个加在一起就是 b 的最佳选择。所以代数本身并不是很有趣。真正有趣的是我们在这里得到的结果的性质。</p><p>After you do the algebra you find that the best choice for a is this 1, so this is the coefficient next to X.
            This is the optimal a. And the optimal b corresponds of the constant terms. So this term and this times that
            together are the optimal choices of b. So the algebra itself is not very interesting. What is really
            interesting
            is the nature of the result that we get here.</p>
        <h2 id="unknown-462">未知</h2><h2>Unknown</h2>
        <p>如果我们在这个特定示例中绘制结果，您将得到类似这样的曲线。它穿过该图的中间，并且略微倾斜。在此示例中，X 和 Theta 呈正相关。X 值越大，Theta 值越大。因此，在此示例中，X 和 Theta 之间的协方差为正，因此我们的估计值可以按以下方式解释：。</p><p>If we were to plot the result on this particular example you would get the curve that’s something like this.
            It
            goes through the middle of this diagram and is a little slanted. In this example, X and Theta are positively
            correlated. Bigger values of X generally correspond to bigger values of Theta. So in this example the
            covariance
            between X and Theta is positive, and so our estimate can be interpreted in the following way:.</p>
        <p>Theta 的预期值是您在没有任何有关 Theta 的信息的情况下得出的估计值。如果您不进行任何观察，这是估计 Theta 的最佳方法。但是我已经进行了观察，X，我需要将其考虑在内。我查看这个差异，X 中包含的新闻是什么？这就是 X 的平均水平。</p><p>The expected value of Theta is the estimate that you would come up with if you didn’t have any information
            about
            Theta. If you don’t make any observations this is the best way of estimating Theta. But I have made an
            observation, X, and I need to take it into account. I look at this difference, which is the piece of news
            contained in X? That’s what X should be on the average.</p>
        <p>如果我观察到 X 比我预期的要大，并且由于 X 和 Theta 是正相关的，这告诉我 Theta 也应该大于其平均值。每当我看到 X 大于其平均值时，这都会给我一个指示，即 theta 也应该可能大于其平均值。因此，我将这个差值乘以一个正系数。</p><p>If I observe an X which is bigger than what I expected it to be, and since X and Theta are positively
            correlated,
            this tells me that Theta should also be bigger than its average value. Whenever I see an X that’s larger
            than
            its average value this gives me an indication that theta should also probably be larger than its average
            value.
            And so I’m taking that difference and multiplying it by a positive coefficient.</p>
        <h2 id="unknown-463">未知</h2><h2>Unknown</h2>
        <p>这就是我在这里得到的具有正斜率的曲线。因此，这个增量。X 中包含的新信息与我们预期的先验平均值相比，该增量使我们能够对先前对 Theta 的估计进行修正，并且该修正量由 X 与 Theta 的协方差决定。</p><p>And that’s what gives me a curve here that has a positive slope. So this increment. the new information
            contained
            in X as compared to the average value we expected apriori, that increment allows us to make a correction to
            our
            prior estimate of Theta, and the amount of that correction is guided by the covariance of X with Theta.</p>
        <p>如果 X 与 Theta 的协方差为 0，则意味着两者之间没有系统关系，在这种情况下，从 X 获取一些信息并不能为我们如何更改 Theta 的估计值提供指导。如果协方差为 0，我们将只保留这个特定的估计值。我们无法进行更正。</p><p>If the covariance of X with Theta were 0, that would mean there’s no systematic relation between the two, and
            in
            that case obtaining some information from X doesn’t give us a guide as to how to change the estimates of
            Theta.
            If that were 0, we would just stay with this particular estimate. We’re not able to make a correction.</p>
        <p>但是当 X 和 Theta 之间存在非零协方差时，协方差可以作为指导，帮助我们获得更好的 Theta 估计值。那么最终的均方误差如何呢？在这种情况下，有一个非常好的公式可以计算出从最佳线性估计中获得的均方误差。这里面有什么故事呢？我们得到的均方误差与原始随机变量的方差有关。</p><p>But when there’s a non zero covariance between X and Theta that covariance works as a guide for us to obtain
            a
            better estimate of Theta. How about the resulting mean squared error? In this context turns out that there’s
            a
            very nice formula for the mean squared error obtained from the best linear estimate. What’s the story here?
            The
            mean squared error that we have has something to do with the variance of the original random variable.</p>
        <h2 id="unknown-464">未知</h2><h2>Unknown</h2>
        <p>我们的原始随机变量越不确定，我们犯的错误就越大。另一方面，当两个变量相关时，我们会探索这种相关性以改进我们的估计。这行是两个随机变量之间的相关系数。当这个相关系数较大时，这里的这个因子就会变小。我们的均方误差就会变小。所以想想这两个极端情况。
        </p><p>The more uncertain our original random variable is, the more error we’re going to make. On the other hand,
            when
            the two variables are correlated we explored that correlation to improve our estimate. This row here is the
            correlation coefficient between the two random variables. When this correlation coefficient is larger this
            factor here becomes smaller. And our mean squared error become smaller. So think of the two extreme cases.
        </p>
        <p>一种极端情况是当 rho 等于 1 时，X 和 Theta 完全相关。当它们完全相关时，一旦我知道 X，我也会知道 Theta。这两个随机变量是线性相关的。在这种情况下，我的估计正好符合目标，均方误差将为 0。另一种极端情况是 rho 等于 0。这两个随机变量不相关。</p><p>One extreme case is when rho equal to 1 so X and Theta are perfectly correlated. When they’re perfectly
            correlated once I know X then I also know Theta. And the two random variables are linearly related. In that
            case, my estimate is right on the target, and the mean squared error is going to be 0. The other extreme
            case is
            if rho is equal to 0. The two random variables are uncorrelated.</p>
        <p>在这种情况下，测量结果无法帮助我估计 Theta 和剩下的不确定性。均方误差只是 Theta 的原始方差。因此，Theta 中的不确定性并没有减少。因此，道德上来说，估计误差是随机变量 Theta 中原始不确定性量的减少版本，这两个随机变量之间的相关性越大，我们就越能从原始随机变量中消除不确定性。</p><p>In that case the measurement does not help me estimate Theta, and the uncertainty that’s left. the mean
            squared
            error. is just the original variance of Theta. So the uncertainty in Theta does not get reduced. So moral.
            the
            estimation error is a reduced version of the original amount of uncertainty in the random variable Theta,
            and
            the larger the correlation between those two random variables, the better we can remove uncertainty from the
            original random variable.</p>
        <h2 id="unknown-465">未知</h2><h2>Unknown</h2>
        <p>我没有推导出这个公式，但它只是代数运算的问题。我们有一个 Theta 帽公式，从该公式中减去 Theta。取平方，取期望，然后进行几行代数运算（你可以在文本中读到），最终你会得到这个非常简洁的公式。</p><p>I didn’t derive this formula, but it’s just a matter of algebraic manipulations. We have a formula for Theta
            hat,
            subtract Theta from that formula. Take square, take expectations, and do a few lines of algebra that you can
            read in the text, and you end up with this really neat and clean formula.</p>
        <p>我在讲座开始时提到，我们可以用 Theta 和 X 进行推理，它们不仅仅是单个数字，它们可以是向量随机变量。例如，我们可能有多个数据可以提供有关 X 的信息。这里没有向量，所以这次讨论是针对 Theta 和 X 只是标量、一维量的情况。如果我们有多个数据，我们该怎么办？</p><p>Now I mentioned in the beginning of the lecture that we can do inference with Theta’s and X’s not just being
            single numbers, but they could be vector random variables. So for example we might have multiple data that
            gives
            us information about X. There are no vectors here, so this discussion was for the case where Theta and X
            were
            just scalar, one dimensional quantities. What do we do if we have multiple data?</p>
        <p>假设 Theta 仍然是一个标量，它是一维的，但我们做了几个观察。基于这些观察，我们想要估计 Theta。最佳最小均方估计量将再次是给定 X 的 Theta 的条件期望。这是最佳估计量。在这种情况下，X 是一个向量，所以我们使用的一般估计量就是这个。</p><p>Suppose that Theta is still a scalar, it’s one dimensional, but we make several observations. And on the
            basis of
            these observations we want to estimate Theta. The optimal least mean squares estimator would be again the
            conditional expectation of Theta given X. That’s the optimal one. And in this case X is a vector, so the
            general
            estimator we would use would be this one.</p>
        <h2 id="unknown-466">未知</h2><h2>Unknown</h2>
        <p>但是如果我们想让事情简单化，并且希望我们的估计量具有简单的函数形式，我们可能会将估计量限制为数据的线性函数。然后故事就和我们之前讨论的完全一样了。我限制自己使用数据的线性函数来估计 Theta，所以我的信号处理框只应用了线性函数。</p><p>But if we want to keep things simple and we want our estimator to have a simple functional form we might
            restrict
            to estimator that are linear functions of the data. And then the story is exactly the same as we discussed
            before. I constrained myself to estimating Theta using a linear function of the data, so my signal
            processing
            box just applies a linear function.</p>
        <p>我在寻找最佳系数，即导致平方误差最小的系数。这是我的平方误差，这是（我的估计值减去我试图估计的值）的平方，然后取平均值。我们怎么做呢？和以前一样。因为我们有一个期望，所以 X 和 Theta 被平均了。</p><p>And I’m looking for the best coefficients, the coefficients that are going to result in the least possible
            squared error. This is my squared error, this is (my estimate minus the thing I’m trying to estimate)
            squared,
            and then taking the average. How do we do this? Same story as before. The X’s and the Theta’s get averaged
            out
            because we have an expectation.</p>
        <p>剩下的只是 a 和 b 的系数的函数。和之前一样，它变成了一个二次函数。然后我们设置这个 a 和 b 函数关于系数的导数，我们将其设置为 0。这给了我们一个线性方程组。这是一个由这些系数满足的线性方程组。</p><p>Whatever is left is just a function of the coefficients of the a’s and of b’s. As before it turns out to be a
            quadratic function. Then we set the derivatives of this function of a’s and b’s with respect to the
            coefficients, we set it to 0. And this gives us a system of linear equations. It’s a system of linear
            equations
            that’s satisfied by those coefficients.</p>
        <h2 id="unknown-467">未知</h2><h2>Unknown</h2>
        <p>这是一个线性系统，因为这是这些系数的二次函数。因此，为了在这种特殊情况下得到闭式公式，需要引入向量、矩阵、度量逆等等。我们感兴趣的不是具体公式，而是有趣的是，这只需使用线性方程的直接求解器即可完成。</p><p>It’s a linear system because this is a quadratic function of those coefficients. So to get closed form
            formulas
            in this particular case one would need to introduce vectors, and matrices, and metrics inverses and so on.
            The
            particular formulas are not so much what interests us here, rather, the interesting thing is that this is
            simply
            done just using straightforward solvers of linear equations.</p>
        <p>你唯一需要做的就是写下这些非线性方程的正确系数。你得到的典型系数是什么？假设一个典型的快速方程是取你展开的这个二次方程的一个典型项。</p><p>The only thing you need to do is to write down the correct coefficients of those non linear equations. And
            the
            typical coefficient that you would get would be what? Let say a typical quick equations would be let’s take
            a
            typical term of this quadratic one you expanded.</p>
        <p>您将得到诸如 a1x1 乘以 a2x2 之类的项。当您取期望值时，您将得到 a1a2 乘以 x1x2 的期望值。因此，这将涉及诸如 a1 平方 x1 的期望值平方之类的项。您将得到诸如 a1a2、x1x2 的期望值之类的项，并且这里的许多其他项也应该有。因此，您会得到系数中的二次项。</p><p>You’re going to get the terms such as a1x1 times a2x2. When you take expectations you’re left with a1a2 times
            expected value of x1x2. So this would involve terms such as a1 squared expected value of x1 squared. You
            would
            get terms such as a1a2, expected value of x1x2, and a lot of other terms here should have a too. So you get
            something that’s quadratic in your coefficients.</p>
        <h2 id="unknown-468">未知</h2><h2>Unknown</h2>
        <p>方程组中出现的常数与随机变量平方的期望值或随机变量乘积有关。要写下这些数值，你唯一需要知道的是随机变量的均值和方差。如果你知道均值和方差，那么你就知道这个东西是什么。</p><p>And the constants that show up in your system of equations are things that have to do with the expected
            values of
            squares of your random variables, or products of your random variables. To write down the numerical values
            for
            these the only thing you need to know are the means and variances of your random variables. If you know the
            mean
            and variance then you know what this thing is.</p>
        <p>如果你也知道协方差，那么你就知道这是什么了。因此，为了在多个数据的情况下找到最佳线性估计量，你不需要知道所涉及的随机变量的整个概率分布。你只需要知道你的均值和协方差。这些是影响最佳估计量构建的唯一数量。我们已经可以在这个公式中看到这一点了。</p><p>And if you know the covariances as well then you know what this thing is. So in order to find the optimal
            linear
            estimator in the case of multiple data you do not need to know the entire probability distribution of the
            random
            variables that are involved. You only need to know your means and covariances. These are the only quantities
            that affect the construction of your optimal estimator. We could see this already in this formula.</p>
        <p>一旦我知道了模型中随机变量的均值、方差和协方差，我的最佳估计量的形式就完全确定了。我不需要知道这里涉及的随机变量的详细分布情况。所以正如我一般所说的，你可以使用线性方程求解器来找到最佳估计量的形式。在一些特殊示例中，你可以得到闭式解。</p><p>The form of my optimal estimator is completely determined once I know the means, variance, and covariance of
            the
            random variables in my model. I do not need to know how the details distribution of the random variables
            that
            are involved here. So as I said in general, you find the form of the optimal estimator by using a linear
            equation solver. There are special examples in which you can get closed form solutions.</p>
        <h2 id="unknown-469">未知</h2><h2>Unknown</h2>
        <p>你能想到的最简单的估计问题如下。你有一些不确定的参数，并且在有噪声的情况下对该参数进行多次测量。所以 Wi 是噪声。I 对应于你的第 i 次实验。所以这是你在实验室中遇到的最常见的情况。如果你正在处理某个过程，你试图测量一些东西，你会一遍又一遍地测量它。</p><p>The nicest simplest estimation problem one can think of is the following. you have some uncertain parameter,
            and
            you make multiple measurements of that parameter in the presence of noise. So the Wi’s are noises. I
            corresponds
            to your i th experiment. So this is the most common situation that you encounter in the lab. If you are
            dealing
            with some process, you’re trying to measure something you measure it over and over.</p>
        <p>每次测量都会有一些随机误差。然后你需要把所有的测量结果放在一起，得出一个估计值。因此，假设噪声彼此独立，也与真实参数的值无关。在不失一般性的情况下，我们可以假设噪声的均值为 0，并且它们有一些我们假设已知的方差。</p><p>Each time your measurement has some random error. And then you need to take all your measurements together
            and
            come up with a single estimate. So the noises are assumed to be independent of each other, and also to be
            independent from the value of the true parameter. Without loss of generality we can assume that the noises
            have
            0 mean and they have some variances that we assume to be known.</p>
        <p>Theta 本身具有先验分布，具有特定均值和特定方差。因此，最佳线性估计量的形式非常好。好吧，也许您无法立即看到它，因为这看起来很乱，但它到底是什么？它是 X 和先验均值的线性组合。它实际上是 X 和先验均值的加权平均值。在这里，我们收集了顶部的所有系数。</p><p>Theta itself has a prior distribution with a certain mean and the certain variance. So the form of the
            optimal
            linear estimator is really nice. Well maybe you cannot see it right away because this looks messy, but what
            is
            it really? It’s a linear combination of the X’s and the prior mean. And it’s actually a weighted average of
            the
            X’s and the prior mean. Here we collect all of the coefficients that we have at the top.</p>
        <h2 id="unknown-470">未知</h2><h2>Unknown</h2>
        <p>所以整个过程基本上是一个加权平均值。1/(sigma_i 平方) 是我们赋予 Xi 的权重，分母是所有权重的总和。所以最后我们处理的是加权平均值。如果 mu 等于 1，并且所有 Xi 都等于 1，那么我们的估计值也将等于 1。现在，我们拥有的权重的形式很有趣。</p><p>So the whole thing is basically a weighted average. 1/(sigma_i squared) is the weight that we give to Xi, and
            in
            the denominator we have the sum of all of the weights. So in the end we’re dealing with a weighted average.
            If
            mu was equal to 1, and all the Xi’s were equal to 1 then our estimate would also be equal to 1. Now the form
            of
            the weights that we have is interesting.</p>
        <p>任何给定数据点的权重都与方差成反比。这说明了什么？如果我的第 i 个数据点方差很大，如果 Wi 非常嘈杂，那么 Xi 就不是很有用，也不是很可靠。所以我给它一个较小的权重。方差很大，我的 Xi 中有很多错误，这意味着我应该给它较小的权重。</p><p>Any given data point is weighted inversely proportional to the variance. What does that say? If my i th data
            point has a lot of variance, if Wi is very noisy then Xi is not very useful, is not very reliable. So I’m
            giving
            it a small weight. Large variance, a lot of error in my Xi means that I should give it a smaller weight.</p>
        <p>如果两个数据点的方差相同，质量相当，那么我将赋予它们相同的权重。另一个有趣的事情是，先验均值的处理方式与 X 相同。因此，它被视为额外的观察值。因此，我们对先验均值和我们所做的测量值取加权平均值。该公式看起来好像先验均值只是另一个数据点。</p><p>If two data points have the same variance, they’re of comparable quality, then I’m going to give them equal
            weight. The other interesting thing is that the prior mean is treated the same way as the X’s. So it’s
            treated
            as an additional observation. So we’re taking a weighted average of the prior mean and of the measurements
            that
            we are making. The formula looks as if the prior mean was just another data point.</p>
        <h2 id="unknown-471">未知</h2><h2>Unknown</h2>
        <p>这就是贝叶斯估计的思维方式。你有真实的数据点，你观察到的 X，你也有一些先验信息。这起着类似于数据点的作用。有趣的是，如果这个模型中的所有随机变量都是正态的，那么这些最优线性估计量恰好也是条件期望。这就是正态随机变量的优点，条件期望结果是线性的。</p><p>So that’s the way of thinking about Bayesian estimation. You have your real data points, the X’s that you
            observe, you also had some prior information. This plays a role similar to a data point. Interesting note
            that
            if all random variables are normal in this model these optimal linear estimator happens to be also the
            conditional expectation. That’s the nice thing about normal random variables that conditional expectations
            turn
            out to be linear.</p>
        <p>因此，最优估计和最优线性估计结果相同。这给了我们线性估计的另一种解释。线性估计本质上等同于假设所有随机变量都是正态的。所以这是一个次要问题。现在我想用一句话来结束。你进行测量，并根据 X 估计 Theta。</p><p>So the optimal estimate and the optimal linear estimate turn out to be the same. And that gives us another
            interpretation of linear estimation. Linear estimation is essentially the same as pretending that all random
            variables are normal. So that’s a side point. Now I’d like to close with a comment. You do your measurements
            and
            you estimate Theta on the basis of X.</p>
        <p>假设你有一个测量仪器，它测量的是 X 的立方而不是 X 的立方，而你想估计 Theta。你会得到不同的估计值吗？好吧，X 和 X 的立方包含相同的信息。告诉你 X 和告诉你 X 的立方值是一样的。因此，给定 X 的 Theta 的后验分布与给定 X 的立方的 Theta 的后验分布相同。</p><p>Suppose that instead you have a measuring device that’s measures X cubed instead of measuring X, and you want
            to
            estimate Theta. Are you going to get to different a estimate? Well X and X cubed contained the same
            information.
            Telling you X is the same as telling you the value of X cubed. So the posterior distribution of Theta given
            X is
            the same as the posterior distribution of Theta given X cubed.</p>
        <h2 id="unknown-472">未知</h2><h2>Unknown</h2>
        <p>因此这些后验分布的均值将相同。因此，如果您进行的是最优最小二乘估计，那么对数据进行变换并不重要。另一方面，如果您限制自己进行线性估计，那么使用 X 的线性函数与使用 X 立方的线性函数是不一样的。</p><p>And so the means of these posterior distributions are going to be the same. So doing transformations through
            your
            data does not matter if you’re doing optimal least squares estimation. On the other hand, if you restrict
            yourself to doing linear estimation then using a linear function of X is not the same as using a linear
            function
            of X cubed.</p>
        <p>所以这是一个线性估计量，但数据是 X 立方，我们有数据的线性函数。所以这意味着当你使用线性估计时，你可以选择在什么上进行线性化？有时你想在非普通尺度上绘制数据，并尝试通过它们绘制一条线。有时你会在对数尺度上绘制数据，并尝试通过它们绘制一条线。</p><p>So this is a linear estimator, but where the data are the X cube’s, and we have a linear function of the
            data. So
            this means that when you’re using linear estimation you have some choices to make linear on what? Sometimes
            you
            want to plot your data on a not ordinary scale and try to plot a line through them. Sometimes you plot your
            data
            on a logarithmic scale, and try to plot a line through them.</p>
        <p>哪种比例合适？这里是立方比例。你必须考虑你的特定模型来决定哪个版本更合适。最后，当我们有多个数据时，有时这些多个数据可能包含相同的信息。所以 X 是一个数据点，X 平方是另一个数据点，X 立方是另一个数据点。</p><p>Which scale is the appropriate one? Here it would be a cubic scale. And you have to think about your
            particular
            model to decide which version would be a more appropriate one. Finally when we have multiple data sometimes
            these multiple data might contain the same information. So X is one data point, X squared is another data
            point,
            X cubed is another data point.</p>
        <h2 id="unknown-473">未知</h2><h2>Unknown</h2>
        <p>它们三者包含相同的信息，但你可以尝试将它们组成一个线性函数。然后你得到一个线性估计量，它具有更一般的形式，即 X 的函数。因此，如果你想将 Theta 估计为 X 的三次函数，例如，你可以建立这种特定形式的线性估计模型，并找到最佳系数，即 a 和 b。</p><p>The three of them contain the same information, but you can try to form a linear function of them. And then
            you
            obtain a linear estimator that has a more general form as a function of X. So if you want to estimate your
            Theta
            as a cubic function of X, for example, you can set up a linear estimation model of this particular form and
            find
            the optimal coefficients, the a’s and the b’s.</p>
        <p>好的，最后一张幻灯片只是向您概述了贝叶斯推理中发生的事情，供您思考。基本上，我们讨论了三种可能的估计方法。最大后验概率、均方误差估计和线性均方误差估计或最小二乘估计。并且有许多标准示例，您将在复习、教程、家庭作业等中反复看到，甚至在考试中也能看到。</p><p>All right, so the last slide just gives you the big picture of what’s happening in Bayesian Inference, it’s
            for
            you to ponder. Basically we talked about three possible estimation methods. Maximum posteriori, mean squared
            error estimation, and linear mean squared error estimation, or least squares estimation. And there’s a
            number of
            standard examples that you will be seeing over and over in the recitations, tutorial, homework, and so on,
            perhaps on exams even.</p>
        <p>我们对某些未知参数采取一些良好的先验，对噪声或观测采取一些良好的模型，然后您需要计算出各种估计中的后验分布并进行比较。
        </p><p>Where we take some nice priors on some unknown parameter, we take some nice models for the noise or the
            observations, and then you need to work out posterior distributions in the various estimates and compare
            them.
        </p>
        <h1 id="classical-statistical-inference-i">23. 经典统计推断 I</h1><h1>23. Classical Statistical Inference I</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 49:32 <a href="https://youtube.com/watch?v=4UJc0S8APm4">https://youtube.com/watch?v=4UJc0S8APm4</a></p><p> 12 years ago (Nov 10, 2012) — 49:32 <a href="https://youtube.com/watch?v=4UJc0S8APm4">https://youtube.com/watch?v=4UJc0S8APm4</a></p>
        <h2 id="unknown-474">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。教授：所以在最后三节课中，我们将讨论经典统计学，即如果您不想假设未知参数的先验分布，可以采用的统计方法。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality, educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: So for the last three
            lectures
            we’re going to talk about classical statistics, the way statistics can be done if you don’t want to assume a
            prior distribution on the unknown parameters.</p>
        <p>今天我们将主要关注估计方面，将假设检验留到下两节课再讲。因此，有一种通用方法可用于进行参数估计，那就是最大似然法。我们将定义它是什么。然后我们将研究最常见的估计问题，即估计给定分布的平均值。</p><p>Today we’re going to focus, mostly, on the estimation side and leave hypothesis testing for the next two
            lectures. So where there is one generic method that one can use to carry out parameter estimation, that’s
            the
            maximum likelihood method. We’re going to define what it is. Then we will look at the most common estimation
            problem there is, which is to estimate the mean of a given distribution.</p>
        <p>我们将讨论置信区间，它指的是在你的估计值周围提供一个区间，它具有一些属性，例如参数很可能位于该区间内，但我们会谨慎解释这一特定陈述。好的。首先是大框架。该图与我们在贝叶斯统计中看到的几乎相同。我们有一些未知参数。
        </p><p>And we’re going to talk about confidence intervals, which refers to providing an interval around your
            estimates,
            which has some properties of the kind that the parameter is highly likely to be inside that interval, but we
            will be careful about how to interpret that particular statement. Ok. So the big framework first. The
            picture is
            almost the same as the one that we had in the case of Bayesian statistics. We have some unknown parameter.
        </p>
        <h2 id="unknown-475">未知</h2><h2>Unknown</h2>
        <p>我们有一个测量设备。它有一些噪音，一些随机性。我们得到一个观测值 X，其分布取决于参数的值。然而，与贝叶斯设置相比，最大的变化是，这里的这个参数只是一个数字。它不是作为随机变量建模的。它没有概率分布。它没有任何随机性。它是一个常数。只是我们不知道这个常数是什么。</p><p>And we have a measuring device. There is some noise, some randomness. And we get an observation, X, whose
            distribution depends on the value of the parameter. However, the big change from the Bayesian setting is
            that
            here, this parameter is just a number. It’s not modeled as a random variable. It does not have a probability
            distribution. There’s nothing random about it. It’s a constant. It just happens that we don’t know what that
            constant is.</p>
        <p>具体来说，这里的概率分布，即 X 的分布，取决于 Theta。但这不是通常意义上的条件分布。条件分布的定义是，我们有两个随机变量，并且我们将一个随机变量作为另一个随机变量的条件。我们使用横线将 X 与 Theta 分开。为了说明这不是条件分布，我们使用了不同的符号。</p><p>And in particular, this probability distribution here, the distribution of X, depends on Theta. But this is
            not a
            conditional distribution in the usual sense of the word. Conditional distributions were defined when we had
            two
            random variables and we condition one random variable on the other. And we used the bar to separate the X
            from
            the Theta. To make the point that this is not a conditioned distribution, we use a different notation.</p>
        <p>我们在这里放了一个分号。这意味着 X 有一个分布。该分布有一个特定的参数。我们不知道这个参数是什么。例如，这可能是一个正态分布，方差为 1，但平均值为 Theta。我们不知道 Theta 是什么。我们想估计它。现在，一旦我们有了这个设置，你的工作就是设计这个框，即估计器。</p><p>We put a semicolon here. And what this is meant to say is that X has a distribution. That distribution has a
            certain parameter. And we don’t know what that parameter is. So for example, this might be a normal
            distribution, with variance 1 but a mean Theta. We don’t know what Theta is. And we want to estimate it. Now
            once we have this setting, then your job is to design this box, the estimator.</p>
        <h2 id="unknown-476">未知</h2><h2>Unknown</h2>
        <p>估计器是一些数据处理盒，它进行测量并产生未知参数的估计值。现在这里使用的符号好像 X 和 Theta 是一维量。但实际上，如果你将 X 和 Theta 解释为参数向量，我们所说的一切都仍然有效。例如，你可能会获得几个测量值，X1 到 2Xn。背景中可能有几个未知参数。
        </p><p>The estimator is some data processing box that takes the measurements and produces an estimate of the unknown
            parameter. Now the notation that’s used here is as if X and Theta were one dimensional quantities. But
            actually,
            everything we say remains valid if you interpret X and Theta as vectors of parameters. So for example, you
            may
            obtain several measurements, X1 up to 2Xn. And there may be several unknown parameters in the background.
        </p>
        <p>再说一次，我们没有，也不想假设 Theta 的先验分布。它是一个常数。如果你想从数学角度考虑这种情况，就好像你有许多不同的概率模型。因此，具有此均值的正态分布或具有该均值的正态分布或具有该均值的正态分布，这些都是备选的候选概率模型。</p><p>Once more, we do not have, and we do not want to assume, a prior distribution on Theta. It’s a constant. And
            if
            you want to think mathematically about this situation, it’s as if you have many different probabilistic
            models.
            So a normal with this mean or a normal with that mean or a normal with that mean, these are alternative
            candidate probabilistic models.</p>
        <p>我们想要尝试决定哪一个是正确的模型。在某些情况下，我们必须在少数几个模型中进行选择。例如，你有一枚硬币，其偏差未知。偏差是 1/2 或 3/4。你要抛硬币几次。然后尝试确定真正的偏差是这个还是那个。</p><p>And we want to try to make a decision about which one is the correct model. In some cases, we have to choose
            just
            between a small number of models. For example, you have a coin with an unknown bias. The bias is either 1/2
            or
            3/4. You’re going to flip the coin a few times. And you try to decide whether the true bias is this one or
            is
            that one.</p>
        <h2 id="unknown-477">未知</h2><h2>Unknown</h2>
        <p>因此，在这种情况下，我们有两个特定的、可替代的概率模型，我们想要从中区分出来。但有时事情会更复杂一些。例如，你有一枚硬币。你有一个假设，即我的硬币没有偏差。另一个假设是我的硬币有偏差。然后你进行实验。你想要做出一个决定，决定这个假设是真的还是那个假设是真的。</p><p>So in this case, we have two specific, alternative probabilistic models from which we want to distinguish.
            But
            sometimes things are a little more complicated. For example, you have a coin. And you have one hypothesis
            that
            my coin is unbiased. And the other hypothesis is that my coin is biased. And you do your experiments. And
            you
            want to come up with a decision that decides whether this is true or this one is true.</p>
        <p>在这种情况下，我们处理的不仅仅是两个备选概率模型。这个模型是硬币的特定模型。但这个模型实际上对应着许多可能的备选硬币模​​型。因此，这包括 Theta 为 0.6 的模型、Theta 为 0.7 的模型、Theta 为 0.8 的模型等等。因此，我们试图区分一个模型和许多备选模型。如何做到这一点？</p><p>In this case, we’re not dealing with just two alternative probabilistic models. This one is a specific model
            for
            the coin. But this one actually corresponds to lots of possible, alternative coin models. So this includes
            the
            model where Theta is 0.6, the model where Theta is 0.7, Theta is 0.8, and so on. So we’re trying to
            discriminate
            between one model and lots of alternative models. How does one go about this?</p>
        <p>嗯，有一些系统的方法可以解决这类问题。下次我们会开始讨论这些方法。所以今天，我们将重点讨论估计问题。在估计问题中，theta 是一个量，是一个实数，一个连续参数。我们要设计这个盒子，所以我们从这个盒子中得到的是一个估计值。现在请注意，这里的估计值是一个随机变量。</p><p>Well, there’s some systematic ways that one can approach problems of this kind. And we will start talking
            about
            these next time. So today, we’re going to focus on estimation problems. In estimation problems, theta is a
            quantity, which is a real number, a continuous parameter. We’re to design this box, so what we get out of
            this
            box is an estimate. Now notice that this estimate here is a random variable.</p>
        <h2 id="unknown-478">未知</h2><h2>Unknown</h2>
        <p>尽管 Theta 是确定性的，但它是随机的，因为它是我们观察到的数据的函数。数据是随机的。我们将函数应用于数据以构建我们的估计值。因此，由于它是随机变量的函数，因此它本身就是一个随机变量。Theta 的分布取决于 X 的分布。X 的分布受 Theta 的影响。</p><p>Even though theta is deterministic, this is random, because it’s a function of the data that we observe. The
            data
            are random. We’re applying a function to the data to construct our estimate. So, since it’s a function of
            random
            variables, it’s a random variable itself. The distribution of Theta hat depends on the distribution of X.
            The
            distribution of X is affected by Theta.</p>
        <p>因此，最终，您的估计 Theta 帽的分布也会受到 Theta 值的影响。在设计估计量时，我们的总体目标是，最终得到一个误差，即估计误差，这个误差不会太大。但我们必须明确这一点。同样，我们到底是什么意思呢？那么我们如何解决这个问题呢？</p><p>So in the end, the distribution of your estimate Theta hat will also be affected by whatever Theta happens to
            be.
            Our general objective, when designing estimators, is that we want to get, in the end, an error, an
            estimation
            error, which is not too large. But we’ll have to make that specific. Again, what exactly do we mean by that?
            So
            how do we go about this problem?</p>
        <p>一种通用方法是选择一个 Theta，在此 Theta 下，我们观察到的数据是 X 最有可能发生的数据。所以我观察 X。对于任何给定的 Theta，我都可以计算出这个量，它告诉我，在这个特定的 Theta 下，你观察到的 X 有这个发生的概率。在这个 Theta 下，你观察到的 X 有这个发生的概率。</p><p>One general approach is to pick a Theta, under which the data that we observe, that this is the X’s, our most
            likely to have occurred. So I observe X. For any given Theta, I can calculate this quantity, which tells me,
            under this particular Theta, the X that you observed had this probability of occurring. Under that Theta,
            the X
            that you observe had that probability of occurring.</p>
        <h2 id="unknown-479">未知</h2><h2>Unknown</h2>
        <p>您只需选择那个 Theta，它使您观察到的数据最有可能。将这个最大似然估计与您在贝叶斯环境中使用最大逼近理论概率估计时得到的估计进行比较是很有趣的。在贝叶斯环境中，我们所做的是，给定数据，我们使用 Theta 的先验分布。然后我们计算给定 X 的 Theta 的后验分布。
        </p><p>You just choose that Theta, which makes the data that you observed most likely. It’s interesting to compare
            this
            maximum likelihood estimate with the estimates that you would have, if you were in a Bayesian setting, and
            you
            were using maximum approach theory probability estimation. In the Bayesian setting, what we do is, given the
            data, we use the prior distribution on Theta. And we calculate the posterior distribution of Theta given X.
        </p>
        <p>请注意，这与我们这里的情况正好相反。这是 Theta 特定值对应的 X 概率，而这是 Theta 特定 X 对应的概率。因此，这是相反类型的条件。在贝叶斯设置中，Theta 是一个随机变量。因此，我们可以讨论 Theta 的概率分布。</p><p>Notice that this is sort of the opposite from what we have here. This is the probability of X for a
            particular
            value of Theta, whereas this is the probability of Theta for a particular X. So it’s the opposite type of
            conditioning. In the Bayesian setting, Theta is a random variable. So we can talk about the probability
            distribution of Theta.</p>
        <p>那么，除了 X 和 Theta 的顺序颠倒这一语法差异之外，这两者如何比较呢？让我们详细地写下 Theta 的后验分布。根据贝叶斯规则，这个条件分布是从先验和我们拥有的测量过程模型中获得的。我们得到了这个表达式。所以在贝叶斯估计中，我们想要找到 Theta 的最可能值。</p><p>So how do these two compare, except for this syntactic difference that the order X’s and Theta’s are
            reversed?
            Let’s write down, in full detail, what this posterior distribution of Theta is. By the Bayes rule, this
            conditional distribution is obtained from the prior, and the model of the measurement process that we have.
            And
            we get to this expression. So in Bayesian estimation, we want to find the most likely value of Theta.</p>
        <h2 id="unknown-480">未知</h2><h2>Unknown</h2>
        <p>我们需要使这个量在所有可能的 Theta 上最大化。首先要注意的是分母是一个常数。它与 Theta 无关。因此，当你最大化这个量时，你不必关心分母。你只想最大化分子。现在，这里的情况开始看起来更相似了。如果这里的那个项不存在，那么前项也不存在，它们将完全是同一类。</p><p>And we need to maximize this quantity over all possible Theta’s. First thing to notice is that the
            denominator is
            a constant. It does not involve Theta. So when you maximize this quantity, you don’t care about the
            denominator.
            You just want to maximize the numerator. Now, here, things start to look a little more similar. And they
            would
            be exactly of the same kind, if that term here was absent, it the prior was absent.</p>
        <p>如果先验只是一个常数，那么两者将变得相同。因此，如果先验是一个常数，那么最大似然估计的形式与贝叶斯最大后验概率估计完全相同。因此，您可以对最大似然估计给出这种特定的解释。</p><p>The two are going to become the same if that prior was just a constant. So if that prior is a constant, then
            maximum likelihood estimation takes exactly the same form as Bayesian maximum posterior probability
            estimation.
            So you can give this particular interpretation of maximum likelihood estimation.</p>
        <p>最大似然估计本质上就是你在贝叶斯世界中所做的，并且你假设了 Theta 的先验是均匀的，所有 Theta 都具有同等可能性。好的。让我们看一个简单的例子。假设 Xi 是独立的、相同分布的随机变量，具有特定参数 Theta。因此，每个 Xi 的分布都是这个特定项。因此 Theta 是一维的。它是一个一维参数。</p><p>Maximum likelihood estimation is essentially what you have done, if you were in a Bayesian world, and you had
            assumed a prior on the Theta’s that’s uniform, all the Theta’s being equally likely. Okay. So let’s look at
            a
            simple example. Suppose that the Xi’s are independent, identically distributed random variables, with a
            certain
            parameter Theta. So the distribution of each one of the Xi’s is this particular term. So Theta is one
            dimensional. It’s a one dimensional parameter.</p>
        <h2 id="unknown-481">未知</h2><h2>Unknown</h2>
        <p>但是我们有几个数据。给定一个特定的 Theta 值，我们写下特定 X 向量的概率公式。但是，当我使用给定这个词时，这里不是条件意义。它是特定 Theta 选择的密度值。在这里，我写下了，我根据 PMF 定义了最大似然估计。如果 X 是离散随机变量，这就是您要做的事情。</p><p>But we have several data. We write down the formula for the probability of a particular X vector, given a
            particular value of Theta. But again, when I use the word, given, here it’s not in the conditioning sense.
            It’s
            the value of the density for a particular choice of Theta. Here, I wrote down, I defined maximum likelihood
            estimation in terms of PMFs. That’s what you would do if the X’s were discrete random variables.</p>
        <p>这里，X 是连续随机变量，因此我使用 PDF 而不是 PMF。因此，这里的定义推广到连续随机变量的情况。并且您使用 F 而不是 X，这是我们通常的配方。因此，最大似然估计已定义。现在，由于 Xi 是独立的，因此所有 X 的联合密度是各个密度的乘积。因此，您可以查看这个数量。
        </p><p>Here, the X’s are continuous random variables, so instead of I’m using the PDF instead of the PMF. So this a
            definition, here, generalizes to the case of continuous random variables. And you use F’s instead of X’s,
            our
            usual recipe. So the maximum likelihood estimate is defined. Now, since the Xi’s are independent, the joint
            density of all the X’s together is the product of the individual densities. So you look at this quantity.
        </p>
        <p>这是观察到特定 X 序列的密度或概率。我们问一个问题，Theta 的值是多少，使得我们观察到的 X 最有可能出现？所以我们想进行最大化。现在这个最大化只是一个计算问题。我们将通过取这个表达式的对数来进行最大化。最大化表达式与最大化对数相同。
        </p><p>This is the density or sort of probability of observing a particular sequence of X’s. And we ask the
            question,
            what’s the value of Theta that makes the X’s that we observe most likely? So we want to carry out this
            maximization. Now this maximization is just a calculational problem. We’re going to do this maximization by
            taking the logarithm of this expression. Maximizing an expression is the same as maximizing the logarithm.
        </p>
        <h2 id="unknown-482">未知</h2><h2>Unknown</h2>
        <p>因此，这个表达式的对数，乘积的对数是对数之和。你从这个 Theta 项中得到贡献。有 n 个这样的项，所以我们得到一个 n log Theta。然后我们得到这些项的对数之和。它给出了负 Theta。然后是 X 之和。所以我们需要使这个表达式相对于 Theta 最大化。</p><p>So the logarithm of this expression, the logarithm of a product is the sum of the logarithms. You get
            contributions from this Theta term. There’s n of these, so we get an n log Theta. And then we have the sum
            of
            the logarithms of these terms. It gives us minus Theta. And then the sum of the X’s. So we need to maximize
            this
            expression with respect to Theta.</p>
        <p>实现最大化的方法是求 Theta 的导数。然后得到 n/Theta 等于 X 的总和。然后解出 Theta。然后你会发现最大似然估计就是这个量。这有点道理，因为这是 X 样本均值的倒数。在指数分布中，我们知道 Theta 是 1/(指数分布的均值)。</p><p>The way to do this maximization is you take the derivative, with respect to Theta. And you get n over Theta
            equals to the sum of the X’s. And then you solve for Theta. And you find that the maximum likelihood
            estimate is
            this quantity. Which sort of makes sense, because this is the reciprocal of the sample mean of X’s. Theta,
            in an
            exponential distribution, we know that it’s 1 over (the mean of the exponential distribution).</p>
        <p>所以这看起来是个合理的估计。所以无论如何，这是最大似然估计程序告诉我们应该报告的估计值。当然，这里的公式会告诉你如果已经观察到具体数字该怎么做。如果你已经观察到具体数字，那么你将这个特定数字视为你对 Theta 的估计。</p><p>So it looks like a reasonable estimate. So in any case, this is the estimates that the maximum likelihood
            estimation procedure tells us that we should report. This formula here, of course, tells you what to do if
            you
            have already observed specific numbers. If you have observed specific numbers, then you observe this
            particular
            number as your estimate of Theta.</p>
        <h2 id="unknown-483">未知</h2><h2>Unknown</h2>
        <p>如果你想更抽象地描述你的估计过程，你构建的是一个估计器，它是一个盒子，它接收随机变量，大写 X1 到大写 Xn，并产生你的估计，这也是一个随机变量。因为它是这些随机变量的函数，用大写 Theta 表示，表示这现在是一个随机变量。所以这是一个关于数字的等式。
        </p><p>If you want to describe your estimation procedure more abstractly, what you have constructed is an estimator,
            which is a box that’s takes in the random variables, capital X1 up to Capital Xn, and produces out your
            estimate, which is also a random variable. Because it’s a function of these random variables and is denoted
            by
            an upper case Theta, to indicate that this is now a random variable. So this is an equality about numbers.
        </p>
        <p>这是一般程序的描述，即两个随机变量之间的相等性。这为您提供了我们在这里所做事情的更抽象的视图。好的。那么我们可以对我们的估计说些什么呢？它是好还是坏？因此，我们应该研究这个特定的随机变量并讨论它具有的统计特性。</p><p>This is a description of the general procedure, which is an equality between two random variables. And this
            gives
            you the more abstract view of what we’re doing here. All right. So what can we tell about our estimate? Is
            it
            good or is it bad? So we should look at this particular random variable and talk about the statistical
            properties that it has.</p>
        <p>我们希望这个随机变量接近 Theta 的真实值，并且概率很高，无论 Theta 是多少，因为我们不知道 Theta 是什么。让我们更具体一点，我们想要的属性。所以我们以某种方式制作了估计量。因此，这个估计量再次对应于一个接收数据（大写 X）的框，并产生一个估计 Theta 帽。这个估计是随机的。</p><p>What we would like is this random variable to be close to the true value of Theta, with high probability, no
            matter what Theta is, since we don’t know what Theta is. Let’s make a little more specific the properties
            that
            we want. So we cook up the estimator somehow. So this estimator corresponds, again, to a box that takes data
            in,
            the capital X’s, and produces an estimate Theta hat. This estimate is random.</p>
        <h2 id="unknown-484">未知</h2><h2>Unknown</h2>
        <p>有时它会高于 Theta 的真实值。有时它会低于。理想情况下，我们希望它没有系统误差，无论是正值还是负值。因此，对于一个好的估计器来说，合理的期望是，平均而言，它会给出正确的值。现在，让我们更具体地说明一下这个期望是什么。</p><p>Sometimes it will be above the true value of Theta. Sometimes it will be below. Ideally, we would like it to
            not
            have a systematic error, on the positive side or the negative side. So a reasonable wish to have, for a good
            estimator, is that, on the average, it gives you the correct value. Now here, let’s be a little more
            specific
            about what that expectation is.</p>
        <p>这是关于 Theta hat 概率分布的期望。Theta hat 概率分布受 X 概率分布的影响。因为 Theta hat 是 X 的函数。而 X 概率分布受 Theta 真实值的影响。因此，根据哪一个是 Theta 真实值，这将是一个不同的期望。</p><p>This is an expectation, with respect to the probability distribution of Theta hat. The probability
            distribution
            of Theta hat is affected by the probability distribution of the X’s. Because Theta hat is a function of the
            X’s.
            And the probability distribution of the X’s is affected by the true value of Theta. So depending on which
            one is
            the true value of Theta, this is going to be a different expectation.</p>
        <p>因此，如果您要更详细地写出这个期望，它看起来会像这样。您需要写下 Theta hat 的概率分布。这将是某个函数。但这个函数取决于真实 Theta，受真实 Theta 的影响。然后您将其与 Theta hat 积分。这里的重点是什么？同样，Theta hat 是 X 的函数。</p><p>So if you were to write this expectation out in more detail, it would look something like this. You need to
            write
            down the probability distribution of Theta hat. And this is going to be some function. But this function
            depends
            on the true Theta, is affected by the true Theta. And then you integrate this with respect to Theta hat.
            What’s
            the point here? Again, Theta hat is a function of the X’s.</p>
        <h2 id="unknown-485">未知</h2><h2>Unknown</h2>
        <p>因此，Theta 帽的密度受 X 密度的影响。X 密度受 Theta 的真实值的影响。因此，Theta 帽的分布受 Theta 值的影响。换句话说，正如我几分钟前提到的，在这个行业中，我们好像在考虑不同的可能概率模型，每个 Theta 选择都有一个概率模型。</p><p>So the density of Theta hat is affected by the density of the X’s. The density of the X’s is affected by the
            true
            value of Theta. So the distribution of Theta hat is affected by the value of Theta. Another way to put it
            is, as
            I’ve mentioned a few minutes ago, in this business, it’s as if we are considering different possible
            probabilistic models, one probabilistic model for each choice of Theta.</p>
        <p>我们试图猜测这些概率模型中哪一个是真实的。强调这个表达式依赖于真实 Theta 的一个方法是在这里放置一个小下标，即期望，在参数 Theta 的特定值下。因此，根据真实参数 Theta 取什么值，这个期望将具有不同的值。</p><p>And we’re trying to guess which one of these probabilistic models is the true one. One way of emphasizing the
            fact that this expression depends on the true Theta is to put a little subscript here, expectation, under
            the
            particular value of the parameter Theta. So depending on what value the true parameter Theta takes, this
            expectation will have a different value.</p>
        <p>我们希望无论真实值是多少，我们的估计都不会在正值或负值方面产生偏差。所以这是一个理想的特性。它总是正确的吗？不一定，这取决于我们构建的估计量。对于我们的指数示例来说，它是正确的吗？不幸的是，不是，我们在指数示例中的估计结果是有偏差的。</p><p>And what we would like is that no matter what the true value is, that our estimate will not have a bias on
            the
            positive or the negative sides. So this is a property that’s desirable. Is it always going to be true? Not
            necessarily, it depends on what estimator we construct. Is it true for our exponential example?
            Unfortunately
            not, the estimate that we have in the exponential example turns out to be biased.</p>
        <h2 id="unknown-486">未知</h2><h2>Unknown</h2>
        <p>一个极端的观察方法是考虑样本量为 1 的情况。我们试图估计 Theta。在这种情况下，上一张幻灯片中的估计量只是 1/X1。现在 X1 在 0 附近有相当大的密度，这意味着 1/X1 有相当大的概率非常大。如果你进行计算，这最终会使 1/X1 的预期值为无穷大。</p><p>And one extreme way of seeing this is to consider the case where our sample size is 1. We’re trying to
            estimate
            Theta. And the estimator from the previous slide, in that case, is just 1/X1. Now X1 has a fair amount of
            density in the vicinity of 0, which means that 1/X1 has significant probability of being very large. And if
            you
            do the calculation, this ultimately makes the expected value of 1/X1 to be infinite.</p>
        <p>现在无穷大肯定不是正确的值。所以我们的估计值是向上偏差的。而且它实际上向上偏差了很多。事情就是这样。最大似然估计一般会有偏差。但在某些情况下，它们会逐渐变得无偏。</p><p>Now infinity is definitely not the correct value. So our estimate is biased upwards. And it’s actually biased
            a
            lot upwards. So that’s how things are. Maximum likelihood estimates, in general, will be biased. But under
            some
            conditions, they will turn out to be asymptotically unbiased.</p>
        <p>也就是说，随着你得到的数据越来越多，X 向量越来越长，有了独立数据，你得到的估计值，估计量的期望值将越来越接近真实值。所以你确实有一些很好的渐近性质，但我们不会证明任何类似的东西。
        </p><p>That is, as you get more and more data, as your X vector is longer and longer, with independent data, the
            estimate that you’re going to have, the expected value of your estimator is going to get closer and closer
            to
            the true value. So you do have some nice asymptotic properties, but we’re not going to prove anything like
            this.
        </p>
        <h2 id="unknown-487">未知</h2><h2>Unknown</h2>
        <p>说到渐近性质，一般来说，我们希望的是，随着你收集越来越多的数据，在某种意义上，你会得到正确的答案。我们在这里要使用的意义是概率收敛的极限意义，因为这是我们掌握的唯一随机变量收敛概念。例如，这类似于我们在民意测验问题中所遇到的。</p><p>Speaking of asymptotic properties, in general, what we would like to have is that, as you collect more and
            more
            data, you get the correct answer, in some sense. And the sense that we’re going to use here is the limiting
            sense of convergence in probability, since this is the only notion of convergence of random variables that
            we
            have in our hands. This is similar to what we had in the pollster problem, for example.</p>
        <p>如果我们的样本量越来越大，我们就可以越来越确信，我们获得的估计值接近我们已知的分布的未知真实参数。所以这是一个理想的特性。如果你有无限大量的数据，你应该能够或多或少准确地估计一个未知参数。所以这是估计量的理想特性。</p><p>If we had a bigger and bigger sample size, we could be more and more confident that the estimate that we
            obtained
            is close to the unknown true parameter of the distribution that we have. So this is a desirable property. If
            you
            have an infinitely large amount of data, you should be able to estimate an unknown parameter more or less
            exactly. So this is it desirable property of estimators.</p>
        <p>事实证明，在温和条件下，给定独立数据的最大似然估计确实具有这种性质。因此，在这方面，最大似然估计是一种很好的方法。那么让我们看看，在我们的指数示例中，我们是否具有这种一致性属性？在我们的指数示例中，我们使用这个量来估计未知参数 Theta。当 n 趋于无穷大时，这个量具有什么属性？</p><p>It turns out that maximum likelihood estimation, given independent data, does have this property, under mild
            conditions. So maximum likelihood estimation, in this respect, is a good approach. So let’s see, do we have
            this
            consistency property in our exponential example? In our exponential example, we used this quantity to
            estimate
            the unknown parameter Theta. What properties does this quantity have as n goes to infinity?</p>
        <h2 id="unknown-488">未知</h2><h2>Unknown</h2>
        <p>这个量是上面那个量的倒数，也就是样本均值。根据弱大数定律，我们知道样本均值收敛到期望值。所以这个性质来自弱大数定律。从概率上讲，这个量收敛到期望值，对于指数分布来说，期望值为 1/Theta。现在，如果某个量收敛到某个量，那么该量的倒数应该收敛到该量的倒数。</p><p>Well this quantity is the reciprocal of that quantity up here, which is the sample mean. We know from the
            weak
            law of large numbers, that the sample mean converges to the expectation. So this property here comes from
            the
            weak law of large numbers. In probability, this quantity converges to the expected value, which, for
            exponential
            distributions, is 1/Theta. Now, if something converges to something, then the reciprocal of that should
            converge
            to the reciprocal of that.</p>
        <p>这对于数字来说当然是正确的。但你谈论的不是数字的收敛。我们谈论的是概率的收敛，这是一个更复杂的概念。幸运的是，当我们处理概率收敛时，事实证明同样的事情是正确的。虽然我们不会费心去做这件事，但可以证明，这个的倒数，也就是我们的估计，确实在概率上收敛到那个的倒数。</p><p>That’s a property that’s certainly correct for numbers. But you’re not talking about convergence of numbers.
            We’re talking about convergence in probability, which is a more complicated notion. Fortunately, it turns
            out
            that the same thing is true, when we deal with convergence in probability. One can show, although we will
            not
            bother doing this, that indeed, the reciprocal of this, which is our estimate, converges in probability to
            the
            reciprocal of that.</p>
        <p>而这个倒数就是真正的参数 Theta。所以对于这个特定的指数示例，我们确实具有理想的特性，即随着数据数量越来越大，我们构建的估计值将越来越接近真正的参数值。无论 Theta 是多少，这都是正确的。无论真正的参数 Theta 是什么，随着我们收集更多数据，我们都会接近它。好的。</p><p>And that reciprocal is the true parameter Theta. So for this particular exponential example, we do have the
            desirable property, that as the number of data becomes larger and larger, the estimate that we have
            constructed
            will get closer and closer to the true parameter value. And this is true no matter what Theta is. No matter
            what
            the true parameter Theta is, we’re going to get close to it as we collect more data. Okay.</p>
        <h2 id="unknown-489">未知</h2><h2>Unknown</h2>
        <p>因此，这两个粗略的定性属性如果能有就好了。如果你想要更定量一点，你可以开始查看你的估计量给出的均方误差。现在，我再次重申我在这里提出的评论。也就是说，这里的这个期望是关于 Theta hat 概率分布的期望，它对应于小 theta 的一个特定值。所以固定一个小的 theta。</p><p>So these are two rough qualitative properties that would be nice to have. If you want to get a little more
            quantitative, you can start looking at the mean squared error that your estimator gives. Now, once more, the
            comment I was making up there applies. Namely, that this expectation here is an expectation with respect to
            the
            probability distribution of Theta hat that corresponds to a particular value of little theta. So fix a
            little
            theta.</p>
        <p>写下这个表达式。看看那个小的 theta 下的 Theta 帽的概率分布。然后进行这个计算。你会得到一些依赖于小的 theta 的量。所以这个等式中的所有量都应该被解释为那个特定的小的 theta 值下的量。所以如果你想让它更明确，你可以开始在这些表达式的任何地方添加小的下标。让我们看看这些表达式告诉我们什么。</p><p>Write down this expression. Look at the probability distribution of Theta hat, under that little theta. And
            do
            this calculation. You’re going to get some quantity that depends on the little theta. And so all quantities
            in
            this equality here should be interpreted as quantities under that particular value of little theta. So if
            you
            wanted to make this more explicit, you could start throwing little subscripts everywhere in those
            expressions.
            And let’s see what those expressions tell us.</p>
        <p>我们知道，一个随机变量的期望值平方总是等于这个随机变量的方差加上随机变量的期望平方。所以这个随机变量的期望值平方。这里的等式就是我们熟悉的公式，即X 的期望值平方等于 X 的方差加上 X 的期望值平方。所以我们将这个公式应用到 X 等于 Theta hat 减去 Theta。</p><p>The expected value squared of a random variable, we know that it’s always equal to the variance of this
            random
            variable, plus the expectation of the random variable squared. So the expectation value of that random
            variable,
            squared. This equality here is just our familiar formula, that the expected value of X squared is the
            variance
            of X plus the expected value of X squared. So we apply this formula to X equal to Theta hat minus Theta.</p>
        <h2 id="unknown-490">未知</h2><h2>Unknown</h2>
        <p>现在，请记住，在这个经典设置中，Theta 只是一个常数。我们已经固定了 Theta。我们想要计算这个量在特定 Theta 下的方差。当你给一个随机变量添加或减去一个常数时，方差不会改变。这与我们的估计量的方差相同。我们在这里得到的是估计的偏差。它告诉我们，平均而言，我们是高于还是低于这个值。</p><p>Now, remember that, in this classical setting, theta is just a constant. We have fixed Theta. We want to
            calculate the variance of this quantity, under that particular Theta. When you add or subtract a constant to
            a
            random variable, the variance doesn’t change. This is the same as the variance of our estimator. And what
            we’ve
            got here is the bias of our estimate. It tells us, on the average, whether we fall above or below.</p>
        <p>我们将偏差取为 b 平方。如果我们有一个无偏估计量，偏差项将为 0。因此，理想情况下，我们希望 Theta hat 非常接近 Theta。由于 Theta 是一个常数，如果发生这种情况，Theta hat 的方差将非常小。所以 Theta 是一个常数。如果 Theta hat 的分布集中在自己的小 Theta 周围，那么 Theta hat 的方差就会很小。</p><p>And we’re taking the bias to be b squared. If we have an unbiased estimator, the bias term will be 0. So
            ideally
            we want Theta hat to be very close to Theta. And since Theta is a constant, if that happens, the variance of
            Theta hat would be very small. So Theta is a constant. If Theta hat has a distribution that’s concentrated
            just
            around own little theta, then Theta hat would have a small variance.</p>
        <p>所以这是我们的愿望之一。我们将有一个较小的方差。但我们同时也希望有一个较小的偏差。因此，均方误差的一般形式有两个贡献。一个是我们的估计量的方差。另一个是偏差。人们通常希望设计一个同时保持这两个项较小的估计量。</p><p>So this is one desire that have. We’re going to have a small variance. But we also want to have a small bias
            at
            the same time. So the general form of the mean squared error has two contributions. One is the variance of
            our
            estimator. The other is the bias. And one usually wants to design an estimator that simultaneously keeps
            both of
            these terms small.</p>
        <h2 id="unknown-491">未知</h2><h2>Unknown</h2>
        <p>因此，这里有一个估计方法，对于这个术语来说，它表现很好，但对于那个术语来说，它表现很差。假设我的分布是正态分布，具有未知的均值 Theta 和方差 1。我使用一个非常愚蠢的估计量作为我的估计量。我总是产生一个估计值，说我的估计是 100。所以我只是忽略数据并报告 100。这有什么用呢？</p><p>So here’s an estimation method that would do very well with respect to this term, but badly with respect to
            that
            term. So suppose that my distribution is, let’s say, normal with an unknown mean Theta and variance 1. And I
            use
            as my estimator something very dumb. I always produce an estimate that says my estimate is 100. So I’m just
            ignoring the data and report 100. What does this do?</p>
        <p>我的估计量的方差为 0。我报告的估计量中没有随机性。但偏差会非常严重。偏差将是 Theta 帽，即 100 减去 Theta 的真实值。对于某些 Theta，我的偏差会非常严重。如果我的真实 Theta 恰好为 0，我的偏差平方就是一个巨大的项。我会得到一个很大的错误。</p><p>The variance of my estimator is 0. There’s no randomness in the estimate that I report. But the bias is going
            to
            be pretty bad. The bias is going to be Theta hat, which is 100 minus the true value of Theta. And for some
            Theta’s, my bias is going to be horrible. If my true Theta happens to be 0, my bias squared is a huge term.
            And
            I get a large error.</p>
        <p>那么这个例子的寓意是什么呢？有办法让方差变得很小，但在这些情况下，你需要付出偏差的代价。所以你需要做一些更细致的事情，同时让两个项都保持很小。所以当你开始尝试为更复杂的问题设计复杂的估计量时，这些类型的考虑就变得很重要。但我们不会在这堂课中这样做。</p><p>So what’s the moral of this example? There are ways of making that variance very small, but, in those cases,
            you
            pay a price in the bias. So you want to do something a little more delicate, where you try to keep both
            terms
            small at the same time. So these types of considerations become important when you start to try to design
            sophisticated estimators for more complicated problems. But we will not do this in this class.</p>
        <h2 id="unknown-492">未知</h2><h2>Unknown</h2>
        <p>这属于统计学和推理的进阶课程。对于这门课程，对于参数估计，我们基本上会坚持使用两种非常简单的方法。一种方法是我们刚刚讨论过的最大似然法。另一种方法是如果你还在读高中并且不知道任何概率的话你会做的。你得到数据。这些数据来自某个具有未知均值的分布。你想估计这个未知均值。</p><p>This belongs to further classes on statistics and inference. For this class, for parameter estimation, we
            will
            basically stick to two very simple methods. One is the maximum likelihood method we’ve just discussed. And
            the
            other method is what you would do if you were still in high school and didn’t know any probability. You get
            data. And these data come from some distribution with an unknown mean. And you want to estimate that the
            unknown
            mean.</p>
        <p>你会怎么做？你只需要取这些数据并取平均值。让我们更具体一点。我们有来自给定分布的 X。我们可能知道分布的一般形式。我们可能知道该分布的方差，或者，我们可能不知道。但我们不知道平均值。我们想估计该分布的平均值。现在，我们可以写出这种情况。</p><p>What would you do? You would just take those data and average them out. So let’s make this a little more
            specific. We have X’s that come from a given distribution. We know the general form of the distribution,
            perhaps. We do know, perhaps, the variance of that distribution, or, perhaps, we don’t know it. But we do
            not
            know the mean. And we want to estimate the mean of that distribution. Now, we can write this situation.</p>
        <p>我们可以用不同的形式来表示它。Xi 等于 Theta。这是平均值。加上一个 0 均值随机变量，你可以将其视为噪声。因此，这对应于你在实验室中通常会遇到的情况，你去那里尝试测量一个未知量。你会得到很多测量结果。但每次测量时，你的测量结果中都会有一些额外的噪声。</p><p>We can represent it in a different form. The Xi’s are equal to Theta. This is the mean. Plus a 0 mean random
            variable, that you can think of as noise. So this corresponds to the usual situation you would have in a
            lab,
            where you go and try to measure an unknown quantity. You get lots of measurements. But each time that you
            measure them, your measurements have some extra noise in there.</p>
        <h2 id="unknown-493">未知</h2><h2>Unknown</h2>
        <p>你想要消除这种噪音。消除测量噪音的方法是收集大量数据并取平均值。这就是样本均值。这是一种非常合理的方法来估计 X 的未知均值。所以这就是样本均值。它是某个分布的未知均值的合理、可信、一般来说相当好的估计量。</p><p>And you want to kind of get rid of that noise. The way to try to get rid of the measurement noise is to
            collect
            lots of data and average them out. This is the sample mean. And this is a very, very reasonable way of
            trying to
            estimate the unknown mean of the X’s. So this is the sample mean. It’s a reasonable, plausible, in general,
            pretty good estimator of the unknown mean of a certain distribution.</p>
        <p>我们可以在不真正了解 X 的分布的情况下应用此估计量。实际上，我们不需要了解任何有关分布的信息。我们仍然可以应用它，因为例如方差不会显示在这里。我们不需要知道方差来计算该数量。这个估计量具有良好的特性吗？是的，它确实如此。样本均值的预期值是多少？</p><p>We can apply this estimator without really knowing a lot about the distribution of the X’s. Actually, we
            don’t
            need to know anything about the distribution. We can still apply it, because the variance, for example, does
            not
            show up here. We don’t need to know the variance to calculate that quantity. Does this estimator have good
            properties? Yes, it does. What’s the expected value of the sample mean?</p>
        <p>如果这个期望是这个和的期望除以 n。每个 X 的期望值是 Theta。因此样本均值的期望值就是 Theta 本身。所以我们的估计量是无偏的。无论 Theta 是多少，我们的估计量在任何一个方向上都没有系统误差。此外，弱大数定律告诉我们，这个量在概率上收敛到真实参数。</p><p>If the expectation of this, it’s the expectation of this sum divided by n.&nbsp;The expected value for each one of
            the
            X’s is Theta. So the expected value of the sample mean is just Theta itself. So our estimator is unbiased.
            No
            matter what Theta is, our estimator does not have a systematic error in either direction. Furthermore, the
            weak
            law of large numbers tells us that this quantity converges to the true parameter in probability.</p>
        <h2 id="unknown-494">未知</h2><h2>Unknown</h2>
        <p>所以它是一个一致的估计量。这很好。如果你想计算与这个估计量相对应的均方误差。还记得我们如何定义均方误差吗？它就是这个量。那么这是我们到目前为止已经做过很多次的计算。均方误差是 X 分布的方差除以 n。&nbsp;</p><p>So it’s a consistent estimator. This is good. And if you want to calculate the mean squared error
            corresponding
            to this estimator. Remember how we defined the mean squared error? It’s this quantity. Then it’s a
            calculation
            that we have done a fair number of times by now. The mean squared error is the variance of the distribution
            of
            the X’s divided by n.&nbsp;</p>
        <p>因此，随着我们获得越来越多的数据，均方误差会下降到 0。在一些例子中，样本均值也与最大似然估计相同。例如，如果 X 来自正态分布，您可以写下似然，对 Theta 进行最大化，您会发现最大似然估计与样本均值相同。</p><p>So as we get more and more data, the mean squared error goes down to 0. In some examples, it turns out that
            the
            sample mean is also the same as the maximum likelihood estimate. For example, if the X’s are coming from a
            normal distribution, you can write down the likelihood, do the maximization with respect to Theta, you’ll
            find
            that the maximum likelihood estimate is the same as the sample mean.</p>
        <p>在其他情况下，样本均值将不同于最大似然值。然后您可以选择使用两者中的哪一个。在大多数合理的情况下，您可能只会使用样本均值，因为它简单、易于计算且具有良好的特性。好的。然后你去找你的老板。你报告说，好的，我在实验室做了所有的实验。</p><p>In other cases, the sample mean will be different from the maximum likelihood. And then you have a choice
            about
            which one of the two you would use. Probably, in most reasonable situations, you would just use the sample
            mean,
            because it’s simple, easy to compute, and has nice properties. All right. So you go to your boss. And you
            report
            and say, OK, I did all my experiments in the lab.</p>
        <h2 id="unknown-495">未知</h2><h2>Unknown</h2>
        <p>我得到的平均值是一个确定的数字，2.37。那么这对你的老板来说有用吗？你的老板想知道他们能相信这个数字 2.37 多少。好吧，我知道真实值不会完全是这个数字。但它应该有多接近呢？所以请告诉我你认为 Theta 的可能值的范围。情况是这样的。</p><p>And the average value that I got is a certain number, 2.37. So is that the informative to your boss? Well
            your
            boss would like to know how much they can trust this number, 2.37. Well, I know that the true value is not
            going
            to be exactly that. But how close should it be? So give me a range of what you think are possible values of
            Theta. So the situation is like this.</p>
        <p>假设我们观察到来自某个分布的 X。我们试图估计平均值。我们得到数据。也许我们的数据看起来像这样。你计算平均值。你找到样本平均值。因此，让我们假设样本平均值是一个数字，出于某种原因取为 2.37。但你想向你的老板传达这些数据分布有多广的信息。</p><p>So suppose that we observe X’s that are coming from a certain distribution. And we’re trying to estimate the
            mean. We get our data. Maybe our data looks something like this. You calculate the mean. You find the sample
            mean. So let’s suppose that the sample mean is a number, for some reason take to be 2.37. But you want to
            convey
            something to your boss about how spread out these data were.</p>
        <p>因此老板要求你给出一个区间，真实参数 Theta 可能位于该区间。因此老板要求你给出一个区间。因此你最终要报告一个区间。你以某种方式使用你看到的数据来构建这个区间。你还要向老板报告这个区间的端点。让我们给这些端点命名，Theta_n 和 Theta_n+。</p><p>So the boss asks you to give him or her some kind of interval on which Theta, the true parameter, might lie.
            So
            the boss asked you for an interval. So what you do is you end up reporting an interval. And you somehow use
            the
            data that you have seen to construct this interval. And you report to your boss also the endpoints of this
            interval. Let’s give names to these endpoints, Theta_n and Theta_n+.</p>
        <h2 id="unknown-496">未知</h2><h2>Unknown</h2>
        <p>这里的端点只是起到跟踪我们使用了多少数据的作用。所以你向老板报告的也是这个区间。这里的 Theta 是区间的端点，是小写还是大写？它们应该是什么？好吧，你在看到数据后构建这些区间。你在构建区间时要考虑数据。所以这些肯定应该取决于数据。因此它们是随机变量。
        </p><p>The ends here just play the role of keeping track of how many data we’re using. So what you report to your
            boss
            is this interval as well. Are these Theta’s here, the endpoints of the interval, lowercase or uppercase?
            What
            should they be? Well you construct these intervals after you see your data. You take the data into account
            to
            construct your interval. So these definitely should depend on the data. And therefore they are random
            variables.
        </p>
        <p>您的估算器也是一样，一般来说，它将是一个随机变量。但是，当您去向老板报告数字时，您会根据获得的数据给出随机变量的具体实现。因此，您不会只有一个产生估算值的框。因此，我们之前的画面是，您有一个估算器，它采用 X 并产生 Theta 帽。</p><p>Same thing with your estimator, in general, it’s going to be a random variable. Although, when you go and
            report
            numbers to your boss, you give the specific realizations of the random variables, given the data that you
            got.
            So instead of having just a single box that produces estimates. So our previous picture was that you have
            your
            estimator that takes X’s and produces Theta hats.</p>
        <p>现在我们的盒子也会产生 Theta 帽减号和 Theta 帽加号。它还会产生一个间隔。X 是随机的，因此这些数量也是随机的。一旦你去做实验并得到你的数据，那么你的数据就会是一些小写的 x，具体数字。然后你的估计值和估计量也会变成小写。我们希望这个间隔做什么？</p><p>Now our box will also be producing Theta hats minus and Theta hats plus. It’s going to produce an interval as
            well. The X’s are random, therefore these quantities are random. Once you go and do the experiment and
            obtain
            your data, then your data will be some lowercase x, specific numbers. And then your estimates and estimator
            become also lower case. What would we like this interval to do?</p>
        <h2 id="unknown-497">未知</h2><h2>Unknown</h2>
        <p>我们希望它很可能包含参数的真实值。因此，我们可能会施加以下类型的一些规范。我选择一个数字，alpha。通常，这个alpha，可以看作是大误差的概率。alpha的典型值可能是0.05，在这种情况下，这个数字是0.95。你会得到这样的规范。</p><p>We would like it to be highly likely to contain the true value of the parameter. So we might impose some
            specs of
            the following kind. I pick a number, alpha. Usually that alpha, think of it as a probability of a large
            error.
            Typical value of alpha might be 0.05, in which case this number here is point 0.95. And you’re given specs
            that
            say something like this.</p>
        <p>我希望这种情况发生的概率至少为 0.95，也就是说真实参数位于置信区间内。现在让我们试着解释一下这个说法。假设你做了这个实验，最后向老板报告的置信区间是 1.97 到 2.56。这就是你向老板报告的。假设置信区间有这个属性。</p><p>I would like, with probability at least 0.95, this to happen, which says that the true parameter lies inside
            the
            confidence interval. Now let’s try to interpret this statement. Suppose that you did the experiment, and
            that
            you ended up reporting to your boss a confidence interval from 1.97 to 2.56. That’s what you report to your
            boss. And suppose that the confidence interval has this property.</p>
        <p>你能告诉你的老板，Theta 的真实值介于这两个数字之间的概率是 95% 吗？这是一个有意义的陈述吗？因此，这个陈述是，暂定的陈述是，Theta 的真实值介于 1.97 和 2.56 之间的概率是 95%。那么，这个陈述中的随机性是什么？没有什么是随机的。Theta 的真实值是一个常数。1.97 是一个数字。2.56 也是一个数字。</p><p>Can you go to your boss and say, with probability 95%, the true value of Theta is between these two numbers?
            Is
            that a meaningful statement? So the statement is, the tentative statement is, with probability 95%, the true
            value of Theta is between 1.97 and 2.56. Well, what is random in that statement? There’s nothing random. The
            true value of theta is a constant. 1.97 is a number. 2.56 is a number.</p>
        <h2 id="unknown-498">未知</h2><h2>Unknown</h2>
        <p>因此，讨论 theta 处于这个区间的概率没有任何意义。要么 theta 恰好处于这个区间，要么恰好不在。但是没有与此相关的概率。因为 theta 不是随机的。从语法上看，你可以看到这一点。因为这里的 theta 是小写。那么我们在这里谈论的是什么样的概率呢？随机性在哪里？好吧，随机的东西是区间。它不是 theta。</p><p>So it doesn’t make any sense to talk about the probability that theta is in this interval. Either theta
            happens
            to be in that interval, or it happens to not be. But there are no probabilities associated with this.
            Because
            theta is not random. Syntactically, you can see this. Because theta here is a lower case. So what kind of
            probabilities are we talking about here? Where’s the randomness? Well the random thing is the interval. It’s
            not
            theta.</p>
        <p>因此，这里要说明的是，由我们的程序构建的区间应该具有这样的属性：有 95% 的概率，它将落在 theta 的真实值之上。因此，解释 95% 置信区间的正确方法如下。我们有我们不知道的 theta 的真实值。我得到数据。根据数据，我构建了一个置信区间。</p><p>So the statement that is being made here is that the interval, that’s being constructed by our procedure,
            should
            have the property that, with probability 95%, it’s going to fall on top of the true value of theta. So the
            right
            way of interpreting what the 95% confidence interval is, is something like the following. We have the true
            value
            of theta that we don’t know. I get data. Based on the data, I construct a confidence interval.</p>
        <p>我得到了我的置信区间。我很幸运。θ 的真实值就在这里。第二天，我做了同样的实验，获取我的数据，构建一个置信区间。我得到了这个置信区间，又一次幸运。第二天我得到了数据。我用我的数据得出了 θ 和置信区间的估计值。那天，我运气不好。我在那里得到了一个置信区间。</p><p>I get my confidence interval. I got lucky. And the true value of theta is in here. Next day, I do the same
            experiment, take my data, construct a confidence interval. And I get this confidence interval, lucky once
            more.
            Next day I get data. I use my data to come up with an estimate of theta and the confidence interval. That
            day, I
            was unlucky. And I got a confidence interval out there.</p>
        <h2 id="unknown-499">未知</h2><h2>Unknown</h2>
        <p>这里的要求是，在我们使用特定程序构建置信区间的 95% 的日子里，我们都会很幸运。我们将通过置信区间捕获正确的 theta 值。因此，这是关于这些随机置信区间分布的陈述，它们落在真实 theta 之上的可能性有多大，而不是它们落在真实 theta 之外的可能性有多大。</p><p>What the requirement here is, is that 95% of the days, where we use this certain procedure for constructing
            confidence intervals, 95% of those days, we will be lucky. And we will capture the correct value of theta by
            your confidence interval. So it’s a statement about the distribution of these random confidence intervals,
            how
            likely are they to fall on top of the true theta, as opposed to how likely they are to fall outside.</p>
        <p>所以这是关于与置信区间相关的概率的陈述。它们不是关于 theta 的概率，因为 theta 本身不是随机的。所以这就是一般意义上的置信区间，以及我们如何解释它。我们如何构建 95% 的置信区间？让我们在一个特定示例中完成这个练习。计算与我们讨论大数定律和中心极限定理时所做的计算完全相同。</p><p>So it’s a statement about probabilities associated with a confidence interval. They’re not probabilities
            about
            theta, because theta, itself, is not random. So this is what the confidence interval is, in general, and how
            we
            interpret it. How do we construct a 95% confidence interval? Let’s go through this exercise, in a particular
            example. The calculations are exactly the same as the ones that you did when we talked about laws of large
            numbers and the central limit theorem.</p>
        <p>因此，从计算上来说，这没有什么新意，但就我们使用的语言和解释而言，这也许是新的。所以我们从某个分布中得到了样本均值。我们想计算 95% 的置信区间。我们从正态分布表中知道，标准正态分布的尾部有 2.5%，也就是在 1.96 之后。是的，到这个时候，1.96 这个数字应该相当熟悉了。</p><p>So there’s nothing new calculationally but it’s, perhaps, new in terms of the language that we use and the
            interpretation. So we got our sample mean from some distribution. And we would like to calculate a 95%
            confidence interval. We know from the normal tables, that the standard normal has 2.5% on the tail, that’s
            after
            1.96. Yes, by this time, the number 1.96 should be pretty familiar.</p>
        <h2 id="unknown-500">未知</h2><h2>Unknown</h2>
        <p>因此，如果这里的概率是 2.5%，那么这里的数字就是 1.96。现在看看这里的随机变量。这是样本平均值。与真实平均值的差异，由通常的归一化因子归一化。根据中心极限定理，这大致是正常的。因此，小于 1.96 的概率为 0.95。现在拿这个事件来重写它。</p><p>So if this probability here is 2.5%, this number here is 1.96. Now look at this random variable here. This is
            the
            sample mean. Difference, from the true mean, normalized by the usual normalizing factor. By the central
            limit
            theorem, this is approximately normal. So it has probability 0.95 of being less than 1.96. Now take this
            event
            here and rewrite it.</p>
        <p>这个事件，即 Theta 减去 theta 大于这个数字，小于那个数字。这里的这个事件等同于这里的那个事件。因此，这暗示了一种构建 95% 置信区间的方法。</p><p>This the event, well, that Theta hat minus theta is bigger than this number and smaller than that number.
            This
            event here is equivalent to that event here. And so this suggests a way of constructing our 95% percent
            confidence interval.</p>
        <p>我将报告区间，该区间表示置信区间的下限，该区间表示置信区间的上限，换句话说，在实验结束时，我们将报告样本均值，这是我们的估计值。我们还将报告样本均值周围的区间。这是我们的 95% 置信区间。n 越大，置信区间越小。</p><p>I’m going to report the interval, which gives this as the lower end of the confidence interval, and gives
            this as
            the upper end of the confidence interval in other words, at the end of the experiment, we report the sample
            mean, which is our estimate. And we report also, an interval around the sample mean. And this is our 95%
            confidence interval. The confidence interval becomes smaller, when n is larger.</p>
        <h2 id="unknown-501">未知</h2><h2>Unknown</h2>
        <p>从某种意义上说，我们更确信自己做的估算工作很好，因此我们可以采用较小的区间，但仍然可以确信我们的区间能够捕捉参数的真实值。此外，如果我们的数据噪声很小，当您有更准确的测量结果时，您会更有信心您的估算相当不错。这会导致置信区间更小，置信区间的长度也更短。</p><p>In some sense, we’re more certain that we’re doing a good estimation job, so we can have a small interval and
            still be quite confident that our interval captures the true value of the parameter. Also, if our data have
            very
            little noise, when you have more accurate measurements, you’re more confident that your estimate is pretty
            good.
            And that results in a smaller confidence interval, smaller length of the confidence interval.</p>
        <p>但你仍有 95% 的概率可以捕捉到 theta 的真实值。所以我们通过从正态表中获取 95% 的置信区间和相应的值（即 1.96）来进行此练习。当然，如果你将 alpha 设置为其他数字，则可以更普遍地执行此操作。再次查看正态表。</p><p>And still you have 95% probability of capturing the true value of theta. So we did this exercise by taking
            95%
            confidence intervals and the corresponding value from the normal tables, which is 1.96. Of course, you can
            do it
            more generally, if you set your alpha to be some other number. Again, you look at the normal tables.</p>
        <p>您会在这里找到该值，因此尾部的概率 alpha 大于 2。您不用这个 1.96，而是使用从正常表格中得到的任何数字。这会告诉您如何构建置信区间。确切地说，这不一定是 95% 的置信区间。它大约是 95% 的置信区间。为什么会这样？因为我们做了近似。我们使用了中心极限定理。</p><p>And you find the value here, so that the tail has probability alpha over 2. And instead of using these 1.96,
            you
            use whatever number you get from the normal tables. And this tells you how to construct a confidence
            interval.
            Well, to be exact, this is not necessarily a 95% confidence interval. It’s approximately a 95% confidence
            interval. Why is this? Because we’ve done an approximation. We have used the central limit theorem.</p>
        <h2 id="unknown-502">未知</h2><h2>Unknown</h2>
        <p>因此，结果可能是 95.5% 的置信区间，而不是 95%，因为我们的计算并不完全准确。但对于合理的 n 值，使用中心极限定理是一个很好的近似值。人们几乎总是这么做。所以只需从正常表中获取值即可。好的，除了一个问题。我使用了数据。我得到了我的估计值。</p><p>So it might turn out to be a 95.5% confidence interval instead of 95%, because our calculations are not
            entirely
            accurate. But for reasonable values of n, using the central limit theorem is a good approximation. And
            that’s
            what people almost always do. So just take the value from the normal tables. Okay, except for one catch. I
            used
            the data. I obtained my estimate.</p>
        <p>我想向我的老板汇报这个 theta minus 和 theta hat，也就是置信区间。这有什么困难？我知道 n 是什么。但我不知道 sigma 是什么。所以如果我不知道 sigma，我该怎么办？这里有几种选择。第一种选择与我们讨论民意测验问题时的做法很相似。</p><p>And I want to go to my boss and report this theta minus and theta hat, which is the confidence interval.
            What’s
            the difficulty? I know what n is. But I don’t know what sigma is, in general. So if I don’t know sigma, what
            am
            I going to do? Here, there’s a few options for what you can do. And the first option is familiar from what
            we
            did when we talked about the pollster problem.</p>
        <p>我们不知道 sigma 是什么，但也许我们对 sigma 有一个上限。例如，如果是 Xi 的伯努利随机变量，我们已经看到标准差最多为 1/2。因此，对 sigma 使用最保守的值。使用最保守的值意味着您采用比必要更大的置信区间。所以这是一个选择。另一个选择是尝试从数据中估计 sigma。您如何进行此估计？
        </p><p>We don’t know what sigma is, but maybe we have an upper bound on sigma. For example, if the Xi’s Bernoulli
            random
            variables, we have seen that the standard deviation is at most 1/2. So use the most conservative value for
            sigma. Using the most conservative value means that you take bigger confidence intervals than necessary. So
            that’s one option. Another option is to try to estimate sigma from the data. How do you do this estimation?
        </p>
        <h2 id="unknown-503">未知</h2><h2>Unknown</h2>
        <p>在特殊情况下，对于特殊类型的分布，您可以考虑采用启发式方法进行这种估计。例如，在伯努利随机变量的情况下，我们知道 sigma 的真实值（伯努利随机变量的标准差）是 theta1 减 theta 的平方根，其中 theta 是伯努利的平均值。尝试使用此公式。但 theta 是我们首先要估计的东西。
        </p><p>In special cases, for special types of distributions, you can think of heuristic ways of doing this
            estimation.
            For example, in the case of Bernoulli random variables, we know that the true value of sigma, the standard
            deviation of a Bernoulli random variable, is the square root of theta1 minus theta, where theta is the mean
            of
            the Bernoulli. Try to use this formula. But theta is the thing we’re trying to estimate in the first place.
        </p>
        <p>我们不知道。我们该怎么办？好吧，我们有一个估计值，即由我们的估计程序产生的估计值，即样本均值。所以我获得了我的数据。我得到了我的数据。我产生了估计值 theta hat。它是均值的估计值。在这个公式中使用该估计值来得出我的标准差的估计值。
        </p><p>We don’t know it. What do we do? Well, we have an estimate for theta, the estimate, produced by our
            estimation
            procedure, the sample mean. So I obtain my data. I get my data. I produce the estimate theta hat. It’s an
            estimate of the mean. Use that estimate in this formula to come up with an estimate of my standard
            deviation.
        </p>
        <p>然后，在构建置信区间时，使用标准差，假设这是正确的。好吧，你的数据数量很大，那么根据大数定律，我们知道 theta hat 是 theta 的一个相当好的估计。因此，sigma hat 将是 sigma 的一个相当好的估计。因此，使用这种方法我们不会犯大错误。</p><p>And then use that standard deviation, in the construction of the confidence interval, pretending that this is
            correct. Well the number of your data is large, then we know, from the law of large numbers, that theta hat
            is a
            pretty good estimate of theta. So sigma hat is going to be a pretty good estimate of sigma. So we’re not
            making
            large errors by using this approach.</p>
        <h2 id="unknown-504">未知</h2><h2>Unknown</h2>
        <p>因此，在这种情况下，事情很简单，因为我们有一个分析公式。 Sigma 由 theta 决定。 因此，我们可以快速粗略地估计 sigma。 一般来说，如果您没有这种好的公式，您该怎么办？ 好吧，您仍然需要以某种方式估计 sigma。 估计标准差的通用方法是什么？</p><p>So in this scenario here, things were simple, because we had an analytical formula. Sigma was determined by
            theta. So we could come up with a quick and dirty estimate of sigma. In general, if you do not have any nice
            formulas of this kind, what could you do? Well, you still need to come up with an estimate of sigma somehow.
            What is a generic method for estimating a standard deviation?</p>
        <p>同样，估计方差的通用方法是什么？方差是某个随机变量的预期值。方差是括号内随机变量的平均值。如何估计某个随机变量的平均值？你获得该随机变量的大量测量值并取平均值。所以这将是估计分布方差的合理方法。</p><p>Equivalently, what could be a generic method for estimating a variance? Well the variance is an expected
            value of
            some random variable. The variance is the mean of the random variable inside of those brackets. How does one
            estimate the mean of some random variable? You obtain lots of measurements of that random variable and
            average
            them out. So this would be a reasonable way of estimating the variance of a distribution.</p>
        <p>再次，弱大数定律告诉我们，这个平均值会收敛到这个的期望值，也就是分布的方差。所以我们得到了一个很好且一致的方差估计方法。但现在，我们似乎陷入了一个恶性循环，因为要估计方差，我们需要知道平均值。而平均值是我们首先要估计的东西。好的。</p><p>And again, the weak law of large numbers tells us that this average converges to the expected value of this,
            which is just the variance of the distribution. So we got a nice and consistent way of estimating variances.
            But
            now, we seem to be getting in a vicious circle here, because to estimate the variance, we need to know the
            mean.
            And the mean is something we’re trying to estimate in the first place. Okay.</p>
        <h2 id="unknown-505">未知</h2><h2>Unknown</h2>
        <p>但是我们确实有一个平均值的估计值。因此，一个合理的近似值是再次代入平均值的估计值，因为我们不知道平均值。这样你就得到了这个表达式，但是用的是 theta 帽而不是 theta 本身。这是估计方差的另一种合理方法。它确实具有相同的一致性属性。为什么？</p><p>But we do have an estimate from the mean. So a reasonable approximation, once more, is to plug in, here,
            since we
            don’t know the mean, the estimate of the mean. And so you get that expression, but with a theta hat instead
            of
            theta itself. And this is another reasonable way of estimating the variance. It does have the same
            consistency
            properties. Why?</p>
        <p>当 n 很大时，其表现将与 n 相同，因为 θ 收敛到 θ。当 n 很大时，其与 sigma 平方大致相同。因此，对于较大的 n，该量也收敛到 sigma 平方。并且，我们也有方差的一致估计。我们可以采用该一致估计，并将其用于构建置信区间。</p><p>When n is large, this is going to behave the same as that, because theta hat converges to theta. And when n
            is
            large, this is approximately the same as sigma squared. So for a large n, this quantity also converges to
            sigma
            squared. And we have a consistent estimate of the variance as well. And we can take that consistent estimate
            and
            use it back in the construction of confidence interval.</p>
        <p>这里有一个小细节，我们除以 n。这里，我们除以 n 1。我们为什么要这样做？好吧，事实证明，这就是你需要做的，以使这些估计成为方差的无偏估计。人们必须进行一些计算，然后发现这就是你需要的因素，以便无偏。
        </p><p>One little detail, here, we’re dividing by n.&nbsp;Here, we’re dividing by n 1. Why do we do this? Well, it turns
            out
            that’s what you need to do for these estimates to be an unbiased estimate of the variance. One has to do a
            little bit of a calculation, and one finds that’s the factor that you need to have here in order to be
            unbiased.
        </p>
        <h2 id="unknown-506">未知</h2><h2>Unknown</h2>
        <p>当然，如果你得到 100 个数据点，无论你除以 100 还是除以 99，对方差的估计都只会有微小的差异。所以，标准差的估计也只会有微小的差异。这不是什么大问题。而且真的无关紧要。</p><p>Of course, if you get 100 data points, whether you divide by 100 or divided by 99, it’s going to make only a
            tiny
            difference in your estimate of your variance. So it’s going to make only a tiny difference in your estimate
            of
            the standard deviation. It’s not a big deal. And it doesn’t really matter.</p>
        <p>但是如果你想炫耀自己对统计学的深入了解，你就得加上 1/n 1 这个因素。所以现在你基本上需要把这个故事拼凑起来，即如何估计方差。你首先估计样本平均值。然后做一些额外的工作来得出一个合理的方差和标准差估计值。</p><p>But if you want to show off about your deeper knowledge of statistics, you throw in the 1 over n 1 factor in
            there. So now one basically needs to put together this story here, how you estimate the variance. You first
            estimate the sample mean. And then you do some extra work to come up with a reasonable estimate of the
            variance
            and the standard deviation.</p>
        <p>然后，你使用标准差的估计值来得出一个置信区间，该区间具有这两个端点。在执行此过程时，基本上涉及许多近似值。有两种类型的近似值。一种近似值是我们假设样本均值具有正态分布。根据中心极限定理，我们有理由这样做。但这不是精确的。这只是一个近似值。</p><p>And then you use your estimate, of the standard deviation, to come up with a confidence interval, which has
            these
            two endpoints. In doing this procedure, there’s basically a number of approximations that are involved.
            There
            are two types of approximations. One approximation is that we’re pretending that the sample mean has a
            normal
            distribution. That’s something we’re justified to do, by the central limit theorem. But it’s not exact. It’s
            an
            approximation.</p>
        <h2 id="unknown-507">未知</h2><h2>Unknown</h2>
        <p>第二个近似值是，通常情况下，你不必使用正确的标准差，而是必须使用标准差的近似值。好的，所以你将在复习和辅导课中对这些概念进行一些练习。下周我们将继续学习新主题。但期末考试中要讲的内容只到这一步。所以下周只是通识教育。希望有用，但它不在考试中。</p><p>And the second approximation that comes in is that, instead of using the correct standard deviation, in
            general,
            you will have to use some approximation of the standard deviation. Okay so you will be getting a little bit
            of
            practice with these concepts in recitation and tutorial. And we will move on to new topics next week. But
            the
            material that’s going to be covered in the final exam is only up to this point. So next week is just general
            education. Hopefully useful, but it’s not in the exam.</p>
        <h1 id="classical-inference-ii">24. 古典推理 II</h1><h1>24. Classical Inference II</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 51:50 <a href="https://youtube.com/watch?v=tBUHRpFZy0s">https://youtube.com/watch?v=tBUHRpFZy0s</a></p><p> 12 years ago (Nov 10, 2012) — 51:50 <a href="https://youtube.com/watch?v=tBUHRpFZy0s">https://youtube.com/watch?v=tBUHRpFZy0s</a></p>
        <h2 id="unknown-508">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu。约翰·齐西克利斯：今天我们将继续讨论经典统计学。</p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. JOHN TSITSIKLIS: And we’re going to
            continue today with our discussion of classical statistics.</p>
        <p>我们首先快速回顾一下上次讨论的内容，然后讨论两个主题，这两个主题涵盖了现实世界中发生的大量统计数据。所以有两种基本方法。一种是线性回归方法，另一种是进行假设检验的基本方法和工具。好的，这两个主题是任何具有科学素养的人都应该了解的。</p><p>We’ll start with a quick review of what we discussed last time, and then talk about two topics that cover a
            lot
            of statistics that are happening in the real world. So two basic methods. One is the method of linear
            regression, and the other one is the basic methods and tools for how to do hypothesis testing. OK, so these
            two
            are topics that any scientifically literate person should know something about.</p>
        <p>我们将介绍其中涉及的基本思想和概念。在经典统计学中，我们基本上有一系列关于世界的可能模型。世界是我们观察到的随机变量，我们有一个模型，但实际上不只是一个模型，而是几个候选模型。每个候选模型都对应一个我们不知道的参数θ的不同值。</p><p>So we’re going to introduce the basic ideas and concepts involved. So in classical statistics we basically
            have
            essentially a family of possible models about the world. So the world is the random variable that we
            observe,
            and we have a model for it, but actually not just one model, several candidate models. And each candidate
            model
            corresponds to a different value of a parameter theta that we do not know.</p>
        <h2 id="unknown-509">未知</h2><h2>Unknown</h2>
        <p>因此，与贝叶斯统计相反，这个 theta 被假定为我们不知道的常数。它不是作为随机变量建模的，没有与 theta 相关的概率。我们只有关于 X 的概率。那么在这种情况下，选择参数值的合理方法是什么？一种通用方法是最大似然法，它选择这个量最大的 theta。</p><p>So in contrast to Bayesian statistics, this theta is assumed to be a constant that we do not know. It is not
            modeled as a random variable, there’s no probabilities associated with theta. We only have probabilities
            about
            the X’s. So in this context what is a reasonable way of choosing a value for the parameter? One general
            approach
            is the maximum likelihood approach, which chooses the theta for which this quantity is largest.</p>
        <p>那么直观上这是什么意思呢？我试图找到我观察到的数据最有可能出现的 theta 值。因此，思路本质上如下。假设我必须在两个 theta 值之间做出选择。在这个 theta 值下，我观察到的 X 不太可能发生。在那个 theta 值下，我观察到的 X 有相当大的概率发生。</p><p>So what does that mean intuitively? I’m trying to find the value of theta under which the data that I observe
            are
            most likely to have occurred. So is the thinking is essentially as follows. Let’s say I have to choose
            between
            two choices of theta. Under this theta the X that I observed would be very unlikely. Under that theta the X
            that
            I observed would have a decent probability of occurring.</p>
        <p>所以我选择后者作为我对 theta 的估计。与我们上次讨论过的贝叶斯方法进行比较很有趣，在贝叶斯方法中，我们也最大化了 theta，但我们最大化了一个量，在这个量中，X 和 theta 之间的关系是相反的。在贝叶斯世界中，Theta 是一个随机变量。所以它有一个分布。</p><p>So I chose the latter as my estimate of theta. It’s interesting to do the comparison with the Bayesian
            approach
            which we did discuss last time, in the Bayesian approach we also maximize over theta, but we maximize a
            quantity
            in which the relation between X’s and thetas run the opposite way. Here in the Bayesian world, Theta is a
            random
            variable. So it has a distribution.</p>
        <h2 id="unknown-510">未知</h2><h2>Unknown</h2>
        <p>一旦我们观察数据，它就会有一个后验分布，我们会发现 Theta 的值最有可能位于后验分布之下。正如我们上次讨论的那样，当您进行最大化时，后验分布由这个表达式给出。分母并不重要，如果您采用先验，它是平坦的。这是一个独立于 Theta 的常数，那么该项就会消失。</p><p>Once we observe the data, it has a posterior distribution, and we find the value of Theta, which is most
            likely
            under the posterior distribution. As we discussed last time when you do this maximization now the posterior
            distribution is given by this expression. The denominator doesn’t matter, and if you were to take a prior,
            which
            is flat. that is a constant independent of Theta, then that term would go away.</p>
        <p>至少从句法上看，这两种方法看起来是一样的。因此，从句法上或形式上看，最大似然估计与贝叶斯估计相同，在贝叶斯估计中，您假设先验是平坦的，因此 Theta 的所有可能值都同样可能。然而，从哲学上看，它们是完全不同的东西。在这里，我选择最有可能的 Theta 值。在这里，我选择观察到的数据更有可能出现的 Theta 值。</p><p>And syntactically, at least, the two approaches look the same. So syntactically, or formally, maximum
            likelihood
            estimation is the same as Bayesian estimation in which you assume a prior which is flat, so that all
            possible
            values of Theta are equally likely. Philosophically, however, they’re very different things. Here I’m
            picking
            the most likely value of Theta. Here I’m picking the value of Theta under which the observed data would have
            been more likely to occur.</p>
        <p>因此，最大似然估计是一种通用方法，因此它被广泛应用在许多不同类型的估计问题中。有一种特殊的估计问题，在这种问题中，您可能会忘记最大似然估计，而直接得出估计值。这就是您尝试估计 X 分布的平均值的情况，其中 X 是一个随机变量。</p><p>So maximum likelihood estimation is a general purpose method, so it’s applied all over the place in many,
            many
            different types of estimation problems. There is a special kind of estimation problem in which you may
            forget
            about maximum likelihood estimation, and come up with an estimate in a straightforward way. And this is the
            case
            where you’re trying to estimate the mean of the distribution of X, where X is a random variable.</p>
        <h2 id="unknown-511">未知</h2><h2>Unknown</h2>
        <p>您观察到几个独立同分布的随机变量 X1 至 Xn。它们都具有与此 X 相同的分布。因此它们具有共同的均值。我们不知道我们想要估计的均值。有什么比仅取我们观察到的值的平均值更自然的呢？</p><p>You observe several independent identically distributed random variables X1 up to Xn. All of them have the
            same
            distribution as this X. So they have a common mean. We do not know the mean we want to estimate it. What is
            more
            natural than just taking the average of the values that we have observed?</p>
        <p>因此，你生成大量的 X，取它们的平均值，并期望这将是该随机变量真实平均值的合理估计。事实上，根据弱大数定律，我们知道这个估计值在概率上收敛到随机变量的真实平均值。</p><p>So you generate lots of X’s, take the average of them, and you expect that this is going to be a reasonable
            estimate of the true mean of that random variable. And indeed we know from the weak law of large numbers
            that
            this estimate converges in probability to the true mean of the random variable.</p>
        <p>我们上次谈到的另一件事是，除了给出一个点估计，我们可能还想给出一个区间，告诉我们我们可能相信的 theta 位于哪里。1 alpha 置信区间是基于数据生成的区间。所以它是从这个值到那个值的区间。这些值用大写字母表示，因为它们是随机的，因为它们取决于我们看到的数据。</p><p>The other thing that we talked about last time is that besides giving a point estimate we may want to also
            give
            an interval that tells us something about where we might believe theta to lie. And 1 alpha confidence
            interval
            is in interval generated based on the data. So it’s an interval from this value to that value. These values
            are
            written with capital letters because they’re random, because they depend on the data that we have seen.</p>
        <h2 id="unknown-512">未知</h2><h2>Unknown</h2>
        <p>这给了我们一个区间，我们希望这个区间具有这样的属性：theta 有很高的概率位于该区间内。因此，通常我们会将 1 alpha 视为一个数量，例如 95%。在这种情况下，我们有一个 95% 的置信区间。正如我们上次讨论的那样，正确解释 95% 的含义非常重要。它不代表以下含义。</p><p>And this gives us an interval, and we would like this interval to have the property that theta is inside that
            interval with high probability. So typically we would take 1 alpha to be a quantity such as 95% for example.
            In
            which case we have a 95% confidence interval. As we discussed last time it’s important to have the right
            interpretation of what’s 95% means. What it does not mean is the following.</p>
        <p>未知值有 95% 的概率位于我们生成的区间内。这是因为未知值不是随机变量，而是常数。一旦我们生成区间，它要么在区间内，要么在区间外，但并不涉及概率。相反，概率应在随机区间本身上进行解释。</p><p>The unknown value has 95% percent probability of being in the interval that we have generated. That’s because
            the
            unknown value is not a random variable, it’s a constant. Once we generate the interval either it’s inside or
            it’s outside, but there’s no probabilities involved. Rather the probabilities are to be interpreted over the
            random interval itself.</p>
        <p>类似这样的陈述意味着，如果我有一个生成 95% 置信区间的程序，那么每当我使用这个程序时，我都会得到一个随机区间，并且它有 95% 的概率捕捉到 theta 的真实值。所以大多数时候，当我使用这个特定的程序来生成置信区间时，真实的 theta 恰好会以 95% 的概率位于该置信区间内。
        </p><p>What a statement like this says is that if I have a procedure for generating 95% confidence intervals, then
            whenever I use that procedure I’m going to get a random interval, and it’s going to have 95% probability of
            capturing the true value of theta. So most of the time when I use this particular procedure for generating
            confidence intervals the true theta will happen to lie inside that confidence interval with probability 95%.
        </p>
        <h2 id="unknown-513">未知</h2><h2>Unknown</h2>
        <p>因此，此陈述中的随机性与我的置信区间有关，而不是与 theta 有关，因为 theta 不是随机的。如何构建置信区间？有多种方法可以实现这一点，但在我们处理随机变量的均值估计的情况下，使用中心极限定理可以很容易地做到这一点。</p><p>So the randomness in this statement is with respect to my confidence interval, it’s not with respect to
            theta,
            because theta is not random. How does one construct confidence intervals? There’s various ways of going
            about
            it, but in the case where we’re dealing with the estimation of the mean of a random variable doing this is
            straightforward using the central limit theorem.</p>
        <p>基本上，我们取估计平均值，即样本平均值，然后取样本平均值左右两侧的对称区间。我们通过查看正态分布表来选择该区间的宽度。因此，如果这个数量，1 alpha 是 95%，我们将查看正态分布的 97.5 百分位数。
        </p><p>Basically we take our estimated mean, that’s the sample mean, and we take a symmetric interval to the left
            and to
            the right of the sample mean. And we choose the width of that interval by looking at the normal tables. So
            if
            this quantity, 1 alpha is 95% percent, we’re going to look at the 97.5 percentile of the normal
            distribution.
        </p>
        <p>从正态表中找到与该值相对应的常数，并根据此公式构建置信区间。这样，当您估计样本均值时，就可以非常机械地构建置信区间。因此，以这种方式构建置信区间涉及近似值。近似值是中心极限定理。我们假设样本均值是一个正态随机变量。</p><p>Find the constant number that corresponds to that value from the normal tables, and construct the confidence
            intervals according to this formula. So that gives you a pretty mechanical way of going about constructing
            confidence intervals when you’re estimating the sample mean. So constructing confidence intervals in this
            way
            involves an approximation. The approximation is the central limit theorem. We are pretending that the sample
            mean is a normal random variable.</p>
        <h2 id="unknown-514">未知</h2><h2>Unknown</h2>
        <p>当 n 很大时，这或多或少是正确的。这就是中心极限定理告诉我们的。有时我们可能需要做一些额外的近似工作，因为我们通常不知道 sigma 的真实值。所以我们需要做一些工作来从数据中估计 sigma。所以 sigma 当然是 X 的标准差。</p><p>Which is, more or less, right when n is large. That’s what the central limit theorem tells us. And sometimes
            we
            may need to do some extra approximation work, because quite often we do not know the true value of sigma. So
            we
            need to do some work either to estimate sigma from the data. So sigma is, of course, the standard deviation
            of
            the X’s.</p>
        <p>我们可能想根据数据来估计它，或者我们可能对 sigma 有一个上限，我们只使用这个上限。现在让我们转到一个新话题。现实世界中的许多统计数据都属于以下类型。假设 X 是高中学生的 SAT 成绩，Y 是同一学生的 MIT GPA。
        </p><p>We may want to estimate it from the data, or we may have an upper bound on sigma, and we just use that upper
            bound. So now let’s move on to a new topic. A lot of statistics in the real world are of the following
            flavor.
            So suppose that X is the SAT score of a student in high school, and Y is the MIT GPA of that same student.
        </p>
        <p>所以你认为这两者之间存在某种关系。所以你去收集不同学生的数据，并记录下典型学生的 SAT 成绩，这可能是他们的 MIT GPA。然后将所有这些数据绘制在 (X,Y) 图上。现在有理由相信这两者之间存在某种系统关系。因此，高中时 SAT 成绩较高的人可能在大学时 GPA 较高。</p><p>So you expect that there is a relation between these two. So you go and collect data for different students,
            and
            you record for a typical student this would be their SAT score, that could be their MIT GPA. And you plot
            all
            this data on an (X,Y) diagram. Now it’s reasonable to believe that there is some systematic relation between
            the
            two. So people who had higher SAT scores in high school may have higher GPA in college.</p>
        <h2 id="unknown-515">未知</h2><h2>Unknown</h2>
        <p>嗯，这可能是真的，也可能不是。你想构建一个这种类型的模型，看看这种类型的关系在多大程度上是真实的。所以你可能会假设现实世界是由这种类型的模型描述的。SAT 分数和大学 GPA 之间存在线性关系。所以它是线性关系，有一些我们不知道的参数，theta0 和 theta1。</p><p>Well that may or may not be true. You want to construct a model of this kind, and see to what extent a
            relation
            of this type is true. So you might hypothesize that the real world is described by a model of this kind.
            That
            there is a linear relation between the SAT score, and the college GPA. So it’s a linear relation with some
            parameters, theta0 and theta1 that we do not know.</p>
        <p>因此，我们假设数据具有线性关系，并且根据 theta0 和 theta1 的选择，这些数据中可能存在不同的直线。现在我们想找到这种最佳模型来解释数据。当然会有一些随机性。因此，一般来说，找到一条穿过所有数据点的直线是不可能的。</p><p>So we assume a linear relation for the data, and depending on the choices of theta0 and theta1 it could be a
            different line through those data. Now we would like to find the best model of this kind to explain the
            data. Of
            course there’s going to be some randomness. So in general it’s going to be impossible to find a line that
            goes
            through all of the data points.</p>
        <p>因此，让我们尝试找到最能解释这些数据的最佳直线。我们的操作方法如下。假设我们尝试一些特定的 theta0 和 theta1 值。这些值会给我们一条特定的直线。根据这条直线，我们可以做出预测。对于有这个 x 的学生，我们现有的模型会预测 y 是这个值。</p><p>So let’s try to find the best line that comes closest to explaining those data. And here’s how we go about
            it.
            Suppose we try some particular values of theta0 and theta1. These give us a certain line. Given that line,
            we
            can make predictions. For a student who had this x, the model that we have would predict that y would be
            this
            value.</p>
        <h2 id="unknown-516">未知</h2><h2>Unknown</h2>
        <p>实际的 y 是其他值，因此这个量就是我们的模型在预测该特定学生的 y 时产生的误差。我们希望选择一条预测尽可能好的线。那么我们所说的尽可能好是什么意思呢？我们将采用以下标准作为我们的标准。我们将查看我们的模型对每个特定学生的预测误差。</p><p>The actual y is something else, and so this quantity is the error that our model would make in predicting the
            y
            of that particular student. We would like to choose a line for which the predictions are as good as
            possible.
            And what do we mean by as good as possible? As our criteria we’re going to take the following. We are going
            to
            look at the prediction error that our model makes for each particular student.</p>
        <p>取其平方，然后将它们加到我们所有的数据点上。所以我们要看的是这个数量的平方、那个数量的平方、那个数量的平方等等的总和。我们将所有这些平方相加，我们希望找到一条线，使这些平方预测误差的总和尽可能小。这就是程序。我们有数据，X 和 Y。</p><p>Take the square of that, and then add them up over all of our data points. So what we’re looking at is the
            sum of
            this quantity squared, that quantity squared, that quantity squared, and so on. We add all of these squares,
            and
            we would like to find the line for which the sum of these squared prediction errors are as small as
            possible. So
            that’s the procedure. We have our data, the X’s and the Y’s.</p>
        <p>我们将通过最小化平方误差之和来找到这种类型的最佳模型，即最佳模型。所以这是一个可以随心所欲的方法，说“好吧，这就是我要建立模型的方法”。这听起来很合理。即使你对概率一无所知，这听起来也很合理。但它有概率依据吗？</p><p>And we’re going to find theta’s the best model of this type, the best possible model, by minimizing this sum
            of
            squared errors. So that’s a method that one could pull out of the hat and say OK, that’s how I’m going to
            build
            my model. And it sounds pretty reasonable. And it sounds pretty reasonable even if you don’t know anything
            about
            probability. But does it have some probabilistic justification?</p>
        <h2 id="unknown-517">未知</h2><h2>Unknown</h2>
        <p>事实证明，是的，在某些假设下，你可以用概率考虑来激励这种方法。所以让我们建立一个概率模型，它将引导我们找到这些估计参数的特定方法。所以这是一个概率模型。我选择一个有特定 SAT 分数的学生。这可以随机完成，也可以以系统的方式完成。</p><p>It turns out that yes, you can motivate this method with probabilistic considerations under certain
            assumptions.
            So let’s make a probabilistic model that’s going to lead us to these particular way of estimating the
            parameters. So here’s a probabilistic model. I pick a student who had a specific SAT score. And that could
            be
            done at random, but also could be done in a systematic way.</p>
        <p>也就是说，我挑选一个 SAT 成绩为 600 的学生，一个 610 的学生，一直到 1,400 或 1,600 的学生，无论正确的数字是多少。我挑选所有这些学生。我假设对于这种类型的学生，有一个真正的模型告诉我他们的 GPA 将是一个随机变量，这是由他们的 SAT 分数加上一些随机性、一些随机噪声预测的。</p><p>That is, I pick a student who had an SAT of 600, a student of 610 all the way to 1,400 or 1,600, whatever the
            right number is. I pick all those students. And I assume that for a student of this kind there’s a true
            model
            that tells me that their GPA is going to be a random variable, which is something predicted by their SAT
            score
            plus some randomness, some random noise.</p>
        <p>我用具有 0 均值和一定方差的独立正态随机变量来建模该随机噪声。因此，这是一个特定的概率模型，现在我可以考虑对这个特定模型进行最大似然估计。因此，要在这里进行最大似然估计，我需要写下我观察到的 y 的似然。我观察到的 y 的似然是多少？</p><p>And I model that random noise by independent normal random variables with 0 mean and a certain variance. So
            this
            is a specific probabilistic model, and now I can think about doing maximum likelihood estimation for this
            particular model. So to do maximum likelihood estimation here I need to write down the likelihood of the y’s
            that I have observed. What’s the likelihood of the y’s that I have observed?</p>
        <h2 id="unknown-518">未知</h2><h2>Unknown</h2>
        <p>嗯，一个特定的 w 具有 e 减去 w 平方除以 (2 sigma 平方) 的形式的似然性。这是特定 w 的似然性。观察到 y 的特定值的概率或似然性，与 w 取 y 减去这个减去那个的值的似然性相同。因此，y 的似然性具有这种形式。将其视为 w_i 平方。因此，这是密度</p><p>Well, a particular w has a likelihood of the form e to the minus w squared over (2 sigma squared). That’s the
            likelihood of a particular w. The probability, or the likelihood of observing a particular value of y,
            that’s
            the same as the likelihood that w takes a value of y minus this, minus that. So the likelihood of the y’s is
            of
            this form. Think of this as just being the w_i squared. So this is the density</p>
        <p>如果我们有多个数据，则将不同 y 的似然值相乘。因此，您必须写出类似这样的内容。由于 w 是独立的，这意味着 y 也是独立的。y 向量的似然值是各个 y 的似然值的乘积。每个单独的 y 的似然值都具有这种形式。其中 w 是 y_i 减去这两个量。</p><p>And if we have multiple data you multiply the likelihoods of the different y’s. So you have to write
            something
            like this. Since the w’s are independent that means that the y’s are also independent. The likelihood of a y
            vector is the product of the likelihoods of the individual y’s. The likelihood of every individual y is of
            this
            form. Where w is y_i minus these two quantities.</p>
        <p>因此，这是该特定模型下似然函数将采用的形式。在最大似然方法下，我们希望最大化这个量相对于 theta0 和 theta1 的值。现在，为了实现这个最大化，你不妨考虑对数并最大化对数，也就是上面的指数。因为我们有一个负号，所以最大化这个指数与最小化没有负号的指数是一样的。</p><p>So this is the form that the likelihood function is going to take under this particular model. And under the
            maximum likelihood methodology we want to maximize this quantity with respect to theta0 and theta1. Now to
            do
            this maximization you might as well consider the logarithm and maximize the logarithm, which is just the
            exponent up here. Maximizing this exponent because we have a minus sign is the same as minimizing the
            exponent
            without the minus sign.</p>
        <h2 id="unknown-519">未知</h2><h2>Unknown</h2>
        <p>Sigma 平方是一个常数。所以你最终要做的就是最小化这个量，这与我们在线性回归方法中所做的一样。所以总而言之，你可能会选择用这种特定的方式进行线性回归，只是因为它看起来合理或可信。</p><p>Sigma squared is a constant. So what you end up doing is minimizing this quantity here, which is the same as
            what
            we had in our linear regression methods. So in conclusion you might choose to do linear regression in this
            particular way, just because it looks reasonable or plausible.</p>
        <p>或者，你可能将你所做的解释为最大似然估计，即你假设一种模型，其中噪声项是具有相同分布的独立同分布的正态随机变量。因此，线性回归隐含地做出了这种假设。它进行最大似然估计，就好像世界真的由这种形式的模型描述，并且 W 是随机变量。</p><p>Or you might interpret what you’re doing as maximum likelihood estimation, in which you assume a model of
            this
            kind where the noise terms are normal random variables with the same distribution independent identically
            distributed. So linear regression implicitly makes an assumption of this kind. It’s doing maximum likelihood
            estimation as if the world was really described by a model of this form, and with the W’s being random
            variables.</p>
        <p>因此，这至少给了我们一些理由，即这种将线拟合到数据的特定方法并不是那么随意，而是有坚实的基础。好的，那么一旦你接受这个公式是合理的，下一步是什么？下一步是看看如何进行这种最小化。这不是一个很难实现的最小化。</p><p>So this gives us at least some justification that this particular approach to fitting lines to data is not so
            arbitrary, but it has a sound footing. OK so then once you accept this formulation as being a reasonable one
            what’s the next step? The next step is to see how to carry out this minimization. This is not a very
            difficult
            minimization to do.</p>
        <h2 id="unknown-520">未知</h2><h2>Unknown</h2>
        <p>方法是将此表达式的导数设置为 0。因为这是 theta0 和 theta1 的二次函数。当你对 theta0 和 theta1 求导时，你会得到 theta0 和 theta1 的线性函数。最后你就可以求解 theta0 和 theta1 的线性方程组。事实证明，对于数据而言，存在非常好且简单的参数最佳估计公式。
        </p><p>The way it’s done is by setting the derivatives of this expression to 0. Now because this is a quadratic
            function
            of theta0 and theta1. when you take the derivatives with respect to theta0 and theta1. you get linear
            functions
            of theta0 and theta1. And you end up solving a system of linear equations in theta0 and theta1. And it turns
            out
            that there’s very nice and simple formulas for the optimal estimates of the parameters in terms of the data.
        </p>
        <p>这些公式就是这些。我说这些公式既简洁又好用。让我们看看为什么。我们如何解释它们？假设世界由这种模型描述，其中 X 和 Y 是随机变量。其中 W 是与 X 无关的噪声项。因此，我们假设线性模型确实正确，但并不完全正确。</p><p>And the formulas are these ones. I said that these are nice and simple formulas. Let’s see why. How can we
            interpret them? So suppose that the world is described by a model of this kind, where the X’s and Y’s are
            random
            variables. And where W is a noise term that’s independent of X. So we’re assuming that a linear model is
            indeed
            true, but not exactly true.</p>
        <p>我们获得的任何特定数据点总是会有一些噪声。因此，如果这种模型是正确的，并且 W 的均值为 0，那么 Y 的预期值将是 theta0 加上 X 的 theta1 预期值。由于 W 的均值为 0，因此没有额外的项。因此，具体来说，theta0 将等于 Y 的预期值减去 X 的 theta1 预期值。</p><p>There’s always some noise associated with any particular data point that we obtain. So if a model of this
            kind is
            true, and the W’s have 0 mean then we have that the expected value of Y would be theta0 plus theta1 expected
            value of X. And because W has 0 mean there’s no extra term. So in particular, theta0 would be equal to
            expected
            value of Y minus theta1 expected value of X.</p>
        <h2 id="unknown-521">未知</h2><h2>Unknown</h2>
        <p>因此，让我们使用这个方程来尝试得出 theta0 的合理估计值。我不知道 Y 的预期值，但我可以估计它。我该如何估计它？我查看我获得的所有 y 的平均值。所以我用我看到的数据的平均值替换它，然后估计它。这里，X 也类似。</p><p>So let’s use this equation to try to come up with a reasonable estimate of theta0. I do not know the expected
            value of Y, but I can estimate it. How do I estimate it? I look at the average of all the y’s that I have
            obtained. so I replace this, I estimate it with the average of the data I have seen. Here, similarly with
            the
            X’s.</p>
        <p>我可能不知道 X 的期望值，但我有 x 的数据点。我查看所有数据点的平均值，得出这个期望值的估计值。现在我不知道 theta1 是什么，但我的程序将生成一个 theta1 的估计值，称为 theta1 hat。一旦我有了这个估计值，一个理性的人就会用这种特定的方式估计 theta0。</p><p>I might not know the expected value of X’s, but I have data points for the x’s. I look at the average of all
            my
            data points, I come up with an estimate of this expectation. Now I don’t know what theta1 is, but my
            procedure
            is going to generate an estimate of theta1 called theta1 hat. And once I have this estimate, then a
            reasonable
            person would estimate theta0 in this particular way.</p>
        <p>这就是我对 theta0 的估计。就是这个公式。我们还没有解决更难的问题，也就是如何首先估计 theta1。所以为了估计 theta0，我假设我已经对 theta1 有一个估计。好的，theta1 估计的正确公式恰好是这个。它看起来很乱，但让我们试着解释一下。</p><p>So that’s how my estimate of theta0 is going to be constructed. It’s this formula here. We have not yet
            addressed
            the harder question, which is how to estimate theta1 in the first place. So to estimate theta0 I assumed
            that I
            already had an estimate for a theta1. OK, the right formula for the estimate of theta1 happens to be this
            one.
            It looks messy, but let’s try to interpret it.</p>
        <h2 id="unknown-522">未知</h2><h2>Unknown</h2>
        <p>我要做的是，为了简单起见，我将采用这个模型，假设它们是具有 0 均值的随机变量。看看我们如何估计如何估计 theta1。让我们将这个等式的两边乘以 X。因此，我们得到 Y 乘以 X 等于 theta0 加上 theta0 乘以 X 加上 theta1 乘以 X 平方，再加上 X 乘以 W。现在取两边的期望值。</p><p>What I’m going to do is I’m going to take this model for simplicity let’s assume that they’re the random
            variables have 0 means. And see how we might estimate how we might try to estimate theta1. Let’s multiply
            both
            sides of this equation by X. So we get Y times X equals theta0 plus theta0 times X plus theta1 times X
            squared,
            plus X times W. And now take expectations of both sides.</p>
        <p>如果我有 0 均值的随机变量，则 Y 乘以 X 的预期值只是 X 与 Y 的协方差。我假设我的随机变量有 0 均值，所以它的期望值为 0。这将是 X 的方差，所以我有 X 的 theta1 乘以方差。</p><p>If I have 0 mean random variables the expected value of Y times X is just the covariance of X with Y. I have
            assumed that my random variables have 0 means, so the expectation of this is 0. This one is going to be the
            variance of X, so I have theta1 times variance of X.</p>
        <p>由于我假设我的随机变量有 0 均值，并且我还假设 W 独立于 X，所以最后一项也有 0 均值。因此，在这样的概率模型下，这个等式是正确的。如果我们知道方差和协方差，那么我们就会知道 theta1 的值。但我们只有数据，我们不一定知道方差和协方差，但我们可以估计它。</p><p>And since I’m assuming that my random variables have 0 mean, and I’m also assuming that W is independent of X
            this last term also has 0 mean. So under such a probabilistic model this equation is true. If we knew the
            variance and the covariance then we would know the value of theta1. But we only have data, we do not
            necessarily
            know the variance and the covariance, but we can estimate it.</p>
        <h2 id="unknown-523">未知</h2><h2>Unknown</h2>
        <p>方差的合理估计是多少？方差的合理估计是这个量除以 n，协方差的合理估计是分子除以 n。所以这是我对平均值的估计。我查看了与平均值的平方距离，然后对大量数据取平均值。这是估计我们分布的方差的最合理方法。</p><p>What’s a reasonable estimate of the variance? The reasonable estimate of the variance is this quantity here
            divided by n, and the reasonable estimate of the covariance is that numerator divided by n.&nbsp;So this is my
            estimate of the mean. I’m looking at the squared distances from the mean, and I average them over lots and
            lots
            of data. This is the most reasonable way of estimating the variance of our distribution.</p>
        <p>同样，这个量的期望值是 X 与 Y 的协方差，然后我们有很多很多数据点。这里的这个量将是协方差的一个非常好的估计值。所以基本上这个公式的作用是。一种思考方式。</p><p>And similarly the expected value of this quantity is the covariance of X with Y, and then we have lots and
            lots
            of data points. This quantity here is going to be a very good estimate of the covariance. So basically what
            this
            formula does is. one way of thinking about it.</p>
        <p>就是从这个完全正确的关系开始，但根据数据估计协方差和方差，然后使用这些估计值得出 theta1 的估计值。因此，这为我们提供了估计值构建方式的公式的概率解释。
        </p><p>Is that it starts from this relation which is true exactly, but estimates the covariance and the variance on
            the
            basis of the data, and then using these estimates to come up with an estimate of theta1. So this gives us a
            probabilistic interpretation of the formulas that we have for the way that the estimates are constructed.
        </p>
        <h2 id="unknown-524">未知</h2><h2>Unknown</h2>
        <p>如果你愿意假设这是真实世界模型，真实世界模型的结构，只是你不知道均值、协方差和方差。那么这是一种估计这些未知参数的自然方法。好吧，所以我们有一个闭式公式，只要我们有数据，我们就可以应用它。现在线性回归是一个有整门课程和整本书的主题。
        </p><p>If you’re willing to assume that this is the true model of the world, the structure of the true model of the
            world, except that you do not know means and covariances, and variances. Then this is a natural way of
            estimating those unknown parameters. All right, so we have a closed form formula, we can apply it whenever
            we
            have data. Now linear regression is a subject on which there are whole courses, and whole books that are
            given.
        </p>
        <p>原因是，你可以将很多内容带入这个主题，并且有很多方法可以阐述我们在两个参数和两个随机变量的情况下得到的简单解决方案。所以，让我给你介绍一下，当你开始更深入地研究线性回归时会出现哪些主题。</p><p>And the reason for that is that there’s a lot more that you can bring into the topic, and many ways that you
            can
            elaborate on the simple solution that we got for the case of two parameters and only two random variables.
            So
            let me give you a little bit of flavor of what are the topics that come up when you start looking into
            linear
            regression in more depth.</p>
        <p>因此，在迄今为止的讨论中，我们建立了线性模型，试图用一个变量的值来解释另一个变量的值。我们试图用 SAT 分数来解释 GPA，或者我们试图用 SAT 分数来预测 GPA。但也许你的 GPA 受到多种因素的影响。</p><p>So in our discussions so far we made the linear model in which we’re trying to explain the values of one
            variable
            in terms of the values of another variable. We’re trying to explain GPAs in terms of SAT scores, or we’re
            trying
            to predict GPAs in terms of SAT scores. But maybe your GPA is affected by several factors.</p>
        <h2 id="unknown-525">未知</h2><h2>Unknown</h2>
        <p>例如，你的 GPA 可能受到 SAT 成绩、家庭收入、祖母受教育年限以及许多其他类似因素的影响。因此，你可以写下一个模型，我认为 GPA 与该模型之间存在某种关系，它是我提到的所有其他变量的线性函数。</p><p>For example maybe your GPA is affected by your SAT score, also the income of your family, the years of
            education
            of your grandmother, and many other factors like that. So you might write down a model in which I believe
            that
            GPA has a relation, which is a linear function of all these other variables that I mentioned.</p>
        <p>也许你有一个关于大学表现的决定因素的理论，你想建立一个这样的模型。在这种情况下，我们该怎么做？好吧，我们再次收集数据点。我们看看第 i 个学生，他的大学 GPA。我们记录他们的 SAT 分数、家庭收入和祖母的受教育年限。所以这是针对某个特定学生的一个数据点。</p><p>So perhaps you have a theory of what determines performance at college, and you want to build a model of that
            type. How do we go about in this case? Well, again we collect the data points. We look at the i th student,
            who
            has a college GPA. We record their SAT score, their family income, and grandmother’s years of education. So
            this
            is one data point that is for one particular student.</p>
        <p>我们假设这种形式的模型。对于第 i 名学生，如果我们为这些参数选择了特定值，那么这将是我们的模型所犯的错误。然后我们去选择那些将再次给出最小可能的平方误差总和的参数。</p><p>We postulate the model of this form. For the i th student this would be the mistake that our model makes if
            we
            have chosen specific values for those parameters. And then we go and choose the parameters that are going to
            give us, again, the smallest possible sum of squared errors.</p>
        <h2 id="unknown-526">未知</h2><h2>Unknown</h2>
        <p>因此从哲学上讲，它与我们之前讨论的完全相同，只是现在我们在模型中包含了多个解释变量，而不是单个解释变量。这就是公式。接下来你要做什么？好吧，为了进行这种最小化，你需要在获得数据后求导，这样你就有了这三个参数的函数。</p><p>So philosophically it’s exactly the same as what we were discussing before, except that now we’re including
            multiple explanatory variables in our model instead of a single explanatory variable. So that’s the
            formulation.
            What do you do next? Well, to do this minimization you’re going to take derivatives once you have your data,
            you
            have a function of these three parameters.</p>
        <p>你对参数求导数，将导数设为 0，你就得到了线性方程组。你把这个线性方程组输入计算机，你就能得到最优参数的数值。当你处理多个变量时，没有我们在上一张幻灯片中提到的那种很好的闭式公式。除非你愿意使用矩阵符号。</p><p>You take the derivative with respect to the parameter, set the derivative equal to 0, you get the system of
            linear equations. You throw that system of linear equations to the computer, and you get numerical values
            for
            the optimal parameters. There are no nice closed form formulas of the type that we had in the previous slide
            when you’re dealing with multiple variables. Unless you’re willing to go into matrix notation.</p>
        <p>在这种情况下，您可以再次写下闭式公式，但它们的直观性会比我们之前的公式略差一些。但这个故事的寓意是，从数字上讲，这是一个非常简单的过程。这是一个问题，一个计算机可以为您解决的优化问题。它可以非常快速地为您解决。因为它所涉及的只是求解线性方程组。</p><p>In that case you can again write down closed form formulas, but they will be a little less intuitive than
            what we
            had before. But the moral of the story is that numerically this is a procedure that’s very easy. It’s a
            problem,
            an optimization problem that the computer can solve for you. And it can solve it for you very quickly.
            Because
            all that it involves is solving a system of linear equations.</p>
        <h2 id="unknown-527">未知</h2><h2>Unknown</h2>
        <p>现在，当你选择解释变量时，你可能会有一些选择。一个人可能认为你的 GPA 与你的 SAT 分数有关。另一个人可能认为你的 GPA 与你的 SAT 分数的平方有关。而那个人可能想尝试建立这种模型。现在你什么时候想这样做？</p><p>Now when you choose your explanatory variables you may have some choices. One person may think that your GPA
            a
            has something to do with your SAT score. Some other person may think that your GPA has something to do with
            the
            square of your SAT score. And that other person may want to try to build a model of this kind. Now when
            would
            you want to do this?</p>
        <p>假设您拥有的数据如下所示。如果数据如下所示，那么您可能会说线性模型看起来不正确，但也许二次模型可以更好地拟合数据。
        </p><p>Suppose that the data that you have looks like this. If the data looks like this then you might be tempted to
            say
            well a linear model does not look right, but maybe a quadratic model will give me a better fit for the data.
        </p>
        <p>因此，如果你想用二次模型拟合数据，那么你要做的就是用 X 平方而不是 X 作为解释变量，然后构建这种模型。这种模型与那种模型没有什么不同。它们仍然是线性模型，因为我们的 θ 以线性方式显示。</p><p>So if you want to fit a quadratic model to the data then what you do is you take X squared as your
            explanatory
            variable instead of X, and you build a model of this kind. There’s nothing really different in models of
            this
            kind compared to models of that kind. They are still linear models because we have theta’s showing up in a
            linear fashion.</p>
        <h2 id="unknown-528">未知</h2><h2>Unknown</h2>
        <p>无论你选择什么作为解释变量，它是 X，还是 X 的平方，还是你选择的其他函数。X 的某个一般函数 h 都没有区别。因此，将 X 的 h 视为新的 X。因此，你可以用完全相同的方式制定问题，只不过你不是使用 X，而是选择 X 的 h。</p><p>What you take as your explanatory variables, whether it’s X, whether it’s X squared, or whether it’s some
            other
            function that you chose. Some general function h of X, doesn’t make a difference. So think of you h of X as
            being your new X. So you can formulate the problem exactly the same way, except that instead of using X’s
            you
            choose h of X’s.</p>
        <p>所以基本上，问题是我想建立一个基于 X 值解释 Y 的模型，还是我想建立一个基于 X 的 h 值解释 Y 的模型。哪个值才是正确的？通过这张图片，我们可以看出，这会产生不同的结果。</p><p>So it’s basically a question do I want to build a model that explains Y’s based on the values of X, or do I
            want
            to build a model that explains Y’s on the basis of the values of h of X. Which is the right value to use?
            And
            with this picture here, we see that it can make a difference.</p>
        <p>X 中的线性模型可能不太合适，但二次模型可能会给我们更好的拟合效果。因此，这引出了这样一个话题：如果你正在处理一个现实世界的问题，那么如何选择 X 的函数 h。因此，在现实世界的问题中，你只需要给出 X 和 Y。你可以自由地构建任何类型的模型。</p><p>A linear model in X might be a poor fit, but a quadratic model might give us a better fit. So this brings to
            the
            topic of how to choose your functions h of X if you’re dealing with a real world problem. So in a real world
            problem you’re just given X’s and Y’s. And you have the freedom of building models of any kind you want.</p>
        <h2 id="unknown-529">未知</h2><h2>Unknown</h2>
        <p>你可以自由选择任意类型的 X 函数 h。所以这是一个相当困难和棘手的话题。因为你可能会想做得过头。例如，我得到了 10 个数据点，我可以说，好的，我要选择一个 X 的 h。</p><p>You have the freedom of choosing a function h of X of any type that you want. So this turns out to be a quite
            difficult and tricky topic. Because you may be tempted to overdo it. For example, I got my 10 data points,
            and I
            could say OK, I’m going to choose an h of X.</p>
        <p>我将选择 X 的 h 值，实际上是 X 的多个 h 值，进行多元线性回归，其中我将建立一个使用 10 次多项式的模型。如果我选​​择使用 10 次多项式拟合数据，我将完美地拟合数据，但我可能会得到一个执行类似操作的模型，并遍历我的所有数据点。</p><p>I’m going to choose h of X and actually multiple h’s of X to do a multiple linear regression in which I’m
            going
            to build a model that’s uses a 10th degree polynomial. If I choose to fit my data with a 10th degree
            polynomial
            I’m going to fit my data perfectly, but I may obtain a model is does something like this, and goes through
            all
            my data points.</p>
        <p>因此，如果我使用大量参数，并且适当选择 h 函数，我可以使预测误差非常小。但显然这会是垃圾。如果您获得这些数据点，并且您说这是我的模型来解释它们。其中有一个多项式上下波动，那么您可能做错了什么。因此，选择这些函数（h）的复杂程度。</p><p>So I can make my prediction errors extremely small if I use lots of parameters, and if I choose my h
            functions
            appropriately. But clearly this would be garbage. If you get those data points, and you say here’s my model
            that
            explains them. That has a polynomial going up and down, then you’re probably doing something wrong. So
            choosing
            how complicated those functions, the h’s, should be.</p>
        <h2 id="unknown-530">未知</h2><h2>Unknown</h2>
        <p>而使用多少个解释变量是一个非常微妙而深奥的话题，有深奥的理论告诉你应该做什么，不应该做什么。但主要应该避免的是，当你的数据太少时，模型中的参数太多。所以如果你只有 10 个数据点，你就不应该有 10 个自由参数。</p><p>And how many explanatory variables to use is a very delicate and deep topic on which there’s deep theory that
            tells you what you should do, and what you shouldn’t do. But the main thing that one should avoid doing is
            having too many parameters in your model when you have too few data. So if you only have 10 data points, you
            shouldn’t have 10 free parameters.</p>
        <p>使用 10 个自由参数，您将能够完美拟合数据，但您无法真正依赖所看到的结果。好的，现在在实践中，当人们运行线性回归时，他们不仅仅给出参数 theta 的点估计。</p><p>With 10 free parameters you will be able to fit your data perfectly, but you wouldn’t be able to really rely
            on
            the results that you are seeing. OK, now in practice, when people run linear regressions they do not just
            give
            point estimates for the parameters theta.</p>
        <p>但与我们估计随机变量平均值的情况类似，您可能希望给出置信区间，以便告诉您在估计每个特定参数时有多少随机性。有公式可用于为 theta 的估计值构建置信区间。我们不打算研究它们，因为这会花费太多时间。</p><p>But similar to what we did for the case of estimating the mean of a random variable you might want to give
            confidence intervals that sort of tell you how much randomness there is when you estimate each one of the
            particular parameters. There are formulas for building confidence intervals for the estimates of the
            theta’s.
            We’re not going to look at them, it would take too much time.</p>
        <h2 id="unknown-531">未知</h2><h2>Unknown</h2>
        <p>您可能还想估计模型中噪声的方差。如果您假设您的真实模型是我们之前讨论过的那种，即 Y 等于 theta1 乘以 X 加上 W，并且 W 的方差为 sigma 平方。您可能想估计一下，因为它会告诉您一些有关模型的信息，这称为标准误差。</p><p>Also you might want to estimate the variance in the noise that you have in your model. That is if you are
            pretending that your true model is of the kind we were discussing before, namely Y equals theta1 times X
            plus W,
            and W has a variance sigma squared. You might want to estimate this, because it tells you something about
            the
            model, and this is called standard error.</p>
        <p>它限制了模型的预测准确度。即使你有正确的 theta0 和 theta1，当有人告诉你 X 时，你也可以对 Y 做出预测，但这个预测并不准确。因为存在这种额外的随机性。如果这种额外的随机性很大，那么你的预测也会有很大的误差。通常会报告另一个数量。</p><p>It puts a limit on how good predictions your model can make. Even if you have the correct theta0 and theta1,
            and
            somebody tells you X you can make a prediction about Y, but that prediction will not be accurate. Because
            there’s this additional randomness. And if that additional randomness is big, then your predictions will
            also
            have a substantial error in them. There’s another quantity that gets reported usually.</p>
        <p>这是使用统计软件包（称为 R 平方）时获得的计算机输出的一部分。它是衡量您构建的线性回归模型的解释力的指标。使用线性回归。我不会准确定义 R 平方，而是给您一个与之相关的类似量。完成线性回归后，您可以查看以下量。</p><p>This is part of the computer output that you get when you use a statistical package which is called R square.
            And
            its a measure of the explanatory power of the model that you have built linear regression. Using linear
            regression. Instead of defining R square exactly, let me give you a sort of analogous quantity that’s
            involved.
            After you do your linear regression you can look at the following quantity.</p>
        <h2 id="unknown-532">未知</h2><h2>Unknown</h2>
        <p>你看看 Y 的方差，这是你可以通过数据估算出来的。这就是 Y 中有多少随机性。将它与 Y 中的随机性进行比较，但要以 X 为条件。所以这个量告诉我，如果我知道 X，我的 Y 中还会有多少随机性？所以如果我知道 X，我就会有更多的信息，所以 Y 受到更多的限制。Y 中的随机性更少。</p><p>You look at the variance of Y, which is something that you can estimate from data. This is how much
            randomness
            there is in Y. And compare it with the randomness that you have in Y, but conditioned on X. So this quantity
            tells me if I knew X how much randomness would there still be in my Y? So if I know X, I have more
            information,
            so Y is more constrained. There’s less randomness in Y.</p>
        <p>如果我对 X 一无所知，这就是 Y 中的随机性。所以这个量自然会小于 1，如果这个量很小，就意味着只要我知道 X，Y 就广为人知。这实际上告诉我，知道 x 可以让我很好地预测 Y。知道 X 意味着我正在解释 Y 中的大部分随机性。</p><p>This is the randomness in Y if I don’t know anything about X. So naturally this quantity would be less than
            1,
            and if this quantity is small it would mean that whenever I know X then Y is very well known. Which
            essentially
            tells me that knowing x allows me to make very good predictions about Y. Knowing X means that I’m explaining
            away most of the randomness in Y.</p>
        <p>因此，如果您阅读使用线性回归的统计研究，您可能会遇到以下形式的陈述：学生 GPA 的 60% 由家庭收入决定。如果您阅读此类陈述，它实际上是指此类数量。在 Y 的总方差中，我们建立模型后还剩下多少方差？
        </p><p>So if you read a statistical study that uses linear regression you might encounter statements of the form 60%
            of
            a student’s GPA is explained by the family income. If you read the statements of this kind it’s really
            refers to
            quantities of this kind. Out of the total variance in Y, how much variance is left after we build our model?
        </p>
        <h2 id="unknown-533">未知</h2><h2>Unknown</h2>
        <p>因此，如果我们建立模型后 Y 的方差只剩下 40%，则意味着 X 解释了 Y 中 60% 的变化。因此，这个想法是 Y 中的随机性是由多种来源引起的。我们的解释变量和随机噪声。我们要问的是，Y 中总随机性的百分之多少是由 X 参数的变化解释的？</p><p>So if only 40% of the variance of Y is left after we build our model, that means that X explains 60% of the
            variations in Y’s. So the idea is that randomness in Y is caused by multiple sources. Our explanatory
            variable
            and random noise. And we ask the question what percentage of the total randomness in Y is explained by
            variations in the X parameter?</p>
        <p>Y 中的总随机性中有多少仅归因于随机效应？因此，如果你有一个模型可以解释 Y 中的大部分变化，那么你可以认为你有一个好的模型，它可以告诉你一些关于现实世界的有用信息。现在，当你使用线性回归时，有很多事情可能会出错，而且有很多陷阱。当你遇到这种称为异质性的情况时，就会出现一个陷阱。</p><p>And how much of the total randomness in Y is attributed just to random effects? So if you have a model that
            explains most of the variation in Y then you can think that you have a good model that tells you something
            useful about the real world. Now there’s lots of things that can go wrong when you use linear regression,
            and
            there’s many pitfalls. One pitfall happens when you have this situation that’s called heteroskedacisity.</p>
        <p>假设你的数据属于这种类型。那么这里发生了什么？你似乎有一个线性模型，但当 X 很小时，你有一个非常好的模型。这意味着当 X 在这里时，W 的方差很小。另一方面，当 X 在那里时，你有很多随机性。</p><p>So suppose your data are of this kind. So what’s happening here? You seem to have a linear model, but when X
            is
            small you have a very good model. So this means that W has a small variance when X is here. On the other
            hand,
            when X is there you have a lot of randomness.</p>
        <h2 id="unknown-534">未知</h2><h2>Unknown</h2>
        <p>在这种情况下，W 不是均匀分布的，但 W 的方差（噪声的方差）与 X 有关。因此，对于 x 空间的不同区域，噪声量也不同。在这种情况下会出现什么问题？由于我们试图最小化平方误差之和，因此我们真正关注的是最大的误差。</p><p>This would be a situation in which the W’s are not identically distributed, but the variance of the W’s, of
            the
            noise, has something to do with the X’s. So with different regions of our x space we have different amounts
            of
            noise. What will go wrong in this situation? Since we’re trying to minimize sum of squared errors, we’re
            really
            paying attention to the biggest errors.</p>
        <p>这意味着我们要关注这些数据点，因为那里会出现很大的误差。因此，线性回归公式最终会根据这些数据（最嘈杂的数据）建立一个模型。而不是那些按顺序排列的数据。显然，这不是正确的做法。</p><p>Which will mean that we are going to pay attention to these data points, because that’s where the big errors
            are
            going to be. So the linear regression formulas will end up building a model based on these data, which are
            the
            most noisy ones. Instead of those data that are nicely stacked in order. Clearly that’s not to the right
            thing
            to do.</p>
        <p>因此你需要改变一些东西，并利用 W 的方差随 X 变化的事实，并且有办法处理它。这是需要小心的事情。另一种陷入麻烦的可能性是，如果你使用多个彼此非常密切相关的解释变量。</p><p>So you need to change something, and use the fact that the variance of W changes with the X’s, and there are
            ways
            of dealing with it. It’s something that one needs to be careful about. Another possibility of getting into
            trouble is if you’re using multiple explanatory variables that are very closely related to each other.</p>
        <h2 id="unknown-535">未知</h2><h2>Unknown</h2>
        <p>例如，假设我试图通过查看你第一次参加 SAT 考试的成绩和第二次参加 SAT 考试的成绩来预测你的 GPA。我假设几乎每个人都参加过不止一次 SAT。假设你有一个这样的模型。那么，你第一次参加 SAT 考试的成绩和第二次参加 SAT 考试的成绩很可能非常接近。</p><p>So for example, suppose that I tried to predict your GPA by looking at your SAT the first time that you took
            it
            plus your SAT the second time that you took your SATs. I’m assuming that almost everyone takes the SAT more
            than
            once. So suppose that you had a model of this kind. Well, SAT on your first try and SAT on your second try
            are
            very likely to be fairly close.</p>
        <p>你可以考虑忽略这个因素，得出估计值。然后你基于此建立一个模型，或者忽略这个因素的替代模型，并根据第二个 SAT 进行预测。这两个模型可能基本上与另一个模型一样好，因为这两个量本质上是相同的。</p><p>And you could think of coming up with estimates in which this is ignored. And you build a model based on
            this, or
            an alternative model in which this term is ignored, and you make predictions based on the second SAT. And
            both
            models are likely to be essentially as good as the other one, because these two quantities are essentially
            the
            same.</p>
        <p>因此，在这种情况下，您估计的 theta 将对数据的细节非常敏感。您更改数据，您有数据，数据会告诉您这个系数很大，那个系数很小。您只要稍微更改数据，您的 theta 就会发生巨大变化。</p><p>So in that case, your theta’s that you estimate are going to be very sensitive to little details of the data.
            You
            change your data, you have your data, and your data tell you that this coefficient is big and that
            coefficient
            is small. You change your data just a tiny bit, and your theta’s would drastically change.</p>
        <h2 id="unknown-536">未知</h2><h2>Unknown</h2>
        <p>因此，在这种情况下，您有多个解释变量，但它们是冗余的，因为它们彼此非常紧密相关，并且可能具有线性关系。因此，必须小心处理这种情况，并进行特殊测试以确保不会发生这种情况。最后，最大和最常见的错误是，您运行线性回归，得到线性模型，然后您说哦，好吧。</p><p>So this is a case in which you have multiple explanatory variables, but they’re redundant in the sense that
            they’re very closely related to each other, and perhaps with a linear relation. So one must be careful about
            the
            situation, and do special tests to make sure that this doesn’t happen. Finally the biggest and most common
            blunder is that you run your linear regression, you get your linear model, and then you say oh, OK.</p>
        <p>根据这个特定公式，Y 是由 X 引起的。好吧，我们所做的只是确定 X 和 Y 之间的线性关系。这并没有告诉我们任何事情。是 Y 导致了 X，还是 X 导致了 Y，或者 X 和 Y 都是由我们没有想到的其他变量引起的。</p><p>Y is caused by X according to this particular formula. Well, all that we did was to identify a linear
            relation
            between X and Y. This doesn’t tell us anything. Whether it’s Y that causes X, or whether it’s X that causes
            Y,
            or maybe both X and Y are caused by some other variable that we didn’t think about.</p>
        <p>因此，建立一个误差较小的良好线性模型并不能告诉我们两个变量之间的因果关系。它只能告诉我们两个变量之间存在密切的关联。如果你知道其中一个变量，你就可以预测另一个变量。但它并没有告诉你任何有关底层物理的信息，即存在一些引入这些变量之间关系的物理机制。好的，这就是关于线性回归的内容。</p><p>So building a good linear model that has small errors does not tell us anything about causal relations
            between
            the two variables. It only tells us that there’s a close association between the two variables. If you know
            one
            you can make predictions about the other. But it doesn’t tell you anything about the underlying physics,
            that
            there’s some physical mechanism that introduces the relation between those variables. OK, that’s it about
            linear
            regression.</p>
        <h2 id="unknown-537">未知</h2><h2>Unknown</h2>
        <p>让我们开始下一个主题，即假设检验。下次我们将继续讨论。因此，在这里，我们不是试图估计连续参数，而是对 X 随机变量的分布有两个备选假设。</p><p>Let us start the next topic, which is hypothesis testing. And we’re going to continue with it next time. So
            here,
            instead of trying to estimate continuous parameters, we have two alternative hypotheses about the
            distribution
            of the X random variable.</p>
        <p>例如，我们的随机变量既可以在 H0 下服从这个分布，也可以在 H1 下服从这个分布。我们想决定哪个分布是正确的？因此，我们给出了这两个分布，以及一些常用术语，其中一个是零假设，即默认假设，我们还有一些备选假设。</p><p>So for example our random variable could be either distributed according to this distribution, under H0, or
            it
            might be distributed according to this distribution under H1. And we want to make a decision which
            distribution
            is the correct one? So we’re given those two distributions, and some common terminologies that one of them
            is
            the null hypothesis. sort of the default hypothesis, and we have some alternative hypotheses.</p>
        <p>我们想检查这个是真的，还是那个是真的。所以你得到一个数据点，你想做出决定。在这幅图中，一个理性的人会怎么做来做出决定？他们可能会选择一个特定的阈值 Xi，并决定如果你的数据落在这个区间，H1 为真。如果你的数据落在一边，H0 为真。</p><p>And we want to check whether this one is true, or that one is true. So you obtain a data point, and you want
            to
            make a decision. In this picture what would a reasonable person do to make a decision? They would probably
            choose a certain threshold, Xi, and decide that H1 is true if your data falls in this interval. And decide
            that
            H0 is true if you fall on the side.</p>
        <h2 id="unknown-538">未知</h2><h2>Unknown</h2>
        <p>所以这将是解决问题的合理方法。更一般地，你取所有可能的X的集合，并将可能的X的集合分为两个区域。一个是拒绝区域，你在这个区域决定H1，或者拒绝H0。该区域的补集是你决定H0的地方。所以这是你的数据的x空间。在这个例子中，x是一维的。</p><p>So that would be a reasonable way of approaching the problem. More generally you take the set of all possible
            X’s, and you divide the set of possible X’s into two regions. One is the rejection region, in which you
            decide
            H1, or you reject H0. And the complement of that region is where you decide H0. So this is the x space of
            your
            data. In this example here, x was one dimensional.</p>
        <p>但一般来说，X 是一个向量，其中所有可能的数据向量都被分为两种类型。如果它属于这一组，你会做出一种决定。如果它属于那一组，你会做出另一种决定。好的，那么你如何描述特定决策方式的表现？假设我选择了阈值。我可能会犯两种可能的错误。</p><p>But in general X is going to be a vector, where all the possible data vectors that you can get, they’re
            divided
            into two types. If it falls in this set you’d make one decision. If it falls in that set, you make the other
            decision. OK, so how would you characterize the performance of the particular way of making a decision?
            Suppose
            I chose my threshold. I may make mistakes of two possible types.</p>
        <p>也许 H0 是正确的，但我的数据恰好落在了这里。在这种情况下，我犯了一个错误，这将是对 H0 的错误拒绝。如果我的数据落在了这里，我就会拒绝 H0。我决定 H1。而 H0 是正确的。发生这种情况的概率是多少？我们称之为 alpha。但还有另一种错误。假设 H1 是正确的，但我的数据恰好落在了那一边。</p><p>Perhaps H0 is true, but my data happens to fall here. In which case I make a mistake, and this would be a
            false
            rejection of H0. If my data falls here I reject H0. I decide H1. Whereas H0 was true. The probability of
            this
            happening? Let’s call it alpha. But there’s another kind of error that can be made. Suppose that H1 was
            true,
            but by accident my data happens to falls on that side.</p>
        <h2 id="unknown-539">未知</h2><h2>Unknown</h2>
        <p>然后我又会犯错误。即使 H1 是正确的，我也会决定 H0。这种情况发生的可能性有多大？这将是这里曲线下的面积。这是可能犯的另一种错误，beta 是这种特定类型错误的概率。这两种都是错误。alpha 是某种错误的概率。</p><p>Then I’m going to make an error again. I’m going to decide H0 even though H1 was true. How likely is this to
            occur? This would be the area under this curve here. And that’s the other type of error than can be made,
            and
            beta is the probability of this particular type of error. Both of these are errors. Alpha is the probability
            of
            error of one kind.</p>
        <p>Beta 是另一种错误发生的概率。您希望错误发生的概率很小。因此，您希望使 alpha 和 beta 都尽可能小。不幸的是，这是不可能的，这是有代价的。如果我以这种方式达到我的阈值，那么 alpha 会变小，但 beta 会变大。所以这是有代价的。</p><p>Beta is the probability of an error of the other kind. You would like the probabilities of error to be small.
            So
            you would like to make both alpha and beta as small as possible. Unfortunately that’s not possible, there’s
            a
            trade off. If I go to my threshold it this way, then alpha become smaller, but beta becomes bigger. So
            there’s a
            trade off.</p>
        <p>如果我缩小拒绝域，一种错误发生的可能性就会减小，而另一种错误发生的可能性就会增大。所以我们得到了这种权衡。那么我们该怎么做呢？我们如何系统地行动？我们如何得出拒绝域？好吧，理论基本上告诉你应该如何创建这些区域。但它并没有告诉你具体如何做。</p><p>If I make my rejection region smaller one kind of error is less likely, but the other kind of error becomes
            more
            likely. So we got this trade off. So what do we do about it? How do we move systematically? How do we come
            up
            with rejection regions? Well, what the theory basically tells you is it tells you how you should create
            those
            regions. But it doesn’t tell you exactly how.</p>
        <h2 id="unknown-540">未知</h2><h2>Unknown</h2>
        <p>它告诉你这些区域的大致形状。例如，这里的理论告诉我们，正确的做法是设置阈值，然后做出决定，一个方向向右，一个方向向左。但它不一定告诉我们把阈值放在哪里。不过，知道做出正确决定的方法取决于特定的阈值，这已经足够有用了。</p><p>It tells you the general shape of those regions. For example here, the theory who tells us that the right
            thing
            to do would be to put the threshold and make decisions one way to the right, one way to the left. But it
            might
            not necessarily tell us where to put the threshold. Still, it’s useful enough to know that the way to make a
            good decision would be in terms of a particular threshold.</p>
        <p>让我更具体一点。我们可以从贝叶斯案例中假设检验问题的解决方案中获得灵感。在贝叶斯案例中，我们只需选择在给定数据的情况下更有可能的假设。使用贝叶斯规则产生的后验概率，它们是这样写的。这个术语与那个术语相同。它们抵消了，然后让我在这里和那里收集术语。我在这里得到了一个表达式。
        </p><p>Let me make this more specific. We can take our inspiration from the solution of the hypothesis testing
            problem
            that we had in the Bayesian case. In the Bayesian case we just pick the hypothesis which is more likely
            given
            the data. The produced posterior probabilities using Bayesian rule, they’re written this way. And this term
            is
            the same as that term. They cancel out, then let me collect terms here and there. I get an expression here.
        </p>
        <p>我认为你讲义上的版本是正确的。幻灯片上的版本不正确，所以我在这里修正它。好的，这就是你在贝叶斯情况下做出决策的方式。在贝叶斯情况下，你要做的是计算这个比率。我们称之为似然比。并将该比率与阈值进行比较。</p><p>I think the version you have in your handout is the correct one. The one on the slide was not the correct
            one, so
            I’m fixing it here. OK, so this is the form of how you make decisions in the Bayesian case. What you do in
            the
            Bayesian case, you calculate this ratio. Let’s call it the likelihood ratio. And compare that ratio to a
            threshold.</p>
        <h2 id="unknown-541">未知</h2><h2>Unknown</h2>
        <p>在贝叶斯情况下，您应该使用的阈值与两个假设的先验概率有关。在非贝叶斯情况下，我们没有先验概率，所以我们不知道如何设置这个阈值。但我们要做的是，无论如何都要保留这个特定的结构，也许会使用一些其他的考虑因素来选择阈值。</p><p>And the threshold that you should be using in the Bayesian case has something to do with the prior
            probabilities
            of the two hypotheses. In the non Bayesian case we do not have prior probabilities, so we do not know how to
            set
            this threshold. But we’re going to do is we’re going to keep this particular structure anyway, and maybe use
            some other considerations to pick the threshold.</p>
        <p>因此，我们将使用似然比检验，也就是我们所说的似然比检验，我们计算一种称为似然的量，并将其与阈值进行比较。那么，这种似然的解释是什么？我们要问，我观察到的 X，如果 H1 为真，它们发生的可能性有多大？如果 H0 为真，它们发生的可能性有多大？</p><p>So we’re going to use a likelihood ratio test, that’s how it’s called in which we calculate a quantity of
            this
            kind that we call the likelihood, and compare it with a threshold. So what’s the interpretation of this
            likelihood? We ask. the X’s that I have observed, how likely were they to occur if H1 was true? And how
            likely
            were they to occur if H0 was true?</p>
        <p>如果我的数据可信，那么这个比率可能会很大，它们可能在 H1 下发生。但它们非常不可信，在 H0 下极不可能发生。那么我的想法是，我看到的数据在 H0 下极不可能发生。所以 H0 可能不正确。</p><p>This ratio could be big if my data are plausible they might occur under H1. But they’re very implausible,
            extremely unlikely to occur under H0. Then my thinking would be well the data that I saw are extremely
            unlikely
            to have occurred under H0. So H0 is probably not true.</p>
        <h2 id="unknown-542">未知</h2><h2>Unknown</h2>
        <p>我将选择 H1。因此，当这个比率很大时，它告诉我们，如果我们假设 H1 为真而不是 H0 为真，我们看到的数据会得到更好的解释。因此，我计算这个数量，将其与阈值进行比较，这就是我做出决定的方式。</p><p>I’m going to go for H1 and choose H1. So when this ratio is big it tells us that the data that we’re seeing
            are
            better explained if we assume H1 to be true rather than H0 to be true. So I calculate this quantity, compare
            it
            with a threshold, and that’s how I make my decision.</p>
        <p>例如，在这张特定图片中，其变化方式是，该图片中的似然比与我的 X 单调变化。因此，将似然比与阈值进行比较与将我的 x 与阈值进行比较相同，而我们面临的问题是如何选择阈值。选择阈值的方式通常是通过固定两个错误概率之一来完成的。</p><p>So in this particular picture, for example the way it would go would be the likelihood ratio in this picture
            goes
            monotonically with my X. So comparing the likelihood ratio to the threshold would be the same as comparing
            my x
            to the threshold, and we’ve got the question of how to choose the threshold. The way that the threshold is
            chosen is usually done by fixing one of the two probabilities of error.</p>
        <p>也就是说，我说，我希望某一特定类型的错误是给定的数字，所以我固定了这个 alpha。然后我试着找到我的阈值应该在哪里。这样这个概率 theta，也就是概率，就等于 alpha。然后另一个错误概率 beta 将是它最终的结果。所以有人提前选择了 alpha。</p><p>That is, I say, that I want my error of one particular type to be a given number, so I fix this alpha. And
            then I
            try to find where my threshold should be. So that this probability theta, probability out there, is just
            equal
            to alpha. And then the other probability of error, beta, will be whatever it turns out to be. So somebody
            picks
            alpha ahead of time.</p>
        <h2 id="unknown-543">未知</h2><h2>Unknown</h2>
        <p>根据基于 alpha 的错误拒绝概率，我找到了我的阈值。我选择我的阈值，这随后决定了 beta 的值。所以我们下次继续讲这个故事，我们就到此为止。</p><p>Based on the probability of a false rejection based on alpha, I find where my threshold is going to be. I
            choose
            my threshold, and that determines subsequently the value of beta. So we’re going to continue with this story
            next time, and we’ll stop here.</p>
        <h1 id="classical-inference-iii">25. 古典推理 III</h1><h1>25. Classical Inference III</h1>
        <p><img src="data:image/jpeg;base64,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">12 年前 (2012 年 11 月 10 日) — 52:07 <a href="https://youtube.com/watch?v=rYefUsYuEp0">https://youtube.com/watch?v=rYefUsYuEp0</a></p><p> 12 years ago (Nov 10, 2012) — 52:07 <a href="https://youtube.com/watch?v=rYefUsYuEp0">https://youtube.com/watch?v=rYefUsYuEp0</a></p>
        <h2 id="unknown-544">未知</h2><h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT OpenCourseWare，网址为 ocw.mit.edu 教授：好的，如果您还没有这样做，请花点时间浏览课程评估网站并输入您对课程的评论。
        </p><p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials
            from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu PROFESSOR: OK, if you have not yet
            done
            it, please take a moment to go through the course evaluation website and enter your comments for the class.
        </p>
        <p>今天我们要做的是总结一下假设检验的世界。看几个假设检验的例子，从简单的开始，比如我们上次讨论的那种情况，你只有两个假设，你要在它们之间做出选择。但也看看更复杂的情况，你只有一个基本假设。</p><p>So what we’re going to do today to wrap things up is we’re going to go through a tour of the world of
            hypothesis
            testing. See a few examples of hypothesis tests, starting from simple ones such as the one the setting that
            we
            discussed last time in which you just have two hypotheses, you’re trying to choose between them. But also
            look
            at more complicated situations in which you have one basic hypothesis.</p>
        <p>假设你有一枚公平的硬币，你想用硬币不公平的假设来测试它，但替代假设实际上有很多不同的假设。那么我的硬币公平吗？我的骰子公平吗？我的随机变量分布是否正确，等等。最后，我将对整个事情发表一些一般性评论。</p><p>Let’s say that you have a fair coin and you want to test it against the hypotheses that your coin is not
            fair,
            but that alternative hypothesis is really lots of different hypothesis. So is my coin fair? Is my die fair?
            Do I
            have the correct distribution for random variable, and so on. And I’m going to end up with a few general
            comments about this whole business.</p>
        <h2 id="unknown-545">未知</h2><h2>Unknown</h2>
        <p>因此，简单假设检验问题中的悲哀之处在于：我们有两个可能的模型，这是经典世界，因此我们对这两个假设没有任何先验概率。通常，我们希望认为这些假设不是完全对称的，而是一个默认假设，通常称为零假设。</p><p>So the sad thing in simple hypothesis testing problems is the following. we have two possible models, and
            this is
            the classical world so we do not have any prior probabilities on the two hypotheses. Usually we want to
            think of
            these hypotheses as not being completely symmetrical, but rather one is the default hypothesis, and usually
            it’s
            referred to as the null hypothesis.</p>
        <p>你想检查零假设是否正确，事情是否如你预期的那样正常，或者它是否被证明是错误的，在这种情况下，备选假设是正确的。那么如何去做呢？无论你使用什么方法，最终你都会做以下事情。你有所有可能获得的简单观察的空间。</p><p>And you want to check whether the null hypothesis is true, whether things are normal as you would have
            expected
            them to be, or whether it turns out to be false, in which case an alternative hypothesis would be correct.
            So
            how does one go about it? No matter what approach you use, in the end you’re going to end up doing the
            following. You have the space of all simple observations that you may obtain.</p>
        <p>所以当你做实验时，你会得到一个 X 向量，一个位于某处的数据向量。对于某些向量，你将决定接受 H。</p><p>So when you do the experiment you’re going to get an X vector, a vector of data that’s somewhere. And for
            some
            vectors you’re going to decide that you accept H.</p>
        <h2 id="unknown-546">未知</h2><h2>Unknown</h2>
        <p>注意，对于某些向量，你会拒绝 H0 而接受 H1。因此，你最终会将所有 X 的空间划分为两部分，一部分是拒绝区域，一部分是接受区域。</p><p>Note for some vectors that you reject H0 and you accept H1. So what you will end up doing is that you’re
            going to
            have some division of the space of all X’s into two parts, and one part is the rejection region, and one
            part is
            the acceptance region.</p>
        <p>所以如果你落在这里，你就会接受 H0，如果你落在这里，你就会拒绝 H0。所以要设计一个假设检验，基本上你需要将 X 空间分成两部分。所以弄清楚如何做到这一点涉及两个要素。一个要素是决定我想要什么样的分割曲线？</p><p>So if you fall in here you accept H0, if you fall here you’d reject H0. So to design a hypothesis test
            basically
            you need to come up with a division of your X space into two pieces. So the figuring out how to do this
            involves
            two elements. One element is to decide what kind of shape so I want for my dividing curve?</p>
        <p>选择了分割曲线的形状后，我到底要把它放在哪里？所以如果你要用直线切割这个空间，你可能会把它放在这里，或者你可以把它放在那里，或者你可以把它放在那里。你到底要把它放在哪里？让我们看看这两个步骤。</p><p>And having chosen the shape of the dividing curve, where exactly do I put it? So if you were to cut this
            space
            using, let’s say, a straight cut you might put it here, or you might put it there, or you might put it
            there.
            Where exactly are you going to put it? So let’s look at those two steps.</p>
        <h2 id="unknown-547">未知</h2><h2>Unknown</h2>
        <p>第一个问题是确定拒绝域的总体形状，也就是测试的结构。对于两个假设的情况，确定拒绝域的方法是写下两个假设之间的似然比。因此，我们将其称为 X 的量 l。这是您可以根据现有数据计算得出的。</p><p>The first issue is to decide the general shape of your rejection region, which is the structure of your test.
            And
            the way this is done for the case of two hypothesis is by writing down the likelihood ratio between the two
            hypothesis. So let’s call that quantity l of X. It’s something that you can compute given the data that you
            have.</p>
        <p>X 的 l 值较高基本上意味着这里的概率往往大于这个概率。这意味着您看到的数据很可能在 H1 下发生，但在 H0 下发生的可能性较小。因此，如果您看到数据在 H1 下更合理，可以更好地解释，那么这个比率很大，您将选择支持 H1 或拒绝 H0。如果您有离散数据，这就是您要做的。您可以使用 PMF。</p><p>A high value of l of X basically means that this probability here tends to be bigger than this probability.
            It
            means that the data that you have seen are quite likely to have occurred under H1, but less likely to have
            occurred under H0. So if you see data that they are more plausible, can be better explained, under H1, then
            this
            ratio is big, and you’re going to choose in favor of H1 or reject H0. That’s what you do if you have
            discrete
            data. You use the PMFs.</p>
        <p>如果您有密度，在连续数据的情况下，您再次考虑两个密度的比率。因此，X 的大 l 证明您的数据更符合 H1 而不是 H0。一旦您接受了这种结构，那么您的决定实际上是根据该单个数字做出的。也就是说，您的数据是某种向量，您将数据压缩为单个数字。</p><p>If you have densities, in the case of continues data, again you consider the ratio of the two densities. So a
            big
            l of X is evidence that your data are more compatible with H1 rather than H0. Once you accept this kind of
            structure then your decision is really made in terms of that single number. That is, you had your data that
            was
            some kind of vector, and you condense your data into a single number.</p>
        <h2 id="unknown-548">未知</h2><h2>Unknown</h2>
        <p>这是一种统计量。在本例中是似然比，你把分界点放在这里的某个地方，称之为 Xi。在这个区域你接受 H1，在这个区域你接受 H0。因此，通过致力于使用似然比进行测试，我们已经从在 x 空间中寻找分界线的复杂图景，转变为在实线上寻找分界点的更简单问题。</p><p>A statistic as it’s called. in this case the likelihood ratio, and you put the dividing point somewhere here
            call
            it Xi. And in this region you accept H1, in this region you accept H0. So by committing ourselves to using
            the
            likelihood ratio in order to carry out the test we have gone from this complicated picture of finding a
            dividing
            line in x space, to a simpler problem of just finding a dividing point on the real line.</p>
        <p>好的，我们接下来怎么做？剩下要做的就是选择这个阈值 Xi。或者说，临界值，以便做出决定。你可以把它放在任何地方，但决定把它放在哪里的一种方法如下。看看这个随机变量 x 的 l 的分布。</p><p>OK, how are we going? So what’s left to do is to choose this threshold, Xi. Or as it’s called, the critical
            value, for making our decision. And you can place it anywhere, but one way of deciding where to place it is
            the
            following. look at the distribution of this random variable, l of X.</p>
        <p>在 H0 下，它具有一定的分布，而在 H1 下，它具有其他分布。如果我将阈值放在此处，将会发生以下情况。当 H0 为真时，我最终做出错误决定的可能性很大。如果 H0 为真，我的似然比仍有可能大于 Xi，这就是做出这种特定类型错误决定的概率。</p><p>It’s has a certain distribution under H0, and it has some other distribution under H1. If I put my threshold
            here, here’s what’s going to happen. When H0 is true, there is this much probability that I’m going to end
            up
            making an incorrect decision. If H0 is true there’s still a probability that my likelihood ratio will be
            bigger
            than Xi, and that’s the probability of making an incorrect decision of this particular type.</p>
        <h2 id="unknown-549">未知</h2><h2>Unknown</h2>
        <p>这就是对 H0 做出错误拒绝。通常，人们会将此概率设置为某个数字，alpha。例如，alpha 为 5%。一旦您决定将其设置为 5%，就决定了 Psi(Xi) 这个数字将位于何处。</p><p>That is of making a false rejection of H0. Usually one sets this probability to a certain number, alpha. For
            example alpha being 5%. And once you decide that you want this to be 5%, that determines where this number
            Psi(Xi) is going to be.</p>
        <p>所以这里的想法是，如果我看到的数据与 H0 完全不相容，我将拒绝 H0。如果它们在 H0 下不太可能发生。我取这个水平，5%。所以我看到我的数据，然后我说，如果 H0 是真的，我看到这种数据的概率将小于 5%。</p><p>So the idea here is that I’m going to reject H0 if the data that I have seen are quite incompatible with H0.
            if
            they’re quite unlikely to have occurred under H0. And I take this level, 5%. So I see my data and then I say
            well if H0 was true, the probability that I would have seen data of this kind would be less than 5%.</p>
        <p>鉴于我看到了这些数据，这表明 H0 不成立，我最终拒绝了 H0。当然，还有另一种类型的错误概率。如果我将阈值放在这里，如果 H1 成立但我的似然比落在这里，我就会犯相反类型的错误。</p><p>Given that I saw those data, that suggests that H0 is not true, and I end up rejecting H0. Now of course
            there’s
            the other type of error probability. If I put my threshold here, if H1 is true but my likelihood ratio falls
            here I’m going to make a mistake of the opposite kind.</p>
        <h2 id="unknown-550">未知</h2><h2>Unknown</h2>
        <p>H1 是正确的，但我的似然比结果很小，所以我决定支持 H0。这是另一种错误，这种错误概率我们称之为 beta。你可以看到 alpha 和 beta 之间存在权衡。如果你以这种方式移动阈值，alpha 会变小，但 beta 会变大。一般情况是，在你的权衡中，取决于你把阈值放在哪里，如下所示。</p><p>H1 is true, but my likelihood ratio turned out to be small, and I decided in favor of H0. This is an error of
            the
            other kind, this probability of error we call beta. And you can see that there’s a trade off between alpha
            and
            beta. If you move your threshold this way alpha become smaller, but beta becomes larger. And the general
            picture
            is, in your trade off, depending on where you put your threshold is as follows.</p>
        <p>如果您在此处设置阈值，则可以将这个 beta 设为 0，但在这种情况下，您肯定会犯相反的错误。因此，beta 等于 0，alpha 等于 1 是一种可能性。如果您将阈值完整发送到另一侧，beta 等于 1，alpha 等于 0 是另一种可能性。一般来说，您会得到某种权衡曲线。</p><p>You can make this beta to be 0 if you put your threshold out here, but in that case you are certain that
            you’re
            going to make a mistake of the opposite kind. So beta equals 0, alpha equals 1 is one possibility. Beta
            equals 1
            alpha equals 0 is the other possibility if you send your thresholds complete to the other side. And in
            general
            you’re going to get a trade off curve of some sort.</p>
        <p>如果你想使用特定的 alpha 值，例如 alpha 为 0.05，那么这将为你确定 beta 的概率。现在统计学中有一个普遍且非常重要的定理，我们无法证明。它告诉我们，当我们使用似然比检验时，我们会得到最佳的权衡曲线。你可以想出其他方法来做出决定。</p><p>And if you want to use a specific value of alpha, for example alpha being 0.05, then that’s going to
            determine
            for you the probability for beta. Now there’s a general, and quite important theorem in statistics, which
            were
            are not proving. And which tells us that when we use likelihood ratio tests we get the best possible trade
            off
            curve. You could think of other ways of making your decisions.</p>
        <h2 id="unknown-551">未知</h2><h2>Unknown</h2>
        <p>还有其他方法可以将 x 空间切分为拒绝和接受区域。但是，任何其他方法最终都会产生高于此特定曲线的错误概率。因此，似然比检验可以为您提供处理 alpha 和 beta 之间权衡的最佳方法。我们无法同时最小化 alpha 和 beta，它们之间存在权衡。</p><p>Other ways of cutting off your x space into a rejection and acceptance region. But any other way that you do
            it
            is going to end up with some probabilities of error that are going to be above this particular curve. So the
            likelihood ratio test turns out to give you the best possible way of dealing with this trade off between
            alpha
            and beta. We cannot minimize alpha and beta simultaneously, there’s a trade off between them.</p>
        <p>但至少我们希望有一个测试能够以最佳方式处理这种权衡。对于给定的 alpha 值，我们希望 beta 值尽可能小。并且根据定理，似然比检验确实具有这种最优性。对于给定的 alpha 值，它们可以最小化不同类型错误的概率。所以让我们将所有这些具体化，并查看简单的例子。</p><p>But at least we would like to have a test that deals with this trade off in the best possible way. For a
            given
            value of alpha we want to have the smallest possible value of beta. And as the theorem is that the
            likelihood
            ratio tests do have this optimality property. For a given value of alpha they minimize the probability of
            error
            of a different kind. So let’s make all these concrete and look at the simple example.</p>
        <p>我们有两个均值不同的正态分布。因此，在 H0 下，均值为 0。在 H1 下，均值为 1。您获得了数据，实际上是从两个分布之一中提取了几个数据。您想做出决定，这两个分布中哪一个是正确的？所以您要做的就是写下似然比。</p><p>We have two normal distributions with different means. So under H0 you have a mean of 0. Under H1 you have a
            mean
            of 1. You get your data, you actually get several data drawn from one of the two distributions. And you want
            to
            make a decision, which one of the two is true? So what you do is you write down the likelihood ratio.</p>
        <h2 id="unknown-552">未知</h2><h2>Unknown</h2>
        <p>如果数据向量是根据 H0 生成的，则为该向量的密度，如果是根据 H1 生成的，则为该向量的密度。由于我们有多个数据，因此向量的密度是各个元素密度的乘积。由于我们处理的是法线，所以我们有这些指数因子。指数的乘积给出了总和的指数。</p><p>The density for a vector of data, if that vector was generated according to H0 which is this one, and the
            density
            if it was generated according to H1. Since we have multiple data the density of a vector is the product of
            the
            densities of the individual elements. Since we’re dealing with normals we have those exponential factors. A
            product of exponentials gives us an exponential of the sum.</p>
        <p>我就不多说细节了，但这是似然比的形式。似然比检验告诉我们，我们应该在获得数据后计算这个数量，并与阈值进行比较。现在你可以在这里做一些代数运算，并简化。通过追踪不等式，你正在对两边取对数，等等。</p><p>I’ll spare you the details, but this is the form of the likelihood ratio. The likelihood ratio test tells us
            that
            we should calculate this quantity after we get your data, and compare with a threshold. Now you can do some
            algebra here, and simplify. And by tracing down the inequalities you’re taking logarithms of both sides, and
            so
            on.</p>
        <p>人们得出的结论是，使用一个对该比率有阈值的检验相当于计算这个数量，并将其与阈值进行比较。基本上，这里的这个数量在该数量上是单调的。这个大于阈值相当于这个大于阈值。所以这告诉我们在这个特定情况下似然比检验的一般结构。</p><p>One comes to the conclusion that using a test that has a threshold on this ratio is equivalent to calculating
            this quantity, and comparing it with a threshold. Basically this quantity here is monotonic in that
            quantity.
            This being larger than the threshold is equivalent to this being larger than the threshold. So this tells us
            the
            general structure of the likelihood ratio test in this particular case.</p>
        <h2 id="unknown-553">未知</h2><h2>Unknown</h2>
        <p>这很好，因为它告诉我们，我们可以通过查看数据的简单摘要来做出决策。我们做出决策所依据的这个数量、这个数据摘要被称为统计数据。因此，您可以获取数据（一个多维向量），并将其压缩为一个数字，然后根据该数字做出决策。</p><p>And it’s nice because it tells us that we can make our decisions by looking at this simple summary of the
            data.
            This quantity, this summary of the data on the basis of which we make our decision is called a statistic. So
            you
            take your data, which is a multi dimensional vector, and you condense it to a single number, and then you
            make a
            decision on the basis of that number.</p>
        <p>这就是测试的结构。如果我得到 Xi 的较大总和，则这是支持 H1 的证据，因为这里的平均值较大。因此，如果总和大于阈值，我将决定支持 H1 或拒绝 H0。我该如何选择阈值？</p><p>So this is the structure of the test. If I get a large sum of Xi’s this is evidence in favor of H1 because
            here
            the mean is larger. And so I’m going to decide in favor of H1 or reject H0 if the sum is bigger than the
            threshold. How do I choose my threshold?</p>
        <p>好吧，我想选择我的阈值，这样当 H0 为真时，错误决策的概率，错误拒绝的概率等于某个数字。Alpha，例如 5%。所以这里给出的是 5%。你知道这个随机变量的分布，它是正态的。你想找到让它为真的阈值。</p><p>Well I would like to choose my threshold so that the probability of an incorrect decision when H0 is true the
            probability of a false rejection equals to a certain number. Alpha, such as for example 5%. So you’re given
            here
            that this is 5%. You know the distribution of this random variable, it’s normal. And you want to find the
            threshold value that makes this to be true.</p>
        <h2 id="unknown-554">未知</h2><h2>Unknown</h2>
        <p>所以这是一类你已经见过多次的问题。你去查看正态表，然后就能算出来。所以 Xi 的总和具有某种分布，它是正态的。所以这就是 Xi 总和的分布。你希望这里的概率是 alpha。要实现这一点，使它为真的阈值是多少？</p><p>So this is a type of problem that you have seen several times. You go to the normal tables, and you figure it
            out. So the sum of the Xi’s has some distribution, it’s normal. So that’s the distribution of the sum of the
            Xi’s. And you want this probability here to be alpha. For this to happen what is the threshold value that
            makes
            this to be true?</p>
        <p>所以您知道如何使用正态表解决此类问题。一个略有不同的例子是，您有两个正态分布，它们的均值相同（我们假设为 0），但它们的方差不同。</p><p>So you know how to solve problems of this kind using the normal tables. A slightly different example is one
            in
            which you have two normal distributions that have the same mean let’s take it to be 0 but they have a
            different
            variance.</p>
        <p>因此，如果您看到的 X 在两边都很大，那么您会选择 H1，这很自然。如果您的 X 接近 0，那么这就是方差较小的证据，您会选择 H0。因此，为了正式进行，您再次将其写成似然比的形式。因此，H0 下的 X 向量密度再次是这个。</p><p>So it’s sort of natural that here, if your X’s that you see are kind of big on either side you would choose
            H1.
            If your X’s are near 0 then that’s evidence for the smaller variance you would choose H0. So to proceed
            formally
            you again write down to the form of the likelihood ratio. So again the density of an X vector under H0 is
            this
            one.</p>
        <h2 id="unknown-555">未知</h2><h2>Unknown</h2>
        <p>它是每个 Xi 的密度的乘积。正态密度的乘积会给出指数的乘积，即总和的指数，这就是您得到的表达式。在另一个假设下，唯一改变的是方差。正态分布中的方差出现在指数的分母中。所以你把它放在那里。所以这是似然比检验的一般结构。</p><p>It’s the product of the densities of each one of the Xi’s. Product of normal densities gives you a product of
            exponentials, which is exponential of the sum, and that’s the expression that you get. Under the other
            hypothesis the only thing that changes is the variance. And the variance, in the normal distribution, shows
            up
            here in the denominator of the exponent. So you put it there. So this is the general structure of the
            likelihood
            ratio test.</p>
        <p>现在你要做一些代数运算。这些项是常数，将这个比率与常数进行比较，就如同将指数的比率与常数进行比较一样。然后你取对数，你想将这个东西的对数与常数进行比较。</p><p>And now you do some algebra. These terms are constants comparing this ratio to a constant is the same as just
            comparing the ratio of the exponentials to a constant. Then you take logarithms, you want to compare the
            logarithm of this thing to a constant.</p>
        <p>你做了一点代数运算，最后你会发现，如果 Xi 的平方和大于阈值，则测试的结构是拒绝 H0。因此，通过进行似然比检验，你会被告知应该根据这种类型的规则做出决定。因此，这固定了决策区域（拒绝区域）的形状或结构。</p><p>You do a little bit of algebra, and in the end you find that the structure of the test is to reject H0 if the
            sum
            of the squares of the Xi’s is bigger than the threshold. So by committing to a likelihood ratio test you are
            told that you should be making it your decision according to a rule of this type. So this fixes the shape or
            the
            structure of the decision region, of the rejection region.</p>
        <h2 id="unknown-556">未知</h2><h2>Unknown</h2>
        <p>剩下的唯一一件事就是再次选择这个阈值，以便具有错误拒绝的概率等于 5% 的属性。所以这是 H0 为真的概率，但平方和恰好大于我的阈值。在这种情况下，我最终决定 H1。我如何找到 Xi prime 的值？</p><p>And the only thing that’s left, once more, is to pick this threshold in order to have the property that the
            probability of a false rejection is equal to say 5%. So that’s the probability that H0 is true, but the sum
            of
            the squares accidentally happens to be bigger than my threshold. In which case I end up deciding H1. How do
            I
            find the value of Xi prime?</p>
        <p>好吧，我需要做的是查看图片，或多或少是这种类型，但现在我需要查看 Xi 平方和的分布。实际上 Xi 平方和是一个非负随机变量。所以它的分布会是这样的。</p><p>Well what I need to do is to look at the picture, more or less of this kind, but now I need to look at the
            distribution of the sum of the Xi’s squared. Actually the sum of the Xi’s squared is a non negative random
            variable. So it’s going to have a distribution that’s something like this.</p>
        <p>我查看该分布，再次希望尾部概率为 alpha，这决定了我的阈值。所以，如果您知道这个数量的分布，这又是一个简单的练习。你知道吗？好吧，我们真的不知道，我们还没有在这堂课中处理过这个特定的分布。但原则上你应该能够找到它是什么。这是一个派生分布问题。</p><p>I look at that distribution, and once more I want this tail probability to be alpha, and that determines
            where my
            threshold is going to be. So that’s again a simple exercise provided that you know the distribution of this
            quantity. Do you know it? Well we don’t really know it, we have not dealt with this particular distribution
            in
            this class. But in principle you should be able to find what it is. It’s a derived distribution problem.</p>
        <h2 id="unknown-557">未知</h2><h2>Unknown</h2>
        <p>你知道 Xi 的分布，它是正态的。因此，通过解决派生分布问题，你可以找到 Xi 平方的分布。而且 Xi 平方是相互独立的，因为 Xi 是独立的。所以你想找到已知分布的随机变量之和的分布。由于它们是独立的，原则上，你可以使用卷积公式来做到这一点。</p><p>You know the distribution of Xi, it’s normal. Therefore, by solving a derived distribution problem you can
            find
            the distribution of Xi squared. And the Xi squared’s are independent of each other, because the Xi’s are
            independent. So you want to find the distribution of the sum of random variables with known distributions.
            And
            since they’re independent, in principle, you can do this using the convolution formula.</p>
        <p>因此，从原则上讲，如果你足够耐心，你就能找到这个随机变量的分布。然后你绘制或制表它，并找出该分布的第 95 个百分位数的确切位置，这决定了你的阈值。因此，这个分布实际上有一个简单而漂亮的闭式公式。因为这是一个非常常见的测试，所以人们已经将该分布制成了表格。它被称为卡方分布。</p><p>So in principle, and if you’re patient enough, you will be able to find the distribution of this random
            variable.
            And then you plot it or tabulate it, and find where exactly is the 95th percentile of that distribution, and
            that determines your threshold. So this distribution actually turns out to have a nice and simple closed
            form
            formula. Because this is a pretty common test, people have tabulated that distribution. It’s called the chi
            square distribution.</p>
        <p>有表格可用。你查看表格，找到分布的第 95 个百分位数，这样就可以确定你的阈值。那么这个故事的寓意是什么？似然比检验的结构告诉你你将拥有什么样的决策区域。</p><p>There’s tables available for it. And you look up in the tables, you find the 95th percentile of the
            distribution,
            and this way you determine your threshold. So what’s the moral of the story? The structure of the likelihood
            ratio test tells you what kind of decision region you’re going to have.</p>
        <h2 id="unknown-558">未知</h2><h2>Unknown</h2>
        <p>它告诉你，对于这个特定的测试，你应该使用 Xi 平方和作为你的统计数据，作为你做出决定的基础。然后你需要解决派生分布问题来找到你的统计数据的概率分布。在 H0 下找到这个数量的分布，最后，基于这个分布，在你得到它之后，确定你的阈值。</p><p>It tells you that for this particular test you should be using the sum of the Xi squared’s as your statistic,
            as
            the basis for making your decision. And then you need to solve a derived distribution problem to find the
            probability distribution of your statistic. Find the distribution of this quantity under H0, and finally,
            based
            on that distribution, after you have derived it, then determine your threshold.</p>
        <p>现在让我们讨论一个稍微复杂一些的情况。你有一枚硬币，有人告诉你我试图制造一枚公平的硬币。这枚硬币公平吗？所以你有一个假设，也就是默认假设。零假设。这枚硬币是公平的。但也许它不是。所以你有一个备选假设，即你的硬币不公平。</p><p>So now let’s move on to a somewhat more complicated situation. You have a coin, and you are told that I tried
            to
            make a fair coin. Is it fair? So you have the hypothesis, which is the default. the null hypothesis. that
            the
            coin is fair. But maybe it isn’t. So you have the alternative hypothesis that your coin is not fair.</p>
        <p>现在，在这种情况下，不同之处在于，你的备选假设不仅仅是一个特定的假设。你的备选假设包含许多替代方案。它包括 p 为 0.6 的假设。它包括 p 为 0.51 的假设。它包括 p 为 0.48 的假设，等等。因此，你要用这个假设来检验所有这些备选假设。你最终要做的基本上是以下几点。你会得到一些数据。</p><p>Now what’s different in this context is that your alternative hypothesis is not just one specific hypothesis.
            Your alternative hypothesis consists of many alternatives. It includes the hypothesis that p is 0.6. It
            includes
            the hypothesis that p is 0.51. It includes the hypothesis that p is 0.48, and so on. So you’re testing this
            hypothesis versus all this family of alternative hypothesis. What you will end up doing is essentially the
            following. you get some data.</p>
        <h2 id="unknown-559">未知</h2><h2>Unknown</h2>
        <p>也就是说，你抛硬币多次。假设你抛了 1,000 次。你观察到一些结果。假设你看到了 472 次正面。然后你会问，如果这个假设成立，那么在这个假设下这个值真的可能吗？或者它是否是一个非常离群的值？如果在这个假设下它看起来像一个极端离群值，那么我会拒绝它，并接受另一种选择。</p><p>That is, you flip the coin a number of times. Let’s say you flip it 1,000 times. You observe some outcome.
            Let’s
            say you saw 472 heads. And you ask the question if this hypothesis is true is this value really possible
            under
            that hypothesis? Or would it be very much of an outlier? If it looks like an extreme outlier under this
            hypothesis then I reject it, and I accept the alternative.</p>
        <p>如果这个数字最终在你预期的范围内，那么你保留或接受你的零假设。那么，什么是离群值？首先，你获取数据，然后将它们压缩为一个数字。因此，你的详细数据实际上应该是一系列正面/反面、正面/反面等等。</p><p>If this number turns out to be something within the range that you would have expected then you keep, or
            accept
            your null hypothesis. OK so what does it mean to be an outlier or not? First you take your data, and you
            condense them to a single number. So your detailed data actually would have been a sequence of heads/tails,
            heads/tails and all that.</p>
        <p>任何有理智的人都会告诉你，你不应该真正关心正面和反面的确切顺序。我们只需根据观察到的正面的次数来做出决定。因此，使用某种推理，可以是数学的、直观的或涉及艺术性的推理。你选择你所看到的数据的一维或标量摘要。</p><p>Any reasonable person would tell you that you shouldn’t really care about the exact sequence of heads and
            tails.
            Let’s just base our decision on the number of heads that we have observed. So using some kind of reasoning
            which
            could be mathematical, or intuitive, or involving artistry. you pick a one dimensional, or scalar summary of
            the
            data that you have seen.</p>
        <h2 id="unknown-560">未知</h2><h2>Unknown</h2>
        <p>在这种情况下，数据的摘要只是头部的数量，这是相当合理的。因此，你承诺根据这个数量做出决定。你问我看到的数量是否看起来像一个异常值？或者它看起来或多或少还好吗？好的，异常值是什么意思？</p><p>In this case, the summary of the data is just the number of heads that’s a quite reasonable one. And so you
            commit yourself to make a decision on the basis of this quantity. And you ask the quantity that I’m seeing
            does
            it look like an outlier? Or does it look more or less OK? OK, what does it mean to be an outlier?</p>
        <p>您想选择拒绝域的形状，但要基于单个数字 s。同样，在这种情况下，合理的做法是进行如下论证。如果我的硬币是公平的，我预计会看到 n 超过 2 个正面。这是预期值。如果我看到的正面数量与预期正面数量相差甚远，那么我认为这是一个异常值。</p><p>You want to choose the shape of this rejection region, but on the basis of that single number s. And again,
            the
            reasonable thing to do in this context would be to argue as follows. if my coin is fair I expect to see n
            over 2
            heads. That’s the expected value. If the number of heads I see is far from the expected number of heads then
            I
            consider this to be an outlier.</p>
        <p>所以如果这个数字大于某个阈值 Xi。我认为它是一个异常值，然后我将拒绝我的假设。所以我们选择了我们的统计数据。我们选择了我们将如何做出决定的一般形式，然后选择我们想要的某个重要性或置信水平。同样，这个著名的 5% 数字。</p><p>So if this number is bigger than some threshold Xi. I consider it to be an outlier, and then I’m going to
            reject
            my hypothesis. So we picked our statistic. We picked the general form of how we’re going to make our
            decision,
            and then we pick a certain significance, or confidence level that we want. Again, this famous 5% number.</p>
        <h2 id="unknown-561">未知</h2><h2>Unknown</h2>
        <p>如果某个事物位于发生概率为 5% 或更低的区域，我们将宣布它为异常值。也就是说，我选择拒绝域，以便如果 H0 在默认或零假设下为真，那么我偶然落入该区域的概率只有 5%，这让我认为 H1 为真。
        </p><p>And we’re going to declare something to be an outlier if it lies in the region that has 5% or less
            probability of
            occurring. That is I’m picking my rejection region so that if H0 is true under the default, or null
            hypothesis,
            there’s only 5% chance that by accident I fall there, and the thing makes me think that H1 is going to be
            true.
        </p>
        <p>现在剩下要做的就是选择这个阈值。这是一种常见的计算。我希望选择我的阈值，我的 Xi 数，以便 s 距离平均值 Xi 的概率小于 5%。或者在接受区域内的概率。以便与默认值的距离小于我的阈值。我希望它是 95%。</p><p>So now what’s left to do is to pick the value of this threshold. This is a calculation of the usual kind. I
            want
            to pick my threshold, my Xi number so that the probability that s is further from the mean by an amount of
            Xi is
            less than 5%. Or that the probability of being inside the acceptance region. so that the distance from the
            default is less than my threshold. I want that to be 95%.</p>
        <p>因此，这是使用中心极限定理和正态表可以得到的等式。正面的次数与正确平均值相差 31 的概率为 95%。因此，练习的方式当然是我们从这个数字 5% 开始。这相当于这个数字 95%。</p><p>So this is an equality that you can get using the central limit theorem and the normal tables. There’s 95%
            probability that the number of heads is going to be within 31 from the correct mean. So the way the exercise
            is
            done of course, is that we start with this number, 5%. Which translates to this number 95%.</p>
        <h2 id="unknown-562">未知</h2><h2>Unknown</h2>
        <p>一旦我们确定了这个数字，你就会问，我们应该取多少数字才能使这个等式成立？这又是一个这样的问题。你有一个数量，你知道它的分布。你为什么知道它？根据中心极限定理，正面的数量近似为正态分布。所以这里讨论的是正态分布。</p><p>And once we have fixed that number then you ask the question what number should we have here to make this
            equality to be true? It’s again a problem of this kind. You have a quantity whose distribution you know. Why
            do
            you know it? The number of heads by the central limit theorem is approximately normal. So this here talks
            about
            the normal distribution.</p>
        <p>您将 alpha 设置为 5%，然后问我应该将阈值设在哪里，以便这个概率只有 5%？现在在我们的特定示例中，阈值是 31。这个数字距离正确平均值只有 28。因此这些距离小于阈值。所以我们最终没有拒绝 H0。所以我们有拒绝域。</p><p>You set your alpha to be 5%, and you ask where should I put my threshold so that this probability of being
            out
            there is only 5%? Now in our particular example the threshold turned out to be 31. This number turned out
            was
            just 28 away from the correct mean. So these distance was less than the threshold. So we end up not
            rejecting
            H0. So we have our rejection region.</p>
        <p>我们的设计方式是，当 H0 成立时，我们只有很小的机会（5%）会得到不属于该范围的数据。我们称该数据为异常值。</p><p>The way we designed it is that when H0 is true there’s only a small chance, 5%, that we get to data out of
            there.
            Data that we would call an outlier.</p>
        <h2 id="unknown-563">未知</h2><h2>Unknown</h2>
        <p>如果我们看到这样的异常值，我们会拒绝 H0。如果我们看到的不是异常值，就像本例中那样，距离有点小，那么我们就不会拒绝 H0。这里有一个有趣的小词，人们通常更喜欢使用这个术语。表示 H0 未被数据拒绝。而不是说 H0 被接受。</p><p>If we see such an outlier we reject H0. If what we see is not an outlier as in this case, where that distance
            turned out to be kind of small, then we do not reject H0. An interesting little piece of language here,
            people
            generally prefer to use this terminology. to say that H0 is not rejected by the data. Instead of saying that
            H0
            is accepted.</p>
        <p>从某种意义上说，它们说的是同一件事，但区别却很微妙。当我说“不被拒绝”时，我的意思是我得到了一些与我的假设相符的数据。也就是说，我得到的数据不会证伪我的假设，即我的零假设。所以我的零假设仍然有效，而且可能是正确的。但从数据来看，你永远无法真正证明假设是正确的。</p><p>In some sense they’re both saying the same thing, but the difference is sort of subtle. When I say not
            rejected
            what I mean is that I got some data that are compatible with my hypothesis. That is the data that I got do
            not
            falsify the hypothesis that I had, my null hypothesis. So my null hypothesis is still alive, and may be
            true.
            But from data you can never really prove that the hypothesis is correct.</p>
        <p>也许我的硬币在其他一些复杂的方面并不公平。也许我只是运气好，尽管我的硬币不公平，但最终的结果是公平的。也许我的硬币翻转并不像我在模型中假设的那样独立。因此，我的零假设可能有很多错误，但我仍然得到数据告诉我我的假设是正确的。</p><p>Perhaps my coin is not fair in some other complicated way. Perhaps I was just lucky, and even though my coin
            is
            not fair I ended up with an outcome that suggests that it’s fair. Perhaps my coin flips are not independent
            as I
            assumed in my model. So there’s many ways that my null hypothesis could be wrong, and still I got data that
            tells me that my hypothesis is OK.</p>
        <h2 id="unknown-564">未知</h2><h2>Unknown</h2>
        <p>所以这就是科学的一般运作方式。人们想出一个模型或理论。这是默认理论，我们根据该理论试图找出是否有违反该理论的例子。如果你发现违反该理论的数据和例子，你的理论就是伪造的，你需要寻找一个新的理论。</p><p>So this is the general way that things work in science. One comes up with a model or a theory. This is the
            default theory, and we work with that theory trying to find whether there are examples that violate the
            theory.
            If you find data and examples that violate the theory your theory is falsified, and you need to look for a
            new
            one.</p>
        <p>但是当你有了自己的理论，实际上再多的数据也无法证明你的理论是正确的。所以我们有一个默认理论，即只要我们没有发现任何与之相反的数据，光速就是恒定的。我们坚持这个理论，但无论我们做了多少实验，都无法真正证明这一点。
        </p><p>But when you have your theory, really no amount of data can prove that your theory is correct. So we have the
            default theory that the speed of light is constant as long as we do not find any data that runs counter to
            it.
            We stay with that theory, but there’s no way of really proving this, no matter how many experiments we do.
        </p>
        <p>但可能会有实验推翻该理论，在这种情况下我们需要寻找新的理论。因此，我们对待备选假设的方式有点不对称。H0 是默认假设，我们会接受它，直到我们看到一些相反的证据。如果我们看到一些相反的证据，我们就会拒绝它。
        </p><p>But there could be experiments that falsify that theory, in which case we need to do look for a new one. So
            there’s a bit of an asymmetry here in how we treat the alternative hypothesis. H0 is the default which we’ll
            accept until we see some evidence to the contrary. And if we see some evidence to the contrary we reject it.
        </p>
        <h2 id="unknown-565">未知</h2><h2>Unknown</h2>
        <p>只要我们没有看到相反的证据，我们就会继续研究它，但始终持保留态度。你永远无法真正证明一枚硬币的偏差恰好等于 1/2。也许偏差等于 0.50001，所以偏差不是 1/2。但是，如果进行 1,000 次抛硬币实验，你将无法看到这种效果。</p><p>As long as we do not see evidence to the contrary then we keep working with it, but always take it with a
            grain
            of salt. You can never really prove that a coin has a bias exactly equal to 1/2. Maybe the bias is equal to
            0.50001, so the bias is not 1/2. But with an experiment with 1,000 coin tosses you wouldn’t be able to see
            this
            effect.</p>
        <p>好的，这就是你测试硬币是否公平的方法。你也可以考虑测试骰子是否公平。因此，对于骰子来说，零假设是掷骰子时每个可能结果的概率都相等，等于 1/6。你还可以假设掷出的骰子在统计上是相互独立的。</p><p>OK, so that’s how you go about testing about whether your coin is fair. You can also think about testing
            whether
            a die is fair. So for a die the null hypothesis would be that every possible result when you roll the die
            has
            equal probability and equal to 1/6. And you also make the hypothesis that your die rolls are statistically
            independent from each other.</p>
        <p>所以我拿起骰子，掷出几次，小数 n，然后计算我得到了多少个 1、多少个 2、多少个 3，这些就是我的数据。我计算我观察到的骰子掷出结果等于总和 i 的次数。现在我要问一个问题。我观察到的 Ni 是否与我的假设相符？</p><p>So I take my die, I roll it a number of times, little n, and I count how many 1’s I got, how many 2’s I got,
            how
            many 3’s I got, and these are my data. I count how many times I observed a specific result in my die roll
            that
            was equal to sum i. And now I ask the question. the Ni’s that I observed, are they compatible with my
            hypothesis
            or not?</p>
        <h2 id="unknown-566">未知</h2><h2>Unknown</h2>
        <p>与我的假设相符是什么意思？在零假设下，Ni 应该近似等于或期望等于 N 乘以小 Pi。在我们的例子中，这个小 Pi 当然是 1/6。因此，如果我的骰子是公平的，我期望看到的 1 的数量等于掷骰次数乘以 1/6。我期望看到的 2 的数量也是相同的数字。</p><p>What does compatible to my hypothesis mean? Under the null hypothesis Ni should be approximately equal, or is
            equal in expectation to N times little Pi. And in our example this little Pi is of course 1/6. So if my die
            is
            fair the number of ones I expect to see is equal to the number of rolls times 1/6. The number of 2’s I
            expect to
            see is again that same number.</p>
        <p>当然存在随机性，所以我并不期望得到确切的数字。但我可以问一下 i 与预期值相差多远？如果我的大写 Ni 与 N/6 相差很大，这证明我的骰子不公平。如果这些数字接近 N 乘以 1/6，那么我会说没有证据可以让我拒绝这个假设。</p><p>Of course there’s randomness, so I do not expect to get exactly that number. But I can ask how far away from
            the
            expected values was i? If my capital Ni’s turn to be very different from N/6 this is evidence that my die is
            not
            fair. If those numbers turn out to be close to N times 1/6 then I’m going to say there’s no evidence that
            would
            lead me to reject this hypothesis.</p>
        <p>所以这个假设仍然存在。所以有人提出了这个想法，也许正确的统计数据，或者量化 Ni 与其平均值之间的距离的正确方法是查看这个数量。所以我在查看零假设下的 Ni 的预期值。看看我得到的结果，取它的平方，并将其加到所有 i 上。但也将这些项加到分母中。</p><p>So this hypothesis remains alive. So someone has come up with this thought that maybe the right statistic to
            use,
            or the right way of quantifying how far away are the Ni’s from their mean is to look at this quantity. So
            I’m
            looking at the expected value of Ni under the null hypothesis. See what I got, take the square of this, and
            add
            it over all i’s. But also throw in these terms in the denominator.</p>
        <h2 id="unknown-567">未知</h2><h2>Unknown</h2>
        <p>至于为什么会有这个术语，那就说来话长了。人们可以写下某些似然比，进行某些泰勒级数近似，并且有一个启发式论证可以证明为什么这是测试的良好形式。因此，这一步涉及一定的技巧，有些人不知何故认为计算是合理的做法。</p><p>And why that term is there, that’s a longer story. One can write down certain likelihood ratios, do certain
            Taylor Series approximations, and there’s a Heuristic argument that justifies why this would be a good form
            for
            the test to use. So there’s a certain art that’s involved in this step that some people somehow decided that
            it’s a reasonable thing to do is to calcelate.</p>
        <p>一旦你得到结果，就可以计算出结果的一维摘要，这将是你的统计数据，并将该统计数据与阈值进行比较。这就是你做出决定的方式。所以到目前为止，我们已经确定了我们将要使用的拒绝域的类型。所以我们选择了测试的定性结构，现在唯一剩下的就是选择我们将要使用的特定阈值。
        </p><p>Once you get your results to calculate this one dimensional summary of your result, this is going to be your
            statistic, and compare that statistic to a threshold. And that’s how you make your decision. So by this
            point we
            have fixed the type of the rejection region that we’re going to have. So we’ve chosen the qualitative
            structure
            of our test, and the only thing that’s now left is to choose the particular threshold we’re going to use.
        </p>
        <p>再次重申，方法是一样的。我们希望设置阈值，使错误拒绝的概率为 5%。我们希望当零假设为真时，数据落入此处的概率仅为 5%。因此，这与设置阈值 Xi 相同，以使我们的检验统计量大于该阈值的概率。</p><p>And the recipe, once more, is the same. We want to set our threshold so that the probability of a false
            rejection
            is 5%. We want the probability that our data fall in here is only 5% when the null hypothesis is true. So
            that’s
            the same as setting our threshold Xi so that the probability that our test statistic is bigger than that
            threshold.</p>
        <h2 id="unknown-568">未知</h2><h2>Unknown</h2>
        <p>我们希望这个概率只有 0.05。那么要解决这类问题，你需要做什么呢？你需要找到大写字母 T 的概率分布。所以再一次，这是同样的情况。你需要进行某种计算，并得出随机变量 T 的分布，其中 T 是这样定义的。</p><p>We want that probability to be only 0.05. So to solve a problem of this kind what is it that you need to do?
            You
            need to find the probability distribution of capital T. So once more it’s the same picture. You need to do
            some
            calculations of some sort, and come up with the distribution of the random variable T, where T is defined
            this
            way.</p>
        <p>您想在假设 H0 下找到这个分布。一旦找到该分布，您就可以解决这个常见问题。我希望这里的概率为 5%。我的阈值应该是多少？那么这归结为什么？从某种意义上说，找到大写字母 T 的分布是一个混乱、困难、派生的分布问题。从这个模型中，我们知道大写字母 Ni 的分布。</p><p>You want to find this distribution under hypothesis H0. Once you find what that distribution is then you can
            solve this usual problem. I want this probability here to be 5%. What should my threshold be? So what does
            this
            boil down to? Finding the distribution of capital T is in some sense a messy, difficult, derived
            distribution
            problem. From this model we know the distribution of the capital Ni’s.</p>
        <p>实际上，我们甚至可以写下大写 Ni 的联合分布。事实上，我们可以在这里做一个近似。大写 Ni 是一个二项式随机变量。假设我在小数 N 中掷出的 1 的数量。所以这是一个二项式随机变量。当小数 n 很大时，这将近似为正态分布。所以我们有正态随机变量，或者近似正态减去一个常数。它们仍然近似为正态分布。</p><p>And actually we can even write down the joint distribution of the capital Ni’s. In fact we can make an
            approximation here. Capital Ni is a binomial random variable. Let’s say the number of 1’s that I got in
            little N
            rolls off my die. So that’s a binomial random variable. When little n is big this is going to be
            approximately
            normal. So we have normal random variables, or approximately normal minus a constant. They’re still
            approximately normal.</p>
        <h2 id="unknown-569">未知</h2><h2>Unknown</h2>
        <p>我们取这些的平方，然后缩放它们，这样您就可以解决派生分布问题，以找到这个数量的分布。您可以做更多的工作，更多的派生分布工作，并找到大写字母 T 的分布。所以这是件乏味的事情，但由于这个测试经常使用，人们又做了这些计算。他们找到了大写字母 T 的分布，并在表格中提供。</p><p>We take the squares of these, scale them so you can solve a derived distribution problem to find the
            distribution
            of this quantity. You can do more work, more derived distribution work, and find the distribution of capital
            T.
            So this is a tedious matter, but because this test is used quite often, again people have done those
            calculations. They have found the distribution of capital T, and it’s available in tables.</p>
        <p>然后你去查看这些表格，找到做出此类决策的适当阈值。现在，为了让你了解一个人可能要处理的假设有多复杂，让我们将事情复杂化一个层次。所以在这里你可以认为这个 X 是一个离散随机变量。这是我掷骰子的结果。</p><p>And you go to those tables, and you find the appropriate threshold for making a decision of this type. Now to
            give you a sense of how complicated hypothesis one might have to deal with let’s make things one level more
            complicated. So here you can think this X is a discrete random variable. This is the outcome of my roll.</p>
        <p>我有一个模型，其中离散随机变量的可能值的概率都等于 1/6。所以我的零假设是随机变量大写 X 的特定 PMF。所以，另一种表述这个问题的方式是，我的 PMF 是否正确？所以这是一次掷骰子结果的 PMF。你问我的 PMF 是否正确？让它变得更复杂。</p><p>And I had a model in which the possible values of my discrete random variables they have probabilities all
            equal
            to 1/6. So my null hypothesis here was a particular PMF for the random variable capital X. So another way of
            phrasing what happened in this problem was the question is my PMF correct? So this is the PMF of the result
            of
            one die roll. You’re asking the question is my PMF correct? Make it more complicated.</p>
        <h2 id="unknown-570">未知</h2><h2>Unknown</h2>
        <p>当我有连续数据时，PDF 是否正确？这个问题怎么样？所以我假设我的概率分布是某种特定的正态分布。我从该随机变量中得到很多结果。我能判断我的结果是否正常吗？有哪些方法可以解决这个问题？
        </p><p>How about the question of the type is my PDF correct when I have continuous data? So I have hypothesized
            that’s
            the probability distribution that I have is let’s say a particular normal. I get lots of results from that
            random variable. Can I tell whether my results look like normal or not? What are some ways of going about
            it?
        </p>
        <p>好吧，我们在上一张幻灯片中看到，有一种方法可以确定您的 PMF 是否正确。因此，您可以获取正常结果，即从实验中获得的数据，然后对其进行离散化，这样现在您就可以处理离散数据了。以前的方法可以解决离散问题，例如我的 PDF 是否正确？</p><p>Well, we saw in the previous slide that there is a methodology for deciding if your PMF is correct. So you
            could
            take your normal results, the data that you got from your experiment, and discretize them, and so now you’re
            dealing with discrete data. And sort of used in previous methodology to solve a discrete problem of the type
            is
            my PDF correct?</p>
        <p>因此，在实践中，这样做的方法是获取所有数据，比如说这种数据点。将空间分成几个箱子，然后计算每个箱子中有多少个数据点。这样，你就会得到这个、那个、那个，什么都没有。这就是你从现有数据中得到的直方图。就像我们每次测验后你看到的非常熟悉的直方图一样。</p><p>So in practice the way this is done is that you get all your data, let’s say data points of this kind. You
            split
            your space into bins, and you count how many you have in each bin. So you get this, and that, and that, and
            nothing. So that’s a histogram that you get from the data that you have. Like the very familiar histograms
            that
            you see after each one of our quizzes.</p>
        <h2 id="unknown-571">未知</h2><h2>Unknown</h2>
        <p>所以如果你看这些直方图，你会问它看起来正常吗？好的，我们需要一个系统的方法来解决它。如果它是正常的，你可以计算出落在这个区间的概率。落在那个区间的概率，落入那个区间的概率。所以你会有一个预期值，即在这个区间会有多少个结果或数据点。</p><p>So if you look at these histogram, and you ask does it look like normal? OK, we need a systematic way of
            going
            about it. If it were normal you can calculate the probability of falling in this interval. The probability
            of
            falling in that interval, probability of falling into that interval. So you would have expected values of
            how
            many results, or data points, you would have in this interval.</p>
        <p>并将每个间隔的这些预期值与您观察到的实际值进行比较。然后取平方和，依此类推，就像上一张幻灯片中那样。这为您提供了一种解决方法。这有点混乱。这很难做到，因为您需要做出如何选择箱体大小的艰难决定？
        </p><p>And compare these expected values for each interval with the actual ones that you observed. And then take the
            sum
            of squares, and so on, exactly as in the previous slide. And this gives you a way of going about it. This is
            a
            little messy. It gets hard to do because you have the difficult decision of how do you choose the bin size?
        </p>
        <p>如果您将箱子设置得非常窄，那么您将得到很多包含 0 的箱子，以及一些只有一个结果的箱子。这可能感觉不对。如果您将箱子设置得非常宽，那么您将丢失大量信息。有没有办法在不创建箱子的情况下进行测试？这只是为了说明统计学家所想到的聪明想法。</p><p>If you take your bins to be very narrow you would get lots of bins with 0’s, and a few bins that only have
            one
            outcome in them. It probably wouldn’t feel right. If you choose your bins to be very wide then you’re losing
            a
            lot of information. Is there some way of making a test without creating bins? This is just to illustrate the
            clever ideas of what statisticians have thought about.</p>
        <h2 id="unknown-572">未知</h2><h2>Unknown</h2>
        <p>这是一种非常巧妙的测试方法，可以测试我的分布是否正确。我们在这里绘制 PMF 或 PDF 的近似值。我们会问它是否像我们假设的 PDF？我们不使用 PDF，而是使用累积分布函数。那么这是怎么做的？我假设的真实正态分布，我假设的密度。我的零假设。</p><p>And here’s a really cute way of going about a test, whether my distribution is correct or not. Here we’re
            essentially plotting a PMF, or an approximation of a PDF. And we ask does it look like the PDF we assumed?
            Instead of working with PDFs let’s work with cumulative distribution functions. So how does this go? The
            true
            normal distribution that I have hypothesized, the density that I’m hypothesizing. my null hypothesis.</p>
        <p>具有我可以绘制的特定 CDF。因此，假设我的假设 H0 是 X 符合我们的标准正态分布，我绘制标准正态分布的 CDF，这是这里的连续曲线。现在我得到了数据，我绘制了经验 CDF。经验 CDF 是什么？在经验 CDF 中，您要问的是数据中有多少比例低于 0？您会得到一个数字。</p><p>Has a certain CDF that I can plot. So supposed that my hypothesis H0 is that the X’s are normal with our
            standard
            normals, and I plot the CDF of the standard normal, which is the sort of continuous looking curve here. Now
            I
            get my data, and I plot the empirical CDF. What’s the empirical CDF? In the empirical CDF you ask the
            question
            what fraction of the data fell below 0? You get a number.</p>
        <p>我的数据中有多少比例低于 1？我得到一个数字。我的数据中有多少比例低于 2，依此类推。所以你谈论的是低于每个特定数字的数据比例。通过将这些分数绘制为该数字的函数，你会得到类似于 CDF 的东西。它是数据所暗示的 CDF。现在，我的实验中低于 0 的数据比例是。</p><p>What fraction of my data fell below 1? I get a number. What fraction of my data fell below 2, and so on. So
            you’re talking about fractions of the data that fell below each particular number. And by plotting those
            fractions as a function of this number you get something that looks like a CDF. And it’s the CDF suggested
            by
            the data. Now the fraction of the data that fall below 0 in my experiment is.</p>
        <h2 id="unknown-573">未知</h2><h2>Unknown</h2>
        <p>如果我的假设是正确的，那么预期为 1/2。1/2 是真实 CDF 的值。我查看得到的分数，预期是该数字。但是存在随机性，因此可能与此略有不同。对于任何特定值，我得到的分数低于某个数字。我们低于 2 的数据分数，其期望是低于 2 的概率，这是正确的 CDF。</p><p>If my hypothesis were true. expected to be 1/2.1/2 is the value of the true CDF. I look at the fraction that
            I
            got, it’s expected to be that number. But there’s randomness, so it’s might be a little different than that.
            For
            any particular value, the fraction that I got below a certain number. the fraction of data that we’re below,
            2,
            its expectation is the probability of falling below 2, which is the correct CDF.</p>
        <p>因此，如果我的假设成立，那么当 n 很大时，我根据数据得到的经验 CDF 应该非常接近真实 CDF。因此，判断我的模型是否正确的一种方法是查看假设的 CDF，即假设 H0 下的 CDF。查看我根据数据构建的 CDF，看看它们是否足够接近。</p><p>So if my hypothesis is true the empirical CDF that I get based on data should, when n is large, be very close
            to
            the true CDF. So a way of judging whether my model is correct or not is to look at the assumed CDF, the CDF
            under hypothesis H0. Look at the CDF that I constructed based on the data, and see whether they’re close
            enough
            or not.</p>
        <p>所谓足够接近，是指我将查看所有可能的 X，并查看这两条曲线之间的最大距离。我将进行测试，如果距离较小，则支持 H0，如果距离较大，则支持 H1。这仍然留给我一个问题，即确定阈值。我到底应该把阈值放在哪里？</p><p>And by close enough, I mean I’m going to look at all the possible X’s, and look at the maximum distance
            between
            those two curves. And I’m going to have a test that decides in favor of H0 if this distance is small, and in
            favor of H1 if this distance is large. That still leaves me the problem of coming up with a threshold. Where
            exactly do I put my threshold?</p>
        <h2 id="unknown-574">未知</h2><h2>Unknown</h2>
        <p>由于这个测试非常重要，而且经常使用，人们努力尝试理解这个相当困难的随机变量的概率分布。人们需要做大量的近似和巧妙的计算，但这些都导致了这个随机变量的概率分布的值和表格值。</p><p>Because this test is important enough, and is used frequently people have made the effort to try to
            understand
            the probability distribution of this quite difficult random variable. One needs to do lots of approximations
            and
            clever calculations, but these have led to values and tabulated values for the probability distribution of
            this
            random variable.</p>
        <p>例如，这些表格值告诉我们，如果我们想要 5% 的错误拒绝概率，那么我们的阈值应该是 1.36 除以 n 的平方根。所以我们知道这个特定值的阈值应该放在哪里。如果我们想要这个特定的错误或错误概率发生。所以这大概就是传统统计学所能达到的最困难和最复杂的程度。你想对那些不那么容易处理的假设进行测试。
        </p><p>And, for example, those tabulated values tell us that if we want 5% false rejection probability, then our
            threshold should be 1.36 divided by the square root of n.&nbsp;So we know where to put our threshold for this
            particular value. If we want this particular error or error probability to occur. So that’s about as hard
            and
            sophisticated classical statistics get. You want to have tests for hypotheses that are not so easy to
            handle.
        </p>
        <p>人们总能想出一些巧妙的方法来做这种测试。如何将理论预测与观察到的预测和观察到的数据进行比较。得出理论和数据之间差异的某种衡量标准，如果差异很大，那么你就拒绝你的假设。好吧，当然这并不是统计学领域的终结，还有更多。</p><p>People somehow think of clever ways of doing tests of this kind. How to compare the theoretical predictions
            with
            the observed predictions with the observed data.come up with some measure of the difference between theory
            and
            data, and if that difference is big, than you reject your hypothesis. OK, of course that’s not the end of
            the
            field of statistics, there’s a lot more.</p>
        <h2 id="unknown-575">未知</h2><h2>Unknown</h2>
        <p>在某种程度上，随着我们今天讲座的进行，我们构建拒绝域的方式越来越临时。我从帽子里拿出一个特定的数据和模型之间的拟合度测量值。我说我们就用基于此的测试吧。</p><p>In some ways, as we kept moving through today’s lecture, the way that we constructed those rejection regions
            was
            more and more ad hoc. I pulled out of a hat a particular measure of fit between data and the model. And I
            said
            let’s just use a test based on this.</p>
        <p>人们尝试或多或少采用系统性方法来得出拒绝域的一般形状，这些拒绝域至少具有一些理想或有利的理论特性。人们研究的一些更具体的问题。而不是进行测试，这是正确的 PDF 吗？是或否。我只是给你数据，我要求你告诉我，给我一个这些数据的模型或 PDF。</p><p>There are attempts at more or less systematic ways of coming up with the general shape of rejection regions
            that
            have at least some desirable or favorable theoretical properties. Some more specific problems that people
            study.
            instead of having a test, is this the correct PDF? Yes or no. I just give you data, and I ask you tell me,
            give
            me a model or a PDF for those data.</p>
        <p>好的，我的想法有很多种。一种通用方法是，你先形成一个直方图，然后你从这个直方图中提取一条平滑的线，这样就可以拟合直方图了。这仍然留下了一个问题，即你如何选择分箱？直方图中的分箱大小。你把它们分成多窄？</p><p>OK, my thoughts of this kind are of many types. One general method is you form a histogram, and then you take
            your histogram and plot a smooth line, that kind of fits the histogram. This still leaves the question of
            how do
            you choose the bins? The bin size in your histograms. How narrow do you take them?</p>
        <h2 id="unknown-576">未知</h2><h2>Unknown</h2>
        <p>这取决于你有多少数据，有很多理论可以告诉你选择 bin 大小的最佳方法，以及平滑数据的最佳方法。一个完全不同的主题是在信号处理中你想要进行推理。你不仅希望它很好，还希望它在计算上很快。</p><p>And that depends on how many data you have, and there’s a lot of theory that tells you about the best way of
            choosing the bin sizes, and the best ways of smoothing the data that you have. A completely different topic
            is
            in signal processing you want to do your inference. Not only you want it to be good, but you also want it to
            be
            fast in a computational way.</p>
        <p>你实时获得大量数据。你想在估算和决策来来去去时不断处理和修改它们。在前几节课中简要提到的另一个主题是，当你建立一个模型，比如线性回归模型时，你会选择一些解释变量，并尝试根据 X，也就是这些变量预测 y。你可以选择将哪些变量作为解释变量。</p><p>You get data in real time, lots of data. You want to keep processing and revising your estimates and your
            decisions as they come and go. Another topic that was briefly touched upon the last couple of lectures is
            that
            when you set up a model, like a linear regression model, you choose some explanatory variables, and you try
            to
            predict y from your X, these variables. You have a choice of what to take as your explanatory variables.</p>
        <p>是否有系统的方法可以挑选正确的 X 变量来估计 Y。例如，我应该尝试根据 X 估计 Y 吗？还是根据 X 平方估计 Y？我该如何在两者之间做出选择？最后，如今的流行趋势与任何大型、高维的东西有关。复杂事物的复杂模型，以及大量数据。因此，如今数据无处不在。数据量非常庞大。</p><p>Are there systematic ways of picking the right X variables to try to estimate a Y. For example should I try
            to
            estimate Y on the basis of X? Or on the basis of X squared? How do I decide between the two? Finally, the
            rage
            these days has to do with anything big, high demensional.complicated models of complicated things, and tons
            and
            tons of data. So these days data are generated everywhere. The amounts of data are humongous.</p>
        <h2 id="unknown-577">未知</h2><h2>Unknown</h2>
        <p>此外，人们感兴趣的问题往往非常复杂，包含大量参数。因此，我需要专门定制的方法，即使面对如此大量的数据，甚至可能存在计算限制，也能给出良好的结果，或者说不错的结果。因此，面对大量数据，您需要简单但仍然可以为您提供有意义答案的方法。</p><p>Also, the problems that people are interested in tend to be very complicated with lots of parameters. So I
            need
            specially tailored methods that can give you good results, or decent results even in the face of these huge
            amounts of data, and possibly with computational constraints. So with huge amounts of data you want methods
            that
            are simple, but still can deliver for you meaningful answers.</p>
        <p>正如我之前提到的，整个统计学领域与概率领域截然不同。从某种意义上说，我们在统计学中所做的一切都是概率计算。这就是理论所做的。但这里面有很大的艺术元素。您看到我们以一种有点临时的方式选择了一些决策区域或拒绝区域的形状。还有更基本的东西。您如何组织数据？</p><p>Now as I mentioned some time ago, this whole field of statistics is very different from the field of
            probability.
            In some sense all that we’re doing in statistics is probabilistic calculations. That’s what the theory kind
            of
            does. But there’s a big element of art. You saw that we chose the shape of some decision regions or
            rejection
            regions in a somewhat ad hoc way. There’s even more basic things. How do you organize your data?</p>
        <p>您如何考虑要测试哪些假设，等等。这里面涉及很多艺术，而且有很多可能出错的地方。所以最后我想说一句，你可以把它看作悲观的，也可以看作乐观的。几年前有一篇著名的论文，被引用了大约 1,000 次左右。</p><p>How do you think about which hypotheses you would like to test, and so on. There’s a lot of art that’s
            involved
            here, and there’s a lot that can go wrong. So I’m going to close with a note that you can take either as
            pessimistic or optimistic. There is a famous paper that came out a few years ago and has been cited about a
            1,000 times or so.</p>
        <h2 id="unknown-578">未知</h2><h2>Unknown</h2>
        <p>这篇论文的标题是《为什么大多数已发表的研究结果都是错误的》。这实际上是一个很好的论据，说明为什么在心理学或医学等领域，你所看到的很多已发表的研究，认为这种药物对某种疾病有效果，实际上是错误的，因为人们没有正确地进行统计。人们所做的研究有很多偏见。</p><p>And the title of the paper is Why Most Published Research Findings Are False. And it’s actually a very good
            argument why, in fields like psychology or the medical science and all that a lot of what you see published.
            that yes, this drug has an effect on that particular disease. is actually false, because people do not do
            their
            statistics correctly. There’s lots of biases in what people do.</p>
        <p>我的意思是，一个明显的偏见是，你只有看到某种药物后才会发表结果。因此，零假设就是药物不起作用。你做了测试，药物没有起作用，好吧，你只能回家哭了。但如果偶然发生了那 5%，即使药物不起作用，你也得到了一些异常数据，而且药物似乎有效。然后你很兴奋，就把它发表了。所以这显然是一种偏见。</p><p>I mean an obvious bias is that you only published a result when you see something. So the null hypothesis is
            that
            the drug doesn’t work. You do your tests, the drug didn’t work, OK, you just go home and cry. But if by
            accident
            that 5% happens, and even though the drug doesn’t work, you got some outlier data, and it seemed to be
            working.
            Then you’re excited, you publish it. So that’s clearly a bias.</p>
        <p>这样就可以发表结果，即使这些结果背后没有坚实的基础。还有另一件事，好吗？我选择了 5%。因此 H0 是正确的，数据看起来像异常值的概率很小，在这种情况下，我会发表我的结果。好吧，只有 5%，这种情况不会经常发生。但是假设我去做 1,000 次不同的测试呢？</p><p>That gets results to be published, even though they do not have a solid foundation behind them. Then there’s
            another thing, OK? I’m picking my 5%. So H0 is true there’s a small probability that the data will look like
            an
            outlier, and in that case I published my result. OK it’s only 5% it’s not going to happen too often. But
            suppose
            that I go and do a 1,000 different tests?</p>
        <h2 id="unknown-579">未知</h2><h2>Unknown</h2>
        <p>针对这个假设测试 H0，针对那个假设测试 H0，针对那个假设测试 H0。其中一些测试可能偶然地支持 H1，这些测试再次被选中发表。
        </p><p>Test H0 against this hypothesis, test H0 against that hypothesis,test H0 against that hypothesis. Some of
            these
            tests, just by accident might turn out to be in favor of H1, and again these are selected to be published.
        </p>
        <p>因此，如果你进行了大量测试，并且每次测试都有 5% 的错误概率，那么当你考虑所有这些测试的集合时，实际上做出错误推断的概率远远超过 5%。系统地进行此类研究的一个基本原则是，你应该首先选择要测试的假设，然后获取数据，并进行假设检验。</p><p>So if you do lots and lots of tests and in each one you have a 5% probability of error, when you consider the
            collection of all those tests, actually the probability of making incorrect inferences is a lot more than
            5%.
            One basic principle in being systematic about such studies is that you should first pick your hypothesis
            that
            you’re going to test, then get your data, and do your hypothesis testing.</p>
        <p>错误的做法是，获取数据，查看数据，然后说“好吧，我现在要测试这 100 个不同的假设，我将选择我的假设作为数据中看起来异常的特征”。好吧，只要有足够的数据，你总能偶然发现一些异常。如果你选择进行统计测试，这种异常存在吗？是的，它会出现。</p><p>What would be wrong is to get your data, look at them, and say OK I’m going now to test for these 100
            different
            hypotheses, and I’m going to choose my hypothesis to be for features that look abnormal in my data. Well,
            given
            enough data, you can always find some abnormalities just by chance. And if you choose to make a statistical
            test. is this abnormality present? Yes, it will be present.</p>
        <h2 id="unknown-580">未知</h2><h2>Unknown</h2>
        <p>因为你先发现了异常，然后才对其进行测试。所以这是事情出错的另一种方式。所以这个故事的寓意是，虽然概率世界真的很美丽、很可靠，但你有你的公理。每个问题都有一个独特的答案，现在你们都可以用一种非常可靠的方式找到答案。统计学是一项肮脏而困难的工作。这就是为什么这个话题还没有结束。</p><p>Because you first found the abnormality, and then you tested for it. So that’s another way that things can go
            wrong. So the moral of this story is that while the world of probability is really beautiful and solid, you
            have
            your axioms. Every question has a unique answer that by now you can, all of you, find in a very reliable
            way.
            Statistics is a dirty and difficult business. And that’s why the subject is not over.</p>
        <p>如果你对此感兴趣，那么值得继续学习该方向的课程。祝你在期末考试中好运，取得好成绩，然后度过一个愉快的假期。</p><p>And if you’re interested in it, it’s worth taking follow on courses in that direction. OK so have good luck
            in
            the final, do well, and have a nice vacation afterwards.</p>
    </div>




<script id="res-script" src="/res/dist/res/main.js" type="text/javascript"></script>
</body></html>
