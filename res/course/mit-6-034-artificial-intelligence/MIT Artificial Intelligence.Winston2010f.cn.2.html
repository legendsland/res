<!DOCTYPE html>
<html class="translated-ltr" style=""><!--
 Page saved with SingleFile 
 url: file:///home/zy/ws/src/prj/babel/src/note/course/mit-6-034-artificial-intelligence/res/lectures.html 
 saved date: Tue Sep 10 2024 12:34:57 GMT+0800 (Hong Kong Standard Time)
-->

<head>
    <meta name="dc.identifier" content="res/636f6a2a10f61624191d36b1814a49bf6c33ecde">
    <meta charset="utf-8">
    <style>
        .VIpgJd-ZVi9od-ORHb-OEVmcd {
            left: 0;
            top: 0;
            height: 39px;
            width: 100%;
            z-index: 10000001;
            position: fixed;
            border: none;
            border-bottom: 1px solid #6B90DA;
            margin: 0;
            box-shadow: 0 0 8px 1px #999
        }

        .VIpgJd-ZVi9od-xl07Ob-OEVmcd {
            z-index: 10000002;
            border: none;
            position: fixed;
            box-shadow: 0 3px 8px 2px #999
        }

        .VIpgJd-ZVi9od-SmfZ-OEVmcd {
            z-index: 10000000;
            border: none;
            margin: 0
        }

        .goog-te-gadget {
            font-family: arial;
            font-size: 11px;
            color: #666;
            white-space: nowrap
        }

        .goog-te-gadget img {
            vertical-align: middle;
            border: none
        }

        .goog-te-gadget-simple {
            background-color: #FFF;
            border-left: 1px solid #D5D5D5;
            border-top: 1px solid #9B9B9B;
            border-bottom: 1px solid #E8E8E8;
            border-right: 1px solid #D5D5D5;
            font-size: 10pt;
            display: inline-block;
            padding-top: 1px;
            padding-bottom: 2px;
            cursor: pointer
        }

        .goog-te-gadget-icon {
            margin-left: 2px;
            margin-right: 2px;
            width: 19px;
            height: 19px;
            border: none;
            vertical-align: middle
        }

        .goog-te-combo {
            margin-left: 4px;
            margin-right: 4px;
            vertical-align: baseline
        }

        .goog-te-gadget .goog-te-combo {
            margin: 4px 0
        }

        .VIpgJd-ZVi9od-l4eHX-hSRGPd,
        .VIpgJd-ZVi9od-l4eHX-hSRGPd:link,
        .VIpgJd-ZVi9od-l4eHX-hSRGPd:visited,
        .VIpgJd-ZVi9od-l4eHX-hSRGPd:hover,
        .VIpgJd-ZVi9od-l4eHX-hSRGPd:active {
            font-size: 12px;
            font-weight: bold;
            color: #444;
            text-decoration: none
        }

        .VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd,
        .VIpgJd-ZVi9od-TvD9Pc-hSRGPd {
            display: block;
            margin: 0 10px
        }

        .VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd {
            padding-top: 2px;
            padding-left: 4px
        }

        .goog-te-combo,
        .VIpgJd-ZVi9od-ORHb *,
        .VIpgJd-ZVi9od-SmfZ *,
        .VIpgJd-ZVi9od-xl07Ob *,
        .VIpgJd-ZVi9od-vH1Gmf *,
        .VIpgJd-ZVi9od-l9xktf * {
            font-family: arial;
            font-size: 10pt
        }

        .VIpgJd-ZVi9od-ORHb {
            margin: 0;
            background-color: #E4EFFB;
            overflow: hidden
        }

        .VIpgJd-ZVi9od-ORHb img {
            border: none
        }

        .VIpgJd-ZVi9od-ORHb-bN97Pc {
            color: #000
        }

        .VIpgJd-ZVi9od-ORHb-bN97Pc img {
            vertical-align: middle
        }

        .VIpgJd-ZVi9od-ORHb-Tswv1b {
            color: #666;
            vertical-align: top;
            margin-top: 0;
            font-size: 7pt
        }

        .VIpgJd-ZVi9od-ORHb-KE6vqe {
            width: 8px
        }

        .VIpgJd-ZVi9od-LgbsSe {
            border-color: #E7E7E7;
            border-style: none solid solid none;
            border-width: 0 1px 1px 0
        }

        .VIpgJd-ZVi9od-LgbsSe div {
            border-color: #CCC #999 #999 #CCC;
            border-right: 1px solid #999;
            border-style: solid;
            border-width: 1px;
            height: 20px
        }

        .VIpgJd-ZVi9od-LgbsSe button {
            background: transparent;
            border: none;
            cursor: pointer;
            height: 20px;
            overflow: hidden;
            margin: 0;
            vertical-align: top;
            white-space: nowrap
        }

        .VIpgJd-ZVi9od-LgbsSe button:active {
            background: none repeat scroll 0 0#CCC
        }

        .VIpgJd-ZVi9od-SmfZ {
            margin: 0;
            background-color: #FFF;
            white-space: nowrap
        }

        .VIpgJd-ZVi9od-SmfZ-hSRGPd {
            text-decoration: none;
            font-weight: bold;
            font-size: 10pt;
            border: 1px outset #888;
            padding: 6px 10px;
            white-space: nowrap;
            position: absolute;
            left: 0;
            top: 0
        }

        .VIpgJd-ZVi9od-SmfZ-hSRGPd img {
            margin-left: 2px;
            margin-right: 2px;
            width: 19px;
            height: 19px;
            border: none;
            vertical-align: middle
        }

        .VIpgJd-ZVi9od-SmfZ-hSRGPd span {
            text-decoration: underline;
            margin-left: 2px;
            margin-right: 2px;
            vertical-align: middle
        }

        .goog-te-float-top .VIpgJd-ZVi9od-SmfZ-hSRGPd {
            padding: 2px;
            border-top-width: 0
        }

        .goog-te-float-bottom .VIpgJd-ZVi9od-SmfZ-hSRGPd {
            padding: 2px;
            border-bottom-width: 0
        }

        .VIpgJd-ZVi9od-xl07Ob-lTBxed {
            text-decoration: none;
            color: #00C;
            white-space: nowrap;
            margin-left: 4px;
            margin-right: 4px
        }

        .VIpgJd-ZVi9od-xl07Ob-lTBxed span {
            text-decoration: underline
        }

        .VIpgJd-ZVi9od-xl07Ob-lTBxed img {
            margin-left: 2px;
            margin-right: 2px
        }

        .goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed {
            color: #000
        }

        .goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed span {
            text-decoration: none
        }

        .VIpgJd-ZVi9od-xl07Ob {
            background-color: #FFF;
            text-decoration: none;
            border: 2px solid #C3D9FF;
            overflow-y: scroll;
            overflow-x: hidden;
            position: absolute;
            left: 0;
            top: 0
        }

        .VIpgJd-ZVi9od-xl07Ob-ibnC6b {
            padding: 3px;
            text-decoration: none
        }

        .VIpgJd-ZVi9od-xl07Ob-ibnC6b,
        .VIpgJd-ZVi9od-xl07Ob-ibnC6b:link {
            color: #00C;
            background: #FFF
        }

        .VIpgJd-ZVi9od-xl07Ob-ibnC6b:visited {
            color: #551A8B
        }

        .VIpgJd-ZVi9od-xl07Ob-ibnC6b:hover {
            background: #C3D9FF
        }

        .VIpgJd-ZVi9od-xl07Ob-ibnC6b:active {
            color: #00C
        }

        .VIpgJd-ZVi9od-vH1Gmf {
            background-color: #FFF;
            text-decoration: none;
            border: 1px solid #6B90DA;
            overflow: hidden;
            padding: 4px
        }

        .VIpgJd-ZVi9od-vH1Gmf-KrhPNb {
            width: 16px
        }

        .VIpgJd-ZVi9od-vH1Gmf-hgDUwe {
            margin: 6px 0;
            height: 1px;
            background-color: #aaa;
            overflow: hidden
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div {
            padding: 4px
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b .uDEFge {
            display: none
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd .uDEFge {
            display: auto
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd .fmcmS {
            padding-left: 4px;
            padding-right: 4px
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd {
            text-decoration: none
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b:link div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b:visited div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b:active div {
            color: #00C;
            background: #FFF
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b:hover div {
            color: #FFF;
            background: #36C
        }

        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:link div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:visited div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:hover div,
        .VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:active div {
            color: #000;
            font-weight: bold
        }

        .VIpgJd-ZVi9od-l9xktf {
            background-color: #FFF;
            overflow: hidden;
            padding: 8px;
            border: none;
            border-radius: 10px
        }

        .VIpgJd-ZVi9od-l9xktf-OEVmcd {
            background-color: #FFF;
            border: 1px solid #6B90DA;
            box-shadow: 0 3px 8px 2px #999;
            border-radius: 8px
        }

        .VIpgJd-ZVi9od-l9xktf img {
            border: none
        }

        .VIpgJd-ZVi9od-l9xktf-fmcmS {
            margin-top: 6px
        }

        .VIpgJd-ZVi9od-l9xktf-VgwJlc {
            margin-top: 6px;
            white-space: nowrap
        }

        .VIpgJd-ZVi9od-l9xktf-VgwJlc * {
            vertical-align: middle
        }

        .VIpgJd-ZVi9od-l9xktf-VgwJlc .DUGJie {
            background-image: url(data:,)
        }

        .VIpgJd-ZVi9od-l9xktf-VgwJlc .TdyTDe {
            background-image: url(data:,)
        }

        .VIpgJd-ZVi9od-l9xktf-VgwJlc span {
            color: #00C;
            text-decoration: underline;
            cursor: pointer;
            margin: 0 4px
        }

        .VIpgJd-ZVi9od-l9xktf-I9GLp {
            margin: 6px 0 0
        }

        .VIpgJd-ZVi9od-l9xktf-I9GLp form {
            margin: 0
        }

        .VIpgJd-ZVi9od-l9xktf-I9GLp form textarea {
            margin-bottom: 4px;
            width: 100%
        }

        .VIpgJd-ZVi9od-l9xktf-yePe5c {
            margin: 6px 0 4px
        }

        .VIpgJd-ZVi9od-aZ2wEe-wOHMyf {
            z-index: 1000;
            position: fixed;
            -webkit-transition-delay: .6s;
            transition-delay: .6s;
            left: -1000px;
            top: -1000px
        }

        .VIpgJd-ZVi9od-aZ2wEe-wOHMyf-ti6hGc {
            -webkit-transition-delay: 0s;
            transition-delay: 0s;
            left: -14px;
            top: -14px
        }

        .VIpgJd-ZVi9od-aZ2wEe-OiiCO {
            display: -webkit-box;
            display: -webkit-flex;
            display: flex;
            -webkit-box-align: center;
            -webkit-align-items: center;
            align-items: center;
            -webkit-box-pack: center;
            -webkit-justify-content: center;
            justify-content: center;
            width: 104px;
            height: 104px;
            border-radius: 50px;
            background: #FFF url(data:image/png;base64,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)50% 50%no-repeat;
            -webkit-transition: all .6s ease-in-out;
            transition: all .6s ease-in-out;
            -webkit-transform: scale(.4);
            transform: scale(.4);
            opacity: 0
        }

        .VIpgJd-ZVi9od-aZ2wEe-OiiCO-ti6hGc {
            -webkit-transform: scale(.5);
            transform: scale(.5);
            opacity: 1
        }

        .VIpgJd-ZVi9od-aZ2wEe {
            margin: 2px 0 0 2px;
            -webkit-animation: spinner-rotator 1.4s linear infinite;
            animation: spinner-rotator 1.4s linear infinite
        }

        @-webkit-keyframes spinner-rotator {
            0% {
                -webkit-transform: rotate(0deg);
                transform: rotate(0deg)
            }

            100% {
                -webkit-transform: rotate(270deg);
                transform: rotate(270deg)
            }
        }

        @keyframes spinner-rotator {
            0% {
                -webkit-transform: rotate(0deg);
                transform: rotate(0deg)
            }

            100% {
                -webkit-transform: rotate(270deg);
                transform: rotate(270deg)
            }
        }

        .VIpgJd-ZVi9od-aZ2wEe-Jt5cK {
            stroke-dasharray: 187;
            stroke-dashoffset: 0;
            stroke: #4285F4;
            -webkit-transform-origin: center;
            transform-origin: center;
            -webkit-animation: spinner-dash 1.4s ease-in-out infinite;
            animation: spinner-dash 1.4s ease-in-out infinite
        }

        @-webkit-keyframes spinner-dash {
            0% {
                stroke-dashoffset: 187
            }

            50% {
                stroke-dashoffset: 46.75;
                -webkit-transform: rotate(135deg);
                transform: rotate(135deg)
            }

            100% {
                stroke-dashoffset: 187;
                -webkit-transform: rotate(450deg);
                transform: rotate(450deg)
            }
        }

        @keyframes spinner-dash {
            0% {
                stroke-dashoffset: 187
            }

            50% {
                stroke-dashoffset: 46.75;
                -webkit-transform: rotate(135deg);
                transform: rotate(135deg)
            }

            100% {
                stroke-dashoffset: 187;
                -webkit-transform: rotate(450deg);
                transform: rotate(450deg)
            }
        }

        .VIpgJd-yAWNEb-L7lbkb html,
        .VIpgJd-yAWNEb-L7lbkb body,
        .VIpgJd-yAWNEb-L7lbkb div,
        .VIpgJd-yAWNEb-L7lbkb span,
        .VIpgJd-yAWNEb-L7lbkb iframe,
        .VIpgJd-yAWNEb-L7lbkb h1,
        .VIpgJd-yAWNEb-L7lbkb h2,
        .VIpgJd-yAWNEb-L7lbkb h3,
        .VIpgJd-yAWNEb-L7lbkb h4,
        .VIpgJd-yAWNEb-L7lbkb h5,
        .VIpgJd-yAWNEb-L7lbkb h6,
        .VIpgJd-yAWNEb-L7lbkb p,
        .VIpgJd-yAWNEb-L7lbkb a,
        .VIpgJd-yAWNEb-L7lbkb img,
        .VIpgJd-yAWNEb-L7lbkb ol,
        .VIpgJd-yAWNEb-L7lbkb ul,
        .VIpgJd-yAWNEb-L7lbkb li,
        .VIpgJd-yAWNEb-L7lbkb table,
        .VIpgJd-yAWNEb-L7lbkb form,
        .VIpgJd-yAWNEb-L7lbkb tbody,
        .VIpgJd-yAWNEb-L7lbkb tr,
        .VIpgJd-yAWNEb-L7lbkb td {
            margin: 0;
            padding: 0;
            border: 0;
            font: inherit;
            font-size: 100%;
            vertical-align: baseline;
            text-align: left;
            line-height: normal
        }

        .VIpgJd-yAWNEb-L7lbkb ol,
        .VIpgJd-yAWNEb-L7lbkb ul {
            list-style: none
        }

        .VIpgJd-yAWNEb-L7lbkb table {
            border-collapse: collapse;
            border-spacing: 0
        }

        .VIpgJd-yAWNEb-L7lbkb caption,
        .VIpgJd-yAWNEb-L7lbkb th,
        .VIpgJd-yAWNEb-L7lbkb td {
            text-align: left;
            font-weight: normal
        }

        .VIpgJd-yAWNEb-L7lbkb input::-moz-focus-inner {
            border: 0
        }

        div>.VIpgJd-yAWNEb-L7lbkb {
            padding: 10px 14px
        }

        .VIpgJd-yAWNEb-L7lbkb {
            color: #222;
            background-color: #fff;
            border: 1px solid #eee;
            box-shadow: 0 4px 16px rgba(0, 0, 0, .2);
            -moz-box-shadow: 0 4px 16px rgba(0, 0, 0, .2);
            -webkit-box-shadow: 0 4px 16px rgba(0, 0, 0, .2);
            display: none;
            font-family: arial;
            font-size: 10pt;
            width: 420px;
            padding: 12px;
            position: absolute;
            z-index: 10000
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-nVMfcd-fmcmS,
        .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe {
            clear: both;
            font-size: 10pt;
            position: relative;
            text-align: justify;
            width: 100%
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-r4nke {
            color: #999;
            font-family: arial, sans-serif;
            margin: 4px 0;
            text-align: left
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TvD9Pc-LgbsSe {
            display: none
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-l4eHX {
            float: left;
            margin: 0
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-PLDbbf {
            display: inline-block
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-fw42Ze-Z0Arqf-haAclf {
            display: none;
            width: 100%
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-H9tDt {
            margin-top: 20px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-LK5yu {
            float: left
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-qwU8Me {
            float: right
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-cGMI2b {
            min-height: 15px;
            position: relative;
            height: 1%
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-jOfkMb-Ne3sFf {
            background: -webkit-linear-gradient(top, #29910d 0, #20af0e 100%);
            background: -webkit-gradient(linear, left top, left bottom, from(#29910d), to(#20af0e));
            background: linear-gradient(top, #29910d 0, #20af0e 100%);
            background: #29910d;
            border-radius: 4px;
            -moz-border-radius: 4px;
            -webkit-border-radius: 4px;
            box-shadow: inset 0 2px 2px #1e6609;
            -moz-box-shadow: inset 0 2px 2px #1e6609;
            -webkit-box-shadow: inset 0 2px 2px #1e6609;
            color: white;
            font-size: 9pt;
            font-weight: bolder;
            margin-top: 12px;
            padding: 6px;
            text-shadow: 1px 1px 1px #1e6609
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-hSRGPd {
            color: #15c;
            cursor: pointer;
            font-family: arial;
            font-size: 11px;
            margin-right: 15px;
            text-decoration: none
        }

        .VIpgJd-yAWNEb-L7lbkb>textarea {
            font-family: arial;
            resize: vertical;
            width: 100%;
            margin-bottom: 10px;
            border-radius: 1px;
            border: 1px solid #d9d9d9;
            border-top: 1px solid silver;
            font-size: 13px;
            height: auto;
            overflow-y: auto;
            padding: 1px
        }

        .VIpgJd-yAWNEb-L7lbkb textarea:focus {
            box-shadow: inset 0 1px 2px rgba(0, 0, 0, .3);
            border: 1px solid #4d90fe;
            outline: none
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-IbE0S {
            margin-right: 10px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp {
            min-height: 25px;
            vertical-align: middle;
            padding-top: 8px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp {
            margin-bottom: 5px;
            margin-bottom: 0
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input {
            display: inline-block;
            min-width: 54px;
            *min-width: 70px;
            border: 1px solid #dcdcdc;
            border: 1px solid rgba(0, 0, 0, .1);
            text-align: center;
            color: #444;
            font-size: 11px;
            font-weight: bold;
            height: 27px;
            outline: 0;
            padding: 0 8px;
            vertical-align: middle;
            line-height: 27px;
            margin: 0 16px 0 0;
            box-shadow: 0 1px 2px rgba(0, 0, 0, .1);
            -moz-box-shadow: 0 1px 2px rgba(0, 0, 0, .1);
            -webkit-box-shadow: 0 1px 2px rgba(0, 0, 0, .1);
            border-radius: 2px;
            -webkit-transition: all .218s;
            transition: all .218s;
            background-color: #f5f5f5;
            background-image: -webkit-gradient(linear, left top, left bottom, from(#f5f5f5), to(#f1f1f1));
            background-image: -webkit-linear-gradient(top, #f5f5f5, #f1f1f1);
            background-image: linear-gradient(top, #f5f5f5, #f1f1f1);
            -webkit-user-select: none;
            -moz-user-select: none;
            cursor: default
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover {
            border: 1px solid #c6c6c6;
            color: #222;
            -webkit-transition: all 0s;
            transition: all 0s;
            background-color: #f8f8f8;
            background-image: -webkit-gradient(linear, left top, left bottom, from(#f8f8f8), to(#f1f1f1));
            background-image: -webkit-linear-gradient(top, #f8f8f8, #f1f1f1);
            background-image: linear-gradient(top, #f8f8f8, #f1f1f1)
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active {
            border: 1px solid #c6c6c6;
            color: #333;
            background-color: #f6f6f6;
            background-image: -webkit-gradient(linear, left top, left bottom, from(#f6f6f6), to(#f1f1f1));
            background-image: -webkit-linear-gradient(top, #f6f6f6, #f1f1f1);
            background-image: linear-gradient(top, #f6f6f6, #f1f1f1)
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus:active {
            box-shadow: inset 0 0 0 1px rgba(255, 255, 255, .5);
            -webkit-box-shadow: inset 0 0 0 1px rgba(255, 255, 255, .5);
            -moz-box-shadow: inset 0 0 0 1px rgba(255, 255, 255, .5)
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe {
            outline: none;
            border: 1px solid #4d90fe;
            z-index: 4 !important
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.gk6SMd {
            background-color: #eee;
            background-image: -webkit-gradient(linear, left top, left bottom, from(#eee), to(#e0e0e0));
            background-image: -webkit-linear-gradient(top, #eee, #e0e0e0);
            background-image: linear-gradient(top, #eee, #e0e0e0);
            box-shadow: inset 0 1px 2px rgba(0, 0, 0, .1);
            border: 1px solid #ccc;
            color: #333
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf {
            color: white;
            border-color: #3079ed;
            background-color: #4d90fe;
            background-image: -webkit-gradient(linear, left top, left bottom, from(#4d90fe), to(#4787ed));
            background-image: -webkit-linear-gradient(top, #4d90fe, #4787ed);
            background-image: linear-gradient(top, #4d90fe, #4787ed)
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active {
            border-color: #3079ed;
            background-color: #357ae8;
            background-image: -webkit-gradient(linear, left top, left bottom, from(#4d90fe), to(#357ae8));
            background-image: -webkit-linear-gradient(top, #4d90fe, #357ae8);
            background-image: linear-gradient(top, #4d90fe, #357ae8)
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover {
            box-shadow: inset 0 0 0 1px #fff, 0 1px 1px rgba(0, 0, 0, .1);
            -webkit-box-shadow: inset 0 0 0 1px #fff, 0 1px 1px rgba(0, 0, 0, .1);
            -moz-box-shadow: inset 0 0 0 1px #fff, 0 1px 1px rgba(0, 0, 0, .1)
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover {
            border-color: #3079ed
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-mrxPge {
            color: #999;
            font-family: arial, sans-serif
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-W0vJo-fmcmS {
            color: #999;
            font-size: 11px;
            font-family: arial, sans-serif;
            margin: 15px 0 5px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-u0pjoe-fmcmS {
            color: #800;
            display: none;
            font-size: 9pt
        }

        .VIpgJd-yAWNEb-VIpgJd-fmcmS-sn54Q {
            background-color: #c9d7f1;
            box-shadow: 2px 2px 4px #99a;
            box-sizing: border-box;
            -webkit-box-sizing: border-box;
            -moz-box-sizing: border-box;
            position: relative
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-xl07Ob {
            background: #fff;
            border: 1px solid #ddd;
            box-shadow: 0 2px 4px #99a;
            min-width: 0;
            outline: none;
            padding: 0;
            position: absolute;
            z-index: 2000
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb {
            cursor: pointer;
            padding: 2px 5px 5px;
            margin-right: 0;
            border-style: none
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb:hover {
            background: #ddd
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb h1 {
            font-size: 100%;
            font-weight: bold;
            margin: 4px 0
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb strong {
            color: #345aad
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo {
            text-align: right;
            position: absolute;
            right: 0;
            left: auto
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-j7LFlb-SIsrTd .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo {
            text-align: left;
            position: absolute;
            left: 0;
            right: auto
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-fmcmS,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q {
            background-color: #f1ea00;
            border-radius: 4px;
            -webkit-border-radius: 4px;
            -moz-border-radius: 4px;
            box-shadow: rgba(0, 0, 0, .5) 3px 3px 4px;
            box-sizing: border-box;
            -webkit-box-sizing: border-box;
            -moz-box-sizing: border-box;
            color: #f1ea00;
            cursor: pointer;
            margin: -2px -2px -2px -3px;
            padding: 2px 2px 2px 3px;
            position: relative
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q {
            color: #222
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe {
            color: white;
            position: absolute !important
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf,
        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q {
            background-color: #c9d7f1;
            border-radius: 4px 4px 0 0;
            -webkit-border-radius: 4px 4px 0 0;
            -moz-border-radius: 4px 4px 0 0;
            box-shadow: rgba(0, 0, 0, .5) 3px 3px 4px;
            box-sizing: border-box;
            -webkit-box-sizing: border-box;
            -moz-box-sizing: border-box;
            cursor: pointer;
            margin: -2px -2px -2px -3px;
            padding: 2px 2px 3px 3px;
            position: relative
        }

        .VIpgJd-yAWNEb-L7lbkb span:focus {
            outline: none
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-DyVDA {
            background-color: transparent;
            border: 1px solid #4d90fe;
            border-radius: 0;
            -webkit-border-radius: 0;
            -moz-border-radius: 0;
            margin: -2px;
            padding: 1px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-LzX3ef {
            border-left: 2px solid red;
            margin-left: -2px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-YIAiIb {
            border-right: 2px solid red;
            margin-right: -2px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf {
            padding: 2px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS {
            font-size: 11px;
            padding: 2px 2px 3px;
            margin: 0;
            background-color: #fff;
            color: #333;
            border: 1px solid #d9d9d9;
            border-top: 1px solid #c0c0c0;
            display: inline-block;
            vertical-align: top;
            height: 21px;
            box-sizing: border-box;
            -webkit-box-sizing: border-box;
            -moz-box-sizing: border-box;
            -webkit-border-radius: 1px
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:hover {
            border: 1px solid #b9b9b9;
            border-top: 1px solid #a0a0a0;
            box-shadow: inset 0 1px 2px rgba(0, 0, 0, .1)
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:focus {
            box-shadow: inset 0 1px 2px rgba(0, 0, 0, .3);
            outline: none;
            border: 1px solid #4d90fe
        }

        .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-sFeBqf {
            font-size: 11px;
            padding: 2px 6px 3px;
            margin: 0 0 0 2px;
            height: 21px
        }

        .VIpgJd-yAWNEb-hvhgNd {
            font-family: "Google Sans", Arial, sans-serif
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-l4eHX-i3jM8c {
            position: absolute;
            top: 10px;
            left: 14px
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-l4eHX-SIsrTd {
            position: absolute;
            top: 10px;
            right: 14px
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif-i3jM8c,
        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif-SIsrTd {
            margin: 16px;
            padding: 0
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-IuizWc {
            margin: 0 0 0 36px;
            padding: 0;
            color: #747775;
            font-size: 14px;
            font-weight: 500
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif-SIsrTd .VIpgJd-yAWNEb-hvhgNd-IuizWc {
            text-align: right;
            margin: 0 36px 0 0
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1 {
            width: auto;
            padding: 12px 0 0;
            color: #1f1f1f;
            font-size: 16px;
            text-align: initial
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1 .VIpgJd-yAWNEb-SIsrTd {
            text-align: right
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid {
            border-radius: 0 0 12px 12px;
            margin: 0;
            background: #f1f4f9;
            position: relative;
            min-height: 50px
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid .VIpgJd-yAWNEb-SIsrTd {
            text-align: right
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od {
            display: inline-block;
            width: 77%;
            padding: 12px
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd {
            text-align: right
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-UTujCb {
            color: #1f1f1f;
            font-size: 12px;
            font-weight: 500
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-UTujCb {
            text-align: right
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-eO9mKe {
            color: #444746;
            font-size: 12px;
            padding-top: 4px
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-eO9mKe {
            text-align: right
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5 {
            position: absolute;
            top: 10px;
            right: 5px
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5 .VIpgJd-yAWNEb-SIsrTd {
            left: 5px;
            right: auto
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-THI6Vb {
            fill: #0b57d0
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf {
            margin: -4px 2px 0 0;
            padding: 2px 0 0;
            width: 48px;
            height: 48px;
            border: none;
            border-radius: 24px;
            cursor: pointer;
            background: none
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf:hover {
            background: #e8ebec
        }

        .VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-aXYTce {
            display: none
        }

        sentinel {}
    </style>
    <meta name="referrer" content="no-referrer">
    <style>
        img[src="data:,"],
        source[src="data:,"] {
            display: none !important
        }
    </style>
    <title>MIT 6.034 Artificial Intelligence Fall 2010</title>
    <link id="res-style" rel="stylesheet" href="/res/dist/res/style.css" type="text/css">
</head>

<body>
    <div id="book-container">
        <h1 id="introduction-and-scope">1. 简介和范围</h1>
        <h1>1. Introduction and Scope</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 47:19 <a
                href="https://youtube.com/watch?v=TjZBTDzGeGg">https://youtube.com/watch?v=TjZBTDzGeGg</a></p>
        <p> 11 years ago (Jan 11, 2014) — 47:19 <a
                href="https://youtube.com/watch?v=TjZBTDzGeGg">https://youtube.com/watch?v=TjZBTDzGeGg</a></p>
        <h2 id="summary">概括</h2>
        <h2>Summary</h2>
        <p>帕特里克·温斯顿欢迎学生来到
            6034，他讨论了人工智能的范围，将其定义为支持建立模型以促进对思维、感知和行动的理解的表征研究。他强调了模型和表征在人工智能中的重要性，并提供了陀螺仪和农夫、狐狸、鹅和谷物问题等例子。温斯顿强调，人工智能还涉及由表征所揭示的约束所支持的算法，这些约束是对思维、感知和行动进行建模。他介绍了生成和测试的概念，这是一种简单但功能强大的问题解决方法。温斯顿最后提到了这门课程的目的：构建更智能的程序并研究智能的本质。
        </p>
        <p>Patrick Winston, welcoming students to 6034, discusses the scope of artificial intelligence, defining it as
            the study of representations that support the making of models to facilitate understanding of thinking,
            perception, and action. He emphasizes the importance of models and representations in artificial
            intelligence, providing examples such as gyroscopes and the farmer, fox, goose, and grain problem. Winston
            highlights that artificial intelligence is also about algorithms enabled by constraints exposed by
            representations that model thinking, perception, and action. He introduces the concept of generate and test,
            a simple yet powerful problem-solving method. Winston concludes by mentioning the course’s purpose: to build
            smarter programs and study the nature of intelligence.</p>
        <h2 id="introduction">介绍</h2>
        <h2>Introduction</h2>
        <p>帕特里克·温斯顿：欢迎来到
            6034。我不知道我是否能应付这个麦克风。我们拭目以待。今年会是丰收的一年。我们迎来了一群有趣的人。看看二十年前人们给孩子起什么名字总是很有趣。我发现他们被艾米丽的名字淹没了。彼得、保罗和玛丽的名字并不多，但足以在某个时候唱出一首合适的歌曲。
        </p>
        <p>PATRICK WINSTON: Welcome to 6034. I don’t know if I can deal with this microphone. We’ll see what happens.
            It’s going to be a good year. We’ve got a bunch of interesting people. It’s always interesting to see what
            people named their children two decades ago. And I find they were overwhelmed with Emilys. And there are not
            too many Peters, Pauls, and Marys, but enough to call forth a suitable song at some point.</p>
        <p>我们有很多 Jesses，有男有女。我们有一个 Duncan，Duncan 在哪里？Duncan，你在这里。你换了发型。我想向大家保证，考德爵士本学期不会上这门课。今天我要讲的是人工智能，以及这个主题的内容。在过去
            24 小时内，名册的人员流动率约为 10%。</p>
        <p>We have lots of Jesses of both genders. We have a of both genders. And we have a Duncan, where’s Duncan?
            There you are, Duncan. You’ve changed your hairstyle. I want to assure use that the Thane of Cawdor is not
            taking the course this semester. What I’m going to do is tell you about artificial intelligence today, and
            what this subject is about. There’s been about a 10% percent turnover in the roster in the last 24 hours.
        </p>
        <p>我预计在接下来的 24 小时内，营业额还会再增加
            10%。所以我知道你们中的许多人都是观光客，想知道这是否是你们想做的事情。所以我将告诉你们我们这学期要做什么，以及你们离开这里后会知道什么。我将带你们了解这个大纲。我将首先讨论什么是人工智能，以及我们为什么要这样做。
        </p>
        <p>I expect another 10% turnover in the next 24 hours, too. So I know many of you are sightseers, wanting to
            know if this is something you want to do. So I’m going to tell you about what we’re going to do this
            semester, and what you’ll know when you get out of here. I’m going to walk you through this outline. I’m
            going to start by talking about what artificial intelligence is, and why we do it.</p>
        <p>然后我会给你们讲一点人工智能的历史，最后讲一下我们上课时遵守的一些约定。其中一条约定就是，请不要带笔记本电脑。</p>
        <p>And then I’ll give you a little bit of the history of artificial intelligence, and conclude with some of the
            covenants by which we run the course. One of which is no laptops, please.</p>
        <h2 id="artificial-intelligence">人工智能</h2>
        <h2>Artificial Intelligence</h2>
        <p>我会在最后解释为什么我们有这些约定。那么它是什么呢？嗯，它一定与思考有关。那么让我们从这里开始，人工智能的定义，说它与思考有关，不管思考是什么。</p>
        <p>I’ll explain why we have these covenants at the end. So what is it? Well, it must have something to do with
            thinking. So let’s start up here, a definition of artificial intelligence, by saying that it’s about
            thinking, whatever that is.</p>
        <p>我对人工智能的定义相当广泛。所以我们会说人工智能不仅与思考有关。它还与感知有关，与行动有关。如果这是一门哲学课，那么我会就此打住，说，在这门课上，我们将讨论涉及思考、感知和行动的问题。但这不是一门哲学课。这是第六门课程。这是一门工程学院的课程。这是麻省理工学院的课程。
        </p>
        <p>My definition of artificial intelligence has to be rather broad. So we’re going to say it’s not only about
            thinking. It’s also about perception, and it’s about action. And if this were a philosophy class, then I’d
            stop right there and just say, in this subject we’re going to talk about problems involving thinking,
            perception, and action. But this is not a philosophy class. This a Course six class. It’s an engineering
            school class. It’s an MIT class.</p>
        <p>所以我们需要的不止这些。因此我们将讨论针对思考、感知和行动的模型。这对你来说应该不陌生，因为模型制作正是麻省理工学院的特色。你在酒吧遇到某人，或者亲戚问你在麻省理工学院做什么，你的下意识反应是说，我们学会了如何建立模型。这就是我们在麻省理工学院所做的事情。
        </p>
        <p>So we need more than that. And therefore we’re going to talk about models that are targeted at thinking,
            perception, and action. And this should not be strange to you, because model making is what MIT is about.
            You run into someone at a bar, or relative asks you what you do at MIT, the right knee jerk reaction is to
            say, we learned how to build models. That’s what we do at MIT.</p>
        <p>我们用微分方程建立模型。我们用概率建立模型。我们用物理和计算模拟建立模型。无论我们做什么，我们都会建立模型。即使在人文学科课堂上，麻省理工学院的方法也是建立模型，我们可以用这些模型来解释过去、预测未来、理解主题和控制世界。这就是麻省理工学院的意义所在。这也是这门学科的意义所在。现在，我们的模型是思维模型。
        </p>
        <p>We build the models using differential equations. We build models using probabilities. We build models using
            physical and computational simulations. Whatever we do, we build models. Even in humanities class, MIT
            approach is to make models that we can use to explain the past, predict the future, understand the subject,
            and control the world. That’s what MIT is about. And that’s what this subject is about, too. And now, our
            models are models of thinking.</p>
        <p>所以你可能会说，如果我学习这门经典课程，我会变得更聪明吗？答案是肯定的。你会变得更聪明。因为你会有更好的自我思维模型，不仅仅是学科的主题，还有更好的自我思维模型。所以这些模型针对的是思考、感知和行动。我们知道这还不够，因为为了有一个模型，你必须有代表性。
        </p>
        <p>So you might say, if I take this classic will I get smarter? And the answer is yes. You will get smarter.
            Because you’ll have better models of your own thinking, not just the subject matter of the subject, but
            better models of your own thinking. So models targeted at thinking, perception, and action. We know that’s
            not quite enough, because in order to have a model, you have to have representation.</p>
        <p>假设人工智能是关于支持模型制作的表征，以促进对思维、感知和行动的理解。现在你可能会问我，那么表征是什么？它能有什么用处？所以我想花点时间向你们介绍一下陀螺仪。你们中的许多人都有从事机械工程的朋友。让他们难堪的最好方法之一就是说这是自行车车轮。
        </p>
        <p>So let’s say that artificial intelligence is about representations that support the making of models to
            facilitate an understanding of thinking, perception, and action. Now you might say to me, well what’s a
            representation? And what good can it do? So I’d like to take a brief moment to tell you about gyroscopes.
            Many of you have friends in mechanical engineering. One of the best ways embarrass them is to say here’s a
            bicycle wheel.</p>
        <p>如果我旋转它，然后用力吹一下，在轮子的边缘，它会朝这边还是那边转动？我保证他们会将手放在一种叫做右手螺旋定则的姿势下，这个名字很贴切，因为使用这个定则的人往往有 50%
            的时间能得到正确的答案。但我们再也不会犯这种错误了。</p>
        <p>And if I spin it, and blow hard on it right here, on the edge of the wheel, is going to turn over this way or
            this way? I guarantee that what they will do is they’ll put their hand in an arthritic posture called the
            right hand screw rule, aptly named because people who use it tend to get the right answer about 50% of the
            time. But we’re never going to make that mistake again.</p>
        <p>因为我们是电气工程师，不是机械工程师。我们了解表示法。我们要做的就是稍微思考一下。我们将使用一些胶带来帮助我们思考车轮的某个部分。因此，我希望你们只考虑车轮飞过顶部时的那个部分，然后我像那样吹它。</p>
        <p>Because we’re electrical engineers, not mechanical engineers. And we know about representation. What we’re
            going to do is we’re going to think about it a little bit. And we’re going to use some duct tape to help us
            think about just one piece of the wheel. So I want you to just think about that piece of the wheel as the
            wheel comes flying over the top, and I blow on it like that.</p>
        <p>那一块会发生什么？它会朝那个方向掉下去，对吧？下一块也会朝那个方向掉下去。所以当它过来时，它必须朝那个方向掉下去。让我在这里做一些基础工作以确保万无一失。这种感觉非常强烈。试试看。我们需要一个演示。我不会让任何人认为我在作弊。所以让我们把它扭转成这样或那样。
        </p>
        <p>What’s going to happen to that one piece? It’s going to go off that way, right? And the next piece is going
            to go off that way too. So when it comes over, it has to go that way. Let me do some ground f here just to
            be sure. It’s very powerful feeling. Try it. We need a demonstration. I don’t anybody think that I’m
            cheating, here. So let’s just twist it one way or the other.</p>
        <p>所以这很有吸引力，不是吗？亚历克斯现在再也不会弄错陀螺仪了，因为他得到了正确的表示。你们在这门课上要积累的大部分知识是一套表示，它们将帮助你构建智能程序。但我想给你们举第二个例子，一个更具有计算性的例子。</p>
        <p>So that’s powerful pull, isn’t it. Alex is now never going to get the gyroscope wrong, because he’s got the
            right representation. So much of what you’re going to accumulate in this subject is a suite of
            representations that will help you to build programs that are intelligent. But I want to give you a second
            example, one a little bit more computational.</p>
        <p>但在大多数情况下，到你上一年级的时候，你对其中一个问题就非常熟悉了。那就是农夫、狐狸、鹅和谷物的问题。有一条河，一艘漏水的划艇，只能载着农夫和他的四件物品之一。那么，这个问题的正确表现是什么呢？它可能是一张农夫的照片。它可能是一首关于这种情况的诗，也许是一首俳句。
        </p>
        <p>But one of which was very familiar to you by the time you went to first grade, in most cases. It’s the
            problem of the farmer, the fox, the goose, and the grain. There’s a river, a leaky rowboat that can only
            carry the farmer, and one of his four possessions. So what’s the right representation for this problem? It
            might be a picture of the farmer. It might be a poem about the situation, perhaps a haiku.</p>
        <p>我们知道这些都不是正确的表示。不知何故，我们觉得正确的表示最能体现出这种场景中参与者的位置。所以我们可以画一张这样的图。这是场景，这里是鲜艳的绿色，代表着我们被藻类侵袭的河流。这是农夫、狐狸、鹅和谷物。这是初始情况。
        </p>
        <p>We know that those are not the right representation. Somehow, we get the sense that the right representation
            most involve something about the location of the participants in this scenario. So we might draw a picture
            that looks like this. There’s the scenario, and here in glorious green, representing our algae infested
            rivers is the river. And here’s the farmer, the fox, the goose, and the grain. An initial situation.</p>
        <p>现在还有其他类似情况。我们有一条河，农夫和鹅在河的那边。狐狸和谷物也在河的那边。我们知道农夫可以从一种情况移动到另一种情况。所以现在我们开始解决这个问题了。这是麻省理工学院解决农夫、狐狸、鹅和谷物问题的方法。</p>
        <p>Now there are other situations like this one, for example. We have the river, and the farmer, and the goose
            is on that side. And the fox and the grain is on that side. And we know that the farmer can execute a
            movement from one situation to another. So now we’re getting somewhere where with the problem. This is at
            MIT approach to the farmer, fox, goose, and grain problem.</p>
        <p>当你还是个小孩的时候，你可能被这个问题难住了。有多少这样的情况？你怎么看，Tanya？在我看来，所有四个人都可以在河的这一边或另一边。所以对于农夫可以处于的每个位置，其他每个东西都可以在河的两边。所以是二的四次方，她毫不犹豫地说道。
        </p>
        <p>It might have stumped you when you were a little kid. How many such situations are there? What do you think,
            Tanya? It looks to me like all four of individuals can be on one side or the other. So for every position
            the farmer can be, each of the other things can be on either side of the river. So it would be two to the
            fourth she says aggressively and without hesitation.</p>
        <p>是的，2 的 4 次方，16
            种可能性。所以我们实际上可以画出整个图表。它足够小。这里还有另一个位置，有农夫、狐狸、鹅和谷物。事实上，这就是我们想要的。如果我们画出整个图表，它看起来就像这样。这是情况和它们之间允许的联系的图表。为什么不是
            16？因为另一个。我有多少？四个？十个？</p>
        <p>Yes, two to the fourth, 16 possibilities. So we could actually draw out the entire graph. It’s small enough.
            There’s another position over here with the farmer, fox, goose, and grain. And in fact that’s the one we
            want. And if we draw out the entire graph, it looks like this. This is a graph of the situations and the
            allowed connections between them. Why are there not 16? Because the other. how many have I got? Four? 10?
        </p>
        <p>其他情况是有人被吃掉。所以我们不想去任何那些地方。所以有了表示，神奇的事情就发生了。我们暴露了我们的约束。这就是我们建立表示的原因。这就是你在高中学习代数的原因，因为代数符号揭示了约束，这使得我们能够真正计算出你在报纸上刊登的广告数量能吸引多少客户。
        </p>
        <p>The others are situations in which somebody gets eaten. So we don’t want to go to any of those places. So
            having got the representation, something magical has happened. We’ve got our constraints exposed. And that’s
            why we build representations. That’s whey you algebra in high school, because algebraic notation exposes the
            constraints that make it possible to actually figure out how many customers you get for the number of
            advertisements you place in the newspaper.</p>
        <p>因此，人工智能是关于表征所暴露的约束，这些约束支持针对思考的模型。实际上还有一件事。还没有完成。因为毕竟，最终我们必须构建程序。因此，它是关于由表征所暴露的约束支持的算法，这些表征模拟针对思考、感知和行动。</p>
        <p>So artificial intelligence is about constraints exposed by representations that support models targeted to
            thinking. actually there’s one more thing, too. Not quite done. Because after all, in the end, we have to
            build programs. So it’s about algorithms enabled by constraints exposed by representations that model
            targeted thinking, perception, and action.</p>
        <h2 id="algorithms">算法</h2>
        <h2>Algorithms</h2>
        <p>因此，这些算法，或者我们可以称它们为程序，或者我们可以称它们为方法，无论您喜欢什么。</p>
        <p>So these algorithms, or we might call them just as well procedures, or we might call them just as well
            methods, whatever you like.</p>
        <p>这些就是人工智能的内容。方法、算法、表示。我想再举一个例子。在人工智能中，我们称之为生成测试。这是一个非常简单的想法，你再也不会在这个主题中听到它了。但你需要将它添加到你的问题解决方法、技术、程序和算法库中。这就是它的工作原理。
        </p>
        <p>These are the stuff of what artificial intelligence is about. methods, algorithms, representations. I’d like
            to give you one more example. It’s something we call, in artificial intelligence, generated test. And it’s
            such a simple idea, you’ll never hear it again in this subject. But it’s an idea you need to add to your
            repertoire of problem solving methods, techniques, procedures, and algorithms. So here’s how it works.</p>
        <p>也许我可以通过举例来更好地解释。这是我在去上课的路上从一棵树上摘下的一片树叶。我希望这不是这个物种的最后一片。它是什么，什么树？我不知道。我从来没有学过我的树木，我的颜色，或者我的乘法表。所以我必须回到这本书，奥杜邦学会北美树木实地指南。
        </p>
        <p>Maybe I can explain to best by starting off with an example. Here’s a tree leaf I picked off a tree on the
            way over to class. I hope it’s not the last of the species. What is it, what kind of tree? I don’t know. I
            never did learn my trees, or my colors, or my multiplication tables. So I have to go back to this book, the
            Audubon Society Field Guide to North American Trees.</p>
        <p>那么我该如何解决这个问题呢？这很简单。我只需一页一页地翻，直到找到看起来像这片叶子的东西。然后我发现它是一棵梧桐树，或者别的什么。麻省理工学院到处都是这样的树。所以当我这样做的时候，我会做一些非常直观、非常自然的事情，这也是你经常做的事情。但我们要给它起个名字。我们称之为生成和测试。
        </p>
        <p>And how would I solve the problem? It’s pretty simple. I just turn the pages one at a time, until I find
            something that looks like this leaf. And then I discover it’s a sycamore, or something. MIT’s full of them.
            So when I do that, I do something very intuitive, very natural, something you do all the time. But we’re
            going to give it a name. We’re going to call it generate and test.</p>
        <p>生成和测试方法包括生成一些可能的解决方案，将它们放入测试它们的盒子中，然后从另一端出来的大多是失败。</p>
        <p>And generate and test method consists of generating some possible solutions, feeding them into a box that
            tests them, and then out the other side comes mostly failures.</p>
        <h2 id="generating-tests">生成测试</h2>
        <h2>Generating Tests</h2>
        <p>但偶尔我们也会得到一些成功并让我们高兴的事情。这就是我对那片叶子所做的。但现在你有了它的名字。一旦你给某样东西起了名字，你就拥有了控制它的权力。你可以开始谈论它了。</p>
        <p>But every once in a while we get something that succeeds and pleases us. That’s what I did with the leaf. But
            now you have a name for it. Once you have a name for something, you get power over it. You can start to talk
            about it.</p>
        <p>所以我可以说，如果你正在使用生成和测试方法解决问题，你最好构建一个具有某些属性的生成器，这些属性可以使生成器更好。例如，它们不应该是多余的。它们不应该给你两次相同的解决方案。它们应该是可参考的。它们应该能够吸收信息，例如，这是一棵落叶树。不要费心去看针叶树。所以一旦你有了某样东西的名字，你就可以开始谈论它了。
        </p>
        <p>So I can say, if you’re doing a generate and test approach to a problem, you better build a generator with
            certain properties that make generators good. For example, they should not be redundant. They shouldn’t give
            you the same solution twice. They should be informable. They should be able to absorb information such as,
            this is a deciduous tree. Don’t bother looking at the conifers. So once you have a name for something, you
            can start talking about.</p>
        <h2 id="rumpelstiltskin">侏儒怪</h2>
        <h2>Rumpelstiltskin</h2>
        <p>而这些词汇会给你力量。所以我们可能称之为 Rumpelstiltskin 原则。这是我们今天的第一个强大想法。这个主题充满了强大的想法。每节课都会有一些。Rumpelstiltskin
            原则说，一旦你能说出某样东西的名字，你就拥有了控制它的力量。你知道鞋带末端的那个小东西是什么吗？很有趣。她疯狂地做着手势。这也是我们稍后会讨论的事情。</p>
        <p>And that vocabulary gives you power. So we call this the Rumpelstiltskin Principle perhaps. The first of our
            powerful ideas for the day. This subject is full of powerful ideas. There will be some in every class.
            Rumpelstiltskin Principle says that once you can name something, you get power over it. You know what that
            little thing is on the end of your shoelace? It’s interesting. She’s gesturing like mad. That’s something
            we’ll talk about later, too.</p>
        <p>运动方面的东西，以及它如何帮助我们思考。它是什么？没人知道吗？它是农业方面的，对吧？它是肩带，很好。所以一旦你知道了名字，你就可以开始谈论了。你可以说肩带的用途就像绳子末端的鞭子。它防止绳子松开。现在你有地方挂上这些知识了。
        </p>
        <p>Motor stuff, and how it helps us think. What is it? No one knows? It’s an ag something, right? It’s an aglet,
            very good. So once you have the name, you can start to talk about. You can say the purpose of an aglet is
            pretty much like the whipping on the end of a rope. It keeps the thing from unwinding. Now you have a place
            to hang that knowledge.</p>
        <p>因此，从现在到本学期的剩余时间，我们都会经常谈论命名事物的力量。符号标签赋予我们控制概念的力量。既然我们在这里，我还应该说这是一个非常简单的想法，即生成和测试。你可能会想对某人说，我们今天学习了生成和测试。但这是一个微不足道的想法。
        </p>
        <p>So we’re talking about this frequently from now into the rest of the semester, the power of being able to
            name things. Symbolic labels give us power over concepts. While we’re here I should also say that this is a
            very simple idea, generate and test. And you might be tempted to say to someone, we learned about generate
            and test today. But it’s a trivial idea.</p>
        <p>我希望你们从词汇表中剔除“琐碎”这个词，因为它是一个非常危险的标签。它之所以危险是因为琐碎和简单之间是有区别的。这是什么区别？给某件事贴上琐碎的标签和称其为简单有什么区别？是吗？正是如此。他说简单可以很有力量，而琐碎听起来不仅简单，而且价值不大。
        </p>
        <p>The word trivial is a word I would like you to purge from your vocabulary, because it’s a very dangerous
            label. The reason it’s dangerous is because there’s a difference between trivial and simple. What is it?
            What’s the difference between labeling something as trivial and calling it simple? Yes? Exactly so. He says
            that simple can be powerful, and trivial makes it sound like it’s not only simple, but of little worth.</p>
        <p>很多麻省理工学院的人都错失了机会，因为他们倾向于认为，除非想法很复杂，否则想法就不重要。但人工智能中最简单的想法往往是最强大的。我们可以教你一门人工智能课程，这门课程充满了数学，甚至会让第 18
            门课程的教授感到恶心。但这些想法只是过于复杂、过于数学化，而且过于不简单。简单的想法往往是最强大的。</p>
        <p>So many MIT people miss opportunities, because they have a tendency to think that ideas aren’t important
            unless they’re complicated. But the most simple ideas in artificial intelligence are often the most
            powerful. We could teach an artificial intelligence course to you that would be so full of mathematics it
            would make a Course 18 professor gag. But those ideas would be merely gratuitously complicated, and
            gratuitously mathematical, and gratuitously not simple. Simple ideas are often the most powerful.</p>
        <p>那么我们目前讲到了哪里？我们讨论了定义。我们讨论了方法的示例。向您展示了一种表示，也许还讨论了第一个想法。您已经正确理解了表示，通常您已经快完成了。因为有了这个表示，他们可以立即看到这个问题只有两个解决方案，这是我们小时候不会想到的，我们当时没有想到要画出这个图表。
        </p>
        <p>So where are we so far? We talked about the definition. We talked about an example of a method. Showed you a
            representation, and perhaps also talked about the first idea, too. You’ve got the representation right,
            you’re often almost done. Because with this representation, they can immediately see that there are just two
            solutions to this problem, something that wouldn’t have occurred to us when we were little kids, and didn’t
            think to draw the diagram.</p>
        <p>还有一件事。在过去和其他地方，人工智能通常被教导为纯粹的推理。但我们用眼睛和符号装置解决问题。而你用眼睛解决了这个问题。所以我想通过给你一个小谜题来强调这一点。让我们看看，谁在这里？我没看到，但我敢打赌他来自非洲。有人来自非洲吗？没有人来自非洲？没有？
        </p>
        <p>There’s still one more thing. In the past, and in other places, artificial intelligence is often taught as
            purely about reasoning. But we solve problems with our eyes, as well as our symbolic apparatus. And you
            solved that problem with your eyes. So I like to reinforce that by giving you a little puzzle. Let’s see,
            who’s here? I don’t see but I’ll bet he’s from Africa. Is anyone from Africa? No one’s from Africa? No?</p>
        <p>那当然更好了。因为他们知道这个谜题的答案。谜题是这样的。赤道穿过非洲多少个国家？有人愿意用生命来回答吗？可能没有人。好吧，现在让我重复一下这个问题。赤道穿过非洲多少个国家？是的，六个。这真是一个奇迹。</p>
        <p>Well so much the better. because they would know the answer to the puzzle. Here’s the puzzle. How many
            countries in Africa does the Equator cross? Would anybody be willing to stake their life on their answer?
            Probably not. Well, now let me repeat the question. How many countries in Africa does the Equator cross?
            Yeah, six. What happened is a miracle.</p>
        <p>奇迹在于我通过语言与你交流，你的语言系统命令你的视觉系统执行一个程序，扫描那条线，边走边数数。然后你的视觉系统反馈给你的语言系统，说：六。这就是奇迹。如果不理解这个奇迹，我们就永远无法完全理解智力的本质。</p>
        <p>The miracle is that I have communicated with you through language, and your language system commanded your
            visual system to execute a program that involves scanning across that line, counting as you go. And then
            your vision system came back to your language system and said, six. And that is a miracle. And without
            understanding that miracle, we’ll never have a full understanding of the nature of intelligence.</p>
        <p>但我希望我们能教你很多关于这种问题解决的方法。但我们不能教你我们不理解的东西。我们就是这样。以上是关于定义和一些例子。它有什么用？我们可以很快处理这个问题。如果我们是工程师，它是为了构建更智能的程序。</p>
        <p>But that kind of problem solving is the kind of problem solving I wish we could teach you a lot about it. But
            we can’t teach you about stuff we don’t understand. We for that. That’s a little bit about the definition
            and some examples. What’s it for? We can deal with that very quickly. If we’re engineers, it’s for building
            smarter programs.</p>
        <p>它是关于构建一个表示和方法工具包，使构建更智能的程序成为可能。你会发现，如今，如果不将我们在主题中讨论的想法嵌入到某个地方，你就无法构建一个大系统。如果你是一名科学家，动机会有所不同。但这相当于研究相同的东西。</p>
        <p>It’s about building a tool kit of representations and methods that make it possible to build smarter
            programs. And you will find, these days, that you can’t build a big system without having embedded in it
            somewhere the ideas that we talk about in the subject. If you’re a scientist, there’s a somewhat different
            motivation. But it amounts to studying the same sorts of things.</p>
        <p>如果你是一名科学家，你就会想知道是什么让我们能够建立一个智能计算账户。这就是我所做的部分。但这个主题的大部分内容将涉及另一部分，即让你能够构建更智能的程序的部分。其中一部分内容将涉及是什么让我们与黑猩猩不同，我们与黑猩猩共享了大部分的
            DNA。</p>
        <p>If you’re a scientist, you’re interested in what it is that enables us to build a computational account of
            intelligence. That’s the part that I do. But most this subject is going to be about the other part, the part
            that makes it possible for you to build smarter programs. And some of it will be about what it is that makes
            us different from the chimpanzees with whom we share an enormous fraction of our DNA.</p>
        <p>人们过去认为我们与黑猩猩的 DNA 有 95% 是相同的。后来这个数字上升到了 98%。谢天谢地，这个数字停在了那里。然后它实际上又回落了一点。我认为我们又回到了
            94%。我们现在来谈谈人工智能的历史，看看我们是如何走到今天这一步的，好吗？</p>
        <p>It used to be thought that we share 95% of our DNA with chimpanzees. Then it went up to 98. Thank God it
            stopped about there. Then it actually went back a little bit. I think we’re back down to 94. How about if we
            talk a little bit now about the history of AI, so we can see how we got to where we are today?</p>

        <h2 id="lady-lovelace">洛夫莱斯夫人</h2>
        <h2>Lady Lovelace</h2>
        <p>这还将是人工智能的历史，向您介绍您将在本课程中学习的内容。这一切都始于世界上第一位程序员洛夫莱斯夫人，她在计算机出现之前大约 100 年编写了程序。但有趣的是，即使在 1842
            年，人们也在质问她计算机是否真的可以变得聪明。她说：“分析引擎并不自命不凡，不具备创造任何东西的能力。它可以做任何我们知道的命令它做的事情。”这个荒唐的想法一直延续到今天。然而，这就是一切的起源。这就是讨论的开始。然后直到
            1950 年左右，阿兰·图灵写了他著名的论文，引入了图灵测试，才有了很大的进展。</p>
        <p>This will also be a history of AI that tells you a little bit about what you’ll learn in this course. It all
            started with Lady Lovelace, the world’s first programmer, Who wrote programs about 100 years before there
            were computers to run them. But it’s interesting that even in 1842, people were hassling her about whether
            computers could get really smart. And she said, "The analytical engine has no pretensions to originate
            anything. It can do whatever we know how to order it to perform." Screwball idea that persists to this day.
            Nevertheless, that was the origin of it all. That was the beginning of the discussions. And then nothing
            much happened until about 1950, when Alan Turing wrote his famous paper, which introduced the Turing test.
        </p>
        <p>当然，阿兰·图灵此前曾通过破译德国密码《超级密码》赢得了第二次世界大战，英国政府为此将他逼自杀以示奖励，因为他恰好是同性恋。</p>
        <p>Of course, Alan Turing had previously won the Second World War by breaking the German code, the Ultra Code,
            for which the British government rewarded him by driving him to suicide, because he happened to be
            homosexual.</p>
        <p>但图灵在 1950 年发表了他的论文，这是 1842 年洛夫莱斯夫人发表评论后的第一个里程碑。然后现代时代真正开始于 1960
            年马文·明斯基发表的一篇题​​为“迈向人工智能”的论文。不久之后，几乎失明的研究生吉姆编写了一个进行符号积分的程序。</p>
        <p>But Turing wrote his paper in 1950, and that was the first milestone after Lady Lovelace’s comment in 1842.
            And then the modern era really began with a paper written by Marvin Minsky in 1960, titled “Steps Toward
            Artificial Intelligence.” And it wasn’t a long after that Jim a nearly blind graduate student, wrote a
            program that did symbolic integration.</p>
        <h2 id="marvin-minsky">马文·明斯基</h2>
        <h2>Marvin Minsky</h2>
        <p>不是把曲线下的面积相加，而是进行符号积分，就像你在高中一年级时学到的那样。现在星期一，我们将讨论这个程序。你将确切了解它的工作原理。你可以自己编写一个。</p>
        <p>Not adding up area under a curve, but doing symbolic integration just like you learn to do in high school
            when you’re a freshman. Now on Monday, we’re going to talk about this program. And you’re going to
            understand exactly how it works. And you can write one yourself.</p>
        <p>我们将回到过去，回顾该计划，因为在一天的时间里，讨论它、谈论它本身就是一门小型人工智能课程。因为它充满了重要的想法。所以那是黎明时代，早期的黎明时代。这是推测的时代，也是这里的黎明时代。所以在那个早期的黎明时代，集成计划风靡全球。
        </p>
        <p>And we’re going to reach way back in time to look at that program because, in one day discussing it, talking
            about it, will be in itself a miniature artificial intelligence course. Because it’s so rich with important
            ideas. So that’s the dawn age, early dawn age. This was the age of speculation, and this was the dawn age in
            here. So in that early dawn age,the integration program took the world by storm.</p>
        <p>因为不是每个人都知道如何进行积分。有些人，每个人都认为，如果我们今天能够进行积分，那么明天就能搞清楚其余的智能。可惜我们这边没有这样做。这是另一个黎明时代的程序，Eliza
            但我想你更喜欢演示而不是阅读它，对吧？你更喜欢演示吗？让我们看看我们能否演示它。</p>
        <p>Because not everybody knows how to do integration. And someone, everyone, thought that if we can do
            integration today, the rest of intelligence will be figured out tomorrow. Too bad for our side it didn’t
            work out that way. Here’s another dawn age program, the Eliza But I imagine you’d prefer a demonstration to
            just reading it, right? Do you prefer a demonstration? Let’s see if we can demonstrate it.</p>
        <p>这是几年前关于 hamentashen 的争论遗留下来的。有人知道 hamentashen
            怎么拼写吗？我真希望拼写正确。这没关系。有趣的事情会随之而来。好吧，你自己选吧。蓝绿色？伯顿之家？蓝绿色。这就是人工智能的黎明时代。没有人认真对待过这些东西，只是觉得制定一些匹配程序是一件有趣的项目级的事情，等等。集成程序是认真的。这个不是。
        </p>
        <p>This is left over from a hamentashen debate of a couple of years ago. How do you spell hamentashen, anybody
            know? I sure hope that’s right. It doesn’t matter. Something interesting will come. OK, your choice. Teal?
            Burton House? Teal. So that’s dawn age AI. And no one ever took that stuff seriously, except that it was a
            fun project level thing to work out some matching programs, and so on. The integration program was serious.
            This one wasn’t.</p>
        <p>这是很严肃的问题，程序会进行几何类比，就像智力测试中遇到的那种问题。你知道答案吗？A 和 B 的关系相当于 C 和什么的关系？我猜是
            2。第二好的答案是什么？解决这些问题的程序的理论与你刚刚弄清楚的几乎完全相同。在第一种情况下，你删除了里面的图形。</p>
        <p>This was serious, programs that do geometric analogy, problems of the kind you find on intelligence tests. Do
            you have the answer to this? A is to B as C is to what? That would be 2, I guess. What’s the second best
            answer? And the theories of the program that solve these problems are pretty much identical to what you just
            figured out. In the first case you deleted the inside figure.</p>
        <p>第二种情况是，你得到 4 的原因是删除了外部部分并增加了内部部分。还有另一种情况。我认为这是它得到的最难的一个，或者它没有得到的最​​简单的一个。我忘了。A 之于 B 相当于 C 之于
            3。在黎明时代后期，我们开始将注意力从纯粹的符号推理转向对感知器官的思考。</p>
        <p>And the second case is, the reason you got four is because you deleted the outside part and grew the inside
            part. There’s another one. I think this was the hardest one it got, or the easiest one it didn’t get. I’ve
            forgotten. A is to B as C is to 3. In the late dawn age, we began to turn our attention from purely symbolic
            reasoning to thinking a little bit about perceptual apparatus.</p>
        <p>还编写了程序来找出形状和形式的本质。有趣的是，这些程序在这方面遇到的困难与你遇到的一样。因为现在，删除了所有的边缘，一切都变得模糊不清。它可能是一系列平台，也可能是一系列。如果你进行反转，你能看到锯片伸出来吗？</p>
        <p>And programs were written that could figure out the nature of shapes and forms, such as that. And it’s
            interesting that those programs had the same kind of difficulty with this that you do. Because now, having
            deleted all the edges, everything becomes ambiguous. And it may be a series of platforms, or it may be a
            series of. can you see the saw blade sticking up if you go through the reversal?</p>
        <p>编写的程序可以从少量示例中学习。许多人认为计算机学习就是通过数千次试验引导一些神经网络服从。在早期编写的程序了解到拱门必须有平坦部分在顶部，两侧不能接触，顶部可能是也可能不是楔形。</p>
        <p>Programs were written that could learn from a small number of examples. Many people think of computer
            learning as involving leading some neural net to submission with thousands of trials. Programs were written
            in the early dawn age that learned that an arch is something that has to have the flat part on top, and the
            two sides can’t touch, and the top may or may not be a wedge.</p>
        <p>然而，在黎明时代，最重要的事情可能是下周三你们和我一起看的东西。这是一个基于规则的专家系统。斯坦福大学编写了一个程序，用于诊断血液细菌感染。事实证明，它比大多数医生、大多数全科医生做得更好。奇怪的是，它从未被使用过。因为没有人关心你的问题到底是什么。
        </p>
        <p>In the late dawn age, though, the most important thing, perhaps, was what you look at with me on Wednesday
            next. It’s a rule based expert systems. And a program was written at Stanford that did diagnosis of
            bacterial infections of the blood. It turned out to do it better than most doctors, most general
            practitioners. It was never used, curiously enough. Because nobody cares what your problem actually is.</p>
        <p>他们只是给你一种广谱抗生素，可以杀死一切细菌。但是这个黎明时代后期的系统，所谓的系统，是催生了上千家公司的系统，因为人们开始建立基于该技术的专家系统。这就是你不知道你使用过的东西，或者有人为你使用了它。例如，如果你经过亚特兰大机场，你的飞机将由一个基于规则的专家系统停放，该系统知道如何有效地停放飞机。
        </p>
        <p>They just give you a broad spectrum antibiotic that’ll kill everything. But this late dawn age system, the so
            called system, was the system that launched a thousand companies, because people started building expert
            systems built on that technology. Here’s that you don’t know you used, or that was used on your behalf. If
            you go through, for example, the Atlanta airport, your airplane is parked by a rule based expert system that
            knows how to park aircraft effectively.</p>
        <p>通过更智能地停放飞机，达美航空每天可节省约 50 万美元的燃油。这就是专家系统为许多人带来好处的一个例子。这就是深蓝。这将我们带入专家系统时代和商业时代的下一个阶段。</p>
        <p>It saves Delta Airlines about to $0.5 million a day of jet fuel by being all smarter about how to park them.
            So that’s an example of an expert system that does a little bit of good for a lot of people. There’s Deep
            Blue. That takes us to the next stage beyond the age of expert systems, and the business age.</p>
        <p>它将我们带入了这个时代，我称之为推土机时代，因为在这个时代，人们开始意识到我们拥有无限的计算能力。而且，计算能力常常可以替代智能。所以没有人会说“深蓝”能做到人类象棋大师能做到的事情。但尽管如此，“深蓝”通过像推土机处理碎石一样处理数据，击败了世界冠军。
        </p>
        <p>It takes us into this age here, which I call the bulldozer age, because this is the time when people began to
            see that we had at our disposal unlimited amounts of computing. And frequently you can substitute computing
            for intelligence. So no one would say that Deep Blue does anything like what a human chess master does. But
            nevertheless, Deep Blue, by processing data like a bulldozer processes gravel, was able to beat the world
            champion.</p>
        <p>那么什么是正确的方法呢？这就是我们现在所处的时代。当我们讨论这个主题时，我当然会为这些时代引入计划。问题是我们现在处于什么时代。我想，当你处于这个时代时，命名一个时代总是很危险的。我喜欢称呼正确的时代。</p>
        <p>So what’s the right way? That’s the age we’re in right now. I will of course be inducing programs for those
            ages as we go through the subject. There is a question of what age we’re in right now. And it’s always
            dangerous to name an age when you’re in it, I guess. I like to call at the age of the right way.</p>
        <p>在这个时代，我们开始意识到上面的定义实际上有些不完整，因为我们的智力很大程度上与思考、感知和行动无关，而是与将所有这些联系在一起的循环有关。我们在非洲举了一个例子。这是另一个例子，取自我实验室正在开发的一个程序，并且仍在继续开发。我们将要求系统想象一些东西。系统：好的。
        </p>
        <p>And this is an age when we begin to realize that definition up there is actually a little incomplete, because
            much of our intelligence has to do not with thinking, perception, and action acting separately, but with
            loops that tie all those together. We had one example with Africa. Here’s another example drawn from a
            program that has been under development, and continues to be, in my laboratory. We’re going to ask the
            system to imagine something. SYSTEM: OK.</p>
        <p>我会想象一个球掉进碗里。好的。我会想象一个男人撞上了一个女人。帕特里克·温斯顿：你看，如果它对这些情况的实际内容没有很好的记忆，它就会尽力而为。但是想象了这个场景后，它就可以了。系统：是的。我从经验中了解到，男人和女人之间的接触是因为男人撞上了一个女人。
        </p>
        <p>I will imagine that a ball falls into a bowl. OK. I will imagine that a man runs into a woman. PATRICK
            WINSTON: You see, it does the best that it can if it doesn’t have a good memory of what these situations
            actually involve. But having imagined the scene it can then. SYSTEM: Yes. I have learned from experience
            that contact between a man and a woman appeared because a man runs into a woman.</p>
        <p>帕特里克·温斯顿：想象出场景后，它就可以利用视觉器官读取想象场景中的答案。就像你在非洲所做的那样，只不过现在它正在利用自己的视觉记忆，使用视觉程序。系统：好的。我会想象一个男人把球给了另一个男人。帕特里克·温斯顿：我知道这看起来像是鼻涕虫，但他们实际上是杰出的教授。它总是尽其所能。系统：好的。
        </p>
        <p>PATRICK WINSTON: Having imagined the scene, it can then read the answers using its visual apparatus on the
            scene that it imagined. So just like what you did with Africa, only now it’s working with its own visual
            memory, using visual programs. SYSTEM: OK. I will imagine that a man gives a ball to a man. PATRICK WINSTON:
            I know this looks like slugs, but they’re actually distinguished professors. It always does the best it can.
            SYSTEM: OK.</p>
        <p>我会想象一个人飞翔。帕特里克·温斯顿：这是它能做到的最好的事情了。至此，我们对历史的讨论就到此结束了。我为您简要介绍了一下本学期我们将要讨论的内容。是的，克里斯？克里斯：它实际上是在演示什么吗？它有一个大型视频数据库吗？帕特里克·温斯顿：不，它有一个小型视频数据库。
        </p>
        <p>I will imagine that a man flies. PATRICK WINSTON: It’s the best that it can do. So that concludes our
            discussion of the history. And I’ve provided you with a little bit of a glimpse of what we’re going to look
            at as the semester unfolds. Yes, Chris? CHRIS: Is it actually a demonstration of something? Does it have a
            large database of videos? PATRICK WINSTON: No, it has a small database videos.</p>
        <p>CHRIS：但它会根据内容进行智能选择。PATRICK
            WINSTON：根据内容。所以如果你说想象一个学生把一个球给了另一个学生，它就会想象。你说，现在另一个学生有球了吗？另一个学生拿球了吗？它可以回答这些问题，因为它可以查看同一个视频，并在同一视频中看到拿球和送球。
        </p>
        <p>CHRIS: But it’s intelligently picking among them based on. PATRICK WINSTON: Based on their content. So if you
            say imagine that a student gave a ball to another student, it imagines that. You say, now does the other
            student have the ball? Does the other student take the ball? It can answer those questions because it can
            review the same video and see the take as well as the give in the same video.</p>
        <p>所以现在我们必须思考为什么我们应该对未来保持乐观。因为我们在这里已经有了很长的历史，但我们还没有解决这个问题。但我们对未来感到乐观的一个原因是，我们所有的朋友都在前进。我们的朋友包括认知心理学家、心理学家、语言学家，有时还有哲学家，尤其是古人类学家。
        </p>
        <p>So now we have to think about why we ought to be optimistic about the future. Because we’ve had a long
            history here, and we haven’t solved the problem. But one reason why we can feel optimistic about future is
            because all of our friends have been on the march. And our friends include the cognitive psychologists, the
            psychologists, the linguists, sometimes the philosophers, and especially the paleoanthropologists.</p>
        <p>因为我们与黑猩猩的不同之处以及我们是如何变成这样的，这一点正变得越来越清晰。高中生的想法是，我们是通过缓慢、渐进和持续的改进而进化的。但事实似乎并非如此。我们物种的一些特征在指导像我这样的人的活动时非常有用。以下是化石记录中的故事。
        </p>
        <p>Because it is becoming increasingly clear why we’re actually different from the chimpanzees, and how we got
            to be that way. The high school idea is that we evolved through slow, gradual, and continuous improvement.
            But that doesn’t seem to be the way it happened. There are some characteristics of our species that are
            informative when it comes to guiding the activities of people like me. And here’s what the story seems to be
            from the fossil record.</p>
        <p>首先，我们人类以现在的解剖形态存在了大概 20 万年。如果现在有人从 20 万年前走进门，我想他们一定很脏，但除此之外，可能还赤身裸体。除此之外，你不会看出区别，尤其是在麻省理工学院。因此，接下来的 15
            万年是我们人类实际上并没有什么成就的时期。</p>
        <p>First of all, we humans have been around for maybe 200,000 years in our present anatomical form. If someone
            walked through the door right now from 200,000 years ago, I imagine they would be dirty, but other than
            that. probably naked, too. other than that, you wouldn’t be able to tell the difference, especially at MIT.
            And so the ensuing 150,000 years was a period in which we humans didn’t actually amount to much.</p>
        <p>但不知何故，就在 5
            万年前不久，我们中的一小部分人发展出了一种将我们与其他物种区分开来的能力。这是进化的偶然。这些偶然可能会发生，也可能不会发生，但它恰好产生了我们。同样，我们作为一个物种可能缩小到几千人，甚至几百人，这使得这些偶然的变化、偶然的进化产物更容易持久。
        </p>
        <p>But somehow, shortly before 50,000 years ago, some small group of us developed a capability that separated us
            from all other species. It was an accident of evolution. And these accidents may or may not happen, but it
            happened to produce us. It’s also the case that we probably necked down as a species to a few thousand, or
            maybe even a few hundred individuals, something which made these accidental changes, accidental evolutionary
            products, more capable of sticking.</p>
        <h2 id="noam-chomsky">诺姆·乔姆斯基</h2>
        <h2>Noam Chomsky</h2>
        <p>这让我们猜测 5 万年前到底发生了什么。古人类学家诺姆·乔姆斯基和许多人得出了类似的结论。这个结论是。我引用乔姆斯基的话。他是权威的声音。</p>
        <p>This leads us to speculate on what it was that happened 50,000 years ago. And paleoanthropologists, Noam
            Chomsky, a lot of people reached similar conclusions. And that conclusion is. I’ll quote Chomsky. He’s the
            voice of authority.</p>
        <p>“似乎在 5
            万年前不久，我们中的一小部分人获得了将两个概念组合成第三个概念的能力，而不会干扰原来的两个概念，没有限制。”从我这样的人工智能人士的角度来看，乔姆斯基似乎想说的是，我们学会了如何开始描述事物，这种方式与语言密切相关。
        </p>
        <p>“It seems that shortly before 50,000 years ago, some small group of us acquired the ability to take two
            concepts, and combine them to make a third concept, without disturbing the original two concepts, without
            limit.” And from a perspective of an AI person like me, what Chomsky seems to be saying is, we learned how
            to begin to describe things, in a way that was intimately connected with language.</p>
        <p>而这，最终就是我们与黑猩猩的区别所在。所以你可能会说，那我们就只研究语言吧。不，你不能那样做，因为我们用眼睛思考。所以语言有两个作用。第一，它使我们能够进行描述。描述使我们能够讲故事。</p>
        <p>And that, in the end, is what separates us from the chimpanzees. So you might say, well let’s just study
            language. No, you can’t do that, because we think with our eyes. So language does two things. Number one, it
            enables us to make descriptions. Descriptions enable us to tell stories.</p>
        <h2 id="language">语言</h2>
        <h2>Language</h2>
        <p>讲故事和理解故事是教育的核心。这正在不断进步。</p>
        <p>And storytelling and story understanding is what all of education is about. That’s going up.</p>
        <p>而向下移动使我们能够调动感知系统的资源，甚至命令我们的感知系统想象我们从未见过的事物。举个例子。想象一下提着满满一桶水在街上奔跑。会发生什么？你的腿被打湿了。水溅了出来。你永远不会在网上找到这个事实。你可能从来没有被告知当你提着满满一桶水在街上奔跑时会发生什么。
        </p>
        <p>And going down enables us to marshal the resources of our perceptual systems, and even command our perceptual
            systems to imagine things we’ve never seen. So here’s an example. Imagine running down the street with a
            full bucket of water. What happens? Your leg gets wet. The water sloshes out. You’ll never find that fact
            anywhere on the web. You’ve probably never been told that’s what happens when you run down the street with a
            full bucket of water.</p>
        <p>但你很容易想象到这种情况，而且你知道会发生什么。有内部想象模拟。除非我们能理解这一点，否则我们永远无法理解人类智能。这是另一个例子。想象一下带着满满一桶镍币在街上奔跑？会发生什么？镍币很重。你会弯下腰。你会摇摇晃晃。但没有人告诉过你这些。你在网上找不到它。
        </p>
        <p>But you easily imagine this scenario, and you know what’s going to happen. There was internal imagination
            simulation. We’re never going to understand human intelligence until we can understand that. Here’s another
            example. Imagine running down the street with a full bucket of nickels? What happens? Nickels weigh a lot.
            You’re going to be bent over. You’re going to stagger. But nobody ever told you that. You won’t find it
            anywhere on the web.</p>
        <p>因此，语言是一切的核心，因为它使故事叙述向上发展，使感知器官的资源向下流动。这就是我们本学期要结束的话题，我们将尝试更多地了解这一现象。至此，我想说的关于材料和主题的所有内容都结束了。现在，我想稍微谈谈我们将如何操作这个主题。
        </p>
        <p>So language is at the center of things because it enables storytelling going up, and marshalling the
            resources of the perceptual apparatus, going down. And that’s where we’re going to finish the subject the
            semester, by trying to understand more about that phenomenon. So that concludes everything I wanted to say
            about the material and the subject. Now I want to turn my attention a little bit to how we are going to
            operate the subject.</p>
        <p>因为这个学科有很多让人困惑的特点。首先，我们在课程中有四种活动。每种活动都有不同的目的。所以我做了讲座。讲座应该持续一个小时，介绍材料和大局。它们是关于强有力的想法。它们是关于课程的体验方面。让我站在一边说几句话。
        </p>
        <p>Because there are many characteristics of the subject that are confusing. First of all, we have four kinds of
            activities in the course. And each of these has a different purpose. So I did the lectures. And the lectures
            are supposed to be an hour about introducing the material and the big picture. They’re about powerful ideas.
            They’re about the experience side of the course. Let me step aside and make a remark.</p>
        <h2 id="remarks">评论</h2>
        <h2>Remarks</h2>
        <p>麻省理工学院 (MIT) 涉及两件事。</p>
        <p>MIT is about two things.</p>
        <p>这关乎技能培养，关乎伟大创意。因此，你可以在家乡、达特茅斯、哈佛、普林斯顿或所有这些地方培养技能。但只有在麻省理工学院才能获得这种体验。我认识人工智能领域的每一个人。我可以告诉你他们的想法。我可以告诉你我的想法。这是你在其他地方得不到的东西。
        </p>
        <p>It’s about skill building, and it’s about big ideas. So you can build a skill at home, or at Dartmouth, or at
            Harvard, or Princeton, or all those kinds of places. But the experience you can only get at MIT. I know
            everybody there is to know in artificial intelligence. I can tell you about how they think. I can tell you
            about how I think. And that’s something you’re not going to get any other place.</p>
        <p>在我看来，这就是我上这些课的职责。复习课巩固和扩展了内容，提供了一个足够小的讨论场所。大型复习课是课程的组成部分。它们在周五的同一时间教授。我的研究生 Mark Seifter
            将教授这些课程。这些课程围绕过去的测验问题展开。Mark 将向您展示如何解决这些问题。这是该主题非常重要的组成部分。</p>
        <p>So that’s my role, as I see it, in giving these lectures. Recitations are four buttressing and expanding on
            the material, and providing a venue that’s small enough for discussion. Mega recitations are components of
            the course. They’re taught at the same hour on Fridays. Mark Seifter, my graduate student, will be teaching
            those. And those are wrapped around past quiz problems. And Mark will show you how to work them. It’s very
            important component to the subject.</p>
        <p>最后，辅导课是为了帮助你完成家庭作业。所以你可能会问我，我真的需要去上课吗？我想说，答案是，只有当你想通过这门课时才需要去上课。但你是麻省理工学院的学生。麻省理工学院的人总是喜欢看数据。</p>
        <p>And finally the tutorials are about helping you with the homework. So you might say to me, well, do I really
            need to go to class? I like to say that the answer is, only if you like to pass the subject. But you are MIT
            students. And MIT people always like to look at the data.</p>
        <p>这是我们在去年秋季开课后制作的散点图，显示了课程出勤率和课程成绩之间的关系。如果你不确定这一切意味着什么，这是回归线。因此，这些信息有点可疑，原因有两个，其中之一是我们要求人们自我报告他们认为他们参加了多少场讲座。
        </p>
        <p>So this is a scattergram we made after the subject was taught last fall, which shows the relationship between
            attendance at lectures and the grades awarded in the course. And if you’re not sure what that all means,
            here’s the regression line. So that information is a little suspect for two reasons, one of which is we
            asked people to self report on how many lectures they thought they attended.</p>
        <p>我们分配这些数字成绩的机制有点奇怪。还有第三件事，那就是，永远不能把相关性和因果性混为一谈。你可以想出其他解释来解释为什么趋势线会上升，而不是它是否与讲座产生好成绩有关。你可能会问我对左上角那边的人有什么看法。</p>
        <p>And our mechanism for assigning these numerical grades is a little weird. And there’s a third thing, too, and
            that is, one must never confuse correlation with cause. You can think of other explanations for why that
            trend line goes up, different from whether it has something to do with lectures producing good grades. You
            might ask how I feel about the people up there on the other upper left hand corner.</p>
        <p>有一两个学生在某门学科上名列前茅，但根本没有去上课。我对此有复杂的感受。你们都是成年人。这是你们自己的选择。另一方面，如果这是你们在麻省理工学院学习的所有科目中惯常的做法，我希望你们辞职去别的地方，让别人来接替他们的位置。
        </p>
        <p>There are one or two people who were near the top of the subject who didn’t go to class at all. And I have
            mixed feelings about that. You’re adults. It’s your call. On the other hand, I wish that if that’s what you
            do habitually in all the subjects you take at MIT, that you would resign and go somewhere else, and let
            somebody else take their slot.</p>
        <p>因为你没有从强有力的想法和其他涉及与教师互动的事情中受益。所以这是可以做到的。但我不推荐这样做。顺便说一句，我们这里的所有四项活动都显示出类似的回归线。但是那个五分制呢？让我向你解释一下它是如何运作的。我们喜欢有人问我们班级的平均成绩是多少。
        </p>
        <p>Because you’re not benefiting from the powerful ideas, and the other kinds of things that involve interaction
            with faculty. So it can be done. But I don’t recommend it. By the way, all of the four activities that we
            have here show similar regression lines. But what about that five point scale? Let me explain how that works
            to you. We love to have people ask us what the class average is on a quiz.</p>
        <p>因为那时我们只能一脸茫然。因为我们根本不知道任何测验的班级平均分是多少。这就是我们的所作所为。像其他人一样，我们从 0 到 100
            的分数开始。但随后我们问自己，如果你彻底理解了材料，你会得到多少分？我们说，好吧，对于这次考试，就是这个数字。</p>
        <p>Because that’s when we get to use our blank stare. Because we have no idea what the class average ever is on
            any quiz. Here’s what we do. Like everybody else, we start off with a score from zero to 100. But then we
            say to ourselves, what score would you get if you had a thorough understanding of the material? And we say,
            well, for this particular exam, it’s this number right here.</p>
        <p>如果你对材料有很好的理解，你会得到多少分？这就是那个分数。如果你的分数低于这个分数，那说明你超出了我们认为你需要做更多工作的范围。所以我们的做法是，如果你的分数在这个范围内，按照麻省理工学院的 GPA
            和其他惯例，你会得到 5 分。</p>
        <p>And what score would you get if you had a good understanding of the material? That’s that score. And what
            happens if you’re down here is that you’re following off the edge of the range in which we think you need to
            do more work. So what we do is, we say that if you’re in this range here. following MIT convention with GPAs
            and stuff, that gets you a five.</p>
        <p>如果你处于这个范围内，就会急剧下降到 4。如果你处于这个范围内，就会急剧下降到
            3。所以这意味着如果你处于其中一个平台的中间，那么争论这个是没有意义的。因为这不会给你带来任何好处。我们有这些我们认为表现断点的界限。所以你会说，这似乎有点苛刻。</p>
        <p>If you’re in this range down here, there’s a sharp drop off to four. If you’re in this range down here,
            there’s a sharp fall off to three. So that means if you’re in the middle of one of those plateaus there’s no
            point in arguing with this. Because it’s not going to do you any good. We have these boundaries where we
            think performance break points are. So you say, well that seems a little harsh.</p>
        <p>等等等等，然后开始争论。但接下来我们会回到课程中的第二个重大创新。那就是你的成绩分为几个部分计算。第一部分是你第一季度的最高成绩，第二部分是期末成绩。换句话说，你有两次机会。</p>
        <p>Blah, blah, blah, blah, blah, and start arguing. But then we will come back with a second major innovation we
            have in the course. That is that your grade is calculated in several parts. Part one is the max of your
            grade on Q1, and part one of the final. So in other words, you get two shots at everything.</p>
        <p>因此，如果你在第一次测验中得了绝对糟糕的
            F，那么如果你在期末考试中表现良好，那么它就会在期末考试中被抹去。因此，每次测验在期末考试中都有相应的镜像。你会得到这两部分的最高分。现在你对我说，我是麻省理工学院的学生。我很有胆量。我只参加期末考试。已经有人这么做了。
        </p>
        <p>So if you have complete glorious undeniable horrible F on the first quiz, it gets erased on the final if you
            do well on that part of the final. So each quiz has a corresponding mirror on the final. You get the max of
            the score you got on those two pieces. And now you say to me, I’m an MIT student. I have a lot of guts. I’m
            only going to take the final. It has been done.</p>
        <p>我们不推荐这样做。我们不推荐这样做的原因是，我们不希望每个人都完成所有的期末考试。所以如果你必须完成所有的期末考试，即期末考试的所有五个部分，那么时间压力就会很大。所以我们有四次测验。</p>
        <p>We don’t recommend it. And the reason we don’t recommend it is that we don’t expect everybody to do all of
            the final. So there would be a lot of time pressure if you had to do all of the final, all five parts of the
            final. So we have four quizzes.</p>
        <p>期末考试有第五部分，因为我们在截止日期之后会教你一些内容，根据学院规则，我们可以给你期末考试。但大致就是这样。你可以在主题主页的常见问题解答中阅读更多详细信息。现在我们快完成了。</p>
        <p>And the final has a fifth part because there’s some material that we teach you after the last date on which
            we can give you a final by institute rules. But that’s roughly how it works. And you can read about more of
            the details in the FAQ on the subject homepage. So now we’re almost done.</p>
        <p>我只想简单谈谈接下来几天我们将如何与你们沟通，同时我们还要进行组织。所以，第一，我能否请助教帮我分发这些。我们需要安排你们参加辅导课。所以我们将请你们填写这张表格，并在离开前交给我们。我们整理好表格后，你们就会收到我们的消息。
        </p>
        <p>I just want to talk a little bit about how we’re going to communicate with you in the next few days, while
            we’re getting ourselves organized. So, number one. if I could ask the TAs to help me pass these out. we need
            to schedule you into tutorials. So we’re going to ask you to fill out this form, and give it to us before
            you leave. So you’ll be hearing from us once we do the sort.</p>
        <p>存在一个问题，即我们本周是否要进行普通朗诵和大型朗诵。</p>
        <p>There’s the issue of whether we’re going to have ordinary recitation and a mega recitation this week.</p>
        <h2 id="announcements">公告</h2>
        <h2>Announcements</h2>
        <p>所以要注意。否则，你就会被困在教室里无事可做。本周我们不会有常规复习。本周我们会有常规复习吗？没有。本周我们可能会，也可能会有一个大型复习，专门用于 Python 复习。</p>
        <p>So pay attention. Otherwise, you’re going to be stranded in a classroom with nothing to do. We’re not going
            to have any regular recitations this week. Are we having regular recitation this week, No.&nbsp;We may, and
            probably will, have a mega recitation this week that’s devoted to a Python review.</p>
        <p>现在我们知道你们中的许多人在星期五庆祝宗教节日，所以我们将把大量资源放在网上，以便你们可以通过另一种方式获得评审。我们可能在星期五举行 Python
            评审。我们希望你们在本周内查看我们的主页以获取有关评审的更多信息。好了，就这些了。今天我们要做的事情到此结束。只要你给我们提交表格，我们就结束了。</p>
        <p>Now we know that there are many of you who are celebrating a religious holiday on Friday, and so we will be
            putting a lot of resources online so you can get that review in another way. We probably will have a Python
            review on Friday. And we ask that you look at our home page for further information about that as the week
            progresses. So that’s all folks. That concludes what we’re going to do today. And as soon as you give us
            your form, we’re through.</p>
        <h1 id="reasoning-goal-trees-and-problem-solving">2. 推理：目标树和问题解决</h1>
        <h1>2. Reasoning: Goal Trees and Problem Solving</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 45:58 <a
                href="https://youtube.com/watch?v=PNKj529yY5c">https://youtube.com/watch?v=PNKj529yY5c</a></p>
        <p> 11 years ago (Jan 11, 2014) — 45:58 <a
                href="https://youtube.com/watch?v=PNKj529yY5c">https://youtube.com/watch?v=PNKj529yY5c</a></p>
        <h2 id="unknown">未知</h2>
        <h2>Unknown</h2>
        <p>今天我们要讨论的是目标。所以，作为一点热身练习，我希望你们看一下那边的集成问题。那个已经消失了的问题。所以问题是，你能在脑子里做到这一点吗？可能不行。问题是，如果一个程序可以做到这一点，那么从某种意义上来说，这个程序是智能的吗？
        </p>
        <p>What we’re going to talk about today, is goals. So just by way of a little warm up exercise, I’d like you to
            look at that integration problem over there. The one that’s disappeared. So the question is, can you do it
            in your head? Probably not. The question is, if a program can do that, is a program, in any sense of the
            word, intelligent?</p>
        <p>这是今天我演讲时希望你们完成的一项背景任务。所以今天我们将模拟一些人类解决问题的方法，也就是符号积分时需要的那种方法。现在，你们都学会了如何做到这一点。你们可能再也无法解决那个特定的问题了，但你们都在高中 1801
            或类似的课程中学习了如何积分。</p>
        <p>That’s a background task I’d like you to work on as I talk today. So today we’re going to be modeling a
            little bit of human problem solving, the kind that is required when you do symbolic integration. Now, you
            all learned how to do that. You may not be able to do that particular problem anymore, but you all learned
            how to integrate in high school 1801, or something like that.</p>
        <p>问题是，你是怎么做到的，我们试图通过构建一个进行符号积分的程序来模拟的解决问题的技术，是人们解决问题时所做的一种常见描述吗？所以这个问题的答案是肯定的。你今天将看到的解决问题的方法就像生成测试，你上次看到了。</p>
        <p>The question is, how did you do it, and is the problem solving technique that we are trying to model by
            building a program that does symbolic integration, is that a common kind of description of what people do
            when they solve problems. So the answer to the question is, yes. The kind of problem solving you’ll see
            today is like generating tests, which you saw last time.</p>
        <h2 id="unknown-1">未知</h2>
        <h2>Unknown</h2>
        <p>这是我们所有人都会参与的一种非常常见的解决问题的方法，我们都参与其中，但从未思考过，也没有给它起过名字。但一旦我们给它起了一个名字，我们就拥有了控制它的力量。然后我们就能运用它，它就会成为一种技能。我们不仅会见证它，我们不仅会理解它，我们还会本能地把它当作一种技能来使用。
        </p>
        <p>It’s a very common kind of problem solving that we all engage in, that we all engage in without thinking
            about it, and without having a name for it. But once we get a name for it, we’ll get power over it. And then
            we’ll be able to deploy it, and it will become a skill. We’ll not just witness it, we’ll not just understand
            it, we’ll use it instinctively, as a skill.</p>
        <p>所以，你遇到了这个问题，这就是你的问题，你要怎么做才能解决它？我不知道，在表格中查找吗？你永远无法在表格中找到它，因为那个减号和那个
            5。所以你必须做些比这更好的事情。所以你要做的，就是当你看到这样的问题时你总是会做的事情。</p>
        <p>So there you are, you’ve got that problem, there’s your problem, and what do you do to solve it? I don’t
            know, look it up in a table? You’ll never find it in a table because of that minus sign and that 5. So
            you’re going to have to do something better than that. So what you’re going to do, is what you always do
            when you see a problem like that.</p>
        <p>你尝试应用变换，将其变成一个更容易解决的不同问题。最终，你希望将其简化到足够程度，简化后的部分可以在某个小的积分表中找到。那么这个表有多长？我们不会查看包含 388 个元素的表，因为这不是一个很大的积分表。</p>
        <p>You try to apply a transform, and make it into a different problem that’s easier to solve. And eventually,
            what you hope is that you’ll simplify it sufficiently, that the pieces that you’ve simplified to will be
            found in some small table of integrals. So how long is this table? It’s not the case that we’re going to
            look at a table with 388 elements, because this is not a big table of integrals.</p>
        <h2 id="unknown-2">未知</h2>
        <h2>Unknown</h2>
        <p>这是大一新生在学习完微积分课程后可能会想到的想法。一个有趣的问题是，表格中必须包含多少元素才能在这门课上获得 A？我们感兴趣的是涉及多少知识，这是我在那里列出的教义问答元素之一，它将成为当天金星创意套件的一部分。
        </p>
        <p>This is what a freshman might have in a freshman’s head, after taking a course in integral calculus. One of
            the interesting questions is, how many elements have to be in that table to get an A in the course? We’re
            interested in how much knowledge is involved, that’s one of the elements of catechism that I’ve listed over
            there, that will be part of the gold star ideas suite of the day.</p>
        <p>因此，我们希望解决这个问题，并找到一种方法，将其变成另一个更可能或更接近表格中的问题。因此，我们要做的事情非常简单，以图形方式呈现。我们将把给定的问题转化为另一个更简单的问题。我们将为这个过程命名，我们称之为问题简化。
        </p>
        <p>So we’d like to take that problem, and find a way to make it into another problem that’s more likely, or
            closer to being found in the table. So what we’re going to do is very simple, graphically. We’re going to
            take the problem we’re given, and convert it into another problem that’s simpler. And we’re going to give
            that process and name, and we’re going to call it problem reduction.</p>
        <p>因此，在积分学领域，我们可以尝试各种简单的方法、简单的变换，将难题变成更简单的问题。其中一些变换非常简单，而且总是安全的。有些只是，好吧，让我们试试看会发生什么。但有些是安全的，我现在想列出一个安全变换的简短列表。
        </p>
        <p>And so, in the world of integral calculus, there are all sorts of simple methods, simple transformations, we
            can try that will take a hard problem and make it into an easier problem. And some of these transformations
            are extremely simple and always safe. Some of them are just, well let’s try it and see what happens. But
            some of them are safe, and I’d like to make a short list of safe transformations right now.</p>
        <h2 id="unknown-3">未知</h2>
        <h2>Unknown</h2>
        <p>现在我要讲一些细节。这些细节可能比较模糊。问题是，我为什么要这么做？这就是教育哲学，我为什么要这么做。这就是教育哲学。在一个层面上，你想拥有一项技能。但是如果你想拥有一项技能，你必须理解它。所以如果你想拥有一项技能，你必须理解下一级的技能。
        </p>
        <p>Now I’m going to be going into some detail. And that detail will be grungy. And the question is, why do I do
            it? And it’s educational philosophy, is why I do it. So here’s the educational philosophy. At one level, you
            want to have a skill. But if you’re going to have a skill, you have to understand it. So if you’re going to
            have a skill you have to understand it one level down.</p>
        <p>如果你要理解它，你必须在更低的层次上亲眼目睹它。所以我不只是谈论问题减少的想法，因为如果我只这样做，那么我们现在就可以回家了。所以我将向你们展示一个具体的例子，这样你们就能更好地理解它，我将在更低的层次上向你们展示细节。
        </p>
        <p>If you’re going to understand it, you have to have witnessed it on a level lower than that. So I’m not just
            going to talk about the idea of problem reduction, because if I were just going to do that, then we could
            all go home now. So I’m going to show you a particular example of it, so you understand it better, and I’m
            going to show you the detail at an even lower level than that.</p>
        <p>因此，您将见证使技能成为可能的东西，了解使技能成为可能的东西，从而培养技能。这就是我要讲这些细节的原因。所以我不知道，让我们看看。也许我们可以从这个例子中得到一些提示，但我想知道是否有人可以自愿提出一个简单的转换，这总是一件好事。是的，塞巴斯蒂安。观众：去掉常数。说话者
            1：去掉常数。</p>
        <p>So you will witness the stuff that makes it possible, to understand the stuff that makes it possible, to
            build a skill. So that’s why I’m going through the grungy detail. So I don’t know, let’s see. Maybe we can
            get some hints from that example, but I wonder if somebody could volunteer a simple transformation that
            always is a good thing to do. Yes, Sebastian. AUDIENCE: Take the constants out. SPEAKER 1: Take the
            constants out.</p>
        <h2 id="unknown-4">未知</h2>
        <h2>Unknown</h2>
        <p>所以我们将这个数字设为 2。我们会说 x dx 的积分 cf 等于 c 乘以 x dx 的积分 f。还有其他建议吗？是的。听众：三角代换。说话者
            1：三角代换。现在这是。不，那是第二天的事。我们不在这里做三角代换，因为这是安全的，总是有效的，毫无疑问，有更简单的东西。这些是安全的变换。你给我的是启发式变换。</p>
        <p>So we’ll make that number two. And we’ll say that the integral c f of x dx is equal to c times the integral f
            of x dx. Other suggestions? Yes. AUDIENCE: Trig substitution. SPEAKER 1: Trig substitution. Now this is. no,
            that’s for day two. We don’t do trig substitution here under stuff that’s safe, always works, never any
            doubt, there are simpler things. These are the safe transformations. What you’re giving me is a heuristic
            transformation.</p>
        <p>经常是有用的，但不一定总是有效。我们将把我们的变换分为这两类。所以我需要另一个安全的。听众：说话者 1：建筑师坐在那边。不仅按国籍划分，还按课程划分。什么？听众：积分之和是总和的积分。说话者
            1：积分之和是总和的积分。现在缺少什么？第一是什么？</p>
        <p>Often is helpful, doesn’t necessarily always work. We’re going to divide our transformations into those two
            categories. So I need another safe one. AUDIENCE: SPEAKER 1:. The architects are sitting over there. Divided
            not only by nationality, but by course. What? AUDIENCE:. The sum of integrals is the integral of the sum.
            SPEAKER 1:. The sum of integrals is the integral of the sum. Now what’s missing? What’s number one?</p>
        <p>你可能认为它已经存在了，因为你给了我一个涉及常数的变换。你可以把-1看作一个常数。但是否使用单独的变换，当然取决于你如何表示知识。所有这些知识，所有这些事情，都是用早期的 Lisp 形式编写的。</p>
        <p>You’re probably thinking it’s already there, because you’ve given me the transformation that involves a
            constant. And you can think of minus 1 as a constant. But whether you use a separate transformation or not,
            of course depends on how you represent the knowledge. And all of this knowledge, all of this whole thing,
            was written in an early form of Lisp.</p>
        <h2 id="unknown-5">未知</h2>
        <h2>Unknown</h2>
        <p>因此，表示负数的方式与表示负 1 的方式不同。所以我们还需要一个变换。或者更确切地说，Jim Slagle 在编写他著名的变换程序时还需要一个变换。那就是，如果你有 x 的负 f 的积分，那就等于减去 x 的 f
            的积分。所以这几乎完成了我们的安全变换集。</p>
        <p>As a consequence, the way in which minus was represented is different from the way minus 1 is represented. So
            we need one more transformation. Or rather, Jim Slagle needed one more transformation, when he wrote his
            famous transformation program. And that was that if you have the integral of minus f of x, that’s equal to,
            minus the integral of f of x. So that almost completes our safe transformation set.</p>
        <p>我还想再给你一个，因为我觉得你猜不到。你为什么要猜呢？这是第四个。还有​​更多，这只是个例子。为了举例说明，这些就是我们解决这个问题所需要的。所以第四个是，如果你有 p 的 x 积分，除以 q 的 x，那么你除以 p。
        </p>
        <p>There’s one more that I’m going to supply you, because I don’t think you’d guess it. Why should you? It’s
            number four. There are more than this, this is a sample. And these are the ones we’re going to need in order
            to solve that problem, by way of illustration. So the fourth one is that, if you have the integral of p of
            x, over q of x, then you divide.</p>
        <p>如果你能回到高中时代，弄清楚如何除多项式。但如果分子的次数大于分母的次数，那么这是一个下意识的必胜之举，你必须这样做，把它除掉。因此，这构成了一个积分程序的核心，它将几乎什么都积分不了。但实际上，几乎没有任何东西是可积的，所以这是一个很好的开端。
        </p>
        <p>If you can reach way back into high school and figure out how to divide polynomials. But if the degree of the
            numerator is greater than the degree of the denominator, then it’s a knee jerk always win, you must do it,
            divide it out. So this, then, forms the core of an integration program, that will integrate almost nothing.
            But actually, almost nothing is integrable anyway, so it’s a good head start.</p>
        <h2 id="unknown-6">未知</h2>
        <h2>Unknown</h2>
        <p>那么让我们看看如何将其纳入某种程序。某种用于部署我们开始开发的知识的框架。我们要做的就是应用所有安全转换。这是我们的第一步。然后我们将查看表格，然后进行测试以查看我们是否已完成。如果已完成，我们将报告成功。但是，我们不太可能仅凭这些就完成。
        </p>
        <p>So let’s see how we would put this into some kind of procedure. Some kind of framework for deploying the
            knowledge that we’re beginning to develop. What we’re going to do is, apply all safe transforms. That’s our
            first step. Then we’re going to look in the table, and then we’re going to do a test to see if we’re done.
            And if we are, we report success. But, we’re not likely to get done with just that stuff.</p>
        <p>但你知道吗，这里有一个变换，它打破了我的小图。是哪一个？是第三个，对吧？因为这个图没有反映出当你应用第三个时会发生什么。因为它把问题分解成一堆问题，而不是一个问题。所以我们必须稍微扩展一下我们讨论这个问题的图形设备，并展示所谓的“和节点”。
        </p>
        <p>But you know what, there was one transformation up here, which breaks my little diagram. Which one is it?
            It’s the third one, right? Because this picture does not reflect what happens when you apply number three.
            Because it breaks the problem up, not into just one problem, but into a whole bunch. So we have to extend
            our graphical device for talking about this by a little bit, and show what is called an “and node”.</p>
        <p>所以我们有一个程序核心，一个积分表，一些变换，一个架构，一种把这些东西放在一起的方法。现在我们可以在示例问题上尝试一下。让我们试一试。让我们看看，这个立即变成了 5x 的四次方除以 1 减去 x 的平方的 5/2
            dx。</p>
        <p>So we’ve got a program core, we’ve got a table of integrals, we’ve got a few transformations, we’ve got an
            architecture, a way of putting that stuff together. And now we can try it out on our sample problem. So
            let’s have a go at that. Let’s see, this one immediately transforms into 5x to the fourth over 1 minus x
            squared to the 5/2 dx.</p>
        <h2 id="unknown-7">未知</h2>
        <h2>Unknown</h2>
        <p>反过来，这又立即变成了 x 的 1/4 减去 x 的 5/2 平方的积分，dx。顺便说一下，这个程序是一个黎明时代的程序。它是由一位名叫詹姆斯·斯莱格尔的近乎失明、后来完全失明的研究生在 1960
            年编写的，那是很久以前的事情了。</p>
        <p>And that in turn, immediately transforms into the integral of x to the fourth over 1 minus x squared to the
            5/2, dx. This program, by the way, is a dawn age program. This was written by a nearly blind, and
            subsequently completely blind, graduate student by the name of James Slagle in 1960, a long time ago.</p>
        <p>我今天之所以要介绍它，是因为通过描述它，我相当于给你们上了一堂人工智能课程。他预见了接下来 20
            年的很多事情，以至于谈论他的程序（一天内就能实现）是对整个领域的一个小型介绍。所以斯莱格尔在一台古董计算机上做这件事，几乎没有内存，几乎没有速度，只比在跑步机上跑来跑去的老鼠快一点。</p>
        <p>The reason I gave it to you today is because, that by describing it, I am giving you a one lecture course in
            artificial intelligence. He anticipated so much of the subsequent 20 years, that talking about his program,
            which is possible in one day, is a miniature introduction to the whole field. So Slagle, as he was doing
            this on an antique computer, almost no memory, almost no speed, only slightly faster than mice running
            around on a treadmill.</p>
        <p>他编写的程序在与新生进行对比时表现得非常好。当然，与新生进行对比的方式是给它一个测试，测试内容是从 MIT
            过去四五年期末考试中抽取的最难的问题。而这个就是它解决的最难的问题。所以在这一点上，就我们目前所拥有的而言，我们陷入了困境。</p>
        <p>He was able to write a program that did extremely well when benchmarked against freshmen. And the way you
            benchmark against freshman, of course, is you give it an examination, drawn from the previous MIT finals for
            four or five years, the hardest problems. And this was the hardest problem that it solved. So at this point,
            with what we’ve got so far, we would be stuck.</p>
        <h2 id="unknown-8">未知</h2>
        <h2>Unknown</h2>
        <p>我们没有能够让我们走得更远的变换，所以我们需要其他东西。而我们需要的其他东西，就是一些我们称之为启发式变换的变换。这个词很有趣，意思是一种经常有效的方法并不保证一定有效。它不是我们通常所说的算法。而是一种尝试。</p>
        <p>We have no transformation that can take us further, so we need something else. And what we need by way of
            something else, is some transformations that we will describe as. perhaps we’ll call them, heuristic
            transformations. A funny word, meaning a method that often works isn’t guaranteed to work. It’s not an
            algorithm in the usual sense that we talk about algorithms. But rather, it’s an attempt.</p>
        <p>所以我现在要讲的这些东西，有时有用，但并不总是有用。有时会把你带入死胡同，并不总是有用。但如果不了解其中的一些，你就不可能在微积分中获得 A。所以你说，某种三角代换。所以这里有某种三角代换。我们称之为启发式变换 A。
        </p>
        <p>So these things I’m going to talk about now, are sometimes useful, not always useful. Sometimes take you into
            a blind alley, don’ always work. But you can’t get an A in calculus without knowing some of them. So you
            said, some kind of trig substitution. So here is some kind of trig substitution. We’ll call this heuristic
            transformation A.</p>
        <p>有一个函数，x 的正弦、x 的余弦、x 的正切、x 的余切、x 的正割和 x 的余割。我们从高中三角学中都知道，我们可以将其重写为 x 的正弦和 x 的余弦函数。或者，我们可以将其重写为 x 的正切和 x
            的余割函数。或者，我们可以将其重写为 x 的余切和 x 的正割函数。</p>
        <p>You have a function sine x, cosine x, tangent of x, cotangent of x, secant of x, and cosecant of x. And we
            all know from high school trigonometry, that we can rewrite that as a function of sine x, and cosine x. Or
            we can rewrite that as a function of tangent of x, and cosecant of x. Or we can rewrite that as function of
            cotangent of x, and the secant of x.</p>
        <h2 id="unknown-9">未知</h2>
        <h2>Unknown</h2>
        <p>这就是从三角形式到另一种三角形式的转换。这并不总是一个好主意，但有时它很有用。好吧，这只是我们启发式变换套件的一部分。停下来。为了解决问题，我们需要在我们的库中加入其他东西。其中之一是变换系列，我只会向你们展示其中一种。
        </p>
        <p>So that’s a transmission from trigonometric form, into another trigonometric form. It’s not always a good
            idea, sometimes it helps. Well that’s just part one of our suite of heuristic transformations. Stop. There
            are others that we need to have in our repertoire, in order to solve the problem. One of them is a family of
            transformations, which I’ll show you only one.</p>
        <p>就像这样，如果你有一个函数的积分，即 x 的正切，那么你可以将其重写为 y 除以 1 加上 y 平方 dy 的函数的积分。所以这是从三角函数形式到多项式形式的变换。所以它摆脱了我们不想处理的所有三角函数垃圾。</p>
        <p>It goes like this, if you have the integral of a function, of the tangent of x, then you can rewrite that as
            the integral of a function of y over 1 plus y squared dy. So that’s a transformation from a trigonometric
            form into a polynomial form. So it gets rid of all that trigonometric garbage we don’t want to deal with.
        </p>
        <p>像这样的东西有很多，就像像这样的变换有很多，但这足以让你有所领悟。现在我们还需要一个 C。当你看到 1 减 x
            平方这样的数字时，这将是你正确的下意识反应。当你看到它时，你会怎么做？观众：那是什么，Rhana？Rhana：1 + 6 * 1 6 等一下。</p>
        <p>And there’s a whole family of things like that, just as there’s a family of transformations like so, but this
            is enough to give you flavor. Now there’s a C that we need as well. And that’s going to be your proper knee
            jerk reaction when you see something of the form 1 minus x squared. What do you do when you see that?
            AUDIENCE: What’s that Rhana? Rhana: 1 + 6 * 1 6 Well wait a second.</p>
        <h2 id="unknown-10">未知</h2>
        <h2>Unknown</h2>
        <p>我们可以这样做。但是我们还可以做另一件事。克里斯蒂安，你有什么建议吗？我们的匈牙利人在哪里？我们的土耳其人，我们的年轻土耳其人。是的，你觉得呢？听众：我其实不记得了。我的意思是，我认为可能是 10。说话者
            1：好吧，让我们看看。余弦平方加正弦平方等于 1。那么，这对你有什么建议？所以它建议我们做一个涉及 x 等于正弦 y 的变换。</p>
        <p>We could do that. But there’s another thing we can do. Christian, have you got something you can suggest?
            Where’s our Hungarian? Our Turk, our young Turk. Yeah, what do you think? AUDIENCE: I actually don’t
            remember. I mean, I think it might have been 10. SPEAKER 1: Well, let’s see. Cosine squared plus sine
            squared equals 1. So, what’s that suggest to you? So it suggests that we make a transformation that involves
            x equals sine y.</p>
        <p>所以实际上不再需要记住这一点，因为今后她将永远不需要在生活中亲自整合任何东西，她只需模拟程序即可。所以这些从多项式形式回到三角形式。所以你有三个这样的启发式变换。我们有四个安全变换。让我们看看我们能否在集成问题上取得进展。
        </p>
        <p>So doesn’t actually have to remember that anymore because going forward, she will never have to integrate
            anything personally in her life, she can just simulate the program. So these go from polynomial form, back
            into trigonometric form. So you have three of these heuristic transformations. We’ve got four safe
            transformations. Let’s see if we can make any progress on our integration problem.</p>
        <p>好的，让我们继续跟踪我们一直在使用的变换，这是安全变换一号，这是安全变换二号。接下来我们做什么？我们决定不再有适用的安全变换。但现在我们可以看看我们的启发式变换，看，我们看到了什么？听众：C 发言者
            1：什么？听众：应用变换 C。发言者 1：变换 C 建议我们做 x 等于 y 的正弦。</p>
        <p>OK so keeping track of what we’ve been using, this is safe transformation number one, this is safe
            transformation number two. What do we do next? We decided there were no more safe transformations that
            apply. But now we can look at our heuristic transformations and behold, we see what? AUDIENCE: C SPEAKER 1:
            What? AUDIENCE: Applying transformation C. SPEAKER 1: Transformation C suggests that we do x equals the sine
            y.</p>
        <h2 id="unknown-11">未知</h2>
        <h2>Unknown</h2>
        <p>现在我们得到了 y 的四次方的正弦除以 y 的四次方的余弦 dy 的积分，对吧。一切都很好，我看到一些困惑、担心、关切的表情。也许我犯了一个错误，也许我应该做笔记。哦，不，等一下。对于那些有关切表情的人，请记住，如果
            x 等于 y 的正弦，那么 dx 等于 y 的余弦 dy。</p>
        <p>And now we get the integral of sine to the fourth y over cosine to the fourth y dy, right. All good, I see
            some confused, worried, concerned looks. Maybe I’ve made a mistake, perhaps I should use notes. Well no,
            wait a minute. For those of you who have a concerned look, remember that if x equals a sine y, then dx is
            equal to cosine y dy.</p>
        <p>这就是为什么它是四次方的余弦而不是五次方的余弦，正如你可能认为的那样。现在我们取得了一些进展。我们看看这个，我们说，有没有适用的安全变换？答案是，没有。现在我们寻找可能适用的启发式变换，我说，你看到了什么？哪一个？那是什么？
        </p>
        <p>That’s why it’s cosine to the fourth not cosine to the fifth, as you were perhaps thinking it might be. So
            now we’ve made some progress. We look at this, we say, are there any safe transformations that apply? And
            the answer is, no. Now we look for a heuristic transformation that might apply, and I say, what do you see?
            Which one? What’s that?</p>
        <p>听众：说话者 1：她说了一些难以理解的话，但她可能说的是，这看起来像是一种可能与启发式变换 A
            相匹配的模式，对吗？因为我们有一个函数，其中变量被隐藏在正弦、余弦、正切、余切、正割或余割中。我们知道我们可以用三种方式之一重写它。它已经写成了正弦和余弦的函数。</p>
        <p>AUDIENCE: SPEAKER 1: She said something unintelligible, but what she probably said is, that this looks like a
            pattern that might match with the heuristic transformation A, right? Because we have a function in which the
            variable is buried, universally in sines, or cosines, or tangents, or cotangents, or secants, or cosecants.
            And we know we can rewrite that in one of three ways. It’s already written as a function of sine and cosine.
        </p>
        <h2 id="unknown-12">未知</h2>
        <h2>Unknown</h2>
        <p>但是我们也可以用正切和余割来重写它。或者用余切和正割来重写它。所以当我们这样做时，我们可以这样做，得到 1 对 x dx 的余切的积分。这就是上面的 g3。或者我们可以沿着这条路径进行操作，得到 x dx
            的正切的积分。当然，它们都是四次方。</p>
        <p>But we can also rewrite that in terms of tangent and cosecant. Or cotangent and secant. So when we do that,
            we can go this way, and we can get the integral of 1 over the cotangent of x dx. That’s g3 up there. Or we
            can do it down this path, and get the integral of tangent of x dx. And of course, those are both to the
            fourth.</p>
        <p>但你知道吗，我又把我的小图表弄坏了。它去哪了，它消失了。它就在那里。我怎么把它弄坏了？因为通过转换
            A，我引入了一种可能性，即一个特定的问题可以转换成多种类型的问题，其中任何一种都可以成为我的问题的解决方案。到目前为止，我有一个与节点，但现在我必须引入一个或节点。</p>
        <p>But know what, I’ve broken my little graphical diagram again. Where did it go, it’s disappeared. There it is.
            How have I broken it? Because with transformation A, I’ve introduced a possibility that a particular problem
            can be transformed into more than one kind of problem, any of which will be the solution to my problem. So
            far I’ve got an and node, but now I’ve got to introduce an or node.</p>
        <p>因为现在我们有了一个例子，它可以用两种不同的方法之一来解决，我们不在乎是哪一种。现在你会注意到这里已经有些混乱了，因为你如何区分“与”节点和“或”节点。所以通用惯例是，在“与”节点上画一条弧。这样它看起来像一个
            A，所以很容易记住。</p>
        <p>Because now we have an example of something that can be solved one of two different ways, and we don’t care
            which one it is. Now you’ll notice that there’s already some confusion here, because how can you tell the
            difference between an and node and an or node. So the universal convention is, you draw an arc over the and
            nodes. And that makes it look like an A, so it’s easy to remember.</p>
        <h2 id="unknown-13">未知</h2>
        <h2>Unknown</h2>
        <p>所以那些是和节点。现在，我们有了问题归纳方法，有时也称为问题归纳树。有时它被称为与/或树，有时它被称为目标树，因为这个问题树是一棵显示我们的目标如何相互关联的树。所以这些是你的词汇表中的同义词。问题归纳树、与/或树、目标树，都是同一个东西。
        </p>
        <p>So those are and nodes. And now, we have the method of problem reduction, and this is sometimes called a
            problem reduction tree. Sometimes it’s called an and/or tree, and sometimes it’s called a goal tree, because
            this tree of problems is a tree that shows how our goals are related to one another. So these are items for
            your vocabulary that are all synonymous. Problem reduction tree, and/or tree, goal tree, all the same thing.
        </p>
        <p>现在你有了它的名字，你对它有一些控制权。所以当我们遇到这种情况时，与之前的情况不同，我们认为这种情况可能会出现在转换 A 中。让我们看看，我们有 1、2、C，这个是 A，它是一个或节点。我们要解决这些问题中的哪一个？
        </p>
        <p>Now you have a name for it, you’ve got some power over it. So when we get a situation like this, unlike the
            previous situation, which we suggested might come up in transformation A. Let’s see, we’ve got one, two, C,
            and this one is A, it’s an or node. Which one of these problems do we work on?</p>
        <p>好吧，斯莱格尔认为自己是在模仿一名新生，模仿一名新生的智力，模仿一些毕竟需要相当聪明才能做的事情，对吧。大多数人都不知道如何做积分。麻省理工学院的每个人都知道如何做积分。因此，你会认为知道如何做积分的人是相当聪明的。当一个聪明人面临这种​​选择时，他会怎么做？
        </p>
        <p>Well Slegle, who considered himself to be modeling a freshman, modeling the intelligence of a freshman,
            modeling something that, after all, you have to be pretty smart to do, right. Most people don’t know how to
            do integration. Everybody at MIT knows how to do integration. You would think that somebody, therefore, that
            knows how to do integration is pretty smart. What would a smart person do, when faced with this choice?</p>
        <h2 id="unknown-14">未知</h2>
        <h2>Unknown</h2>
        <p>那么，聪明的人会说，这两个问题中哪一个更容易？那么你认为如何确定两个或多个代数表达式中哪一个最容易积分？你叫什么名字？听众：安德鲁·卡罗尔。说话者 1：安德鲁，你怎么看？听众：根据哪个感觉更熟悉。说话者
            1：感觉。听众：是的。说话者 1：感觉。听众：你问，我该如何决定。说话者 1：是的，你会如何决定？</p>
        <p>Well, a smart person would say, which of these two problems is easier? So how do you think you might
            determine which of two, or many algebraic expressions is the easiest to integrate? What’s your name?
            AUDIENCE: Andrew Carrol. SPEAKER 1: Andrew, what do you think? AUDIENCE: Based on whichever one feels more
            familiar. SPEAKER 1: Feels. AUDIENCE: Yes. SPEAKER 1: Feels. AUDIENCE: You asked, how would I decide.
            SPEAKER 1: Yeah, how would you decide?</p>
        <p>你会有什么感觉？ 听众：我觉得切线更熟悉。 说话者 1：哪一个？ 听众：我觉得切线 说话者 1：是的，但我想知道我们如何才能让它更精确一点，这个简单的想法。 年轻的土耳其人有一个建议。 什么？
            听众：在你说出这个简单的想法之前，我有一个建议。</p>
        <p>How would you feel it? AUDIENCE: I would feel that the tangent is more familiar. SPEAKER 1: Which one?
            AUDIENCE: I feel that the tangent SPEAKER 1: Yeah, but I wonder how we could make it a little bit more
            precise, this idea of simplicity. The young Turk has a suggestion. What? AUDIENCE: I had a suggestion until
            you said this idea of simplicity.</p>
        <p>所以我意识到我即将提出的建议并不是要阐明简单性，而是要说，无论我们遇到的更多，或者经验更丰富。 说话者 1：是的，如果这里有一个双曲正切，你可能会说，好吧，远离它。 观众：下一步将对其中哪一个应用更简单的变换。</p>
        <p>So then I realized that what I was about to suggest wasn’t going to clarify simplicity, but I was going to
            say, whichever one we’ve had more encounters with, or more experience with. SPEAKER 1: Yeah, if there was
            something here with a hyperbolic tangent, you might say, well, stay away from that. AUDIENCE: To which one
            of those the easier transformation is applied on the next step.</p>
        <h2 id="unknown-15">未知</h2>
        <h2>Unknown</h2>
        <p>说话者 1：比如，有人稍微往前看一看，看看哪种东西会在你的旁边？我不知道，也许吧。哦，我们同时有很多人。我还不知道你们所有人的名字。哎呀。埃里卡，我认识你。观众：在表格中查找并查看说话者
            1：哦，你可以在表格中查找并查看其中是否有东西，你可以这样做。</p>
        <p>SPEAKER 1: Like, somebody do a little look ahead, and see which kind of thing would be next to you? I don’t
            know, maybe. Oh, we’ve got lots of people, all at the same time. I don’t know all your names yet. Shoot.
            Erica, I know you. AUDIENCE: What’s look it up in the table and see SPEAKER 1: Oh, you could look it up in
            the table and see if something is in it, you could do that.</p>
        <p>但这是与第四个相切的，所以不在表中。艾丽尔？ 观众：我选择没有倒数的那个。 说话者 1：为什么？ 观众：因为当人们看到一个时，会觉得，哦，天哪，它根本行不通。 说话者 1：是的，我们走在正确的轨道上。 克莱尔？
            观众：从非常简单的层面上讲，我选择其中符号最少的那个。 说话者 1：其中符号最少的那个。</p>
        <p>But this is tangent to the fourth, so that’s not in the table. Ariel? AUDIENCE: I choose the one without the
            reciprocal. SPEAKER 1: Why? AUDIENCE: It is because when people see one it’s like, oh man, it jut not going
            to work. SPEAKER 1: Yeah, we’re on the right track. Claire? AUDIENCE: On an extremely simple level, I choose
            whichever one has the least symbols in it. SPEAKER 1:. The fewest symbols in it.</p>
        <p>现在我们确实取得了一些进展，因为你可以测量，对吧，有一个小程序为什么布雷特，你在这里。听众：我想说，每个表达式都可以写成，有多个函数，我们可以说所有这些函数，相乘，相除，你可以选择最少量的 说话者
            1：好吧，我听到了，也许其他人没有，但布雷特说的是，他建议我们应该测量函数组合的深度。</p>
        <p>Now we’re really getting somewhere, because you can measure that, right, there’s a little program Why Brett,
            there you are. AUDIENCE: I would say, every expression can be written as, having a number of functions, we
            could say all these functions, multiplied together, divided, and you can just choose with the least amount
            of SPEAKER 1: Well I heard it, perhaps others didn’t but what Brett said, is he suggested that we should
            measure depth of functional composition.</p>
        <h2 id="unknown-16">未知</h2>
        <h2>Unknown</h2>
        <p>因此符号的数量可能并不重要，因为如果有 x 加 x 加 x 加 x，加到一百，那么积分就不难了。但是如果你有一些东西深深地嵌套在很多函数组合中，那可能就是个问题了。事实上，在尝试了几种替代方案后，Slegle
            决定使用这种方法。</p>
        <p>So the number of symbols may not matter, because if you have x plus x plus x plus x, out to a hundred, that
            would not be hard to integrate. But if you’ve got something that is really deeply nested under a lot of
            functional compositions, that could be a problem. And that’s in fact, what Slegle decided to use, after
            trying several alternatives.</p>
        <p>因此，如果我们测量函数组合的深度，这就是赢家，而我们将另一个搁置一旁，至少目前是这样。现在我们有第四个 x dx
            的切线。我需要安全转换供应吗？不需要。哪一个。你知道有些东西必须应用，否则它就不会在这里作为例子。那么启发式转换供应呢？艾略特。观众：说话者 1：是的，B 太棒了。</p>
        <p>So if we measure the depth of the functional composition, this is the winner, and we put the other one on the
            shelf, at least for the moment. And now we have tangent to the fourth x dx. Do I need the safe
            transformation supply? No.&nbsp;Which of the. you know something has to apply, otherwise it wouldn’t be up
            here as an example. So what of the heuristic transformation supply? Elliott. AUDIENCE: SPEAKER 1: Yeah, B
            bravo.</p>
        <p>军事背景或类似的东西。也许他会开飞机。好吧，B 说，它实际上是正切函数。当我们这样做时，我们必须做一个替换，即 y 等于正切。这意味着它变成了 y 的 1/4 加 y 平方的积分。这是通过变换 B 得到的，变换是 y
            等于 x 的正切。正切。</p>
        <p>Military background or something like that. Maybe he flies airplanes. OK so B says, it is in fact a function
            of the tangent. And when we do that, we’ve got to make a substitution, that y is equal to the tangent. So
            that means that this becomes the integral of y to the fourth over 1 plus y squared. And that’s by
            transformation B, and the transformation is y equals tangent of x. The tangent.</p>
        <h2 id="unknown-17">未知</h2>
        <h2>Unknown</h2>
        <p>我猜我已经忘记了我已经转换过 ay 的事实，但重新标记并不重要。好吧，也许这就是进步。但在任何启发式转换中都没有看到这一点，我现在该怎么办？我不必查看启发式转换，因为其中一种安全转换适用。</p>
        <p>I guess I’ve lost track of the fact that I’ve already transformed a y, but relabeling doesn’t matter. All
            right so that’s progress, maybe. But don’t see this in any of the heuristic transformations, what do I do
            now? I didn’t have to look in the heuristic transformations, because one of the safe transformations
            applies.</p>
        <p>因为这个东西是有理函数，分子的次数大于分母的次数，所以我必须除法。当我除法时，顺便说一下，结果是 4，我得到了什么？有没有人高中代数学的很好，可以帮我算一下吗？</p>
        <p>Because this thing is a rational function and the degree of the numerator is greater that the degree of the
            denominator, so I have to divide. And when I divide, and that by the way is number four, I get what? Is
            anybody good high school algebra that can help me out with that?</p>
        <p>听众：y 平方减 2 加 -2/1 加 y 平方 发言者 1：没错，我认为是 y 平方减 1 加 1/1 加 y 平方。现在怎么办？现在我们真的快要搞清楚了，因为这是一个和。</p>
        <p>AUDIENCE: Y squared minus 2 plus negative 2 over 1 plus y squared SPEAKER 1: Exactly, y squared minus 1 plus
            1 over 1 plus y squared, I think. Now what? Now we’re really getting close to getting through this, because
            that is a sum.</p>
        <h2 id="unknown-18">未知</h2>
        <h2>Unknown</h2>
        <p>由于它是一个和，所以它分为三部分，顶部部分是 y 平方的积分，中间部分是负 1 的积分，底部部分在所有情况下都是 1/1 加上 y 平方 dy 的积分。天哪，如果我查一下，我找到了。它就在那里，那是字母
            B。所以我完成了。</p>
        <p>And by virtue of the fact that it’s a sum, that divides into three pieces, and the top piece is the integral
            of y squared, the middle piece is the integral of minus 1, and the bottom piece is the integral of 1 over 1
            plus y squared dy in all cases. Gosh, if I look this up, I’ve found it. That’s up there, that’s letter B. So
            I’m done with that.</p>
        <p>这个我可以再次变换，借助 1，现在我得到了积分 dy。它在那里，那也是 B。至于这个，我不知道。但我最好跟踪我在这里所做的事情。这是在 and 节点中，所以我必须完成所有这些。我不能放弃最后一件事。</p>
        <p>This one I can transform again, by virtue of 1, and now I get the integral dy. That’s in there, that’s B as
            well. As this one, I don’t know. But I’d better keep track of what I’m doing here. This is in the and node,
            so I’ve got to do all of those. I can’t give up on that last thing.</p>
        <p>而这个和变换就是变换 3。所以这是在表格中，这是在表格中，我们还有这个要做，但这是 C，启发式变换 C。我们有 1，加上 y 平方，然后进行变换 C，加上 y。这是 y 平方。y 等于 z 的正切，然后我们得到 dz
            的积分，这是在表格中，我们完成了。所以现在我们已经解决了这个问题。</p>
        <p>And that and transformation is transformation number 3. So this is in the table, this is in the table, we
            still have this to do, but that’s C, heuristic transformation C. We have 1, plus y squared, then with the
            transformation C, with y. this is y squared. y equals tangent of z And then we get to the integral of dz and
            that’s in the table and, we’re done. So now we’ve solved the problem.</p>
        <h2 id="unknown-19">未知</h2>
        <h2>Unknown</h2>
        <p>这是 MIT 1.01
            期末考试中五年来最难的一道题。这道题就是当时给出的题目，只不过是从这里开始的。我把另外两道题放上来只是为了说明几个转换。但这是一道它解决的题目。现在我们已经看过一个例子了，我们可以继续之前讨论的与这个东西的架构有关的内容了。
        </p>
        <p>It’s the hardest problem that appeared in that half decade on MIT 1.01 finals. This is exactly the problem
            that was given, except that it started here. I put the other two pieces on just to illustrate a couple of
            the transformations. But that’s a problem that it solved. And now that we’ve seen an example, we can finish
            up what we talked about a little bit ago, having to do with the architecture of this thing.</p>
        <p>到目前为止，我们所做的只是讨论安全转换，但现在我们知道，如果我们没有完成，我们需要使用深度函数组合业务来找到一个要解决的问题。然后我们应用启发式转换。Slagle
            设计他的程序的方式是，他只找到一个要解决的问题，进行一次转换，然后返回循环。</p>
        <p>So far, all we’ve done is talk about the safe transformations, but now we know that if we’re not done, we
            need to find a problem to work on using that depth of functional composition business. And then after that
            we apply heuristic transformation. And the way Slagle designed his program is, he found just one problem to
            work on, did one transformation, then went back around the loop.</p>
        <p>因为这些启发式转换比安全转换更难应用。所以我将准确描述这个程序做了什么，除了一件事。现在我想回去修补一下。这件事就在这里。如何处理这样的事情。好吧，我们在一块消失的板上找到了这个问题，但是当我们试图处理这个问题时，我们必须找到一个启发式转换。
        </p>
        <p>Because these heuristic transformations are a little harder to apply than the safe ones. So I’ll given you an
            accurate portrayal of what this program did, except for one thing. Which I would like, now, to go back and
            patch up. And that thing is over here. What to do with something like this. Well we got to that in a board
            that’s disappeared, but when we tried to deal with this, we had to find a heuristic transformation.</p>
        <h2 id="unknown-20">未知</h2>
        <h2>Unknown</h2>
        <p>当我们决定研究这个问题时，一定是因为这是尚未解决的叶节点上最简单的问题。那么，这个函数组合深度是多少呢？</p>
        <p>And when we decided to work on this, it must have been the case that this was the simplest problem at a leaf
            node that has not yet been solved. So what’s the functional composition depth of this?</p>
        <p>是 3。回到这里，我们有一个函数组合深度为 2 的东西。所以当程序实际运行这个特定问题时，它会在距离终点线几英寸的地方停下来，然后返回并处理其他问题一段时间，然后它放弃并回到这里。所以它总是查看整棵树，树上的叶子。
        </p>
        <p>It’s 3. Back over here, we have something that has a depth of functional composition of 2. So when the
            program actually ran on this particular problem, it stopped a few inches short of the finish line, And went
            back and screwed around with that other problem for a little bit, before it gave up and came back here. So
            it’s always looking across the whole tree, the leaves of the tree.</p>
        <p>每当它需要找到一个地方来进行启发式转换时，它就会查看树上所有尚未处理的叶子，并尝试找到最容易处理的叶子，而这可能需要大量后退并重新开始之前忽略的分支。这是一个小细节，并不是特别重要。现在我们在哪里。我们已经把那个家伙带到了那里。
        </p>
        <p>Whenever it has to find a place to work on with the heuristic transformation, it happened to look at all the
            leaves of the tree that had not yet been dealt with, tried to find the easiest one, and that could involve a
            lot of backing up and starting over on a branch of the tree that it had previously ignored. A small detail,
            not a particularly important one. Now where are we. We’ve got that guy there.</p>
        <h2 id="unknown-21">未知</h2>
        <h2>Unknown</h2>
        <p>我们已经有了完整的架构。我们已经解决了问题。现在我们可以开始反思我们所做的工作。例如，我们可以说，这是一个多好的集成程序？答案是，它相当不错。</p>
        <p>We’ve got our complete architecture. We’ve got our solved problem. And now we can start reflecting on what
            we’ve done. We can say, for example, how good an integration program is this? And the answer is, it was
            pretty good.</p>
        <p>Slagle 使用的这台机器位于 26 号楼。我们对它感到非常自豪，它位于玻璃后面，你可以去那里观看磁带旋转，这真的是一种享受。32k 内存，就是 32k
            内存。他能用这么大的机器做任何事情真是太神奇了。让我们看看，让我们拿一台干净的。</p>
        <p>This machine that Slagle was using was a machine that was over in building 26. And we were so proud of it,
            that it was behind glass, and you could go there and watch the tape spin, it was really a delight. 32k of
            memory, that’s 32k of memory. It’s amazing that he was able to do anything with a machine of that size.
            Let’s see, let’s get us a clean one.</p>
        <p>无法同时进行棋盘几何运算和对话。我们现在可以问一些关于程序执行情况的问题。它被给予了 56 个最难的问题，其中 54 个答对了。当它没有答对另外两个问题时发生了什么？好吧，如果你说“哦，它可能内存不足了，因为它有
            32k”的话，你可能是对的。</p>
        <p>Can’t do board geometry and talk at the same time. We can now ask some questions about how well the program
            performed. It was given 56 of the hardest problems, and it got 54 right. What happened when it didn’t get
            the other two? Well, you might be right if you said, oh it probably ran out of memory, since it had 32k.</p>
        <h2 id="unknown-22">未知</h2>
        <h2>Unknown</h2>
        <p>但事实上，它只是缺少 2 个转换，而这些转换是解决整个期末测验问题所必需的。所以当一个程序失败时，这往往是你能问的最有趣的问题。这是一个例外。它在 56 个问题中，有 2
            个因无趣的原因而失败。现在你可以问的下一个问题是，在最大情况下树的深度是多少？</p>
        <p>But in fact, it just was lacking 2 transformations that were needed, in order to solve the whole entire set
            of final quiz problems. So when a program fails, that’s often the most interesting question you can ask.
            This is an exception. This failed for uninteresting reasons on 2 of the 56 problems that it was given to.
            And now the next question you can say is, what is the depth of the tree in the maximal case?</p>
        <p>答案是，我们刚刚算出的就是这种情况。由于我又一次丢失了整棵树，我会告诉你，当你减去 5 时，它的深度是 7。所以在最坏的情况下，这个东西必须下降七层。这是最坏的情况，一个更有趣的问题是平均深度是多少？</p>
        <p>And the answer is, it’s that case we just worked out. And since I’ve once again lost the whole tree, I’ll
            tell you that it’s depth was 7 when you take off that minus 5. So in the worst case, this thing had to get
            down seven levels. That’s the worst case, a more interesting question is what was the average depth?</p>
        <p>大约是 3。现在我们开始谈论一些事情，不仅是关于斯莱格尔的新生工作模式，而且我们开始谈论领域的性质。在微积分问题领域，给新生的积分表达式，在这个领域，解决问题所需的问题简化的平均深度是 3。所以这并不太复杂。</p>
        <p>And that was approximately 3. And now we’re beginning to say something, not only about Slagle’s model of how
            a freshman works, but we’re beginning to say something about the nature of the domain. In the domain of
            calculus problems, integrals expressions that are given to freshman, in that domain, the average depth of
            problem reduction needed to solve the problem was 3. So that’s not very complicated.</p>
        <h2 id="unknown-23">未知</h2>
        <h2>Unknown</h2>
        <p>如果是 10，你会说，哇，怎么会有人做得到这些问题？如果是 5，你会说，只有注定成为数学教授的人才能做对。如果是
            3，我们普通人就能做得很好。另一个更有趣的问题是，有多少分支未被使用？这是一个后来被证明未被使用的分支，它没有继续研究。</p>
        <p>If it were 10, you would say, wow, how can anybody ever do those problems? If it were 5, you’d say, well only
            people destined to be math professors are going to get anything right. If it’s 3, us ordinary mortals can do
            a pretty good job. Another question of even greater interest is, how many branches were unused? Here’s a
            branch that turned out to be unused, it didn’t pursue that.</p>
        <p>因此你可能会说，也许有很多未使用的分支。也许你必须非常聪明地确定要解决的问题，否则你会陷入很多困境。猜猜看，这是关于领域的另一个陈述。</p>
        <p>And so you might say, well maybe there are a lot of unused branches. Maybe you have to be pretty smart about
            your method for determining what problem to work on, because otherwise you’ll go down a lot of rat holes.
            And guess what, here’s another statement about the domain.</p>
        <p>在大一学生可以解决的期末问题领域中，未使用的分支数量约为 1。这意味着这棵树保持了自身完整，不会变成一棵非常大、茂密、无用的树。因此，这意味着函数组合的深度（Brett
            建议将其作为识别正确问题的一种技术）实际上并不重要。因为树不会长得很深，所以它不会变宽。</p>
        <p>In the domain of problems that freshmen could work on a final, the number of unused branches is about 1. So
            that means this tree keeps itself together, and doesn’t run down to a very large, bushy, useless tree. So
            this means that the depth of functional composition, which Brett suggested as a technique for recognizing
            the right problem work on, was a choice that didn’t actually matter. Because the tree doesn’t grow deep, it
            doesn’t go broad.</p>
        <h2 id="unknown-24">未知</h2>
        <h2>Unknown</h2>
        <p>你用什么来决定要做什么并不重要，因为在最坏的情况下，你只会生成几个额外的、无用的节点。但它们很快就会找到死路，所以你不必对它们做任何进一步的事情。所以现在我们需要做的下一件事是远离这个程序，问自己一些关于我们所做事情的性质的问题。
        </p>
        <p>It doesn’t matter what you use to decide what to work on, because in the worst case, you’ll just generate a
            couple of extra, useless nodes. But they very quickly run to find a dead end, so you don’t have to do
            anything more with them. So now the next thing we need to do is back even further away from this program,
            and ask ourselves some questions about the nature of what we’ve been doing.</p>
        <p>这让我想到了右上角板上的东西。其中一件事情是作为教义问答与知识有关。我们在进行这个项目时非正式地做过的事情是，我们问过这样的问题：做这件事需要什么样的知识？关于转变的知识。关于目标树如何工作以及我们何时解决问题的知识。
        </p>
        <p>And that brings me to the things I’ve got on that upper right hand board. One of those things as a catechism
            having to do with knowledge. And what we’ve done informally as we went through this program was, we’ve asked
            questions such as, what kind of knowledge is involved in doing this? Well knowledge about transformation.
            Knowledge about how goal trees work and when we’re done with a problem.</p>
        <p>关于什么事物的知识不需要转换，因为你可以在表格中查找它们。这就是做 1.01 所涉及的知识。如果你做 1.0 电路理论、6 0 电路理论或 6 0
            麦克斯韦方程，这都是同样的事情。你必须问这类问题，关于所涉及的知识的性质，而第一个问题总是，涉及什么样的知识？</p>
        <p>Knowledge about what things don’t need to be transformed, because you can look them up in a table. That’s the
            kind of knowledge that is involved in doing 1.01. And if you do 1.0 circuit theory, 6 0 circuit theory or 6
            0 Maxwell’s equations, this is the same thing. You have to ask questions of this sort, about the nature of
            the knowledge involved, and question number one is always, what kind of knowledge is involved?</p>
        <h2 id="unknown-25">未知</h2>
        <h2>Unknown</h2>
        <p>是基尔霍夫定律、麦克斯韦方程，还是其他什么？下一个问题是，知识是如何表示的？我们的答案是，所有这些内容最终都以最佳表达式列表的形式表示出来。一些知识被记录在表达式表中，以显示有哪些变换。有一个类似的积分表。关于目标树的知识嵌入在程序中，因此它是程序化表示的。
        </p>
        <p>Is it Kirchhoff’s laws, Maxwell’s equations, what is it? The next question is, how is the knowledge
            represented? And our answers here are, well all this stuff, ultimately was represented in list best
            expressions. Some of the knowledge was recorded in a table expressions to show what transformations there
            are. There was a similar table of integrals. Knowledge about goal trees was embedded in the procedure, so it
            was procedurally represented.</p>
        <p>因此，对于每个知识类别，都有一种表示方法。如何使用？很简单，转换用于简化问题。表格用于修剪并作为树的底部。这些是使用知识的方式。然后当然还有一个问题，即需要多少知识。</p>
        <p>And so for each of the categories of knowledge, there’s a way it gets represented. How is it used?
            Straightforward, transformations are used to make the problem simpler. The table is used to trim off and to
            serve as the bottom of the tree. Those are the ways in which the knowledge is used. And then there’s the
            question of course of, how much knowledge is required.</p>
        <p>如果时间已经很晚，第二天有两门期末考试，而你又不确定应该学哪门课，那么知道这一点就很有用了。那么你认为这个课程实际上包含多少知识呢？我向你展示了这个课程涉及的知识。我回答了问题 5
            中的一小部分，具体是什么。但涉及多少知识呢？答案可能会让你感到惊讶。</p>
        <p>Something that’s useful to know if it’s late at night, you have 2 finals the next day, and you’re not sure
            which course you should study. So how much knowledge might you suppose was actually in this program? I’ve
            shown you a glimpse of the kind of knowledge that’s involved in the program. I’ve answered a little bit of
            question 5, what exactly. But how much knowledge was involved. You might be surprised by the answer.</p>
        <h2 id="unknown-26">未知</h2>
        <h2>Unknown</h2>
        <p>首先，积分表。我只列出了 3 个。你可以想到很多其他的东西，比如 e 到 x 的积分就是 e 到​​ x 的积分。但最终，Slagle 发现，一个只有 26 个元素的表足以解决所有这些问题。</p>
        <p>First of all, the table of integrals. I’ve listed only 3 things there. There are lots of other things you can
            think of, like integral of e to the x is e to the x. But in the end, what Slagle found is, a table only 26
            elements was enough to solve all of these problems.</p>
        <p>这里的变换怎么样，安全的变换，大约有 12 种。启发式变换怎么样，大约有 12 种。因此，只需零零碎碎地掌握一些知识，就足以完成微积分期末考试中的积分问题。这真是一个惊喜。</p>
        <p>How about the transformations here, the safe ones, about 12. How about the heuristic ones, about 12. So just
            a few bits and pieces of knowledge, here and there, are sufficient to do everything you need to do, in order
            to do the integration problems on a calculus final. That was a surprise.</p>
        <p>另一个类似的惊喜，也是关于知识的，是要使用的方法和问题的特征之间的关系几乎是一个对角表。这意味着，在这个领域，如果你有点聪明，你几乎可以随时做出正确的转换，而且永远不会后退。这是乔尔·摩西 (Joel Moses)
            的观察结果，他后来曾担任麻省理工学院的教务长一段时间。</p>
        <p>Another surprise of a similar kind, also about knowledge, is that the relationship between the method to be
            used, and the characteristics of the problem, was almost a diagonal table. That means that you could, in
            this domain, make the right transformation almost all the time if you’re a little bit smart, and never back
            up. That was an observation made by Joel Moses, who became subsequently our provost here at MIT for a while.
        </p>
        <h2 id="unknown-27">未知</h2>
        <h2>Unknown</h2>
        <p>他编写了一个可以解决任何问题的程序。它在积分方面击败了最敬业的数学家。它的后代今天在 MATLAB
            中。但这就是它的工作原理。现在你可以自己编写其中一个。部分原因是你现在有了这个问答。这是你在处理新领域时应该问的问题。它会让你变得更聪明。</p>
        <p>And he wrote a program that could solve anything. It would beat the most dedicated mathematicians at
            integration. And its descendents are in MATLAB today. But this is how it all works. And now you can write
            one of these things yourself. Partly because you now have this catechism. This is the kind of stuff you
            should ask any time you’re dealing with a new domain. It will make you smarter.</p>
        <p>当然，这是元知识，是关于知识的知识。所以这句陈词滥调的格言并不足以让我们完善自己。我们要说的是，关于知识的知识才是真正的力量所在。现在，这个程序为我们做了最后一件事。它告诉我们一些关于我们对智慧的理解。</p>
        <p>And this is of course, meta knowledge, this is knowledge about knowledge. So this tired aphorism isn’t quite
            what we are going to complete ourselves with. We’re going to say that knowledge about knowledge is where the
            real power is. Now there’s one final thing that this program does for us. It tells us something about our
            appreciation of what it means to be intelligent.</p>
        <p>你知道，在这一个小时的开始，我要求你们思考一个可以进行符号积分的程序是否在任何方面都是智能的，或者应该在任何程度上被认为是智能的。我想，即使在 MATLAB
            等软件盛行的今天，你们中的许多人也会说，是的，我在麻省理工学院或高中后期学会了如何做这件事，所以它一定很智能。</p>
        <p>You know that in the beginning of this hour, I asked you to think about whether a program that could do
            symbolic integration would be, in any way, or should be considered to any degree, intelligent. And I’m
            imagining that even in these days of MATLAB, and whatnot, many of you said well, yes, I learned how to do
            that at MIT, or late in high school, so it must be smart.</p>
        <h2 id="unknown-28">未知</h2>
        <h2>Unknown</h2>
        <p>但是现在我们已经结束了讨论，我也希望你对这个程序的智能感觉有所减弱。因为当我们理解了某些东西的工作原理后，它的智能似乎就消失了。你在朋友身上看到过这种情况，对吧？他们解决了一些问题，他们看起来非常聪明。然后他们告诉你他们是如何做到的，他们看起来就不再那么聪明了。所以，我们今天的讨论可以总结为一个小故事。
        </p>
        <p>But now that we’ve completed this discussion, I also expect that your feeling of intelligence in this program
            is somewhat diminished. Because what happens is that, when we understand how something works, it’s
            intelligence seems to vanish. You’ve seen this in your friends, right? They solve some problem, they seem
            super smart. Then they tell you how they did it, and they don’t seem so smart anymore. So let’s conclude our
            discussion today was a little story.</p>
        <p>很久以前，我曾与一位学生交谈，他说计算机不可能是智能的。我说，好吧，也许你是对的，但让我给你展示一下这个程序。于是我向他展示了这个集成程序，它正在解决类似的问题。在我向他展示了几个例子之后，他说，好吧，我想它们可能可以智能化。我正在学习如何做到这一点，但这并不总是那么容易。
        </p>
        <p>A long time ago I was talking with a student who said, computers cannot be intelligent. And I said, OK, maybe
            you’re right, but let me show you this program. So I showed him the integration program, working on problems
            like this. And after I showed him a couple of those examples, he says, well, all right, I guess maybe they
            can be intelligent. I’m learning how to do that, and it’s not always easy.</p>
        <p>然后我犯了一个致命的错误。我说让我给你展示一下它是如何工作的，然后我们花了一个小时像这样演示它。最后，他转过身对我说，我收回我说的话，它毕竟不是智能的。它集成的方式和我一样。</p>
        <p>Then I made a fatal mistake. I said let me show you how it works, and we spent an hour going through it like
            this. And at the end of that time, he turned to me and said, I take it back, it’s not intelligent after all.
            It does integration the same way I do.</p>
        <h1 id="reasoning-goal-trees-and-rule-based-expert-systems">3. 推理：目标树和基于规则的专家系统</h1>
        <h1>3. Reasoning: Goal Trees and Rule-Based Expert Systems</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:56 <a
                href="https://youtube.com/watch?v=leXa7EKUPFk">https://youtube.com/watch?v=leXa7EKUPFk</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:56 <a
                href="https://youtube.com/watch?v=leXa7EKUPFk">https://youtube.com/watch?v=leXa7EKUPFk</a></p>
        <h2 id="introduction-1">介绍</h2>
        <h2>Introduction</h2>
        <p>帕特里克·温斯顿教授：女士们，先生们，这是工程师们的饮酒歌。过去，我听这首歌喝了很多酒。作为饮酒歌，它还不错。不过，我提醒你，在小孩子面前播放这首歌之前，先试听一下。有些歌词非常恶心，水手们听了都会脸红。这是一首有趣的歌，因为它有无数的歌词。这是数学证明。
        </p>
        <p>PROFESSOR PATRICK WINSTON: Ladies and gentlemen, the engineers drinking song. Back in the day, I’ve drunk
            quite a lot to that song. And as drinking songs go, it’s not bad. I caution you, however, before playing
            this song in the presence of small children, audition it first. Some of the verses are sufficiently gross as
            to make a sailor go beyond blushing. It’s an interesting song because there are an infinite number of
            verses. Here’s the mathematical proof.</p>
        <p>假设诗句的数量是有限的。那么就会有最后一节。如果有最后一节，那么一些喝醉的校友就会创作一首新诗。因此，没有最后一节，诗句的大小不是有限的，诗句的数量是无限的。我今天为你们演奏这首诗，因为我是一名工程师。我喜欢建造东西。我用木头建造东西。我用金属建造东西。
        </p>
        <p>Suppose there were a finite number of verses. Then there would be a last verse. And if there were a last
            verse, then some drunk alumni would compose a new one. Therefore, there is no last verse, the size is not
            finite, and there are an infinite number of verses. I play it for you today because I’m an engineer. I like
            to build stuff. I build stuff out of wood. I build stuff out of metal.</p>
        <p>我用石头造东西。我特别喜欢写程序。我不知道，有时人们会来找我，说我主修计算机科学，但我不喜欢写程序。我一直对此感到困惑。我的意思是，如果你想要展示你有多坚强，你可以去蹦极或用钉子刺穿你的手或类似的东西。</p>
        <p>I build stuff out of rocks. And I especially like to write programs. I don’t know, sometimes people come to
            me and say, I’m majoring in computer science, but I don’t like to write programs. I’ve always been mystified
            by that. I mean, if you want to show how tough you are, you can go bungee jumping or drive a nail through
            your hand or something like that instead.</p>
        <p>但我已经编写了不少程序来演示这个主题的内容。它们都是用 Java 编写的，主要是因为我可以通过 Web Start 将它们提供给您和世界其他地方。</p>
        <p>But I’ve written quite a few programs for demonstrating stuff in this subject. They’re all written in Java,
            principally because I can therefore make them available to you and to the rest of the world by way of Web
            Start.</p>
        <p>几周前，我在摆弄系统时弄坏了服务器上的版本，不到 15
            分钟，我就收到一封来自安纳托利亚深处的某人的电子邮件，抱怨此事并要求我恢复它。这个特定的程序仿照了早期的人工智能经典。它是特里·维诺格拉德编写的程序的业务端，他后来成为斯坦福大学的计算机科学教授。</p>
        <p>A few weeks ago, I was mucking around with the system and broke the version on the server, and within 15
            minutes, I got an email from somebody in the depths of Anatolia complaining about it and asking me to bring
            it back up. This particular program is patterned after an early AI classic. And it was the business end of a
            program written by Terry Winograd, who became, and is, a professor of computer science at Stanford
            University.</p>
        <p>对于你们这些人来说，这位于西海岸。他在这个程序的自然语言前端所做的工作非常出色。但是自然语言部分并不是我们今天感兴趣的内容。我们更感兴趣的是其他类型的东西。让我们把这些东西堆起来。现在，我要要求做点什么，也许把 B2
            放在 B7 上面。还不错。把 B6 放在 B3 上怎么样？这个程序很聪明。</p>
        <p>Which is on the west coast for those of you. on the strength of his work on the natural language front end of
            this program. But the natural language part is not what makes it of interest for us today. It’s more the
            other kinds of stuff. Let’s pile these things up. Now, I’m going to ask to do something, maybe put B2 on top
            of B7. Not bad. How about B6 on B3? This program’s kind of clever.</p>
        <p>我再来做一次。让我们把 B7 放在 B2 上。好的，现在让我们看看。也许 B5 放在 B2 上？也许先把 B4 放在 B3 上？哦，我一定是按错了按钮。哦，就是这样。好的。让我们把 B4 放在 B1
            上。啊，我的鼠标一直失控。现在，让我们把 B1 放在 B2 上。这是我实际上要在黑板上练习的一个例子。哦，我明白了。我的触摸板意外激活了。</p>
        <p>Let me do one more. Let’s put B7 on B2. OK, now let’s see. Maybe B5 on B2? B4 on B3 first, maybe? Oh, I must
            have clicked the wrong button. Oh, there it goes. OK. Let’s put B4 on B1. Agh, my mouse keeps getting out of
            control. Now, let’s put B1 on B2. This is an example I’m actually going to work out on the board. Oh, I see.
            My touch pad accidentally got activated.</p>
        <p>B1 放在 B2 上。现在，我们来问一个问题。好的。嗯。发言人 2：帕特里克·温斯顿教授：停。发言人 3：帕特里克·温斯顿教授：说得够多了。让我们看看。你为什么要放。你为什么要去掉 B4？好的，一个。发言人
            2：帕特里克·温斯顿教授：也许他们认为，当你使用自己编写的软件时就会发生这种情况。你为什么要清除 B2 的顶部？我这样做了吗？</p>
        <p>B1 on B2. Now, let’s ask a question. OK. Well. SPEAKER 2: PROFESSOR PATRICK WINSTON: Stop. SPEAKER 3:
            PROFESSOR PATRICK WINSTON: Had enough of that. Let’s see. Why did you put. why did you want to get rid of
            B4? OK, one. SPEAKER 2: PROFESSOR PATRICK WINSTON: Maybe they think, that’s what happens when you use
            software you write yourself. Why did you want to clear the top of B2? Did I do that?</p>
        <p>你为什么会清除 B1 的顶部？好的。说话者 2：说话者
            3：帕特里克·温斯顿教授：哦，它一直困扰着我。是的。所以我想喝酒歌很容易被冒犯。但我不会再重复这个场景了。我想向你展示的是，这个程序看起来有点聪明，它可以回答有关自身行为的问题。你写过一个回答有关自身行为的问题的程序吗？可能没有。
        </p>
        <p>Why did you clear the top of B1? OK. SPEAKER 2: SPEAKER 3: PROFESSOR PATRICK WINSTON: Oh, it’s haunting me.
            Yeah. So the drinking song is easily offended, I guess. But I won’t develop that scenario again. What I want
            to show you is that this program looks like it’s kind of smart, and it somehow can answer questions about
            its own behavior. Have you ever written a program that’s answered questions about its own behavior? Probably
            not.</p>
        <p>你想学怎么做吗？好的。所以，到这一小时结束时，你将能够编写这个程序以及更多类似的程序，它们知道如何回答有关自身行为的问题。已经有成千上万个这样的程序被编写出来，但只有那些知道我现在要告诉你的东西的人才能编写，好吗？
        </p>
        <p>Would you like to learn how to do that? OK. So by the end of the hour, you’ll be able to write this program
            and many more like it that know how to answer questions about their own behavior. There have been tens of
            thousands of such programs written, but only by people who know the stuff I’m going to tell you about right
            now, OK?</p>
        <h2 id="program-structure">项目结构</h2>
        <h2>Program Structure</h2>
        <p>所以我想要做的是，首先把这个程序拆开放在板上，然后向你介绍一下它包含的模块和子程序。这就是它。我们需要考虑的第一件事是，这里有一些块。如果我要把底部的块放在较大的块上，会发生什么？首先，我必须为它找到空间。</p>
        <p>So what I want to do is I want to start by taking this program apart on the board and talking to you about
            the modules, the subroutines that it contains. So here it is. The first thing we need to think about, here
            are some blocks. What has to happen if I’m going to put the bottom block on the larger block? Well, first of
            all, I have to find space for it.</p>
        <p>然后我必须抓住下面的积木。我必须移动它，我必须松开它。所以，为了实现我想要做的事情，我需要做这四件事。因此，我知道 put on 方法有四个部分。它必须在目标积木上找到空间。它必须抓住它被命令移动的积木。</p>
        <p>Then I have to grasp the lower block. And I have to move it and I have to ungrasp it. So those are four
            things I need to do in order to achieve what I want to do. So therefore, I know that the put on method has
            four pieces. It has to find space on the target block. It has to grasp the block that it’s been commanded to
            move.</p>
        <p>然后它必须移动，然后必须松开。但是从它回答过的一些问题中得到提示，在我被音乐所困扰之前，我们从中得到提示，我们知道，为了抓住某物，在这个特定的世界中，你不能在它上面放任何东西。因此，grasp 可能会调用 clear
            top 以便从目标对象上取下东西。</p>
        <p>Then it has to move, and then it has to ungrasp. But taking hints from some of the questions that it did
            answer before I got haunted by the music, taking our cue from that, we know that in order to grasp
            something, in this particular world, you can’t have anything on top of it. So grasp, therefore, may call
            clear top in order to get stuff off from the target object.</p>
        <p>这可能发生在迭代循环中，因为上面可能有几件事。那么你如何摆脱这些东西呢？好吧，通过调用 get rid of。这可能会循环几次。然后，你摆脱东西的方法是调用 put on。</p>
        <p>And that may happen in an iterative loop because there may be several things on top. And how do you get rid
            of stuff? Well, by calling get rid of. And that may go around a loop several times. And then, the way you
            get rid of stuff is by calling put on.</p>
        <p>这样就得到了递归，正是通过递归，我们得到了程序在解决问题时行为的很多明显复杂性。现在，为了找到空间，你还必须调用 get rid of。这就是我要放置另一个迭代循环的地方，而不是下面。Cleat top
            里面有迭代循环。这就是程序的结构。它非常简单。</p>
        <p>So that gives us recursion, and it’s from the recursion that we get a lot of the apparent complexity of the
            program’s behavior when it solves a problem. Now, in order to find space, you also have to call get rid of.
            So that’s where I meant to put this other iterative loop, not down here. Cleat top has got the iterative
            loop inside of it. So that’s the structure of the program. It’s extremely simple.</p>
        <p>你可能会问我，那么，你如何能从这么简单的程序中得到如此复杂的行为呢？这是一个合理的问题。但在我们开始解决这个问题之前，我想用一个非常简单的块问题模拟一下这个程序。这就是我差点向你展示的那个，但它是这样的。这是
            B1。我们将其称为 BX，因为我忘记了它的名字。这是 BY。</p>
        <p>And you might say to me, well, how can you get such apparently complex looking behavior out of such a simple
            program? A legitimate question. But before we tackle that one head on, I’d like to do a simulation of this
            program with a very simple blocks problem. And it’s the one I almost showed you, but it goes like this.
            Here’s B1. We’ll call this BX because I forgot its name. Here’s BY.</p>
        <p>这是 B2。任务是将 B1 放在 B2 上。根据我们的系统图，这会导致对子程序的四次调用。我们必须找到空间。我们必须抓住
            B1。我们必须移动，然后我们松开。现在，我们抓住某物的方式是，我们要做的第一件事就是清除它的顶部。因此，抓取会调用清除顶部。而清除顶部又会调用摆脱。</p>
        <p>And here’s B2. And the task is to put B1 on B2. And according to our system diagram, that results in four
            calls to subroutines. We have to find space. We have to grasp B1. We have to move, and then we ungrasp. Now,
            the way we grasp something is the first thing we have to do is clear off its top. So grasp calls clear top.
            And clear top in turn calls get rid of.</p>
        <p>让我看看。让我跟踪一下这些。这是清除 B1 的顶部，这是摆脱 BX。摆脱 BX 的方法是将 BX
            放在桌子上。然后这又会导致调用另一个查找空间、另一个抓取、另一个移动和另一个取消抓取。所以这是程序在解决这个简单问题时的一些痕迹。</p>
        <p>And let me see. Let me keep track of these. This is clearing the top of B1, and this is getting rid of BX.
            And the way we get rid of BX is by putting BX on the table. And then that in turn causes calls to another
            find space, another grasp, another move, and another ungrasp. So that’s a little trace of the program as it
            works on this simple problem.</p>
        <p>那么它如何回答我刚才向您演示的问题呢？让我们使用此跟踪来回答。那么，例如，它如何回答这个问题，为什么您要摆脱 BX？您怎么看？它如何回答这个问题？发言人
            4：帕特里克·温斯顿教授：因此，它会上升一级并报告它所看到的内容。</p>
        <p>So how does it go about answering the questions that I demonstrated to you a moment ago? Let’s do that by
            using this trace. So how, for example, does it answer the question, why did you get rid of BX? what do you
            think? How can it answer that question? SPEAKER 4: PROFESSOR PATRICK WINSTON: So it goes up one level and
            reports what it sees.</p>
        <p>所以它说，并在演示中说，我摆脱了 BX，因为我试图清除 B1 的顶部。所以如果我问你为什么清除 B1 的顶部，它会说因为我试图抓住它。</p>
        <p>So it says, and said in the demonstration, I got rid of BX because I was trying to clear the top of B1. So if
            I were to say why did you clear the top of B1, it would say because I was trying to grasp it.</p>
        <p>如果我说，你为什么理解 B1，它会说因为我把 B1 放在了 B2 上。如果我说，你为什么把 B1 放在 B2
            上，它会说，因为你告诉我这么做。好吧，这就是它处理为什么问题的方式。那么如何问题呢？蒂莫西，你觉得这个问题怎么样？它会如何回答关于你如何做某事的问题？你有什么想法吗？</p>
        <p>If I were to say, why did you grasp B1, it would say because I was putting B1 on B2. If I say, why did you
            put B1 on B2, it would say, slavishly, because you told me to. OK, so that’s how it deals with why
            questions. How about how questions? Timothy, what do you think about that one? How would it go about
            answering a question about how you did something? Do you have a thought?</p>
        <p>蒂莫西：嗯，是的，它会思考我想要完成什么。帕特里克·温斯顿教授：是的，但它要怎么做呢？程序要怎么做呢？我们知道回答“为什么”的问题会让它上升一个层次。它如何回答“如何”的问题呢？塞巴斯蒂安？塞巴斯蒂安：它会下降一个层次。帕特里克·温斯顿教授：你下降一个层次。所以你从这里开始，一路上升。
        </p>
        <p>TIMOTHY: Um, yeah, it would think about what I was trying to accomplish. PROFESSOR PATRICK WINSTON: Yeah, but
            how would it do that? How would the program do that? We know that answering a why question makes it go up
            one level. How does it answer a how question? Sebastian? SEBASTIAN: It goes down one level. PROFESSOR
            PATRICK WINSTON: You go down one level. So you start off all the way up here with a put on.</p>
        <p>你会说，哦，我做了这四件事。你说，你为什么要抓住 B1？它会说因为我试图清除它的顶部。你为什么要清除它的顶部？因为我试图摆脱它。你为什么要摆脱它？因为我试图把它放在桌子上。</p>
        <p>You will say, oh, well I did these four things. You say, why did you grasp B1? It will say because I was
            trying to clear its top. Why did you clear its top? Because I was trying to get rid of it. Why were you
            trying to get rid of it? Because I was trying to put it on the table.</p>
        <p>这就是它回答“如何”问题的方式，通过深入研究这棵树和行动计划的踪迹，看看事情是如何组合在一起的。这些东西组合在一起的是什么？我一直避免使用什么词来将这个问题推向高潮？这些目标是什么，这些事情想要做什么？它们是目标。所以这个东西留下了一个踪迹，也就是一棵目标树。
        </p>
        <p>So that’s how it answers how questions, by going down in this tree and this trace of the program of action so
            as to see how things are put together. What are these things that are being put together? What’s the word
            I’ve been avoiding so as to bring this to a crescendo? What are these objectives, these things it wants to
            do? They’re goals. So this thing is leaving a trace, which is a goal tree.</p>
        <p>这听起来熟悉吗？三天前，我们讨论了与积分相关的目标树。所以这个东西就是构建一个目标树，也称为与或树。所以这一定是一棵与树。如果这是一棵与树，有没有与节点？当然，那里有一个。那么你认为只要你构建了一棵与或树，你就可以回答有关你自己行为的问题吗？当然。
        </p>
        <p>Does that sound familiar? Three days ago, we talked about goal trees in connection with integration. So this
            thing is building a goal tree, also known as an and or tree. So this must be an and tree. And if this is an
            and tree, are there any and nodes? Sure, there’s one right there. So do you think then that you can answer
            questions about your own behavior as long as you build an and or tree? Sure.</p>
        <p>这是否意味着整合程序可以回答有关其自身行为的问题？当然。因为它们都构建了目标树，并且只要您有目标树，就可以回答有关您自身行为的某些问题。</p>
        <p>Does this mean that the integration program could answer questions about its own behavior? Sure. Because they
            both build goal trees, and wherever you got a goal tree, you can answer certain kinds of questions about
            your own behavior.</p>
        <h2 id="goal-trees">目标树</h2>
        <h2>Goal Trees</h2>
        <p>所以让我看看它是否真的在解决问题时为自己构建了一个目标树。所以这次，我们将 B6 放在 B3 上。但要观察它发展其目标树。</p>
        <p>So let me see if in fact it really does build itself a goal tree as it solves problems. So this time, we’ll
            put B6 on B3 this time. But watch it develop its goal tree.</p>
        <p>因此，与我在黑板上练习的简单示例相比，这是一个相当复杂的目标树。但我仍然可以回答有关行为的问题。例如，我可以说，你为什么把 B6 放在 B3 上？因为你告诉我这么做。</p>
        <p>So in contrast to the simple example I was working on the board, this gets to be a pretty complicated goal
            tree. But I could still answers questions about behavior. For example, I could say, why did you put B6 on
            B3? Because you told me to.</p>
        <h2 id="herb-simon">赫伯·西蒙</h2>
        <h2>Herb Simon</h2>
        <p>好吧，所以行为的复杂性在很大程度上不是由这个特定案例中的程序复杂性造成的，而是由于问题的复杂性而构建的这个巨大的目标树。这让我们回到了我们之前讨论的一件事。回到今天的金星想法之一。</p>
        <p>All right, so the complexity of the behavior is largely a consequence not of the complexity of the program in
            this particular case, but the building of this giant goal tree as a consequence of the complexity of the
            problem. This brings us to one of our previous matters. early on to one of the gold star ideas of today.</p>
        <p>这个金星理念可以追溯到 60 年代末 Herb Simon 的一次演讲，他是伪诺贝尔经济学奖的第一位诺贝尔奖获得者。对吗，Bob？他是第一个吗？是的，他是第一个获得诺贝尔奖（伪诺贝尔经济学奖）的人。</p>
        <p>And this gold star idea goes back to a lecture given in the late ’60s by Herb Simon, who was the first Nobel
            Laureate in the pseudo Nobel Prize for economics. Is that right, Bob? Was he the first? All right, he was
            the first winner of the Nobel Prize, pseudo Nobel Prize in economics.</p>
        <p>在题为“人造科学”的讲座中，他说，想象一下你在海滩上看着一只蚂蚁的路径。他说，嗯，你知道，蚂蚁的路径看起来极其复杂。你很容易认为蚂蚁是某种天才或大脑怪兽。</p>
        <p>And in this lecture, which was titled “The Sciences of the Artificial,” he said imagine that you’re looking
            on a beach at the path of a ant. And he said, well, you know, the path of the ant looks extremely
            complicated. And you’re tempted to think the ant is some kind of genius or monster brain ant.</p>
        <p>但事实上，当你仔细观察时，你会发现海滩上有很多鹅卵石，而蚂蚁回家时所做的一切就是避开这些鹅卵石。因此，行为的复杂性，西蒙说，是环境复杂性的结果，而不是程序复杂性的结果。这就是隐喻中的西蒙的蚂蚁。</p>
        <p>But in fact, when you take a closer look, what you discover is that there are a bunch of pebbles on the
            beach, and all the ant is doing is avoiding those pebbles on his way home. So the complexity of the
            behavior, said Simon, is a consequence of the complexity of the environment, not the complexity of the
            program. So that’s the metaphoric Simon’s ant.</p>
        <p>它所说的是行为的复杂性是程序复杂性和环境复杂性的最大值。</p>
        <p>And what it says is that the complexity of the behavior is the max of the complexity of the program and the
            complexity of the environment.</p>
        <h2 id="complex-behavior-simple-program">复杂行为简单程序</h2>
        <h2>Complex Behavior Simple Program</h2>
        <p>所以，我们会在本学期余下的时间里多次看到这种情况。复杂的行为，简单的程序。你认为它会很复杂。结果却很简单，因为问题具有复杂性，而不是程序。</p>
        <p>So that’s something we’ll see many times during the rest of the semester.complex behavior, simple program.
            You think it’s going to be complicated. It turns out to be simple because the problem has the complexity,
            not the program.</p>
        <p>因此，这引出了我们今天演讲的第三个复选框，这里有点场景化，因为现在我想停止谈论以目标为中心的编程，开始谈论基于规则的专家系统。基于规则的专家系统是在 20 世纪 80
            年代中期人们对人工智能商业应用前景的热情高涨下开发的。</p>
        <p>So that brings us to check box three in today’s talk, and there’s a little bit of a scene here because now I
            want to stop talking about goal centered programming and start talking about rule based expert systems. The
            rule based expert systems were developed in a burst of enthusiasm about the prospects for commercial
            applications of artificial intelligence in the mid 1980s.</p>
        <h2 id="simple-rules">简单规则</h2>
        <h2>Simple Rules</h2>
        <p>当时，人们认为长篇文章是可以写的，但你可以通过将所有知识以简单规则的形式写下来，来解释人类智能的有用方面。所以如果这是真的，那也是真的。如果你想实现这个，那就这么做吧。但所有的知识都必须以简单规则的形式封装起来。那么你想用它做什么呢？各种各样的事情。
        </p>
        <p>At that time, it was supposed lengthy articles are written, but you could account for useful aspects of human
            intelligence by writing all the knowledge in the form of simple rules. So if this is true, then that’s true.
            If you want to achieve this, then do that. But all the knowledge had to be encapsulated in the form of
            simple rules. So what might you want to do with this? All sorts of things.</p>
        <p>正如我之前指出的，已经编写了数千个这样的系统。但这里有一个例子。我将找出一个与识别有关的例子。这个例子模仿了一个经典程序，奇怪的是，它也是在斯坦福编写的，叫做
            MYCIN。它是为诊断血液中的细菌感染而开发的。所以你来医院。你得了某种可怕的疾病，医生很好奇哪种抗生素最适合你的疾病。</p>
        <p>Thousands of these systems were written, as I indicated before. But here’s an example. I’m going to work out
            an example having to do with identification. And this example is patterned off of a classic program,
            strangely also written at Stanford, called MYCIN. It was developed to diagnose bacterial infections in the
            blood. So you come in. You got some horrible disease, and the doctor gets curious about what antibiotic
            would be perfect for your disease.</p>
        <p>他开始问很多问题。</p>
        <p>He starts asking a lot of questions.</p>
        <h2 id="identifying-animals">识别动物</h2>
        <h2>Identifying Animals</h2>
        <p>所以我不会处理这个问题，因为这个世界有各种无法发音的术语，如拟杆菌和厌氧菌等。所以这完全类似于谈论在小动物园（有点像小镇动物园）中识别动物。</p>
        <p>So I’m not going to deal with that because that world has all kinds of unpronounceable terms like
            bacterioides and anaerobic and stuff like that. So it’s completely analogous to talk about identifying
            animals in a little zoo, sort of a small town type of zoo.</p>
        <p>所以我建议我们把我们能观察到的关于某种动物的所有特征都写在一张纸上，然后我们再试着弄清楚这种动物是什么。所以我不知道，我们可以从哪里开始？有毛发。然后是以下形式的一些特征。有爪子。锋利的牙齿。还有向前的眼睛。这些都是食肉动物的特征。
        </p>
        <p>So I’m going to suggest that we write down on a piece of paper all the things we can observe about an animal,
            and then we’ll try to figure out what the animal is. So I don’t know, what can we start with? Has hair. Then
            there are some characteristics of the following form. Has claws. Sharp teeth. And forward pointing eyes. And
            these are all characteristics of carnivores.</p>
        <p>我们的眼睛也恰好是朝前看的，但那更多的是因为我们过去经常在树上转来转去，我们需要立体视觉。我们没有爪子和与之相配的尖牙。但无论如何，这些都是食肉动物的典型特征，就像吃肉一样。我们看到的这只小动物身上也有斑点，而且跑得很快。它是什么？嗯，每个人都说它是猎豹。
        </p>
        <p>We happen to have forward pointing eyes too, but that’s more because we used to swing around the trees a lot,
            and we needed the stereo vision. And we don’t have the claws and the sharp teeth that go with it. But
            anyhow, those are typically characteristics of carnivores, as is eating meat. And this particular little
            animal we’re looking at has also got spots, and it’s very fast. What is it? Well, everybody says it’s a
            cheetah.</p>
        <p>让我们看看我们的程序如何解决这个问题。好吧，程序可能会说，让我们看看它是否有头发，那么规则一就说这意味着它一定是哺乳动物。我们可以想象另一条规则，如果你有锋利的爪子、锋利的牙齿和向前的眼睛，那么你就是食肉动物。我在这里使用某种硬件符号。这是一个与门，对吧？
        </p>
        <p>Let’s see how our program would figure that out. Well, program might say, let’s see if it has hair, then rule
            one says that means it must be a mammal. We can imagine another rule that says if you have sharp claws,
            sharp teeth, and forward pointing eyes, then you’re a carnivore. And I’m using sort of hardware notation
            here. That’s an and gate, right?</p>
        <p>所以这意味着我们必须具备所有这些特征，才能断定该动物是食肉动物。现在，这种动物也被观察到吃肉。所以这意味着我们有额外的证据证明这种动物是食肉动物。现在，由于这种动物是哺乳动物和食肉动物，有斑点，而且速度很快，所以这种动物是猎豹。
        </p>
        <p>So that means we have to have all of these characteristics before we will conclude that the animal is a
            carnivore. Now, this animal has been also observed to eat meat. So that means we’ve got extra evidence that
            the animal is carnivorous. And now, because the animal is a mammal and a carnivore and has spots, and it’s
            very fast, then the animal is a cheetah.</p>
        <p>我希望我们所有的非洲学生都同意，它一定是一只猎豹。这是一个小动物园。我的意思是，一个大动物园。谁知道它是什么？它可能有一个无法发音的名字。有可能。但对于我们的小动物园来说，这样就行了。所以我们现在以这些与门的形式写下了组。几条规则，R1，R2。这里需要一个与门。
        </p>
        <p>And I hope all of our African students agree that it must be a cheetah. It’s a small zoo. I mean, a big zoo.
            Who knows what it is? It’s probably got some unpronouncable name. there’s possibilities. But for our small
            zoo, that will do. So we have group now written down in the form of these and gates. Several rules, R1, R2.
            and there needs to be an and gate here.</p>
        <p>那是 R3 和
            R4。所有这些都表明这种动物是猎豹。所以我们建立了一个基于规则的专家系统，它可以识别一种动物，但你可以想象用其他规则填充这个系统，这样你就可以识别长颈鹿和企鹅以及你在小型动物园里发现的所有其他种类的动物。</p>
        <p>That’s R3 and an R4. All of which indicate that this animal is a cheetah. So we built ourself a little rule
            based expert system that can recognize exactly one animal, but you could imagine filling out this system
            with other rules so that you could recognize giraffes and penguins and all the other sorts of things you
            find in a small zoo.</p>
        <p>因此，当您拥有一个如我所指出的那样工作的系统时，我们将给它起一个特殊的名称，并将其称为基于前向链接规则的专家系统，因为它使用规则。</p>
        <p>So when you have a system like this that works as I’ve indicated, then what we’re going to call that, we’re
            going to give that a special name, and we’re going to call that a forward chaining rule based. because it
            uses rules. expert system.</p>
        <h2 id="rulebased-expert-systems">基于规则的专家系统</h2>
        <h2>RuleBased Expert Systems</h2>
        <p>我们要把“专家”放在括号里，因为当这些东西被开发出来时，出于营销原因，他们称它们为专家系统，而不是新手系统。但他们真的是人类意义上的专家吗？</p>
        <p>And we’re going to put expert in parentheses because when these things were developed, for marketing reasons,
            they called them expert systems instead of novice systems. But are they really experts in a human sense?</p>
        <p>不是，因为它们有这些下意识的规则。它们不具备任何你可能想要称之为常识的东西。它们没有能力处理以前的病例，就像我们在医学院学习时那样。所以它们应该被称为基于规则的新手系统，因为它们像新手一样根据规则进行推理。但传统上称它们为基于规则的专家系统。
        </p>
        <p>Not really, because they have these knee jerk rules. They’re not equipped with anything you might want to
            call common sense. They don’t have an ability to deal with previous cases, like we do when we go to medical
            school. So they really ought to be called rule based novice systems because they reason like novices on the
            basis of rules. But the tradition is to call them rule based expert systems.</p>
        <p>这个系统从我们给出的事实开始向前推导，直到右边的结论。这就是为什么它是一个前向链接系统。这个系统能回答有关其自身行为的问题吗？你怎么看？ 发言人 5：帕特里克·温斯顿教授：为什么？ 发言人
            5：帕特里克·温斯顿教授：因为它看起来像一棵目标树。对。</p>
        <p>And this one works forward from the facts we give it to the conclusion off on the right. That’s why it’s a
            forward chaining system. Can this system answer questions about its own behavior? what do you think? SPEAKER
            5: PROFESSOR PATRICK WINSTON: Why? SPEAKER 5: PROFESSOR PATRICK WINSTON: Because it looks like a goal tree.
            Right.</p>
        <p>这实际上是在构建一个目标树，因为这些要求几件事为真的规则中的每一个都在创建一个“与”节点。而这里的每一种情况，如果你有多个理由相信这个东西是食肉动物，这就创建了一个“或”节点。我们已经知道，如果你留下目标树的痕迹，你就可以回答有关你自己行为的问题。所以看看这个。
        </p>
        <p>This is, in fact, building a goal tree because each of these rules that require several things to be true is
            creating an and node. And each of these situations here where you have multiple reasons for believing that
            the thing is a carnivore, that’s creating an or node. And we already know that you can answer questions
            about your own behavior if you leave behind a trace of a goal tree. So look at this.</p>
        <p>如果我问它，你为什么对动物的爪子感兴趣？因为我想看看它是不是食肉动物。你怎么知道这种动物是哺乳动物？因为它有毛发。你为什么认为它是猎豹？因为它是哺乳动物，食肉动物，身上有斑点，而且跑得很快。因此，通过在这个目标树中前后移动，它也可以回答有关其自身行为的问题。
        </p>
        <p>If I say to it, why were you interested in the animal’s claws? Because I was trying to see if it was a
            carnivore. How did you know that the animal is a mammal? Because it has hair. Why did you think it was a
            cheetah? Because it’s a mammal, a carnivore, has spots, and very fast. So by working forward and backward in
            this goal tree, this too can answer questions about its own behavior.</p>
        <p>现在您知道了如何编写程序来回答有关其行为的问题。您可以编写子程序，使每个子程序都围绕一个目标，这样您就拥有了以目标为中心的编程，或者使用规则构建所谓的专家系统，在这种情况下，很容易让它留下目标树的痕迹，这使得它能够回答有关其自身行为的问题，就像这个程序一样。
        </p>
        <p>So now you know how, going forward, you can write programs that answer questions about their behavior. Either
            you write the subroutines so that each one is wrapped around a goal, so you’ve got goal centered
            programming, or you build a so called expert system using rules, in which case it’s easy to make it leave
            behind a trace of a goal tree, which makes it possible to answer questions about its own behavior, just as
            this program did.</p>
        <p>但现在，我要多学一点词汇。为了节省时间，我要把之前画的这些联系都删掉。我要用一种不同的方式来介绍这个动物园。我不会问任何有关动物的问题。相反，我会说，妈妈，我看到的这个东西是猎豹吗？妈妈该怎么弄清楚呢？</p>
        <p>But now, a little more vocabulary. I’m going to save time by erasing all of these things that I previously
            drew by way of connections. And I’m going to approach this zoo in a little different way. I’m going to not
            ask any questions about the animal. Instead, I’m going to say, mommy, is this thing I’m looking at a
            cheetah? And how would mommy go about figuring it out.</p>
        <p>她心里会想，好吧，我不知道。如果它是一只猎豹，那么它必须是食肉动物，它身上必须有斑点。它必须跑得很快。到目前为止，我们已经确定，如果它是一只猎豹，它必须具备妈妈发现的四个特征。</p>
        <p>In her head, she would say, well, I don’t know. If it’s going to be a cheetah, then it must be the case that
            it’s a carnivore, and it must be the case that it has spots. And it must be the case that it’s very fast. So
            so far, what we’ve established is that if it’s going to be a cheetah, it has to have the four
            characteristics that mommy finds behind this rule are four.</p>
        <p>因此，我不会从事实出发，而是从假设出发，逆向而行。假设是，这个东西是猎豹。我该如何证明这是真的还是假的？嗯，到目前为止我还没有做任何事情，因为我只知道如果所有这些都是真的，它就是一只猎豹，但它们是真的吗？嗯，要确定它是否是哺乳动物，我可以使用规则一。
        </p>
        <p>So instead of working forward from facts, what I’m going to do is work backward from a hypothesis. So here
            the hypothesis is this thing is a cheetah. How do I go about showing whether that’s true or not? Well, I
            haven’t done anything so far because all I know is a cheetah if all these things are true, but are they
            true? Well, to find out if it’s a mammal, I can use rule one.</p>
        <p>如果我知道或能确定动物有毛发，那么这部分就解决了。我也可以同样通过食肉动物来判断。我说，如果它有爪子、锋利的牙齿和向前的眼睛，那它就是食肉动物。然后，只要这个动物有，我就可以结束了。我知道它是食肉动物。我不必再通过另一种方式来证明它是食肉动物。
        </p>
        <p>And if I know or can determine that the animal has hair, then that part of it is taken care of. And I can
            similarly work my way back through carnivore. I say, well, it’s a carnivore if it has claws, sharp teeth,
            and forward pointing eyes. And then as much as the animal in question does, then I’m through. I know it’s a
            carnivore. I don’t have to go through and show that it’s a carnivore another way.</p>
        <p>所以我从来没有真正问过它是否吃肉。最后，最后两个条件只需对动物进行检查即可满足。也就是说，它在数据库中。我不需要使用任何规则来确定动物有斑点并且速度非常快。</p>
        <p>So I never actually ask questions about whether it eats meat. Finally, the final two conditions are met by
            just an inspection of the animal. That’s to say, it’s in the database. I don’t have to use any rules to
            determine that the animal has spots and is very fast.</p>
        <p>现在，我已经知道了它是猎豹的所有信息，因为它是食肉动物，因为它有爪子、锋利的牙齿和向前的眼睛，而其他所有信息都是通过逆向推理确定的，从假设到事实，而不是从事实到结论。因此，构建一个这样工作的系统，我就有了一个基于逆向链式规则的专家系统。
        </p>
        <p>So now, I’ve got everything in place to say that it’s a cheetah, because it’s a carnivore, because it has
            claws, sharp teeth, and forward pointing eyes, and all the rest of this stuff is similarly determined by
            going backwards, backwards from the hypothesis toward the facts, instead of from the facts forward to the
            conclusions. So building a system that works like that, I have a backward chaining rule based expert system.
        </p>
        <p>但是这个系统无论是在后向模式还是在正向模式中，都有一个很重要的特点，那就是这个东西是一个演绎系统。</p>
        <p>But there’s a very important characteristic of this system in both backward and forward mode, and that is
            that this thing is a deduction system.</p>
        <h2 id="deduction">扣除</h2>
        <h2>Deduction</h2>
        <p>这是因为它利用事实来产生新的事实。当你有一个演绎系统时，你永远不能拿走任何东西。但这些基于规则的系统也用于另一种模式，在这种情况下，可以拿走一些东西。看，在事实世界里，在演绎世界中，你谈论的是证明事物。</p>
        <p>That’s because it’s working with facts to produce new facts. When you have a deduction system, you can never
            take anything away. But these rule based systems are also used in another mode, where it’s possible to take
            something away. See, in fact world, in deduction world, you’re talking about proving things.</p>
        <p>一旦你证明某件事是真的，它就不可能是假的。如果是假的，你的系统就会出现矛盾。但如果你把它看作是一种编程语言，如果你把使用规则看作是一种编程语言，那么你就可以考虑安排它，让这些规则在数据库中增加或减少。让我给你举几个系统的例子。
        </p>
        <p>And once you prove something is true, it can’t be false. If it is, you’ve got a contradiction in your system.
            But if you think of this as a programming language, if you think of using rules as a programming language,
            then you can think of arranging it so these rules add or subtract from the database. Let me show you an
            example of a couple of systems.</p>
        <p>首先，既然我已经谈到了MYCIN系统，那么让我向大家展示一个MYCIN对话的例子。</p>
        <p>First of all, since I’ve talked about the MYCIN system, let me show you an example of a MYCIN dialogue.</p>
        <h2 id="mice-and-dialogue">老鼠与对话</h2>
        <h2>Mice and Dialogue</h2>
        <p>这是 MYCIN 对话。你可以看到一些你必须去医学院学习的单词。这是典型的 MYCIN 规则，就像进行动物园分析的规则一样，只是领域更复杂。</p>
        <p>That’s a MYCIN dialogue. And you can see the appearance of words you have to go to medical school to learn.
            And here’s a typical MYCIN rule, just like the rules for doing zoo analysis, only a more complicated domain.
        </p>
        <p>但这里有另一个系统的例子，它不是在 80
            年代写的，而是几年前由建筑系的一名学生在博士论文中写的。他对一位名叫西扎的葡萄牙建筑师的建筑很感兴趣。西扎做了很多大规模住宅项目。西扎认为你应该能够设计自己的房子。于是葡萄牙学生、博士何塞·杜阿尔特 (Jose
            Duarte) 提出了这个想法。&nbsp;</p>
        <p>But here’s another example of a system that was written, not in the ’80s, but just a couple of years ago by a
            student in the architecture department, Ph.D.&nbsp;thesis. He was interested in the architecture of a
            Portuguese architect named Siza. And Siza’s done a lot of mass housing stuff. And Siza has the idea that you
            ought to be able to design your own house. And so Jose Duarte, a Portuguese student, a Ph.D.&nbsp;</p>
        <p>建筑专业的学生编写了一个基于规则的系统，该系统能够根据即将入住的人的要求、建议和愿望设计出像西扎一样的房屋。所以，这件事、这个练习最引人注目的部分是，杜阿尔特把程序中的一些设计与西扎的一些设计混合在一起，然后把它们放在西扎面前，然后问他，你做了哪些？
        </p>
        <p>Student in architecture, wrote a rule based system that was capable of designing Siza like houses in response
            to the requirements and recommendations and desires of the people who are going to occupy the houses. So the
            most compelling part of this thing, of this exercise, was that Duarte took some of the designs of the
            program, mixed them up with some of the designs of Siza, and put them in front of Siza, and said, which ones
            did you do?</p>
        <p>而西扎却无法分辨。所以不知何故，使用这种技术建立的规则系统足以让模仿的专家也感到困惑。</p>
        <p>And Siza couldn’t tell. So somehow, the rule based system that was built using this kind of technology was
            sufficient to confuse even the expert that they were patterned after.</p>
        <h2 id="example-problem">示例问题</h2>
        <h2>Example Problem</h2>
        <p>但这个程序有点复杂。它也有自己的专业术语。所以我不打算详细讨论它，而是讨论一个类似的问题。</p>
        <p>But this program is a little complicated. It, too, has its own specialized lingo. So I’m not going to talk
            about it in detail, but rather talk instead about an analogous problem.</p>
        <p>这是每个人都曾遇到过的问题，也就是在杂货店把杂货装进袋子的问题。这是一样的，对吧？你不是把房间装进房子里，而是把杂货装进袋子里。而且必须有一些关于如何做到这一点的规则。事实上，你们中的一些人可能曾经是专业的杂货店装袋工？杂货店专业装袋工。你是。哪一个？
        </p>
        <p>And that is a problem that everyone has faced at one point or another, and that is the problem of putting
            groceries in a bag at a grocery store. It’s the same thing, right? Instead of putting rooms in a house,
            you’re putting groceries in a bag. And there must be some rules about how to do that. In fact, maybe some of
            you have been professional grocery store baggers? a grocery store professional bagger. You’re a. which one?
        </p>
        <p>丽莎：帕特里克·温斯顿教授：是的，你叫什么名字？丽莎：丽莎。帕特里克·温斯顿教授：丽莎。好的，我们有两名专业的杂货店装袋工。我现在要模拟一位高薪知识工程师，他想开发一个知道如何装袋杂货的程序。所以我要访问你的网站
            Market Basket，我要问丽莎，她现在担心失去工作，她是否愿意告诉我她是如何装袋杂货的。</p>
        <p>LISA: PROFESSOR PATRICK WINSTON: Yeah, what is your name? LISA: Lisa. PROFESSOR PATRICK WINSTON: Lisa. OK,
            well we got two professional grocery store baggers. And I’m going to be now simulating a highly paid
            knowledge engineer desirous of building a program that knows how to bag groceries. So I’m going to visit
            your site, Market Basket, and I’m going to ask Lisa, now fearful of losing her job, if she would tell me
            about how she bags groceries.</p>
        <p>那么你能提出一条规则吗？ 丽莎：当然可以。大件物品放在底部。
            帕特里克·温斯顿教授：大件物品放在底部。你看，这就是我成为高薪知识工程师的原因，因为我将她说的话转化为“如果那么”规则。所以如果大，那么放在底部。所以现在我。 说话者
            3：帕特里克·温斯顿教授：那么你呢，你有什么建议吗？关于如何装袋杂货？ 说话者 6：小东西放在顶部。</p>
        <p>So could you suggest a rule? LISA: Sure. Large items in the bottom. PROFESSOR PATRICK WINSTON: Large items in
            the bottom. You see, that’s why I’m a highly paid knowledge engineer, because I translate what she said into
            an if then rule. So if large, then bottom. So now I. SPEAKER 3: PROFESSOR PATRICK WINSTON: So how about you,
            Have you got a suggestion? About how to bag groceries? SPEAKER 6:. The small things on top.</p>
        <p>帕特里克·温斯顿教授：如果小，就放在最上面。丽莎，你还有什么可以告诉我的吗？丽莎：不要把太多重的东西放在同一个袋子里。帕特里克·温斯顿教授：不要把太多重的东西放在同一个袋子里。所以如果重物超过三个，就换一个袋子，或者类似的东西。这就是我们能做的一切。还有谁想做志愿者吗？你在土耳其打包过杂货吗？
        </p>
        <p>PROFESSOR PATRICK WINSTON: If small, then on top. Lisa, have you got anything else you could tell me? LISA:
            Don’t put too many heavy things in the same bag. PROFESSOR PATRICK WINSTON: Don’t put too many heavy things
            in the same bag. So if heavy greater than three, then new bag, or something like that. Is that all we’re
            going to be able to. does anybody else want to volunteer? have you bagged groceries in Turkey?</p>
        <p>丽莎：所以他们没有装袋工，所以我们必须自己装。帕特里克·温斯顿教授：所以在土耳其每个人都是专业的装袋工。是的。这项工作外包给了客户。发言人
            7：所以底部不会有挤压。所以如果你有。帕特里克·温斯顿教授：底部不会有挤压。发言人 7：如果你有西红柿。帕特里克·温斯顿教授：很好。西红柿。发言人 7：你不想让它们被挤压。</p>
        <p>LISA: So they don’t have grocery baggers, so we have to. PROFESSOR PATRICK WINSTON: So everybody’s a
            professional bagger in Turkey. Yeah. It’s outsourced to the customers. SPEAKER 7: So no squishies on the
            bottom. So if you have. PROFESSOR PATRICK WINSTON: No squishies on the bottom. SPEAKER 7: If you have
            tomatoes. PROFESSOR PATRICK WINSTON: That’s good. Tomatoes. SPEAKER 7: You don’t want them to get squished.
        </p>
        <p>帕特里克·温斯顿教授：现在，软糖和西红柿之间有很大不同，因为西红柿是具体的，而软糖不是。现在，麻省理工学院学生的一个倾向当然是我们都倾向于概括。我曾经认识斯隆管理学院的一位教授，他看起来非常聪明。而且。发言者
            3：帕特里克·温斯顿教授：然后我明白了他是怎么想的。如果我要说，我在想一个红苹果。</p>
        <p>PROFESSOR PATRICK WINSTON: Now, there’s a very different thing about squishies and tomatoes because tomato is
            specific, and squishy isn’t. Now, one tendency of MIT students, of course, is that we all tend to
            generalize. I once knew a professor in the Sloan School who seemed real smart. And. SPEAKER 3: PROFESSOR
            PATRICK WINSTON: Then I figured out what he did. If I were to say, I’m thinking about a red apple.</p>
        <p>他们会坐下来说，哦，我看你今天在思考彩色水果。他们只是把它提升了一个抽象层次。不是天才。他在黑板上画了一个三角形后还能讲一个小时。真是了不起的人。无论如何，我们说到哪儿了？哦，是的，装袋杂货。所以我们取得了一些进展，但没有我希望的那么多。
        </p>
        <p>They’d sit back and say, oh, I see you’re contemplating colored fruit today. They’re just taking it up one
            level of abstraction. Not a genius. He also was able to talk for an hour after he drew a triangle on the
            board. Amazing people. Anyhow, where were we? Oh, yes, bagging groceries. So we’re making some progress, but
            not as much as I would like.</p>
        <p>因此，为了真正在这样的任务上取得进展，你必须锻炼。你知道一些知识工程的原理。</p>
        <p>And so in order to really make progress on tasks like this, you have to exercise. you know about some
            principles of knowledge engineering.</p>
        <h2 id="knowledge-engineering-principles">知识工程原理</h2>
        <h2>Knowledge Engineering Principles</h2>
        <p>因此，我在这里列出的第一条原则是处理具体案例，这也是金星理念的一部分。因此，当你在现场时，如果你所做的只是与 Lisa 这样的专家交谈，你得到的只是模糊的概括，因为他们不会考虑到所有事情。</p>
        <p>So principle number one, which I’ve listed over here as part of a gold star idea, is deal with specific
            cases. So while you’re at the site, if all you do is talk to the experts like Lisa and all you’re going to
            get is vague generalities because they won’t think of everything.</p>
        <p>所以你要做的就是说，好吧，让我在生产线上看着你。然后你会发现他们必须有某种方法来处理牛奶。然后你会发现他们必须有某种方法来处理薯片。没有人提到薯片，除非它们可能很软。我们没有软的定义。没有人谈论通心粉。也没有人谈论机油。
        </p>
        <p>So what you do is you say, well, let me watch you on the line. And then you’ll see that they have to have
            some way of dealing with the milk. And then you’ll see that they have to have some way of dealing with the
            potato chips. Nobody mentioned potato chips, except insofar as they might be squishy. We don’t have a
            definition for squishy. Nobody talked about the macaroni. And no one talked about the motor oil.</p>
        <p>这是一家便利店。我可不想把它和肉放在同一个袋子里。然后没人谈论罐头食品。这是一罐橄榄。因此，通过查看具体案例，你可以从人们那里获取他们原本不会想到要给你的知识。这是知识工程的第一条规则。只需几分钟，你就会掌握所有三条知识工程规则，并准备成为一名高薪知识工程师。
        </p>
        <p>This is a convenience store. I don’t want that in the same bag with the meat. And then no one talked about
            canned stuff. Here’s a can of olives. So by looking at specific cases, you elicit from people knowledge they
            otherwise would not have thought to give you. That’s knowledge engineering rule number one. And within a
            very few minutes, you’ll have all three knowledge engineering rules and be prepared to be a highly paid
            knowledge engineer.</p>
        <p>启发式，我们称其为启发式。启发式一号是针对具体案例。启发式二号是针对看似相同但实际处理方式不同的事物提出问题。所以这里有一些 Birds Eye
            冷冻豌豆，还有这里。呃，一些新鲜切好的甜豌豆。而对于我这个一生中从未碰过食品袋的人来说。也许我来自火星。我分不清它们的区别。它们都是豌豆。</p>
        <p>Heuristic, let’s call these heuristics. Heuristic number one, specific cases. Heuristic number two is ask
            questions about things that appear to be the same, but are actually handled differently. So there’s some
            Birds Eye frozen peas, and here. ugh, some fresh cut sweet peas. And to me, the person who’s never touched a
            grocery bag in my life. maybe I’m from Mars. I can’t tell the difference. They’re both peas.</p>
        <p>但是我观察到专家们处理这些对象的方式不同。所以我问，为什么你处理这些豌豆的方式与处理那些豌豆的方式不同，他们怎么说？一个是罐装的，一个是冷冻的。那么会发生什么？Bingo，我的词汇表中有一些新词。这些新词汇将赋予我对域的控制权，因为我现在可以在我的规则中使用这些词。
        </p>
        <p>But I observe that the experts are handling these objects differently. So I say, why did you handle those
            peas differently from those peas, and what do they say? One’s canned, and one’s frozen. So what happens?
            Bingo, I’ve got some new words in my vocabulary. And those new vocabulary words are going to give me power
            over the domain because I can now use those words in my rules.</p>
        <p>我可以写一些规则，比如如果冷冻，然后把它们都放进一个小塑料袋里。实际上，这太复杂了，但我们最终还是这么做的，对吧？为什么我们要把它们都放进一个小塑料袋里？ 发言人 8：帕特里克·温斯顿教授：那是什么？ 发言人
            8：帕特里克·温斯顿教授：嗯，有两种解释。一种是麻省理工学院的解释。</p>
        <p>And I can write rules like if frozen, then put them all together in a little plastic bag. Actually, that’s
            too complicated, but that’s what we end up doing, right? Why do we put them all together in a little plastic
            bag? SPEAKER 8: PROFESSOR PATRICK WINSTON: What’s that? SPEAKER 8: PROFESSOR PATRICK WINSTON: Well, there
            are two explanations. There’s the MIT explanation.</p>
        <p>我们知道温度流等于温差的四次方和表面积等。我们希望将它们全部聚在一起形成一个球体。正常的解释是它们无论如何都会融化，所以它们最好不要把其他东西都弄湿。好的。发言者 3：帕特里克·温斯顿教授：所以这是第二个启发式方法。
        </p>
        <p>We know that temperature flow is equal to the fourth power of the temperature difference and the surface area
            and all that kind of stuff. We want to get them all together in a ball, sphere. The normal explanation is
            that they’re going to melt anyway, so they might as well not get everything else wet. All right. SPEAKER 3:
            PROFESSOR PATRICK WINSTON: So that’s heuristic number two.</p>
        <p>实际上，这是我第一次与你分享第三个启发式方法，因为在过去的这个夏天，我一直在研究它。第三个启发式方法就是，你构建了一个系统，然后你会看到它何时会崩溃。当它崩溃时，就是你没有执行所需的规则之一。为了让程序按照你希望的方式执行，你需要制定规则。
        </p>
        <p>And actually, there’s heuristic number three, that I just want to relate to you for the first time because I
            have been dealing with it a lot over this past summer. Heuristic number three is you build a system, and you
            see when it cracks. And when it cracks is when you don’t have one of the rules you need in order to execute.
            in order to get the program to execute as you want it to execute.</p>
        <p>因此，如果我编写一个杂货店装袋程序并让它装一些杂货，同样，它最终要么出错，要么突然停止，然后我就知道缺少了一条规则。这难道不是当你做一组问题并陷入僵局时会发生的情况吗？你在对自己进行实验，然后发现自己没有完整的程序。
        </p>
        <p>So if I were to write a grocery store bagging program and have it bag some groceries, again, eventually it
            would either make a mistake or come to a grinding halt, and bingo, I know that there’s a missing rule. Isn’t
            that what happens when you do a problem set, and you hit an impasse? You’re performing an experiment on
            yourself, and you’re discovering that you don’t have the whole program.</p>
        <p>事实上，我将其列为与自我工程有关的金星想法，因为所有这些知识工程所能做的事情都是你自己学习新学科时可以做的事情。因为本质上，当你学习电路理论或电磁学或诸如此类的东西时，你会把自己变成一个专家系统。你会对自己说，好吧，让我们看一些具体案例。
        </p>
        <p>In fact, I’ve listed this as a gold star idea having to do with engineering yourself because all of these
            things that you can do for knowledge engineering are things you can do when you learn a new subject
            yourself. Because essentially, you’re making yourself into an expert system when you’re learning circuit
            theory or electromagnetism or something of that sort. You’re saying to yourself, well, let’s look at some
            specific cases.</p>
        <p>那么，这里的哪些词汇可以告诉我为什么这个问题与那个问题不同？哦，这是一个圆柱体而不是球体。或者你正在处理一个问题集，你发现你无法解决这个问题，你需要获得另一块知识才能做到这一点。</p>
        <p>Well, what are the vocabulary items here that tell me why this problem is different from that problem? Oh,
            this is a cylinder instead of a sphere. Or you’re working with a problem set, and you discover you can’t
            work with the problem, and you need to get another chunk of knowledge that makes it possible for you to do
            it.</p>
        <p>所以，你认为这类东西主要是一种机制，是进行知识工程的启发式方法，也是使自己变得更聪明的机制。这就是我今天要和你们讨论的内容。但最重要的是，如果你建立一个基于规则的专家系统，它可以回答有关其自身行为的问题。如果你建立一个以目标为中心的程序，它可以回答有关其自身行为的问题。
        </p>
        <p>So this sort of thing, which you think of primarily as a mechanism, heuristics for doing knowledge
            engineering, are also mechanisms for making yourself smarter. So that concludes what I want to talk with you
            about today. But the bottom line is, that if you build a rule based expert system, it can answer questions
            about its own behavior. If you build a program that’s centered on goals, it can answer questions about its
            own behavior.</p>
        <p>如果你构建了一个集成程序，它可以回答有关其自身行为的问题。如果你想构建这样的系统，并且需要从专家那里获取知识，你需要采用这类启发式方法，因为除非你通过具体案例、通过询问差异问题以及通过最终进行一些实验来查看你的程序是否正确，否则专家不会思考该告诉你什么。
        </p>
        <p>If you build an integration program, it can answer questions about its own behavior. And if you want to build
            one of these systems, and you need to extract knowledge from an expert, you need to approach it with these
            kinds of heuristics because the expert won’t think what to tell you unless you elicit that information by
            specific cases, by asking questions about differences, and by ultimately doing some experiments to see where
            your program is correct.</p>
        <p>所以这实际上就是我要说的，但我想问一个问题，这就是我们需要了解的有关人类智能的全部内容吗？</p>
        <p>So that really concludes what I had to say, except I want to ask the question, is this all we need to know
            about human intelligence?</p>
        <h2 id="is-human-intelligence-really-smart">人类智能真的很聪明吗</h2>
        <h2>Is Human Intelligence Really Smart</h2>
        <p>这些东西真的聪明吗？这些东西真的聪明吗？传统的答案是否定的，它们并不是真的聪明，因为它们的智能只是一层薄薄的表皮。当你试图揭开它的真面目时，正如所写的那样，它们往往会破裂。</p>
        <p>Can these things be. are these things really smart? And the traditional answer is no, they’re not really
            smart because their intelligence is this sort of thin veneer. And when you try to get underneath it, as
            written, they tend to crack.</p>
        <p>例如，我们讨论一条规则，我们可以讨论一条知道你应该把薯片放在袋子顶部的规则。但是一个知道这一点的程序不知道你为什么要把薯片放在袋子顶部。他们不知道如果你把它们放在袋子底部，它们会被压碎。</p>
        <p>For example, we talk about a rule, we could talk about a rule that knows that you should put the potato chips
            on the top of the bag. But a program that knows that would have no idea why you would want to put the potato
            chips on top of the bag. They wouldn’t know that if you put them on the bottom of the bag, they’ll get
            crushed.</p>
        <p>而且它不知道如果薯片被压碎，顾客会生气，因为人们不喜欢吃压碎的薯片。所以当我说这些知识往往是一种假象时，我的意思就是如此。因此，MYCIN 程序在调试期间曾开出一桶青霉素给病人治疗其疾病。他们不知道，他们没有任何常识。
        </p>
        <p>And it wouldn’t know that if they get crushed, the customer will get angry, because people don’t like to eat
            crushed potato chips. So that’s what I mean when I say the knowledge of these things tends to be a veneer.
            So the MYCIN program, during debugging, once prescribed a barrel of penicillin to be administered to a
            patient for its disease. They don’t know, they don’t have any common sense.</p>
        <p>那么问题来了，我不知道。基于规则的规则与常识有什么关系吗？而我对这个问题有点不确定。</p>
        <p>So the question then becomes, well, I don’t know. Does rule based. do rules have anything to do with common
            sense? And I’m becoming a little bit agnostic on that subject.</p>
        <h2 id="rulebased-reasoning">基于规则的推理</h2>
        <h2>RuleBased Reasoning</h2>
        <p>因为有某些迹象，有某些情况，可以说规则在我们对事物的日常理解中发挥了作用。你想看演示吗？当剪辑说话时，我将向你展示什么。</p>
        <p>Because there are certain indications, there are certain situations, in which rules could be said to play a
            role in our ordinary understanding of things. Would you like to see a demonstration? What I’m going to show
            you, when the clip speaks up.</p>
        <p>好吧，在我做出任何承诺之前，让我看看我是否真的连接到了网络。麻省理工学院，很好。麻省理工学院。客人。是的，那就是我。听起来不错。好的，我刚刚测试了系统，我发现它确实连接到了网络。我要在这里调整一些系统选项。我会删除文本框。我们会稍微删除那些更改比例。
        </p>
        <p>Well, before I make any promises, let me see if I’m actually connected to the web. MIT, good. MIT. Guest.
            Yeah, that’s me. Sounds good. OK, I just tested the system, and I’ve seen it is actually connected to the
            web. And I’m going to adjust some systems options here. I’ll get rid of the text box. And we’ll get rid of
            those changes scale a little bit.</p>
        <p>我要做的是读一读《麦克白》剧情简介。你们是麻省理工学院的学生。我相信你们都受过古典教育，对莎士比亚的剧情非常熟悉。所以我要读一读。我要读一个《麦克白》剧情的版本。它会像这样进行。到目前为止，基本上就是阅读一个规则库。
        </p>
        <p>What I’m going to do is I’m going to read a little synopsis of the Macbeth plot. You’re MIT students. I’m
            sure you’re all classically educated and very familiar with Shakespearean plots. So I’m going to read one.
            I’m going to read a version of a Macbeth plot. And it’s going to go along like this. It’s basically reading
            a rule base so far.</p>
        <p>很快，它就会越过规则库，开始阅读麦克白的故事。它就在那里。它读完了麦克白的故事。让我向你展示一下系统实际保存的麦克白故事是什么样子的。就是这样。读一下。好吧，你没时间了，因为机器已经读完了。读完这个故事大约需要五秒钟。
        </p>
        <p>And pretty soon, it’s going to get beyond the rule base and start reading the Macbeth story. And there it is.
            It’s read the Macbeth story. Let me show you what the Macbeth story looks like as it’s actually retained by
            the system. That’s it. Read that. OK, you ran out of time because the machine’s already finished. It takes
            about five seconds to read this story.</p>
        <p>现在，当你看《麦克白》的这个简短梗概时，有几件事需要注意。首先，它说邓肯被谋杀了。邓肯，我希望这不会困扰你。邓肯是被麦克白谋杀的。但它从来没有说过邓肯死了。但你知道邓肯死了，因为他被谋杀了。如果被谋杀，那就死了。
        </p>
        <p>Now, as you look at this little synopsis of Macbeth, there are a couple things to note. For one thing, it
            says that Duncan is murdered. Duncan, I hope this doesn’t bother you. Duncan is murdered by Macbeth. But at
            no time does it say that Duncan is dead. But you know Duncan’s dead because he was murdered. If murdered,
            then dead.</p>
        <p>发言者
            3：帕特里克·温斯顿教授：如果你再往下看，你会看到麦克德夫杀死了麦克白。从下往上第四行。麦克德夫为什么要杀死麦克白？这个故事没有说明原因，但你很容易猜出这是因为他生气了。当你生气时，你不一定会杀人，但这是可能的。
        </p>
        <p>SPEAKER 3: PROFESSOR PATRICK WINSTON: So if you look a little further down, what you see is that Macduff
            kills Macbeth. Fourth line up from the bottom. Why did Macduff kill Macbeth? Doesn’t say why in this story,
            but you have no trouble figuring out that it’s because he got angry. And when you get angry, you don’t
            necessarily kill somebody, but it’s possible.</p>
        <p>发言者
            3：帕特里克·温斯顿教授：现在您已经了解了故事的内容，让我带您回到这个展示。这就是我们所说的详尽图表。当我把它放大时，您会看到里面有一些熟悉的东西。例如，在左上角，麦克白谋杀了邓肯，就在那里。在这里，麦克德夫杀死了麦克白。
        </p>
        <p>SPEAKER 3: PROFESSOR PATRICK WINSTON: So now that you see what’s in the story, let me take you back to this
            display. It’s what we call an elaboration graph. And when I blow it up, you can see that there’s some
            familiar looking things in there. For example, up here in the left hand corner, Macbeth murders Duncan,
            right over there. And over here, Macduff kills Macbeth.</p>
        <p>如果你看看这样做的后果，你会发现似乎一定有一条规则规定，如果你谋杀了某人，你就伤害了他们。如果你谋杀了某人，他们就会死。你可能会杀人的一个原因是他们激怒了你。如果你反过来想，杀人的一个后果就是你伤害了他们，他们也会死。
        </p>
        <p>And if you look at what is a consequence of that, it looks like there must be a rule that says if you murder
            somebody, you harm them. And if you murder somebody, then they’re dead. And one reason why you might kill
            somebody is because they angered you. And if you go the other way, one consequence of killing somebody is
            that you harm them, and that they die too.</p>
        <p>如果你伤害了某人，他们会生气，他们的状态就会变得消极。所以这表明我们手头上有些东西是经过精心编排的，非常奇怪的是，非常符合他们的性格。现在，最后，我要读一下《哈姆雷特》。《哈姆雷特》的示范很像《麦克白》。事实上，《哈姆雷特》和《麦克白》的情节非常相似。
        </p>
        <p>And if you harm somebody, they get angry, and their state goes negative. So that suggests that there are some
            things that we have on our hands that are very compiled and very, strangely enough, very rule like in their
            character. Now, to close, I’m just going to read Hamlet. The Hamlet demonstration is much like the Macbeth
            one. In fact, Hamlet and Macbeth are very alike in their plot.</p>
        <p>但是，我们在这里抓捕哈姆雷特的情节很好地说明了一件事。那就是你会注意到灰色物质与白色物质的比例相当大。灰色物质是通过规则推断出来的物质。哈姆雷特故事中灰色物质如此之多的原因是每个人都与其他人有关系。所以当你杀死任何人时，你都会激怒其他人。所以看看这个。
        </p>
        <p>But there’s one thing that’s well illustrated by our particular capturing of Hamlet here. And that is that
            you’ll note that the ratio of gray stuff to white stuff is considerable. The gray stuff is stuff that has
            been deduced by rules. And the reason there’s so much gray stuff in this Hamlet story is because everybody’s
            related to everybody else. So when you kill anybody, you irritate everybody else. So look at that.</p>
        <p>一些白色的东西，那些是故事中明确的东西，还有许多灰色的东西。所以这表明当我们讲故事时，它主要是受控的幻觉。我知道你脑子里的规则，所以我可以利用这一点来讲故事，而不必告诉你任何我相信你会知道的事情。这就是为什么我们发现，讲故事在很大程度上只是控制你如何进行，一种受控的幻觉。
        </p>
        <p>A few white things, those are the things that are explicit in the story, and lots of gray stuff. So what this
            is suggesting is that when we tell a story, it’s mostly a matter of controlled hallucination. I know what
            rules are in your head, so I could take advantage of that in telling the story and not have to tell you
            anything I’m sure you’re going to know. And so that’s why, we’ve discovered, that storytelling is largely a
            matter of just controlling how you’re going along, a kind of controlled hallucination.</p>
        <h1 id="search-depth-first-hill-climbing-beam">4. 搜索：深度优先、爬山法、光束法</h1>
        <h1>4. Search: Depth-First, Hill Climbing, Beam</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 48:42 <a
                href="https://youtube.com/watch?v=j1H3jAAGlEA">https://youtube.com/watch?v=j1H3jAAGlEA</a></p>
        <p> 11 years ago (Jan 11, 2014) — 48:42 <a
                href="https://youtube.com/watch?v=j1H3jAAGlEA">https://youtube.com/watch?v=j1H3jAAGlEA</a></p>
        <h2 id="unknown-29">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：今天我们要讨论搜索。我知道你们又要听一次关于搜索的讲座了。那些正在学习计算机科学课程的人，你们可能在 601
            课上见过。你们会再次看到这门理论课。但我们的目的会有所不同。我希望你们对各种搜索工作有一定了解。</p>
        <p>PATRICK WINSTON: Today we’re going to be talking about Search. I know you’re going to turn blue with yet
            another lecture on Search. Those of you who are taking computer science subjects, you’ve probably seen in
            601. You’ll see it again as theory course. But we’re going to do it for a little different purpose. I want
            you to develop some intuition about various kinds of Search work.</p>
        <p>我想谈谈“搜索”这个模型，它是我们头脑中发生的事情。最后，如果有时间，我想为你们演示一个从未向 603.4
            班展示过的东西，因为它是去年春天才完成的。今天早上我添加了一些收尾工作。这总是很危险，但我们会看看会发生什么。这就是剑桥。当然，你们都认识它。</p>
        <p>And I want to talk a little bit about Search as a model of what goes on in our heads. And toward the end, if
            there’s time, I’d like to do a demonstration for you of something never before demonstrated to a 603.4
            class, because it was only completed last spring. And some finishing touches were added by me this morning.
            Always dangerous, but we’ll see what happens. There’s Cambridge. You all recognize it, of course.</p>
        <p>您可能想从某个起始位置 s 到达某个目标位置
            g。因此，您会雇一辆出租车并希望获得最佳结果。因此，以下是可能发生的情况，不太好。让我们将起始位置移到这里。我遇到过纽约这样的出租车司机。但这不是一条很好的路径。这是小偷的路径。让我们将搜索方式改为初学者、诚实的初学者的方式。
        </p>
        <p>You might want to get from some starting position s to some goal position g. So, you’ll hire a cab and hope
            for the best. So, here’s what might happen, not too hot. Let’s move the starting position over here. I’ve
            had cab drivers like this New York. But it’s not a very good path. It’s the path of a thief. Let’s change
            the way that the search is done to that of a beginner, an honest beginner.</p>
        <h2 id="unknown-30">未知</h2>
        <h2>Unknown</h2>
        <p>还不错。现在，让我们看看如果出租车司机在第三次博士后研究后成为物理学博士，搜索会如何进行。这些实际上不是遍历。这些只是司机正在考虑的事情，这是所有可能路径中最好的。所以，小偷做得很糟糕。初学者做得相当不错，但不是最佳的。
        </p>
        <p>Not too bad. Now, let’s have a look at how the Search would happen if the cab driver was a Ph.D.&nbsp;in
            physics after his third post doc. These are not actually traverse. These are just things that the driver is
            thinking about, and that is the very best of all possible paths. So, the thief does a horrible job. The
            beginner does a pretty good job, but not an optimal job.</p>
        <p>这是物理学博士在第三次博士后研究后得出的最优结果。那么，你想了解它们是如何工作的吗？答案当然是肯定的。我将与你讨论与你刚刚解决这个问题的方式不同的程序。我想，如果我让你找到一条从 s 到 g
            的路径，你会在几秒钟内找到一条相当不错的路径。</p>
        <p>This is the optimal job as produced by the Ph.D.&nbsp;in physics after his third post doc. So, would you like
            to understand how those all work? The answer, of course, is yes. I’m going to talk to you about procedures
            that are different from the way that you just solved this problem. I imagine that if I said to you, please
            find a path for s to g, you would, within a few seconds, find a pretty good path.</p>
        <p>虽然不是最佳方法，但效果还不错。用眼睛。我们不会告诉你它是如何工作的，因为我们不知道它是如何工作的。但我们知道用眼睛解决问题是我们整体智力的重要组成部分。</p>
        <p>Not the optimal one, but a pretty good one. using your eyes. And we’re not going to tell you about how that
            works, because we don’t know how that works. But we do know that problem solving with the eyes is an
            important part of our total intelligence.</p>
        <h2 id="unknown-31">未知</h2>
        <h2>Unknown</h2>
        <p>除非我们能够理解人类视觉系统对解决日常问题（比如在地图上找到一条相当好的路径）的贡献，否则我们永远无法拥有完整的人类智能理论。但是，唉，我们不能谈论这个，因为我们不知道怎么做。我们正在努力。但我们不知道怎么做。所以，我不会在我的插图中使用剑桥。
        </p>
        <p>And we’ll never have a complete theory of human intelligence until we can understand the contributions of the
            human visual system to solving everyday problems like finding a pretty good path in that map. But, alas, we
            can’t talk about that, because we don’t know how to do it. We’re working on it. But we don’t know how to do
            it. So, I’m not going to use Cambridge in my illustrations.</p>
        <p>一个小时内要处理的事情太多了。因此，我们将使用这张图，它旨在说明一些要点。您也可以用眼睛轻松地通过该图找到一条路径。我们的程序没有眼睛，也没有基于视觉的算法，因此它们必须做其他事情。</p>
        <p>There’s too much there to work through in an hour. So, we’re going to use this map over here which has been
            designed to illustrate a few important points. You, too, can find a path through that graph pretty easily
            with your eyes. Our programs don’t have eyes, and they don’t have visually grounded algorithms, so they’re
            going to have to do something else.</p>
        <p>我们要讨论的第一种搜索方法称为大英博物馆方法。这种方法至少是对大英博物馆的侮辱，如果不是对整个国家的话，因为搜索大英博物馆的方法就是找到所有可能的路径。因此，在板上画一张所有可能路径的图表会很有帮助。我们将从搜索大英博物馆开始。
        </p>
        <p>And the very first kind of search we want to talk about is called the British Museum approach. This is a slur
            against at least the British Museum, if not the entire nation, because the way you do a British Museum
            search is you find every possible path. So, it’ll be helpful to have a diagram of all possible paths on the
            board. We’re going to start with a British Museum search.</p>
        <h2 id="unknown-32">未知</h2>
        <h2>Unknown</h2>
        <p>从起始位置来看，很明显，您可以从我的 s 转到 a 或 b。而且已经有一个重要的测验点。每当我们在测验中遇到这类问题时，我们都会要求您按词汇顺序开发与搜索相关的树。因此，s
            下的节点按字母顺序列出，只是为了有条不紊地进行。因此，从 a 我们可以转到 b 或 d。&nbsp;</p>
        <p>From the starting position, it’s clear, you can go from my s to either a or b. And already there’s an
            important quiz point. Whenever we have these kinds of problems on a quiz, we ask you to develop the tree
            associated with a search in lexical order. So, the nodes there under s are listed alphabetically, just to
            have an orderly way of doing it. So, from a we can go either b or d.&nbsp;</p>
        <p>另一个主题惯例，测验中你必须记住的另一件事是，我们不能让这些搜索咬自己的尾巴。所以，我可以说，如果我在 a，我也可以回到 s。但是没有路径允许它们咬自己，绕行并进入并回到已经在路径上的位置。</p>
        <p>And another convention of the subject, another thing you have to keep in mind in quizzes, is it we don’t have
            these searches bite their own tail. So, I could have said that if I’m at a, I can also go back to s. But no
            path is ever allowed them to bite itself, to go around and enter and get back to a place that’s already on
            the path.</p>
        <p>现在，如果我先去 b，那就意味着从 b 可以去 a 或 c。这很快就会变得臃肿起来。但是让我们看看，s、a、b。我唯一能去的地方是 c，然后是 e。s、a、d，如果不咬自己的尾巴回到 a，我唯一能去的地方是
            g。sb、a，我只能去 d，然后是 g。</p>
        <p>Now if I go on to b first, that means that from b I can go to either a or c.&nbsp;This is getting fat pretty
            fast. But let’s see, s, a, b. The only place I can go is c and then to e. s, a, d, without biting my own
            tail and going back to a, the only place I can go is g. s b, a, I can only go to d and then to g.</p>
        <h2 id="unknown-33">未知</h2>
        <h2>Unknown</h2>
        <p>最后，s、b、c，我只能到
            e。所以，这是由任何你想编写的程序生成的路径的完整集合，它可以找到所有可能的路径。我没有非常精确地说明如何做到这一点，因为你不必这样做。你无法通过聪明来节省很多工作，因为你必须找到所有的东西。所以，这就是大英博物馆树的扩展。
        </p>
        <p>And finally, s, b, c, I can only go to e. So, that is a complete set of paths as produced by any program that
            you will feel you’d like to write that finds all possible paths. I haven’t been very precise about how to do
            that, because you don’t have to be. You can’t save much work by being clever, because you have to find
            everything. So, that’s the British Museum expansion of the tree.</p>
        <p>那么，我做了什么？我一直在研究地图。我向你展示了一个地图示例。很快你就会认为搜索与地图有关。因此，在迈出哪怕一小步之前，我想强调一下，搜索不等于地图。搜索与选择有关。我恰好用地图来说明这些搜索，因为它们特别有说服力。但搜索与地图无关。
        </p>
        <p>So, what have I done? I’ve been playing around with a map. I showed you an example of a map. And pretty soon
            you’re going to think that Search is about maps. So, before going even another tiny step, I want to
            emphasize that Search is not equal to maps. Search is about choice. And I happen to illustrate these
            searches with maps, because they are particularly cogent. But Search is not about maps.</p>
        <p>这是关于你在做决定时所做的选择。今天我要和你们谈论的这些事情是你在探索地图时所做的选择。当你探索其他类型的事物时，你可以做出其他类型的选择。事实上，最后，如果有时间，我会向你们展示如何在人文课上解决问题时进行搜索。这就是大英博物馆的算法。
        </p>
        <p>It’s about the choices you make when you’re trying to make decisions. These things I’m going to be talking to
            you about today are choices you make when you explore the map. You can make other kinds of choices when
            you’re exploring other kinds of things. And, in fact, at the end, if there’s time, I’ll show you how you do
            searches when you’re solving problems in a humanities class. That’s the British Museum algorithm.</p>
        <h2 id="unknown-34">未知</h2>
        <h2>Unknown</h2>
        <p>搜索与地图无关。我们的第一个金星想法是搜索与选择有关。但为了便于说明，搜索与地图有关。因此，我们要讨论的第一种真正的搜索是深度优先搜索。深度优先搜索的理念是，你一心一意地向前冲。因此，从 s 开始，你的选择是 a 或
            b。按照惯例，你总是沿着左边的分支走下去。因此，从 s 开始，我们转到 a。</p>
        <p>Search is not about maps. Our first gold star idea, Search is about choice. But for our illustration, Search
            is about maps. So, the first kind of Search we want to talk about that’s real is Depth first Search. And the
            idea of Depth first Search is that you barrel ahead in a single minded way. So, from s, your choices are a
            or b. And you always go down the left branch by convention. So, from s, we go to a.</p>
        <p>从 a 开始我们有两个选择。我们可以按照我们的词汇惯例转到 b 或 d。之后，我们可以转到 c。之后，我们可以转到
            e。但对我们来说太糟糕了，我们被困住了。我们该怎么办呢？我们走进了死胡同，一切都完了。但当然，一切还没有完。</p>
        <p>From a we have two choices. We can go to either b or d following our lexical convention. After that, we can
            go to c.&nbsp;And after that we can go to e. And too bad for us, we’re stuck. What are we going to do. We’ve
            got into a dead end, all is lost. But of course, all isn’t lost.</p>
        <p>因为我们可以选择回到上次做出决定的地方，然后选择另一个分支。所以，这个过程被称为备份或回溯。此时，我们会说，啊，死胡同。当我们回溯做出选择的树时，我们找到的第一个地方是我们选择 b 而不是 d
            的地方。所以，我们回到那里并采取另一条路线。</p>
        <p>Because we have the choice of backing up to the place where we last made a decision and choosing another
            branch. So, that process is called variously back up or backtracking. At this point, we would say, ah, dead
            end. The first place we find when we back up the tree where we made a choice is when we chose b instead of
            d.&nbsp;So, we go back up there and take the other route.</p>
        <h2 id="unknown-35">未知</h2>
        <h2>Unknown</h2>
        <p>S、a、d 现在转到
            g。这样就完成了。我们将在这里制作一个小表格，列出我们可以用来修饰基本搜索的内容。我们可以用来修饰基本搜索的内容之一就是回溯思想。现在，回溯与大英博物馆算法无关，因为你必须找到所有内容。找到一条路径后就不能放弃。
        </p>
        <p>S, a, d now goes to g. And we’re done. We’re going to make up a little table here of things that we can
            embellish our basic searches with. And one of the things we can embellish our basic searches with is this
            backtracking idea. Now, backtrack is not relevant to the British Museum algorithm, because you’ve got to
            find everything. You can’t quit when you’ve found one path.</p>
        <p>但是您总是希望在深度优先搜索中使用回溯，因为您可能会一头扎下去而错过通往目标的路径。现在，您可能会问我，回溯是否始终是深度优先搜索的一部分？您可以阅读教科书，其中介绍了两种方法。相信我。如果我们在测验中给您一个搜索问题，我们会告诉您您的搜索是否应该使用回溯。
        </p>
        <p>But you’d always want to use backtracking with Depth first Search, because you may plunge on down and miss
            the path that gets to the goal. Now, you might ask me, is backtracking, therefore, always part of Depth
            first Search? And you can read textbooks that do it either way. Count on it. If we give you a Search problem
            on a quiz, we’ll tell you whether or not your Search is supposed to use backtracking.</p>
        <p>我们认为这是一个可选的东西。如果你在进行深度优先搜索时不使用此可选的东西，那你就太愚蠢了。但我们会将这些想法分开，并将其称为可选附加功能。所以，这就是深度优先搜索，非常简单。现在，深度优先搜索的自然伴侣将是广度优先搜索，广度优先。
        </p>
        <p>We consider it to be an optional thing. You’d be pretty stupid not to use this optional thing when you’re
            doing Depth first Search. But we’ll separate these ideas out and call it an optional add on. so, that’s
            Depth first Search, very simple. Now, the natural companion to Depth first Search will be Breadth first
            Search, Breadth first.</p>
        <h2 id="unknown-36">未知</h2>
        <h2>Unknown</h2>
        <p>它的工作方式是，你逐级构建这棵树，在某个时刻，当你扫描一个级别时，你会发现你已经完成了一条通往目标的路径。所以，逐级，s 可以去 a 或 b。a 可以去 b 或 d。而 b 可以去 a 或
            c。所以，你看我们在做什么。我们逐级进行。</p>
        <p>And the way it works is you build up this tree level by level, and at some point, when you scan across a
            level, you’ll find that you’ve completed a path that goes to the goal. So, level by level, s can go to
            either a or b. a can go either to b or d.&nbsp;And b can go to either a or c.&nbsp;So, you see what we’re
            doing. We’re going level by level.</p>
        <p>我们还没有达到有目标的水平，所以我们必须继续前进。请注意，我们在这里积累了相当多的东西，我们记住的路径集的大小有相当大的增长。在下一个级别，我们有 b 到 c，d 到 g，a 到 d，c 到
            e。现在，当我们扫描时，我们确实达到了 g。</p>
        <p>And we haven’t hit a level with a goal in it yet, so we’ve got to keep going. Note that we’re building up
            quite a bit of stuff here, quite a lot of growth in the size of the path set that we’re keeping in mind. At
            the next level, we have b going to c, d going to g, a going to d, and c going to e. And now, when we scan
            across, we do hit g.</p>
        <p>因此，我们通过广度优先搜索找到了一条路径，就像我们通过深度优先搜索找到了一条路径一样。现在，您可能会说，好吧，为什么您按下 g
            时不直接退出呢？实现细节。我们将讨论一个示例实现。您可以按照自己想要的任何方式编写它。但现在我们知道了这些搜索是什么，让我们在这里加快一点速度，并执行几个现在有名称的搜索。</p>
        <p>So, we found a path with Breadth first Search, just as we found a path with Depth first Search. Now, you
            might say, well, why didn’t you just quit when you hit g? Implementation detail. We’ll talk about a sample
            implementation. You can write it in any way you want. But now that we know what these searches are, let’s
            speed things up a little bit here and do a couple searches that now have names.</p>
        <h2 id="unknown-37">未知</h2>
        <h2>Unknown</h2>
        <p>第一种类型是深度优先，砰。这就是产生小偷路径的方法。然后我们还可以进行广度优先搜索，我们还没有尝试过。你认为会发生什么？它会很快，很慢，产生一条好路径，还是一条坏路径？我不知道，让我们试试吧。你必须加快速度，因为它正在做大量的搜索。
        </p>
        <p>The first type will be Depth first, boom. That’s the one that produces the thief path. And then we can also
            do a Breadth first Search, which we haven’t tried yet. What do you suppose is going to happen? Is it going
            to be fast, slow, produce a good path, produce a bad path? I don’t know, let’s try it. I had to speed it up,
            you see, because it’s doing an awful lot of Search.</p>
        <p>它生成了大量路径。最后，你得到了一条路径。它是最佳路径吗？我不这么认为。但今天我们不讨论最佳路径。我们只讨论相当不错的路径，启发式路径。让我们将起始位置移到中间。你认为广度优先搜索会很愚蠢吗？我认为它会非常愚蠢。让我们看看会发生什么。
        </p>
        <p>It’s generating an awful lot of paths. Finally, you got a path. Is it the best path? I don’t think so. But
            we’re not going to talk about optimal paths today. We’re just going to talk about pretty good paths,
            heuristic paths. Let’s move the starting position here in the middle. Do you think Breadth first Search is
            going to be stupid? I think it’s going to be pretty stupid. Let’s see what happens.</p>
        <p>这个搜索非常偏左，你用眼睛是绝对看不到的。让我放慢速度来演示一下。它找到了一条更短的路径，因为它就在中间。但它花了很多时间向左看。这很愚蠢。但这就是它的工作原理。</p>
        <p>This Search is a lot to the left, which you would never do with you eye. Let me slow that down just to
            demonstrate it. It finds a shorter path, because it’s right there in the middle. But it spends a lot of its
            time looking off to the left. It’s pretty stupid. But that’s how it works.</p>
        <h2 id="unknown-38">未知</h2>
        <h2>Unknown</h2>
        <p>现在我们已经有了两个搜索示例，我想写一个小流程图来说明搜索的工作原理。因为如果我这样做，那么我们将更容易看到这些不同搜索的实现之间存在哪些细微的差异。因此，我们要做的是开发一个等候名单、队列、排队，无论你想叫它什么。
        </p>
        <p>So, now that we’ve got two examples of searches on the table, I’d like to just write a little flow chart for
            how the search might work. Because if I do that, then it’ll be easier for us to see what kind of small
            differences there are between the implementations of these various searches. So, what we’re going to do is
            we’re going to develop a waiting list, a queue, a line, whatever you’d like to call it.</p>
        <p>我们称之为队列。我们将开发一个正在考虑的路径队列。因此，我们算法的第一步是初始化我们的队列。我想我要做的是使用这个算法模拟左边这个问题的深度优先搜索。我需要有某种方式来表示我的路径。</p>
        <p>Let’s call if a queue. We’re going to develop a queue of paths that are under consideration. So, the first
            step in our algorithm will be to initialize our queue. And I think what I’ll do is I’ll simulate Depth first
            Search on this problem up there on the left using this algorithm. I need to have some way of representing my
            paths.</p>
        <p>我想要做的是背叛我作为列表程序员的传统，因为我要把它们当作 lisp 的 s 表达式。首先，我只有一条路径。它只有一个节点，即 s。这就是整个路径。初始化队列后，我要做的第二件事是扩展队列上的第一个路径。好的，当我扩展
            s 时，我得到了两条路径。</p>
        <p>And what I want to do is I’m going to betray my heritage as a list programmer, because I’m just going to put
            these up as if there were lisp s expressions. To begin with, I just have one path. And it has only one node
            in it, s. That’s the whole path. The next thing I do after I initialize the queue is I extend first path on
            the queue. OK, when I extend s, I get two paths.</p>
        <h2 id="unknown-39">未知</h2>
        <h2>Unknown</h2>
        <p>我得到 s 去 a，得到 s 去
            b。我从队列前面取出第一个。然后放回通过扩展该路径产生的两个。现在，在我扩展了队列中的第一个路径后，我必须将这些扩展的路径放入队列中。这里有一个明确的步骤，我检查了第一个路径是否是赢家。</p>
        <p>I get s goes to a, and I get s goes to b. I take the first one off the front of the queue. And I put back the
            two that are produced by extending that path. Now, after I’ve extended the first path on the queue, I have
            to but those extended paths on to the queue. In here there’s an explicit step where I’ve checked to see if
            that first path is a winner.</p>
        <p>如果不是，我会延长它。我必须将这些路径放入队列中。所以，我会说我所做的就是结束队列。现在，我已经完成了一步。让我再做一步。我要删除第一条路径。我要延长那条路径。如果我进行深度优先搜索，我应该把这些新路径放在队列的哪里？
        </p>
        <p>If it’s not, I extend it. And I have to put those paths onto the queue. So, I’ll say that what I do is I end
            queue. Now, I’ve done one step. And let’s let me do another step. I’m going to take this first path off. I’m
            going to extend that path. And where do I put these new paths on the queue if I’m doing Depth first Search?
        </p>
        <p>好吧，我想使用刚刚生成的路径。我要深入搜索树。既然我想继续深入搜索刚刚生成的内容，那么我要把这两条路径放在哪里呢？放在队列的末尾吗？我不这么认为，因为要花很长时间才能到达那里。我想把它们放在队列的前面。</p>
        <p>Well, I want to work with the path that I’ve just generated. I’m taking this plunge down deep into the search
            tree. So, since I want to keep going down into the stuff that I just generated, where then do I want to put
            these two paths? At the end of the queue? I don’t think so, because it’ll be a long time getting there. I
            want to put them on the front of the queue.</p>
        <h2 id="unknown-40">未知</h2>
        <h2>Unknown</h2>
        <p>对于深度优先搜索，我想将它们放在队列的最前面。这就是为什么 s、a、b 放在这里，s、a、d，然后是 b。所以，s、b 仍然在那里。这仍然是一种有效的可能性。</p>
        <p>For Depth first Search, I want to put them on the front of the queue. And that’s why s, a, b goes here, and
            s, a, d, and then that’s, b. So, s, b is still there. That’s still a valid possibility.</p>
        <p>但是现在我在它前面设置了两条路径，这两条路径都是我从队列前面取出一条路径生成的，发现它没有到达目标，就将其延伸并放回队列。我不妨在这里完成这个例子。趁着这个机会，我把 s、a、b 取出来，s、a、b，然后我只能去
            c。&nbsp;</p>
        <p>But now I’ve stuck two paths in front of it, both of the ones I generated by taking a path off the front of
            the queue, discovering that it doesn’t go to the goal, extending it and putting those back on the queue. I
            might as well complete this illustration here. While I’m at it, I take the s, a, b off, s, a, b, and I can
            go only there to c.&nbsp;</p>
        <p>但是，当然，我把 s、a、d 和 s、b 保留在队列中。现在，我再次将队列的前面取出，得到 s、a、b、c、e，别忘了还有 s、a、d 和
            s、b。我从队列中取出第一个。它没有到达目标。我尝试将其延长，但那里什么也没有。我走进了死胡同。</p>
        <p>But, of course, I keep s, a, d and s, b on the queue. Now, I take the front off the queue again, and I get s,
            a, b, c, e, and not to forget s, a, d and s, b. I take the first one off the queue. It doesn’t go to the
            goal. I try to extend it, but there’s nothing there. I’ve reached a dead end.</p>
        <h2 id="unknown-41">未知</h2>
        <h2>Unknown</h2>
        <p>因此，在此操作中，我所做的就是将最前面的元素从队列中移除并缩短队列。我们快到家了。我从队列中移除 s、a、d。然后我得到了 s、a、d、c。当然，我还有
            s、b。现在，下次我访问这种情况时，在第一步中，我发现了一条确实可以到达目标的路径，然后我就完成了。因此，每次我都会将队列形象化。</p>
        <p>So, in this operation, all I’m doing is taking the front one off the queue and shortening the queue. We’re
            almost home. I take s, a,d off of queue. And I get s, a, d, c.&nbsp;And, of course, I still have s, b. Now,
            the next time I visit the situation, buried in that first step, I discover a path that actually does get to
            goal, and I’m done. So, each time around I visualize the queue.</p>
        <p>我检查是否完成了。如果没有，我会取出扩展并将它们放在队列中的某个位置。然后我返回。然后这里有一个不同的测试来检查我们是否完成了。这就是深度优先搜索算法的工作原理。现在，如果我们进行广度优先搜索，我们是否必须从头开始？不。相同的算法。我们得到的所有代码都需要替换一行，更改一行。
        </p>
        <p>I check to see if I’m done. If not, I take the extensions and put them somewhere on the queue. And then I go
            back in. And then here there’s a varied test which checks to see if we’re done. That’s how the Depth first
            Search algorithm works. And now, would we have to start all over again if we did Breadth first Search? Nope.
            Same algorithm. All the code we’ve got needs one line replaced, one line changed.</p>
        <p>我需要做哪些不同的事情才能从中获得广度优先搜索，而不是深度优先搜索？Tanya？TANYA：更改队列。PATRICK WINSTON：我应该把它放在队列的哪里？她说要更改它。TANYA：放在后面？PATRICK
            WINSTON：放在后面。所以，使用广度优先搜索，我所要做的就是把它放在后面。</p>
        <p>What do I have to do different in order to get a Breadth first Search out of this instead of a Depth first
            Search? Tanya? TANYA: Change on the queue. PATRICK WINSTON: And where do I put it on the queue? She says to
            change it. TANYA: On the back? PATRICK WINSTON: Put it on the back. So, with Breadth first Search all I have
            to do is put on the back.</p>
        <h2 id="unknown-42">未知</h2>
        <h2>Unknown</h2>
        <p>现在，如果我们满足于低效的搜索，而不太关心我们的路径有多好，那么我们就完成了。我们可以回家了。但我们有点担心搜索的效率。我们希望有一条相当好的路径。所以，我们将不得不再坚持一段时间。</p>
        <p>Now, if we were content with a inefficient search, and didn’t care much about how good our path was, we’d be
            done. And we could go home. But we are a little concerned about the efficiency of our search. And we would
            like a pretty good path. So, we’re going to have to stick around for a little while.</p>
        <p>现在，您可能已经注意到，在广度优先搜索的开发中，该算法非常愚蠢。为什么该算法非常愚蠢？Ty，您怎么看？TY：它无法判断它离目标是越来越近还是越来越远。PATRICK
            WINSTON：它当然无法判断它离目标是越来越近还是越来越远。我们稍后会讨论这个问题。但它甚至比这更愚蠢。</p>
        <p>Now, you may have noticed, up there in that the development of the Breadth first Search, that the algorithm
            is incredibly stupid. Why is the algorithm incredibly stupid? Ty, what do you think? TY: It can’t tell
            whether it’s getting closer or further away from the goal. PATRICK WINSTON: It certainly can’t tell whether
            it’s getting closer or further away from the goal. And we’re going to deal with that in a minute. But it’s
            even stupider than that.</p>
        <p>为什么它很蠢？你叫什么名字？迪伦：迪伦。它重复两次相同的节点。帕特里克·温斯顿：迪伦说它延伸了不止一次到达相同节点的路径。让我们看看迪伦在说什么。在这里，它延伸了 a。但它已经在那里延伸了 a。在这里，它延伸了一条通往
            b 的路径。它已经延伸了一条通往 d 的路径。&nbsp;</p>
        <p>Why is it stupid? What’s your name? DYLAN: Dylan. It the same nodes twice. PATRICK WINSTON: Dylan said it’s
            extending paths that go to the same node more than once. Let’s see what Dylan’s talking about. Down here, it
            extends a. But it’s already extended a up there. Down here, it extends a path that goes to b. And it’s
            already extended a path that goes to d.&nbsp;</p>
        <h2 id="unknown-43">未知</h2>
        <h2>Unknown</h2>
        <p>在这里，它可以延伸一条经过 c 的路径，但它已经有一条经过 c
            的路径了。所以，所有这些路径都是重复的。我们仍然要遍历它们。这太愚蠢了。我们要做的就是稍微修改一下我们的算法。除非最终节点从未扩展过，否则我们不会扩展队列中的第一个路径。</p>
        <p>Over here, it could extend a path that went through c, but it’s already got a path that goes through
            c.&nbsp;So, all of these paths are duplicated. And we’re still going through them. That’s incredibly stupid.
            What we’re going to do is we’re going to amend our algorithm just a little bit. And we’re not going to
            extend the first path on the queue unless final node never before extended.</p>
        <p>我们要做的是查看是否有这条路径。然后我们要延伸它。最后要说的是。如果我们曾经延伸过一条通往该最终节点的路径，并且它是该路径上的最终节点，那么我们不会再这样做了。</p>
        <p>What we’re going to do is we’re going to look to see if there. we’ve got this path. And we’re going to extend
            it. And it’s got a final note. If we’ve ever extended a path that goes to that final node, and it was a
            final node on that path, then we’re not going to do it again.</p>
        <p>我们必须保留一个列表，其中列出已经延伸的路径的最后一部分。大家都明白了吗？这么说有点尴尬，因为这是我们关心的最后一个节点。如果一条路径终止于一个节点，并且之前有其他路径终止于该节点并被延伸。我们不会再这样做了。因为这是浪费时间。现在，让我们看看这是否真的有帮助。
        </p>
        <p>We got to keep a list of places that have already been the last piece of a path that was extended. Everybody
            got that? It’s a little awkward to say it, because it’s the last node we care about. If a path terminates in
            a node, and if some other path previously terminated in that node and got extended. we’re not going to do it
            again. Because it’s a waste of time. Now, let’s see if this actually helps.</p>
        <h2 id="unknown-44">未知</h2>
        <h2>Unknown</h2>
        <p>现在，使用扩展列表。让我们看看，好吧，哇，我们找到了中心位置。让我们重复之前的搜索。哇，这花了很长时间。但请注意，它将 103
            条路径放回队列。现在，让我们添加一个过滤器并重试。少了很多。所以，让我们加快速度，我们从这里开始。你还记得那次搜索有多乏味吗？</p>
        <p>Now, use the extended list. Let’s see, well, gee, we got that place in the center there. Let’s just repeat
            the previous search. Wow, it’s taking a long time. But notice it put 103 paths back on the queue. Now, let’s
            add a filter and try again. A lot less. So, let’s speed this up, and we’ll start way over here. You remember
            how tedious that search was.</p>
        <p>现在我们用这个列表重复一遍，砰，就到了。这都是因为我们没有做那种愚蠢的事情，即返回已经经过的最后一个节点。所以，你永远不会不想这样做。我们最好将其列为另一个选项。它对大英博物馆算法没有帮助，因为没有什么可以帮助大英博物馆算法。它对深度优先有帮助吗？是的。它对广度优先有帮助吗？是的。
        </p>
        <p>And now we’ll repeat it with this list, boom, there it is. That’s all because we didn’t do that silly thing
            of going back through the final node that’s already been gone through. So, you would never not want to do
            this. We better list this as another option. It doesn’t help with a British Museum algorithm, because
            nothing helps with the British Museum algorithm. Does it help with Depth first? Yes. Does it help with
            Breadth first? Yes.</p>
        <p>我们是否使用广度优先进行回溯？不，因为回溯对我们没有任何好处。好吧，我们差不多了，只是从中间开始的搜索仍然很愚蠢。广度优先版本和深度优先版本都向左走。无论如何，我们永远不会用眼睛这样做。</p>
        <p>Do we do backtracking with Breadth first? No, because backtracking can’t do us any good. OK, we’re almost,
            except that search that’s starting in the middle is still pretty stupid. Both the Breadth first version and
            the Depth first version are going off to the left. And we would never do that with our eyes in any case.</p>
        <h2 id="unknown-45">未知</h2>
        <h2>Unknown</h2>
        <p>我们接下来要做的是，通过考虑我们是否似乎取得了任何进展，来进行更明智的搜索。所以，总的来说，接近我们想要去的地方是一件好事。</p>
        <p>The next thing we want to do is we want to have ourselves a slightly more informed search by taking into
            consideration whether we seem to be getting anywhere. So, in general, it’s a good thing to get closer to
            where we want to go.</p>
        <p>一般来说，如果我们可以选择去一个靠近目标的节点或一个不太靠近目标的节点，我们总是会选择去靠近目标的节点。一旦我们将其添加到我们正在做的事情中，我们就有了另一种搜索，它被称为爬山法。</p>
        <p>In general, if we’ve got a choice of going to a node that’s close to the goal or a node that’s not so close
            to the goal, we’ll always want to go to the one that’s close to the goal. And as soon as we add that to what
            we’re doing, we have another kind of Search, which goes by the name of Hill Climbing.</p>
        <p>它就像深度优先搜索，只不过我们不是使用词汇顺序来打破平局，而是根据哪个节点更接近目标来打破平局。我费了一番功夫才跟你讲到这个入队列表。既然已经费了这么大劲，我现在就打算忽略它。不是因为它不是一个好主意，而是因为试图跟踪示例中的所有内容会使示例变得混乱。
        </p>
        <p>And it’s just like Depth first Search, except instead of using lexical order to break ties, we’re going to
            break ties according to which node is closer to the goal. I went to some trouble to talk to you about this
            enqueued list. And having gone to that trouble, I’m now going to ignore it. Not because it isn’t a good
            idea, but because trying to keep track of everything in the example is confusing the example.</p>
        <h2 id="unknown-46">未知</h2>
        <h2>Unknown</h2>
        <p>在小示例中，它不会正确运行。将排队的事情放在一边，将排队列表放在一边，而是只考虑朝着让我们更接近目标的方向前进的价值。在爬山搜索中，就像深度优先搜索一样，我们有 a 和
            b。我们仍然会在父节点下按词汇顺序列出它们。但现在哪一个更接近目标？</p>
        <p>It won’t work out right in the small example and all that. Put the queueing thing aside, queued list aside,
            and think instead just about the value of going in the direction that’s getting us closer to the goal. In
            Hill Climbing Search, just like a Depth first Search, we have a and b. And we’re still going to list them
            lexically on underneath the parent node. But now which one is so closer to the goal?</p>
        <p>现在，这次 b 比 a 更接近目标。因此，我们不会遵循“深度优先”路线（会带我们向下穿过 a），而是前往最接近目标的路线（穿过 b）。b 可以前往 a 或 c。b 距离目标有六个单位。a
            大约是七个单位以上，并非完全按比例绘制。请用数字而不是眼睛看。现在我们在哪里？</p>
        <p>Now, this time b is closer to the goal than a. So, instead of following the Depth first course, which would
            take us down through a, we’re going to go to the one that’s closest which goes through b. And b can either
            go to a or c.&nbsp;b is six units away from the goal. a is about seven plus, not drawn exactly to scale. Use
            the numbers not your eyes. Now where are we?</p>
        <p>它是对称的，所以 a 和 c 距离目标的距离一样远。现在我们要使用词汇顺序来打破平局。现在从 s、b、a，我们将转到
            d。现在，哪个离目标最近？这是我们唯一的选择。所以，现在我们别无选择，只能向下到达目标。这就是进行搜索的爬山法。请注意，这次没有回溯。</p>
        <p>It’s symmetric, so a and c are both equally far from the goal. Now we’re going to use the lexical order to
            break the tie. Now from s, b, a, we’ll go to d.&nbsp;And now, which is closest to the goal? That’s the only
            choice we have. So, now we have no choice but to go down to the goal. That’s the Hill Climbing way of doing
            the search. And notice that this time there’s no backtracking.</p>
        <h2 id="unknown-47">未知</h2>
        <h2>Unknown</h2>
        <p>这不是最优路径。这不是最好的路径。但至少没有回溯。这并不总是正确的。这只是这个特定示例的产物。你认为爬山法会产生更快的搜索速度吗？我认为是这样。让我们看看当我们一次添加这些东西时会发生什么。首先，让我们关闭我们的扩展列表。我们关闭了我们的扩展列表。
        </p>
        <p>It’s not the optimal path. It’s not the best path. But at least there’s no backtracking. That’s not always
            true. That’s just an artifact of this particular example. Do you think Hill Climbing would produce a faster
            search? I think so. Let’s see what happens when we add these things at one at a time. First, let’s turn off
            our extended list. We turned off our extended list.</p>
        <p>为了进行比较，我们再次使用深度优先算法。它会产生一条非常迂回的路径，有 48
            次入队。现在，让我们切换到爬山算法。您觉得怎么样？您认为它会产生一条更直的路径，更少的入队次数吗？轰隆隆。您不会不想这样做，对吧？如果您有某种启发式方法告诉您您已经接近目标，您应该使用它。</p>
        <p>And we’re going to do Depth first again just for the sake of comparison. It produces a very roundabout path
            with 48 enqueueings. Now, let’s switch over to Hill Climbing. And what do think? Do you think it will
            produce a straighter path, fewer enqueueings? Boom. You wouldn’t not want to do that, would you? If you’ve
            got some kind of heuristic that tells you that you’re getting close to the goal, you should use it.</p>
        <p>现在，很容易修改我上面的例子，这样接近目标会让你陷入死胡同。这很容易做到。但这只是示例的产物。一般来说，你会想走一条让你更接近目标的路。所以，这是 23。我不知道，让我们看看使用扩展列表过滤器是否有用。</p>
        <p>Now, it’s easy to modify my example over there so that getting close to the goal gets you trapped in a blind
            alley on e. That’s easy to do. But that’s just an artifact of the example. In general, you want to go along
            a path that gets you closer to the goal. So, that’s 23. I don’t know, let’s see if using the extended list
            filter does any good.</p>
        <h2 id="unknown-48">未知</h2>
        <h2>Unknown</h2>
        <p>是的，仍然是
            23。所以，在那个特定情况下，扩展列表实际上对我们没有任何帮助，因为我们正如此直接地朝着目标前进。好的，就是这样。现在，让我们看看，有没有类似的情况。好吧，我们可以说这是区分搜索的另一种方式。也就是说，它是知情搜索吗？它是否利用了任何类型的启发式信息？
        </p>
        <p>Yeah, still 23. So, in that particular case the extension list didn’t actually do us any good, because we’re
            driving so directly toward the goal. OK, that’s that. Now, let’s see, is there any analog to. well, we might
            say that this is yet another way of distinguishing the searches. And that is, is it an informed search? Is
            it making use of any kind of heuristic information?</p>
        <p>当然，大英博物馆不是，深度不是，广度也不是。现在让我们考虑一下我们对于爬山运动有什么收获。我们想使用回溯吗？当然。我们想使用入队列表吗？当然。而且它是有信息的，因为它利用了这些额外的信息。这可能不是你的问题。你通常不会在地图上得到这些信息。
        </p>
        <p>Certainly, a British Museum is not, Depth is not, Breadth is not. And now let’s consider what we got for Hill
            Climbing. Do we want to use backtracking? Sure. Do we want to use an enqueued list? Sure. And it is
            informed, because it’s taking advantage of this extra information. It may not be in your problem. It’s not
            often the case you’ve got this information in a map.</p>
        <p>您的问题可能没有任何启发式测量到目标的距离的方法。在这种情况下，您无法做到这一点。但是如果您有，就应该使用它。哦，是的，还有一个。我已经把它放在我的图表上，把它给了大家。它被称为光束搜索。正如爬山法是深度优先搜索的类似物一样，光束搜索是广度优先搜索的补充或启发式启发法的补充。
        </p>
        <p>Your problem may not have any heuristic measurement of distance to the goal. In which case, you can’t do it.
            But if you’ve got it, you should use it. Oh, yeah, there’s one more. And I’ve already given it away by
            having it on my chart. It’s called Beam Search. And just as Hill Climbing is an analog of Depth first
            Search, Beam Search is a complement or addition of an informing heuristic to Breadth first Search.</p>
        <h2 id="unknown-49">未知</h2>
        <h2>Unknown</h2>
        <p>你要做的是像广度优先搜索一样开始。但是你说我要将我在任何级别考虑的路径数量限制为一个较小的固定数字，比如，在这种情况下，两个怎么样。所以，我要说我的光束搜索有两个光束。否则，我会像广度优先搜索一样继续，b、d、a、g。
        </p>
        <p>What you do is you start off just like Breadth first Search. But you say I’m going to limit the number of
            paths I’m going to consider at any level to some small, fixed number, like, in this case, how about two. So,
            I’m going to say that I have a Beam of two for my Beam Search. Otherwise, I proceed just like Breadth first
            Search, b, d, a, g.</p>
        <p>现在我遇到了一个愚蠢的问题，我复制了我的节点，因为我忘记了入队列表。但为了说明波束搜索，我现在要做的是，我将采用我在第二级得到的所有路径，并且只保留最好的两条。这就是我的波束宽度。最好的两条是最接近目标的两条。</p>
        <p>And now I’ve got that stupid thing where I’m duplicating my nodes, because I’m forgetting about the enqueued
            list. But to illustrate Beam Search, what about I’m going to do now is I’m going to take all these paths
            I’ve got at the second level, and I’m only going to keep the best two. That’s my beam width. And the best
            two are the two that get closest to the goal.</p>
        <p>那么，这四个，b、c、a 和 d，哪两个最接近目标？现在，b 和 d。这些家伙被删掉了。我只在每个级别保留两个。现在，从 b 和 d 往下，在下一个级别，我有 c 和
            g。现在我找到了目标。所以，我完成了。我们也可以在这里这样做。我们可以选择 Beam Search，还不错。让我们看看，让我们从中间试试这个。</p>
        <p>So, those four, b, c, a, and d, which two get closest to the goal? Now, b and d.&nbsp;These guys are trimmed
            off. I’m only keeping two at every level. Now, going down from b and d, I have, at the next level, c and g.
            And now I’ve found the goal. So, I’m done. We could do that here, too. We could choose a Beam Search, not
            bad. Let’s see, let’s try this thing from the middle.</p>
        <h2 id="unknown-50">未知</h2>
        <h2>Unknown</h2>
        <p>让我们稍微放慢一点速度。现在，我们会看到像普通广度优先搜索那样向左偏移的东西吗？不会，因为它很聪明。它不会说，我想去一个离我的目标更远的地方。现在，让我们看看，也许我们现在可以回到我们的算法，讨论一下入队机制，讨论一下爬山法。
        </p>
        <p>Let’s slow my speed down a little bit. Now, are we going to see anything going off to the left like we did
            with ordinary Breadth first Search? No, because it’s smart. It doesn’t say, I want to go to a place that’s
            further away from my goal. Now, let’s see, maybe we can go back to our algorithm now and talk about that
            enqueueing mechanism and talk about Hill Climbing.</p>
        <p>我可以使用相同的基本搜索机制，只需再次更改那一行吗？是的。这次我如何将新路径添加到队列中？嗯，这很像爬山，对吧？我想将它们添加到前面，但要稍微花点功夫。花招是什么？你觉得呢？记住，我想使用我的启发式信息。</p>
        <p>Can I use the same basic search mechanism, just change that one line again? Yes. How do I add new paths to
            the queue this time? Well, it’s very much like Hill Climbing, right? I want to add them to the front but
            with one little flourish. What’s the flourish? what do you think? Remember, I want to use my heuristic
            information.</p>
        <p>所以，我不仅将它们添加到前面，而且在添加到前面的那些中，我该怎么做？观众：检查距离？帕特里克·温斯顿：检查距离。你如何安排它们？观众：帕特里克·温斯顿：是的，如果你愿意，你可以把最小的放在前面。但让我们对它们进行排序。我们会对它们进行排序，这样可以让一切都井然有序。所以，爬山赛是排在前面的。最后，平衡木怎么样？
        </p>
        <p>So, I not only add them to the front, but amongst the ones I’m adding to the front, what do I do?
            AUDIENCE:Check the distance? PATRICK WINSTON: Check the distance. And how do you arrange them? AUDIENCE:
            PATRICK WINSTON: Yeah, you can put the minimum first if you like. But let’s sort them. We’ll sort them, that
            will keep everything straight. So Hill Climbing is front sorted. And, finally, how about Beam?</p>
        <h2 id="unknown-51">未知</h2>
        <h2>Unknown</h2>
        <p>我们如何利用 Beam Search 将它们添加到队列中？好吧，将它们添加到哪里并不重要，因为我们要做的就是保留 w 最佳。因此，对于 Beam，我们只需将其缩写为保留 w
            最佳即可。现在，您的工具包中有一些基本搜索。还有一个有时会被讨论的搜索。我们有深度、广度、最佳和 Beam，还有一个是最佳、最佳优先搜索。</p>
        <p>What do we do with Beam Search to add them to the queue? Well, it doesn’t matter where we add them, because
            all we’re going to do is we’re going to keep the w best. So, with Beam, we’ll just abbreviate that by saying
            keep w best. Now, you have some of the basic searches in you’re toolkit. There’s one more that’s sometimes
            talked about. We’ve got Depth, Breadth, Best, and Beam, one more is Best, Best first Search.</p>
        <p>这是一种变体，你可以说，我有这棵树。它有很多以叶子为终点的路径。让我始终在最接近目标的叶子节点上工作。它可以从一个地方跳到另一个地方。因为当它追寻一条路径时，它可能在另一条相当远的路径上表现不佳。这棵树将成为最好的那棵树。
        </p>
        <p>It’s a variant where you say, I’ve got this tree. It’s got a bunch of paths that terminate in leaves. Let me
            just always work on the leaf node that’s closest to the goal. It can skip around a little bit from one place
            to another. Because as it pursues one path, it may not do very well in some other path quite distant. And
            the tree will become the best one.</p>
        <p>我们实际上在集成程序中看到了一个这样的例子。它能够跳过所有地方，因为它总是在搜索树、与/或树中寻找最简单的问题，然后处理它。这就是最佳优先搜索。您也可以在连续空间中做这些事情。而且您已经在 1802
            年或其他地方做过这方面的数学计算。但在连续空间中，爬山法有时会导致问题或效果不佳。</p>
        <p>We’ve actually seen an instance of that in then integration program. It’s capable of skipping all over the
            place, because it’s always taking the easiest problem in the search tree, in the and/or tree, working on
            that. That’s Best first Search. You can do these sorts of things in continuous spaces, too. And you’ve done
            the mathematics of that in 1802 or something. But in continuous spaces, the Hill Climbing sometimes leads to
            problems or doesn’t do very well.</p>
        <h2 id="unknown-52">未知</h2>
        <h2>Unknown</h2>
        <p>在连续空间中使用爬山法会遇到什么样的问题？那么，如何在连续空间中进行爬山法呢？假设我们在山中，大雾弥漫。我们想在冻死之前到达山顶。我们用指南针向北走几步，向东走几步，向西走几步，向南走几步。</p>
        <p>What kind of a problem can you encounter in a continuous space with Hill Climbing? Well, how would you do
            Hill Climbing in a continuous space? Let’s say we’re in the mountains, and a big fog has come up. We’re
            trying to get to the top of the hill before we freeze to death. And we take a few steps north, a few steps
            east, west, and south using our compass.</p>
        <p>然后我们检查哪个方向似乎最能让我们向上移动。这就是我们的爬山法，对吧？我们已经探索了四个可以走的方向，并选择了最好的一个。从那里，我们选择四个，尝试所有这些，选择最好的一个，然后我们就可以开始了。我们得到了一个爬山算法。它有什么问题？或者它可能有什么问题？有时它工作得很好。
        </p>
        <p>And we check to see which direction seems to be doing the best job of getting us moving upward. And that’s
            our Hill Climbing approach, right? We have explored four directions we can go and pick the best one. And
            from there, we pick four, try all those, pick the best one, and away we go. We’ve got ourselves a Hill
            Climbing algorithm. What’s wrong with it? Or what can be wrong with it? Sometimes it works just fine.</p>
        <p>是的。 说话者 1：你可能会陷入局部最大值。 帕特里克·温斯顿：我们可能会陷入局部最大值。所以，问题字母 a
            是，如果这是你的空间，它可能看起来像那样。你可能会陷入局部最大值。还有其他类型的问题会出现吗？嗯，这完全取决于空间是什么样的。这是一个空间具有局部最大值的问题。</p>
        <p>Yes. SPEAKER 1: You might get stuck in a local maximum. PATRICK WINSTON: We might get stuck in a local
            maximum. So, problem letter a is that if this is your space, it may look like that. And you may get stuck on
            a local maximum. Is there any other kind of problem that can come up? Well, it all depends on what the space
            is like. Here’s a problem where the space has local maxima.</p>
        <h2 id="unknown-53">未知</h2>
        <h2>Unknown</h2>
        <p>现在，当雾气弥漫时，华盛顿山上有很多人丧命。他们确实会被冻死，为什么呢？他们之所以会被冻死，是因为爬山失败了，他们无法到达山顶的护林站。原因是华盛顿山的山肩上有大片草坪。那里相当平坦。所以，这是电线杆的问题。那个地方看起来像这样。
        </p>
        <p>Now, a lot of people have been killed on Mt. Washington when the fog comes up. And they do freeze to death,
            why? The reason they freeze to death is the Hill Climbing fails them, and they can’t get to the top to the
            ranger station. And the reason is that there are large lawns on the shoulders of Mt. Washington. It’s quite
            flat. So, it’s the telephone pole problem. That space looks like this.</p>
        <p>嗯，这不是华盛顿山的样子。但这是电线杆问题。所以，当你在这里徘徊时，尝试几个方向并挑选最好的一个的想法没有任何帮助，因为它是平坦的。这可能是爬山法的问题。现在，爬山法还有一个大多数人不知道的问题。但它是这样运作的。这是一个在高维空间中特别严重的问题。
        </p>
        <p>Well, this isn’t what Mt. Washington looks like. But it’s the telephone pole problem. So, when you’re
            wandering around here, the idea of trying a few directions and picking the one that’s best doesn’t help any,
            because it’s flat. That can be a problem with Hill Climbing. Now, there’s one more problem with Hill
            Climbing that most people don’t know about. But it works like this. This is a particularly acute problem in
            high dimensional spaces.</p>
        <p>我在这里只用两个例子来说明。我要从常规视图切换到等高线图。因此，我的等高线图将显示 45
            度线上的一座尖锐桥梁。现在您知道在那里会遇到什么麻烦了。您遇到麻烦了，因为如果您朝每个方向迈出一步，那么每个方向都会让您走下坡路。而您却以为自己已经到达了顶峰。</p>
        <p>I’ll illustrate it here just in two. And I’m going to switch from a regular kind of view to a contour map.
            So, my contour map is going to betray the presence of a sharp bridge along the 45 degree line. Now you see
            how you can get in trouble there. You get in trouble, because if you take a step in each direction, every
            direction takes you downhill. And you think you’re at the top.</p>
        <h2 id="unknown-54">未知</h2>
        <h2>Unknown</h2>
        <p>假设你在这里，然后向北走。这样你就会越过等高线。如果你向南走，你也会越过等高线。同样，向西和向东走都看起来像是在往下走，但实际上你正在爬山脊。​​而那条等高线是我所展示的最高点。所以，有时你可能会被愚弄。不是被困住，而是被愚弄。你以为自己在山顶，但实际上你不在。
        </p>
        <p>So, suppose you’re right here and you go north. That takes you down over a contour line. If you go south,
            that also takes you down over contour lines. Likewise, going west and east all appear to be taking you down,
            whereas, in fact, you’re climbing a ridge. And that contour line is the highest that I’ve shown. So,
            sometimes you can get fooled. not stuck, but fooled. into thinking you’re at the top when you’re actually
            not.</p>
        <p>现在，这是一个模型。这个主题是关于智能建模的。这是一种你经常需要的算法，以便构建一个智能系统。但是我们的头脑中是否有任何类型的搜索？如果我们要模拟我们头脑中发生的事情，我们是否必须模拟任何类型的搜索才能做我们人类做的事情？我想是的。
        </p>
        <p>Now, this is a model something. This subject is about modeling intelligence. And this is a kind of algorithm
            you frequently need in order to build an intelligent system. But do we have any kind of Search happening in
            our heads? If we’re going to model what goes on inside our heads, do we have to model any kind of searching
            in order to do the kinds of things that we humans do? I suppose so.</p>
        <p>每次制定计划时，我们实际上都在评估一系列选择，并观察它们如何发挥作用。让我看看能否用另一种方式来说明。这是我上次向你们展示过一点的系统。而且，我可能还会在这里回顾一两件事。我给你们展示了一个麦克白的故事。这是我给你们展示的故事。
        </p>
        <p>Anytime we make a plan, we’re actually evaluating a bunch of choices and seeing how they work. Let me see if
            I can illustrate it another way. This is a system that I showed you a little bit of last time. And, shoot, I
            might as well review one or two things here. I showed you a Macbeth story. This is the story I showed you.
        </p>
        <h2 id="unknown-55">未知</h2>
        <h2>Unknown</h2>
        <p>如果你在人文课上学到这个，最简单的问题可能是为什么麦克德夫在最下面杀死了麦克白？我上次演示了如何回答问题，还是只是演示了图表的展开？我不记得了。但无论如何，我们会再做一次。这是有点程式化的英语。你知道，它不必是程式化的英语。
        </p>
        <p>And if you had this in a humanities class, the simplest questions that might be asked is why did Macduff kill
            Macbeth down there at the bottom? Did I demonstrate the answering of questions last time, or just the
            development of the graph? I can’t remember. But we’ll do it again, anyway. This is somewhat stylized
            English. Just so you’ll know, it doesn’t have to be stylized English.</p>
        <p>这是通过 Story Workbench 提供给 Genesis
            系统的英语。天下没有免费的午餐。您可以利用您的人力资源用三年级英语重写故事情节。或者，您可以利用您的人力资源采用更自然、成人版本的故事，并用注释进行修饰，使它更容易被理解。就在这个夏天，在你们中的一个奇迹般的夏天。
        </p>
        <p>This is English that’s made available to the Genesis system by way of something called Story Workbench.
            There’s no free lunch. Either you can use your human resources to rewrite the plot in third grade English.
            Or you can use your human resources to take a more natural, adult type version of the story and decorate it
            with annotations that make it possible to absorb it. Just this summer, in a miracle of summer one of you.
        </p>
        <p>将这两个系统连接在一起。因此，我们现在可以处理用非常自然的英语表达的故事。我们系统中的所有内容都用英语表达，包括常识知识。比如，如果有人杀了你，你就死了。但更重要的是，对于今天的例子来说，反思层面的知识，关于复仇的知识。你在这里。你在上人文课，有人说，故事里到底发生了什么？
        </p>
        <p>Connected these two systems together. So, we can now work with stories that are expressed in pretty natural
            English. Everything in our system is expressed in English, including common sense knowledge. like if
            somebody kills you, you’re dead. but more importantly, for today’s illustration, that reflective level
            knowledge, that knowledge about what revenge is. Here you are. You’re in the humanities class, and someone
            says, what’s really going on in the story?</p>
        <h2 id="unknown-56">未知</h2>
        <h2>Unknown</h2>
        <p>不是关于谁杀了谁的细节，而是是否有一场惨胜？是否有人成功了？是否有报复行为？这些都是你可能在人文课上被问到的问题。所以，让我启动创世系统。为互联网连接祈祷。启动系统，阅读我刚才给你看的麦克白故事。</p>
        <p>Not the details of who kills whom, but is there a Pyrrhic victory? Does somebody have a success? Is there an
            act of revenge? These are all kinds of things you might be asked about in some kind of humanities class. So,
            let me fire up the genesis system. Pray for internet connectivity. Launch the system on a read of that
            Macbeth story that I showed you just a moment ago.</p>
        <p>目前，它正在吸收有关背景知识、反思水平知识和所有这类东西的信息。它正在构建我们称之为精细化图表的东西。它还没有完全做到这一点。它仍在阅读背景知识。现在它正在阅读《麦克白》。它正在构建它的精细化图表，与你上次看到的一样，只是不完全一样。你看到底部的东西了吗？
        </p>
        <p>At the moment, it’s absorbing information about background knowledge, and about reflective level knowledge,
            and all that sort of thing. It’s building itself this thing we call an elaboration graph. It’s not quite
            there yet. It’s still reading background knowledge. Now it’s reading Macbeth. It’s building it’s elaboration
            graph, the same thing you saw last time, except not quite. Do you see that stuff down at the bottom?</p>
        <p>这些是它在麦克白故事中设法找到的更高层次的概念。所以，它找到了复仇。它是怎么做到的？它搜索了。它有一个关于复仇的描述，它查看了该模式是否在精细图中得到展示。</p>
        <p>Those are higher level concepts that it’s managed to find in the Macbeth story. So, its found a revenge. How
            did it do that? It searched. It had a description of what a revenge is, and it looked to see if that pattern
            was exhibited in the elaboration graph.</p>
        <h2 id="unknown-57">未知</h2>
        <h2>Unknown</h2>
        <p>因此，在明确说出的事情和膝跳式“如果/那么”规则产生的事情的组合中，精细化图表充分实例化，可以找到复仇模式。这很有趣，皮洛士式胜利有点难。如果你说，哦，这里有一个皮洛士式胜利，你可能会得到一个
            A。就是这样。所以，我会把它放大一点，这样你就能看到它是什么了。</p>
        <p>So, in a combination of things that were said explicitly and things that were produced by knee jerk if/then
            rules, the elaboration graph was sufficiently instantiated that the revenge pattern could be found. That’s
            interesting, Pyrrhic victory is a little harder. You’d probably get an a if you said, oh, there’s a Pyrrhic
            victory in here. There it is. So, I’ll blow that up a little bit so you can see what that is.</p>
        <p>你知道什么是皮洛士式的胜利。一开始一切似乎都很好，但后来就不那么顺利了。所以，麦克白想成为这里的国王。最终他成为了国王。但对麦克白来说太糟糕了，因为他最终被杀了。所以，这是一场皮洛士式的胜利。所有这些都是由查看此图表的搜索程序生成的。
        </p>
        <p>You know what a Pyrrhic victory is. It’s a situation where everything seems to be going good at first, and
            then not so hot. So, Macbeth wants to be King down here. And eventually that leads to becoming King. But too
            bad for Macbeth, because eventually he gets killed in consequence. So, it’s a Pyrrhic victory. All that
            produced by Search programs who are looking through this graph.</p>
        <p>当然，一旦你有能力做到这一点，你就可以找到各种各样的东西。你可以用英语报道它们。但更有趣的是，你可以回答问题。为什么麦克白会这么做。它根本不关心大小写。人工智能：从常识层面来看，杰基尔博士认为麦克德夫杀死了麦克白，因为麦克白在反思层面上激怒了麦克德夫。
        </p>
        <p>Now once you’ve got the capability of doing that, of course, then you can find all sorts of things. And you
            can report them in English. But, more interestingly, you can answer questions. Why did Macbeth. it cares not
            a hoot about capitalization. ARTIFICIAL INTELLIGENCE: On a common sense level, it looks like Dr.&nbsp;Jekyll
            thinks Macduff killed Macbeth because Macbeth angered Macduff on a reflective level.</p>
        <h2 id="unknown-58">未知</h2>
        <h2>Unknown</h2>
        <p>看起来杰基尔博士认为麦克德夫杀死麦克白是出于错误、皮洛士式的胜利和复仇。帕特里克·温斯顿：相当老套的演讲输出。但你明白我的意思。它是如何将这些东西放到常识层面的？所有那些构建目标树的程序都以同样的方式报告、回答问题。它只是在目标树中的连接中进行局部查找。它是如何将这些东西放到反思层面的？
        </p>
        <p>It looks like Dr.&nbsp;Jekyll thinks Macduff killed Macbeth as part of acts of mistake, Pyrrhic victory, and
            revenge. PATRICK WINSTON: Pretty corny speech output. But you see the point. How did it get the stuff on the
            common sense level? The same way all those programs that build goal trees report, answers the questions.
            It’s just looking locally around in the connections in the goal tree. How did it get the stuff on the
            reflective level?</p>
        <p>通过报告产生信息的搜索。它通过寻找有关其自身想法的更高层次的想法并报告我们询问的事件实际发生在哪些更高层次的想法中来实现这一点。所以，让我们看看，只是为了好玩，我们可能对麦克白谋杀邓肯的原因感兴趣。如果你没有真正读过剧本，这会不会很方便，这就是你必须写那篇论文的原因？
        </p>
        <p>By reporting on the searches that produced information. it does that by looking for higher level thoughts
            about its own thoughts and reporting in which of those higher level thoughts the incident we asked about
            actually occurs. So, let’s see, just for fun, we might be interested in why Macbeth murdered Duncan.
            Wouldn’t this be handy if you hadn’t actually read the play, and here it is, you’ve got to write that paper?
        </p>
        <p>人工智能：从常识层面来看，它看起来像。帕特里克·温斯顿：我会停止使用这个，因为这太烦人了。是的，很好，麦克白想当国王，邓肯就是国王。让我们看看，为什么麦克白会成为国王？哦，除非我拼写正确，否则它不会回答这个问题。直到去年春天我才能向你展示这一点。
        </p>
        <p>ARTIFICIAL INTELLIGENCE: On a common sense level, it looks like. PATRICK WINSTON: I’ll pull the plug on that,
            because that’s just annoying. Yeah, pretty good, Macbeth wants to be King, and Duncan is the King. Let’s
            see, why did Macbeth become King? Oh, it won’t answer the question unless I spell it right. I wouldn’t be
            able to show that to you until last spring.</p>
        <h2 id="unknown-59">未知</h2>
        <h2>Unknown</h2>
        <p>事实上，直到上周早上经过调整，我才能够向你们展示这一点。因为我们刚刚将语言输出连接到解析器系统，该系统正在反向运行，以生成英语。所以，除了我之外，这是任何人都从未见过的东西。所以，这就是我们今天要做的。</p>
        <p>In fact, I wouldn’t have been able to show you this today until last week with a tweak this morning. Because
            we’ve just now connected the language output to, of course, parser system, which is running in reverse, in
            order to generate that English. So, that’s something that has never before been seen by any eyes but me. So,
            that will conclude what we have to do today.</p>
        <h1 id="search-optimal-branch-and-bound-a">5.搜索：最优、分支限界、A*</h1>
        <h1>5. Search: Optimal, Branch and Bound, A*</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 48:37 <a
                href="https://youtube.com/watch?v=gGQ-vAmdAOI">https://youtube.com/watch?v=gGQ-vAmdAOI</a></p>
        <p> 11 years ago (Jan 11, 2014) — 48:37 <a
                href="https://youtube.com/watch?v=gGQ-vAmdAOI">https://youtube.com/watch?v=gGQ-vAmdAOI</a></p>
        <h2 id="route-66">66 号公路</h2>
        <h2>Route 66</h2>
        <p>教授：这是关于 66 号公路的文章，这条公路曾是伊利诺伊州芝加哥和加利福尼亚州洛杉矶之间的主要公路。这是一条非常著名的公路，因为任何想穿越全国的人都会选择 66
            号公路，因为这是一条最短的路。问题是，你如何找到最短的路径？不是任何一条老路或一条好路，而是如何找到最短的路径？</p>
        <p>PROFESSOR: It was written about Route 66, which used to be the main highway between Chicago, Illinois and Los
            Angeles, California. Very famous highway because anybody who wanted to go across country always took route
            66 because it was the shortest way to go. And the question is, how do you find the shortest path? Not just
            any old path or a good path, but how do you find the very shortest path?</p>
        <p>这就是我们今天要讨论的主题。但是，我很遗憾 66 号公路的消失，但它已经被艾森豪威尔总统创建的州际公路系统所取代。猜猜为什么？让我们看看，也许 ROTC
            的人知道。你知道艾森豪威尔为什么创建州际公路系统吗？当然，在公共事务中，解释和原因总是有区别的。解释是。</p>
        <p>And that’ll be the subject that we’re going to discuss today. But route 66, I lament its passing, but it’s
            been largely replaced by the interstate highway system that was created by President Eisenhower. Guess why?
            Let’s see, maybe the ROTC people know. You know why Eisenhower created the interstate highway system? Well,
            in public affairs, of course, there’s always a distinction to be made between the explanation and the
            reason. The explanation was.</p>
        <p>听众：将武器运往全国各地？教授：嗯，将核武器运往全国各地。让我们用更温和、更温和的措辞来表达。艾森豪威尔观察到，尽管我们炸毁了德国的铁路，但由于德国的汽车债券，德国军队仍能迅速调动部队。</p>
        <p>AUDIENCE: To move weapons across the country? PROFESSOR: Well, to move nuclear weapons across the country.
            Let’s put it in, sightly, more benign terms. Eisenhower had observed that the German army was able to move
            its troops rapidly, even though we bombed their railroads into oblivion because of their auto bond.</p>
        <p>因此艾森豪威尔设想，如果美国遭到入侵，我们也希望能够通过公路系统调遣军队。结果，我们的公路系统相当不错，但铁路系统却很糟糕。这很有意思。我以一种有趣的方式受益于此，因为我来自伊利诺伊州的东皮奥里亚。</p>
        <p>So Eisenhower conceived that if there were ever an invasion in the United States, we too would want to be
            able to move our forces around on a highway system. And consequence of that, we have a pretty good highway
            system and pretty awful railroad system. It’s interesting. I’m a beneficiary of that, in a funny way,
            because I’m from East Peoria, Illinois.</p>
        <p>我周围都是卡特彼勒拖拉机公司的因素，该公司制造了修建所有这些道路所需的所有拖拉机。所以我的高中从所有这些因素的庞大税基中大量花钱。无论如何，今天我们想要找到最好的道路，而不仅仅是一条好道路。和上次一样，我们将处理这两个例子，我们可以让我们的程序运行于此。
        </p>
        <p>And I was surrounded by the factors of Caterpillar Tractor Company, which made all of the tractors that built
            all of those roads. So my high school spent money like water from that huge tax base of all those factors.
            Anyway, today we want to find the very best path, instead of just a good path. And like the last time, we’ll
            deal with, both, an example that we can set our program to work on.</p>
        <p>顺便问一下，你能找到 S 和 G 之间的最短路径吗？你愿意用你的生命来打赌 S 和 G
            之间的最短路径吗？可能不会。用你的眼睛，你可以找到一条好的路径。但你找不到最好的路径。今天，我们所做的可能不是对我们头脑中任何明显的属性进行建模。</p>
        <p>By the way, can you find the shortest path between S and G? Would you like to bet your life on the shortest
            path between S and G? Probably, not. With your eye, you can find a good path. But you can’t find the best
            possible path. Today, what we’re doing is probably not modeling any obvious property of what we have inside
            our heads.</p>
        <p>但能够找到最佳路径是任何上过人工智能课程的人都应具备的技能。因此，我们将研究它，即使它与我们做的许多事情不同。它可能是你脑海中正在发生的事情的模型。因此，我们将同时使用剑桥的这个示例和我们的 Blackboard
            示例。让我们看看，我们必须谨慎行事。</p>
        <p>But being able to find the best path is part of the skill set that anybody who’s had a course artificial
            intelligence would be expected to have. So we’re going to look at it, even though it’s not like many of the
            things we do. A model of something that’s, probably, going on in your head. So we’re going to use, both,
            this example from Cambridge and our Blackboard example. Let’s see, we have to caution ourselves.</p>
        <p>Tanya，搜索是关于地图的吗？不，是关于什么的？以 C 开头。下一个字母是
            H。最后是选择。所以我们谈论的是选择。而不是地图。尽管我们的例子取自地图，因为它们很方便，但它们是视觉化的，有助于理解我所说的算法背后的概念。</p>
        <p>Tanya, is search about maps? No, it’s about what? Starts with a C. And the next letter is H. And it ends up
            being choice. So we’re talking about choice. Not about maps. Even though our examples are drawn from maps
            because they’re convenient, they’re visual, and helps understand the concepts behind the algorithms I’m
            talking about.</p>
        <h2 id="heuristic-distance">启发式距离</h2>
        <h2>Heuristic Distance</h2>
        <p>那么我们先来看看课堂上的例子。今天我做了一件上次没做的事。</p>
        <p>So let’s start off by looking at our classroom example. And I did something today that I neglected to do last
            time.</p>
        <p>这就是我所说的启发式距离。它就是我刚刚在地图上画的那些粉色线条。我们说的是两地之间的直线距离，即使两地之间没有道路。所以一般来说，就像我们上次讨论的那样，最好是让自己到达一个直线距离很近的地方，离你的目标很近。</p>
        <p>And that’s talk to you about what I meant by heuristic distance. It’s those pink lines that I just drew on
            the map. We’re talking about the distance as the crow would fly between two places, even though there’s no
            road that goes between those two places. So in general, and we discussed last time, it’s best to get
            yourself into a place that’s close, as the crow flies, to your goal.</p>
        <p>当然，这只是一种启发式方法，它可能会给你带来麻烦，因为它并不总是正确的。看起来，位于节点 E 是个好地方，因为它离 G
            不太远。但在那个旨在说明这一点的特定情况下，靠近实际上并不是一件好事，因为它是一条死胡同。但总的来说，靠近是件好事。</p>
        <p>And of course, that’s a heuristic and it can get you in trouble because it’s not always true. It would appear
            that being at node E is a good place to be because it’s not very far from G. But in that particular case
            designed to illustrate the point, being close is, actually, not a good thing because it’s a dead end. But in
            general, it’s a good thing to be close.</p>
        <p>我们上次谈到了爬山法和光束搜索，接近是这类搜索的目标。在光束搜索的图示中，我们曾有 C、B、A 和 D。我们有终止于这四个节点的路径作为下一轮搜索的候选。</p>
        <p>And we talked last time about hill climbing and beam search, being close was the objective of those kinds of
            searches. And at one point, in a beam search illustration, we had C, B, A, and D. We had paths terminating
            at all four of those nodes as candidates for the next round of search.</p>
        <p>我们根据这些航线距离决定保留 D 和 B，并拒绝 A 和 C，因为它们直线距离较远。现在，我重复一遍，尽管你们中的许多人已经在教程中解决了这个问题，因为我们今天将需要这个启发式距离的概念。我想确保这一点已经澄清。
        </p>
        <p>And we decided on the basis of these airline distances to keep D and B, and reject A and C because they’re
            further away as the crow flies. Now, I repeat this even though many of you have had this fixed already in
            your tutorials because we’re going to need this concept of heuristic distance today. And I wanted to be sure
            that point has been clarified.</p>
        <p>现在，有了这张小地图，我想你可以通过肉眼判断哪条最短路径。那是什么，胡安娜？你能帮我吗？听众：S、A、D、G。教授：S、A、D、G。如果你把这些距离加起来，你会发现从 S 到 A，再到 D，再从 D 到 G
            的路径的距离是 11。</p>
        <p>So now, with this smaller map I imagine you can do, by eye, a determination of what the shortest path is.
            What is it, Juana? Can you help me out with that? AUDIENCE: S, A, D, G. PROFESSOR: S, A, D, G. And if you
            add up those distances, the distance is 11 along that path that goes from S, first to A, and then to D, and
            then from D to G.</p>
        <p>因此，胡安娜断言这是最佳途径。我们将把胡安娜视为先知，因为在最初尝试理解这些算法时，我们将遵循一个非常重要的解决问题的原则。那就是，如果你想解决问题，最简单的方法通常是问一个知道答案的人。或者谷歌，它也很可能知道答案。所以在这个特殊情况下，我们相信胡安娜知道答案。
        </p>
        <p>So Juana asserts that is the best path. And we’re going to treat Juana as an Oracle because we’re going to
            follow, in our initial attempt to understand these algorithms, a very important principle of problem
            solving. And that is that, if you want to solve a problem, the easiest way is, usually, ask somebody who
            knows the answer. Or Google, which also, probably, knows the answer. So in this particular case, we believe
            that Juana knows the answer.</p>
        <p>她说最短路径是 S、A、D、G，路径长度为 11。但我们不相信她，因为我们申请的是同一所医学院，她可能想欺骗我们。教授：所以我们会非常谨慎地接受她的答案，直到我们检查过以确保她没有试图欺骗我们。</p>
        <p>And she said that the shortest path is S,A, D, G, and its path length is 11. But we don’t trust her because
            we’re applying to the same medical school and she may be trying to screw us. PROFESSOR: So we’re going to be
            very cautious about accepting her answer until we’ve checked it to make sure that she hasn’t attempted to
            delude us.</p>
        <h2 id="path-length">路径长度</h2>
        <h2>Path Length</h2>
        <p>那么我们该如何做呢？</p>
        <p>So how do we go about doing that?</p>
        <p>好吧，一种方法是检查我们能开发的所有其他可能路径最终肯定比 Juana 告诉我们的路径长。所以她告诉我们 S、A、D、G。它的总路径长度为 11。现在我要做的就是开发这棵树状图的其余部分。</p>
        <p>Well, one way to do that is to check to be sure that all other possible paths that we could develop end up
            being, for sure, longer than the one that Juana has told us about. So she’s told us about S, A, D, G. And it
            has a total path length of 11. And now what I’m going to do is I’m just going to develop the rest of this
            tree like diagram.</p>
        <p>但是我要做的是，我不会像大英博物馆那样随机地做这件事。我要做的是，我会看看与可以延伸的最短路径相对应的选择。所以可以延伸的最短路径就是这里的这条。它只有起始节点。我本可以用另一条路去 B。</p>
        <p>But what I’m going to do is, I’m not going to do it in a British Museum or random way. What I’m going to do
            is, I’m going to look at the choice that corresponds the shortest path that can be extended. So the shortest
            path that can be extended is this one right here. The one that just has the starting node in it. And I could
            have gone this other way to B.</p>
        <p>如果我朝 B 走另一条路，那么沿那条路的路径长度为 5。同样，如果我查看终止于 A 的路径，其路径长度为 3。所以现在我有两个选择。A 和 B。我有超出这两个地方的选择。所以我总是会延伸长度较短的那个。</p>
        <p>And if I go that other way to B, then the path length along that side is 5. And likewise, if I look at the
            path that terminates in A, that has a path length of 3. So now I’ve got two choices. A and B. I’ve got
            choices that extend beyond those two places. So I’m always going to extend the one that has the shorter
            length.</p>
        <p>在这种情况下，那就是从 S 到 A 的路径。所以如果我从 S 到 A，我不必去 D。我也可以去 B。如果我去 B，那么累积路径长度就是 S、A、B。那是
            7。并且知道我们现在谈论的是路径，比如累积路径长度，是我们迄今为止走过的。</p>
        <p>So in this case, that would be the path that goes from S to A. So if I from S to A, I don’t have to go to D.
            I can also go to B. And if I go to B, then the accumulated path length is S, A, B. That’s 7. And know that
            we’re talking now about the path, like the accumulated path length, that we’ve traveled so far.</p>
        <p>上次我们讨论了很多关于到达目标的距离。启发式估计我们离目标有多远。现在我们做的正好相反。我们不考虑我们还要走多远。我们只考虑我们已经走了多远。所以现在，再次重复这些步骤。我得到了 7 和 5。所以我会回顾并考虑在路径
            S、B 上经过 B 节点的选择。</p>
        <p>Last time we were talking a lot about distances to the goal. Heuristic estimates of how far we are from the
            goal. Now we’re doing exactly the opposite. We’re not considering how far we’ve got to go. We’re only
            thinking about how far we’ve gone so far. So now, repeating these steps again. I’ve got 7 and 5. So I’ll go
            over and consider the choices that go through the B node on the path S, B.</p>
        <p>这样我得到了 S、B、A 和 S、B、C。这些路径长度是多少？让我们看看。S、B、A 是 9。S、B、C 也是 9。现在最短路径就是这条。所以我延伸它。我走 S、A、B。S、A、B。我唯一能去的地方是 C。这又增加了
            4。所以是 11。我对这条路径了解多少？</p>
        <p>And that gives me S, B, A and S, B, C. And what are those path lengths? Well, let’s see. S, B, A would be 9.
            And S, B,C would be 9. And now the shortest path is this one over here. So I extend that. I go S, A, B. S,
            A, B. The only place I can go is C. That adds another 4. So that’s 11. And what do I know about that path?
        </p>
        <p>我不需要再继续下去了，对吧？因为我已经走过那条路了，所以路径长度等于胡安娜告诉我的到达目标的路径长度。所以继续下去是愚蠢的，因为假设这些长度都是非负的，我不可能做得更好。而且我甚至不能做得那么好，除非我得到的长度为
            0。</p>
        <p>I don’t have to take that any further, right? Because the path length, since I’ve gone on that path already,
            is equal to the path length that Juana has told me gets me to the goal. So it’ll be foolhardy to carry on
            because, presuming that these lengths are all non negative, I can’t do any better. And I can’t even do as
            well, unless I’ve got a length that has 0 length.</p>
        <p>既然我有了这个想法，我可以很快得出结论，好吧，让我考虑一下这两条路径。S、B、A 只能去 D。如果我去 D，那么 3.9 加 3 等于 12。那里不会发生其他事情，因为那是 12，而我的目标路径是 11。C，我只能去
            E。</p>
        <p>So now that I have that idea, I can quickly finish up by saying, well, let me consider these two paths. S, B,
            A can only go to D. And if I go to D, that adds 3.9 plus 3 is 12. Nothing else can happen there because
            that’s 12 and I’ve got a path of a goal that’s 11. C, I can only go to E.</p>
        <p>这是一条死路，但我不必考虑这一点，因为我知道这条路径的累计距离是 6 加 9。也就是 15。所以所有这些都不需要进一步延伸，因为它们到目前为止累计的长度等于或小于目标的长度。所以我检查了
            Oracle。虽然我们申请的是同一所医学院，但胡安娜告诉了我真相。</p>
        <p>It’s a dead end but I don’t have to think about that because I know that the accumulated distance along this
            path is 6 plus 9. That’s 15. So all of these need not be extended any further because their length,
            accumulated so far, is equal to or less than a length of a goal. So I’ve checked the Oracle. And although
            we’re applying to the same medical school, Juana has told me the truth.</p>
        <p>所以现在，不幸的是，胡安娜并不总是在身边。而且我身边也没有神谕。所以我必须找到某种方法，在没有神谕的情况下找到可以检查的最短路径。让我们看看。我能做什么？</p>
        <p>So now, unfortunately, Juana’s not always around. And I don’t always have an Oracle. So I’m going to have to
            have some way of finding the shortest path without that Oracle that I can check against. Let’s see. What can
            I do?</p>
        <h2 id="extending-path-length">延长路径长度</h2>
        <h2>Extending Path Length</h2>
        <p>也许我可以做我刚才做的事情。始终延伸迄今为止的最短路径，并希望在某个点上到达目标。</p>
        <p>Maybe I can do the same thing I just did. Always extend the shortest path so far and hope that I run into the
            goal at some point.</p>
        <p>然后我必须问自己一个问题：当我没有 Oracle 时，我需要做多少额外的工作？让我们尝试一下，看看会发生什么。你没有那条路径可以开始。所以我只有 S。这个距离是 0。我可以去 A 或 B。</p>
        <p>And then I have to ask myself the question how much extra work did I need to do when I don’t have the Oracle?
            Let’s just try it and see what happens. You don’t have that path to start. So I just have S. This distance
            is 0. I can go either to A or B.</p>
        <p>如果我去 A，距离是 3。这里，距离是 5。我将延伸从 S、A 到 B 的路径。这样可以到达 B 或 D。这样去 B 或 D 可以得到 7。S、A、D 可以得到
            6。因此，查看所有这些并延伸迄今为止最短的路径可以让我回到 S、B。因此我延伸了它们。</p>
        <p>If I go to A, I’ve got a distance of 3. Here, I’ve got a distance of 5. I’ll extend the path that goes S, A.
            That can either got to B or D. Going to B or D gives me 7 that way. S, A, D gives me 6. So looking across
            all of these and extending the shortest path so far takes me back over to S, B. So I extend those.</p>
        <p>S、B 会带我到 A 或 C。而这些路径的总累计路径长度分别为 9 和 9。现在最短的路径是
            S、A、D。你看到了这个模式。现在让我们看看。我还没有找到目标。所以我可以问自己一个问题：我迄今为止所做的任何工作是否都白费了？</p>
        <p>S, B takes me to A or C. And those, in turn, have total accumulated path lengths of 9 and 9. Now the shortest
            one is S, A, D. You see the pattern. Now let’s see. I haven’t found the goal yet. So I can ask myself the
            question is any of the work that I’ve done so far wasted?</p>
        <p>不是，因为到目前为止我得到的所有路径都比目标路径短，因为目标还没有出现。所以当我在找到目标后进行 oracle 检查时，这些工作都不会白费。所以最后，我实际上不需要
            Oracle。我可以通过延伸迄今为止最短的路径来开发这个图表，直到我达到目标。</p>
        <p>No because all of the paths that I’ve got so far are shorter than the path of the goal because the goal
            hasn’t shown up. So when I do my oracle checking after I found the goal, none of that work’s going to be
            wasted. So in the end, I don’t, actually, need the Oracle. I could just develop this graph by extending the
            shortest path, so far, until I hit the goal.</p>
        <p>然后，也许，再做一些剩余的检查，以确保所有其他路径的延伸长度都大于目标路径的长度。如果这些词让你感到困惑，让我们继续算法，我认为它会更清楚。让我们看看。我们有 7、6 和两个 9。我们将延伸
            6。这样就到达了目标。砰，我们做到了。</p>
        <p>And then, perhaps, do a little remaining checking to make sure that all the other paths extend with a length
            that’s greater than the path of the goal. So if those words are confusing, let’s carry on with the
            algorithm, and I think it’ll be clearer. So let’s see. We’ve got the 7,6, and two 9s. We’re going to extend
            the one that’s 6. That gets this to the goal. Boom, we’ve got it.</p>
        <p>现在我们得到的路径长度为 11。但请注意，我们不能放弃，因为我们必须确保所有其他路径的长度都大于 11。因此，现在我们必须继续使用我们开始时使用的相同算法。Oracle
            检查算法。当我们这样做时，我们会寻找迄今为止尚未扩展的最短路径。即 B、S、A、B。它到达 C。它是 11。所以我们完成了。</p>
        <p>And we’ve got a path length of 11. Note, though, we can’t quit because we have to be sure that all other
            paths are longer than 11. So now we have to carry on with the same algorithm that we started with. The
            Oracle checking algorithm. And when we do that, we look for this shortest path, so far, that has not been
            extended. That’s B, S, A, B. That goes to C. That’s 11. So we’re done there.</p>
        <p>A 加 D。加 3。等于 12。C 加 E。加 6。等于 15。果然，我们完成了。好吗？艾略特？听众：它知道不可能从教授那里得到零距离延伸吗？问题是，它知道没有零距离长度即将出现吗？这是一个实施细节。</p>
        <p>A goes to D. That adds 3. That’s 12. C goes to E. That adds 6. That’s 15. And sure enough, we’re done. OK?
            Elliot? AUDIENCE: Does it know that there’s know that there isn’t a chance that you could have a zero
            distance extension from the PROFESSOR:. The question is, does it know that there’s no zero distance length
            that’s coming up. That’s an implementation detail.</p>
        <p>这保证了你会找到一条尽可能短的路径。但如果它们的长度为零，则可能还有其他路径。只要它们的长度为非负数，我们就安全了。我们找到了一条最短路径。</p>
        <p>This guarantees you’ll find a path that’s as short as any that you can possibly find. But there might be
            others if they’re zero length lengths. As long as they’re non negative lengths, we’re safe. We’ve got a
            shortest path.</p>
        <h2 id="branch-and-bound">分支限界法</h2>
        <h2>Branch and Bound</h2>
        <p>这很简单。现在我们可以用更复杂的剑桥地图重复这个练习。首先，让我们先做深度图，回忆一下它是什么样子的。这当然不是一条捷径。</p>
        <p>So that was easy. And now we can repeat the exercise with our more complicated map of Cambridge. First of
            all, let’s do depth first just to recall what that looks like. That is, certainly, not a short path.</p>
        <p>那么让我们试试这个想法，顺便说一下，这个想法带有分支入站标签。让我们在同一张地图上尝试分支入站。然后它就出现了。每个小闪烁都在尝试另一条路径。所以你可以看到它正在努力寻找最短路径。它几乎已经到达那里，但这几乎是一个病态的情况。或者它几乎在做大英博物馆。在那里它终于找到了最短的路径。
        </p>
        <p>So let’s try this idea, which, by the way, bares the label branch inbound. Let’s try branch inbound on the
            same map. And there it goes. Each of those little flickers is trying another path. So you can see it’s
            working it’s guts out to find the shortest path. It’s almost there but it’s almost a pathological case. Or
            it’s almost doing British Museum. There it’s finally found the shortest path.</p>
        <p>现在我们可以问一些问题。但首先，在我问任何问题之前，我想把流程图放到板上，因为我们会在进行过程中稍微装饰一下这个流程图。所以我们要做的第一件事是初始化队列。然后我们将测试队列上的第一个路径。然后我们可能会很高兴，因为我们可能已经完成了。
        </p>
        <p>Now there are some things we can ask about that. But first of all, before I ask anything about it, I’d like
            to get the flow chart up on the board because we’re going to decorate that flow chart, a little bit, as we
            go. So the first thing we do is initialize queue. Then we’re going to test first path on the queue. Then we
            might be happy because we might be done.</p>
        <p>我们可能有一条到达目标的最短路径。实际上，这并不完全正确，不是吗？我们不能真正放弃，直到所有其他路径都是它。嗯，这很有趣。如果队列中的第一个元素将我们带到目标，并且我们按路径长度对队列进行排序，那么只要队列中的第一个元素将我们带到目标，我们是否就完成了？
        </p>
        <p>We might have a shortest path to the goal. Actually, that’s not quite true, is it? We can’t really quit until
            every other path is it. Well, that’s interesting. If the first element on the queue gets us all the way to
            the goal, and we sorted our queue by path length, are we through as soon as that first element on the queue
            gets us to the goal?</p>
        <p>是的，因为除了它之外的每条路径都必须经过排序。因此，它无法为我们提供一条更短的路径来达到目标​​。因此，如果第一条路径是通往目标的路径，我们就完成了。可惜，通常不是。所以我们将扩展第一条路径。我们将把所有这些扩展放回队列，然后对它们进行排序。所以，这和我们上次做的差不多。
        </p>
        <p>Yeah because every other path must have been sorted beyond it. And therefore, it can’t offer us a shorter
            path to the goal. So if the first path is a path to the goal we’re done. Alas, it usually isn’t. So we’ll
            extend first path. We’re going to put all those extensions back on the queue, and then we’re going to sort
            them. So that’s, pretty much, the same as we did last time.</p>
        <p>我们总是将元素放回队列。我们将查看队列中的第一个元素，看看它是否是赢家。如果是，我们就完成了。如果不是，我们将扩展它。然后回到这里再试一次。好吧，差不多。但我们注意到，这做了大量工作，因为如果我们查看上面的统计数据，它将
            1,354 条路径放入队列。</p>
        <p>We’re always going to put the elements back on the queue. We’re going to look at the first element the queue
            and see if it’s a winner. If it is we’re done. If it’s not, we’re going to extend it. And then go back in
            here and try again. Well, sort of. But we noted that this did a awful lot of work because if we look at
            those statistics up there, it put 1,354 paths onto the queue.</p>
        <p>这就是 N 排队部分。然后它扩展了已排到队列前面的 835,000 条路径。</p>
        <p>That’s the N queueing part. And then it extended 835,000 paths that had come to the front of the queue.</p>
        <h2 id="extended-list">扩展列表</h2>
        <h2>Extended List</h2>
        <p>现在我想给你一个题外话，因为很容易混淆 N 队列和扩展。在我们上次进行的所有搜索中，保留一个我们放入队列的所有路径的列表是完全合理的。N 队列列表。</p>
        <p>Now I’d like to give you an aside because it’s easy to get confused about N queueing and extending. In all of
            the searches we did last time, it would have been perfectly reasonable to keep a list of all the paths that
            we had put onto the queue. An N queueing list.</p>
        <p>如果一条路径终止于某个节点，而其他路径终止于该节点，并且该节点已经进入队列，则永远不要将这条路径添加到我们的队列中。我上次说的是让我们跟踪已扩展的内容，不要再次扩展它们。因此，您可以跟踪已扩展的节点，不要再次扩展它们。
        </p>
        <p>And never add a path to our queue if it terminates in a node that some other path terminate in that has
            already gone to the queue. What I said last time was let us keep track of the things that have been extended
            and not extend them again. So you can either keep track of the nodes that have been extended and not extend
            them again.</p>
        <p>或者查看带有终止节点的路径，等等等等，并将它们放入队列中，即已排队的路径。不要再将东西放回队列中。我想，上次我可能在其中放了一列，上面写着 N 排队。它应该被扩展。尽管上次 N
            排队有效，但这次只有扩展有效，因为我们想确保我们扩展的任何东西都是一条短路径。</p>
        <p>Or look at the paths with nodes that terminate, and blah, blah, blah and been put on the queue, the queued
            ones. And not put things back on the queue again. And I think, last time, I may have put a column in there
            that said N queued. It should have been extended. Even though N queued worked last time, only extended works
            this time because we want to be sure that anything we extend is a short path.</p>
        <p>因此，对于这些最佳路径，N
            排队的想法根本行不通。所以现在我想回到侧边栏，说我们正在跟踪所有节点，所有以节点结尾的路径，除非它们已经延伸到特定位置之外。所以我们需要在这里修饰我们的算法，说测试第一条路径，如果尚未延伸，则延伸第一条路径。</p>
        <p>So the N queued idea doesn’t work, at all, for these optimal paths. So now I want to come back over here off
            the side bar and say that we’re keeping track of all of the nodes, all of the paths that end in nodes unless
            they have already been extended beyond that particular place. So we need to decorate our algorithm here and
            say test first path and extend the first path if not already extended.</p>
        <p>因为您可以看到，在我举的例子中，到目前为止，我们做了上次讨论过的同样愚蠢的事情。我们延长了经过 A 的路径不止一次，就像这样。延长这条路径有意义吗？没有，因为我们已经延长了一条距离更短的路径。延长这条路径有意义吗？
        </p>
        <p>Because you can see that in the example I had, so far, we did that same silliness that we talked about last
            time. We extended paths that went through A more than once, like so. Would it ever make sense to extend this
            path? No because we’ve already extended a path that got there with less distance. Will it ever make sense to
            extend this path?</p>
        <p>不是，因为我们已经扩展了另一条距离更短的路径。因此，如果我们保留一个扩展列表，我们可以将其添加到分支入站中，以发挥我们的优势。让我们看看这在教室示例中如何工作。然后我们将进行剑桥分析。所以这是桥入站，加上一个扩展列表。我的意思是扩展。不在
            N 排队列表中。N 排队列表在这里不起作用。</p>
        <p>No because we’ve already extended another path that gets to be by a shorter distance. So if we keep an
            extended list, we can add that to branch inbound to our advantage. So let’s see how that would work on the
            classroom example. And then we’ll do Cambridge. So this is bridge inbound, plus an extended list. And I do
            mean extended. Not in the N queued list. N queued list won’t work here.</p>
        <p>让我们看看，我以与之前相同的方式开始。S 指向 A 或 B。长度为 3。长度为 5。因此我扩展 A。它指向 B 或 D。但 B 好像根本不存在。哦，抱歉。等一下。B 指向那里。</p>
        <p>So let’s see, I start off the same way as I did before. S goes to either A or B. That’s a length of 3. That’s
            a length of 5. So I extend A. That goes to either B or D. But B is as if it wasn’t there at all. Oh, sorry.
            Hang on. B goes there.</p>
        <p>这些路径长度分别为 7 和 6。现在我环顾棋盘，问目前为止最短的路径是什么？ 是 B。所以我将其延伸到 A 和 C，路径长度分别为 9 和 9。下一个最短的路径是什么？ 是 D。然后到达 G。路径长度为
            11。棋盘上最短的路径是什么？</p>
        <p>And those path lengths are 7 and 6. And now I look around on the board, and I say what is the shortest path
            so far? And it’s B. So I extend that to get to A and C with path lengths of 9 and 9. And what’s the shortest
            one next? It’s D. And that goes to G. And the path length is 11. And what’s the shortest one on the board?
        </p>
        <p>接下来要扩展的是这条路径。这条路径到达 B。但我已经扩展了一条到达 B 的路径。所以我实际上不需要做那条扩展。这样我就省了一些工作。但我必须现在就去这里做这两个。但等一下。我已经扩展了 B。我已经扩展了
            A，所以我也不需要做那条扩展。</p>
        <p>The one that has to be extended next. That’s this one that gets to B. But I’ve already extended a path that
            get to B. So I don’t, actually, do that extension. So I’ve saved some work. But I’ve got to go over here and
            do these two now. But wait. I’ve already extended B. I’ve already extended A, so I don’t have to do that one
            either.</p>
        <p>我唯一要做的就是去往 C 的路径。然后去往 E 的路径长度为 15。这样就完成了。所以如果你将这个路径与前一个路径进行比较，你会发现这棵树上可能有很多区域被剪掉了，不需要全部检查。</p>
        <p>The only one I have to do is the one that goes to C. And that those then to E with a path length of 15. And
            I’m done. So if you compare this one with a previous one, you can see that there might be vast areas of this
            tree that are pruned away and don’t have to be examined all.</p>
        <p>现在，为了说明这一点，我想跟踪其中一个统计数据。扩展的数量。对于这个特定示例，即案例 1，扩展的数量为 835。如果我使用扩展列表的概念，您能猜出结果是什么吗？</p>
        <p>So now, just for the sake of illustrating that, I would like to keep track of just one of those statistics.
            The number of extensions. So for this particular example, case one, the number of extensions was 835. Why
            don’t you see if you can guess to yourself what it would be if I use this concept of an extended list.</p>
        <p>瞧，我不会再扩展任何已经扩展过的东西，因为它的路径长度肯定会比已经到达同一位置的东西更长。所以这样做毫无意义。所以让我将类型更改为带有扩展列表的分支定界。我要把速度调低一点，这样我们就可以看一看。这可能需要剩下的一个小时。谁知道呢？还有很多工作要做。
        </p>
        <p>See, I’m not going to extend anything I’ve already extended because it’s guaranteed to have a longer path
            length then something that already got to that same place. So it makes no sense to do it. So let me change
            the type to branch and bound with an extended list. I’m going to turn the speed down a little bit so we can
            watch it. It might take the rest of the hour. Who knows? Still doing a lot of work.</p>
        <p>仍在检查大量路径。好吧，看看这个。它只做了 38 个扩展，而不是 835 个。</p>
        <p>Still examining a lot of paths. Well, look at that. Instead of 835 extensions it only did 38.</p>
        <h2 id="dead-horse-principle">死马原则</h2>
        <h2>Dead Horse Principle</h2>
        <p>所以这是一笔相当可观的节省。你肯定想这么做。所以请注意，这是在分支之上的分层。这不是不同的算法。这是对算法的调整改进，它使算法更有效率。所以这整个事情都基于我所说的死马原则。</p>
        <p>So that’s a pretty substantial savings. And you would never not want to do that. So note that’s a layering on
            top of branching out. That’s not a different algorithm. It’s an adjustment improvement to the algorithm, and
            it makes it more efficient. So this whole thing is based on what I call the dead horse principle.</p>
        <p>一旦我们发现一条通往特定地点的路径不可能是获胜路径，我们就会将其删除，并且不再费心延伸它。这是一个死马原则。但是如果我们看看这个例子，一条已经从 S 到 B
            的路径的最短可能长度是多少？你怎么看，Tanya？嗯，首先，它不能小于 5，因为我们已经走了那么远。</p>
        <p>As soon as we figure out that a path that goes to a particular place can’t possibly be the winning path, we
            get rid of it, and don’t bother extending it. It’s a dead horse principle. But if we look at this example,
            what’s the shortest possible length of a path that’s already gone from S to B? What do you think, Tanya?
            Well, first of all, it can’t be less than 5 because we’ve already gone that distance.</p>
        <p>因此，当我说从 S 到 D 的任何路径的最短长度是多少时。我们知道它至少是 5。但我们能再说点什么吗？特别是，当我们查看这些航线距离时，注意到这条航线距离是 6，这比 7 稍大一点，这比 7 稍大一点。那么你觉得呢？
        </p>
        <p>So when I say what’s the shortest length of any path that there could possibly be that goes from S to D. We
            know it’s at least 5. But can we say something more about it? Especially, when we look at these airline
            distances, and note that this airline distance is 6, and that’s a little more than 7, and that’s a little
            more than 7. So what do you think?</p>
        <p>所以它从 S 到了 B，问题是从 S 到 B 的最短路径可能是什么？11 对吧？因为我们不可能有比航空距离更短的路径。如果有一条从 A 到 G 的直线路，它的长度将是
            6。但事实并非如此。所以这给了我们沿着这条路径的距离的下限。</p>
        <p>So it’s gone from S to B, and the question is what’s the shortest path that could possibly be that had
            started out going from S to B? 11 right? Because we can’t have a path that’s shorter than the airline
            distance. If there were a straight line road from A to G, its length would be 6. But there isn’t. So that
            gives us a lower bound on the distance that we have along that path.</p>
        <p>因此，我们使用累积距离加上航空距离来得出从 S 到 B 的起始路径的下限。</p>
        <p>So we’re using the accumulated distance, plus the airline distance, to give us a lower bound on the path that
            we’ve started off on that goes from S to B.</p>
        <h2 id="simulation">模拟</h2>
        <h2>Simulation</h2>
        <p>再次，让我们通过模拟搜索并查看结果来巩固一下。不仅仅是上次，我会忘记我有一个扩展列表。我不想同时随身携带这两样东西。</p>
        <p>Once again, let’s solidify a little bit by simulating the search and seeing how it turns out. Not just I did
            last time, I’m going to forget that I’ve got an extended list. I don’t want to carry both of those things
            around with me at the same time.</p>
        <p>所以，别在意我们有一个扩展列表。稍后我们会把所有这些重新整合在一起。所以我们将忘记我们刚才所做的一切。相反，我们将使用航空距离的概念，看看会发生什么。</p>
        <p>So forget that we’ve got an extended list. We’ll bring all those back together a little later. So we’re going
            to forget what we just did there. And instead we’re just going to use this concept of an airline distance
            and see what happens.</p>
        <h2 id="airline-distances">航空距离</h2>
        <h2>Airline Distances</h2>
        <p>和之前一样，我们从起始节点开始。我们一如既往地有两个选择。我们可以去 A 或 B。</p>
        <p>As before we start with a starting node. We have two choices as always. We can go to A or B.</p>
        <p>如果我们去 A，累计距离是 3。如果我们去 B，累计距离是 5。但现在我们要加上航空距离。所以从 A 到 G 的航空距离略大于 7，也就是 10 多。</p>
        <p>And the accumulated distance, if we go to A, is 3. And then accumulated distance if we go to B is 5. But now
            we’re going to add in the airline distances. So the airline distance from A to G is a little more than 7,
            which is 10 plus.</p>
        <p>从 B 到 G 的航空距离正好是 6。所以结果是 11。按照我们到目前为止已经使用过的程序，我们将延伸看起来最短的路径。现在它是从 S 到 G 的最短潜在距离。所以一定是这里的距离。所以从 A 我们可以去 B 或 D。
        </p>
        <p>The airline distance from B to G is exactly 6. So that gives us 11. And following the procedure we’ve all
            been using already so far, we’re going to extend the path that seems to have the shortest potential. Now
            it’s the shortest potential distance S to G. So that must be this one here. So from A we can go to B or D.
        </p>
        <p>S、A、B 的累计距离为 7。航空距离为 6，因此等于 11。标准算术 13。S、A、D 的距离。等于 6 加上略大于 7 的数。那么累计距离是多少？S、A、D 等于 3 加 3 等于 6。听众：教授：什么？听众：从
            D 出发的航空距离为 5。教授：是 5，对吧。因此，在这种情况下，航空距离与实际距离相同。</p>
        <p>The accumulated distance S, A, B, is 7. The airline distance is 6, so that’s equal to 11. Standard arithmetic
            13. The distance S, A, D. That is 6 plus a little more than 7. So what’s the accumulated distance? S, A, D
            is 3 plus 3 is 6. AUDIENCE: PROFESSOR: What? AUDIENCE:. The airline distance from D would be 5. PROFESSOR:
            Would be 5, right. So airline distance, in this case, is the same as the actual distance.</p>
        <p>所以累计距离是 6。实际距离是 5。所以等于 11。所以现在我在板上有两个 11。模拟我们在测验中要求你做的事情，我们不知道哪一个会更好。他们的分数相同。所以我们要做的是选择词汇量最少的那个。所以 B 排在 D
            之前。所以我们将扩展 B。</p>
        <p>So the accumulated distance is 6. The actual distance is 5. So that’s equal to 11. So now I’ve got two 11’s
            on the board. And simulating what we’d ask you do on a quiz, we don’t know which of those is going to be
            better. They’ve got a tie score. So what we’re going to do is we’re going to choose the one that’s lexically
            least. So B comes before D. So we’ll expand B.</p>
        <p>这条路可以通往 A 或 C。我们必须计算沿着这些路径的最佳距离。S、B、A、S、B、A 的累计距离为 9。所以是 9 加 7 加。也就是 16 加。这条路的累计距离为 9。也是加 7 加。也是加 16
            加。好吧，现在让我们看看。事情进展得相当顺利，因为这条路到目前为止得分最低。</p>
        <p>And that can go to either A or C. And we have to calculate the best possible distance that goes along those
            paths. The accumulated distance S, B, A. S, B, A is 9. So that’s 9 plus 7 plus. That’s 16 plus. This has an
            accumulated lead of distance of 9. Also plus 7 plus. Also 16 plus. Well, now let’s see. Things are shaping
            up pretty well because this one has the lowest score so far.</p>
        <p>我们将其扩展到 G。现在累计距离为 11。航空距离为 0，所以是
            11。这比其他所有距离都小。所以我们得到了。现在将这个与我们的分支入站图进行比较。你看，我们再一次做了相当少的工作。在许多实际情况下，这意味着完成计算的时间不是超过宇宙剩余寿命的时间，而是可以在几秒钟内完成。</p>
        <p>We extend that to G. And now the accumulated distance is 11. The airline distance is 0, so that’s 11. And
            that’s smaller than everybody else. So we’ve got. So now compare this one with our branch inbound graph. And
            you see, once again, we’ve done considerably less work. And that, in many practical cases, means that
            instead of taking more than the remaining lifetime of the universe to complete the calculation, it can
            happen in a few seconds.</p>
        <p>但是让我们看看它在示例中是如何运作的。</p>
        <p>But let’s see how it works on the example.</p>
        <h2 id="in-admissible-heuristics">在可接受的启发式方法中</h2>
        <h2>In admissible heuristics</h2>
        <p>所以我不打算使用扩展列表。我只是想使用这个想法，即对剩余距离（航空距离）使用下限，然后看看会发生什么。所以这一次，扩展的数量是 70。所以它单独工作时的效果不如单独使用扩展列表时的效果好。</p>
        <p>So I’m not going to use the extended list. I’m just going to use this idea of using a lower bound on the
            distance remaining, the airline distance, and see what happens. So this time, the number of extensions is
            70. So it didn’t do quite as well as working alone as the extended list did working alone.</p>
        <p>因此，我们立即得出结论，扩展列表比使用这些下限启发式方法更有用。顺便说一句，这被称为可采纳启发式方法。如果保证启发式估计值小于实际距离，则称为可采纳启发式方法。可采纳是因为您可以将其用于此类目的。因此，看起来扩展列表比可采纳的想法更有用。对吗？
        </p>
        <p>So we immediately conclude that the extended list is more useful than using one of these lower bound
            heuristics. By the way, this is called an admissible heuristic. If the heuristic estimate is guaranteed to
            be less than the actual distance, that’s called an admissible heuristic. Admissible because you can use it
            for this kind of purpose. So it looks like the day extended list is a more useful idea than the admissible
            idea. Right?</p>
        <p>布雷特，你对此有何看法？我是在胡说八道吗？我是在开玩笑吗？听众：我认为你下结论太早了。教授：我为什么下结论太早？你认为这可能取决于什么？听众：我们使用扩展和扩展列表这一事实几乎保证了你只能扩展每个节点一次。教授：嗯，布雷特说了一些难以理解的话，我想不出该如何重复。但他想说的是。教授：
        </p>
        <p>What do you think about that, Brett? Am I hacking? Am I joking? AUDIENCE: I think you’re judging prematurely.
            PROFESSOR: Why am I judging prematurely? What do you think it might depend on? AUDIENCE:. The fact that
            we’re using extensions and the extended list pretty much guarantees you can only extend each node once.
            PROFESSOR: Well, Brett has sad something unintelligible that I can’t think how to repeat. What he meant to
            say, though, was that. PROFESSOR:</p>
        <p>在这些情况下，它几乎总是取决于问题本身。如果你改变问题，你可能会得到不同的结果。那么我们为什么不改变问题，看看我们是否会得到不同的结果呢？所以，不要从最左边开始，而是从中间开始，看看会发生什么。所以我会重新调整我的起始位置，让它就在那里。哎呀，那是错误的调整。
        </p>
        <p>In these cases, it almost always depends on the problem itself. If you change the problem, you may get a
            different result. So why don’t we change the problem and see if we get a different result? So instead of
            starting on the extreme left, let’s start in the middle and see what happens. So I’ll readjust my starting
            position to be right there. Oops, that’s the wrong adjustment.</p>
        <p>我们不妨从不做任何事就开始获取基线分支定界。对于这个问题，也许我们可以稍微加快一点速度。这样我们就得到了 57
            个扩展。这是一个比较简单的问题。让我们用可接受的启发式方法试试。速度太快了。哇，还是很快。六个扩展。你认为这个数字会是多少？更接近 6 还是更接近 57？比 6 好？比 6 差？好吧，让我们想想。</p>
        <p>And we might as well start by getting our baseline branch and bound without anything. And for that one,
            maybe, we’ll speed it up a little bit. So that gives us 57 extensions. It’s an easier problem. So let’s try
            it with the admissible heuristic. That went too fast. Wow, still pretty fast. Six extensions. What do you
            think this number’s going to be? Closer to six or closer to 57? Better than six? Worse than six? Well, let’s
            think.</p>
        <p>我们要做的是不再重复通过同一节点的任何移动。但这不会对我们做非常重要的事情。它不会让我们远离左侧，因为它不知道到目标的剩余飞行距离。所以让我们看看这是否属实。确实如此。看看这个。</p>
        <p>What we’re going to do is we’re going to just not repeat any movements through the same node again. But it’s
            not going to do something very important for us. It’s not going to keep us out of the left side because it
            has no idea of the remaining airline distance to the goal. So let’s see if that’s true It sure is. Look at
            that.</p>
        <p>它愚蠢地花费大量时间做我们永远不会做的事情。即查看左侧。因此，这次扩展次数为
            35。因此，在第二种情况下，可接受的启发式方法效果要好得多。在第一种情况下，扩展方法效果要好得多。但等一下，我们难道不想同时使用两者吗？我们不想只使用其中一种，对吧？</p>
        <p>It is, foolishly, spending a lot of its’ time doing something we would never do. Namely, looking over there
            on the left side. So this time, the number of extensions is 35. So in case two, the admissible heuristic
            does very much better. In case one, the extension thing does much better. But wait a minute, would we ever
            not want to use both at the same time? We wouldn’t want to use just one of these, right?</p>
        <p>它们都有可能给我们带来很多好处。所以如果我们把它们结合起来，也许会得到更好的结果。当我们这样做的时候。看这里，扩展列表是分支定界法之上的一层。可采纳启发式是分支定界法之上的另一层。如果我们把它们结合起来，我们就会得到一个叫做
            A 星的东西。</p>
        <p>They both have the possibility of doing us a lot of good. So maybe if we put them in harness together, we’ll
            get something that’s even better. And when we do that. see here, the extended list is a layer on top of
            branch and bound. The admissible heuristic is another layer on top of branch and bound. If we put those
            together, we get something called A star.</p>
        <p>因此，A 星只是分支定界法，加上扩展列表，加上可接受的启发式方法。那么让我们回到最初的问题，并尝试使用 A 星。我们以相当慢的速度运行它，因为我们预计它会比原始的分支定界法更有效率。果然如此。扩展的数量是
            27。所以看看这个。比单独工作的两个方法都要好得多。</p>
        <p>So A star is just branch and bound, plus an extended list, plus and admissible heuristic. So let’s go back to
            our original problem and try A star on that. We’re running this at a pretty slow speed because we’re
            expecting it to be a lot more efficient than the original branch and bound. And sure enough it is. The
            number of extensions is 27. So look at that. A lot better than either of those working independently.</p>
        <p>现在我可以把东西放在中间，看看会发生什么。所以在这个特殊情况下，扩展列表实际上并没有帮助我们，因为我们可接受的启发式方法将我们紧紧地引导到目标上，这并不重要。所以这一切都取决于你试图探索的空间的性质。顺便说一句，你知道整个过程是如何运作的，对吧？
        </p>
        <p>Now I can stick the thing in the center and see what happens then. So in this particular case, the extended
            list didn’t, actually, help us because our admissible heuristic was channeling us so tightly toward the goal
            it didn’t matter. So it all depends on the nature of the space that you’re trying to explore. By the way,
            you know how the whole works, right?</p>
        <p>所以你想要做的是扩展第一条路径并进行排序。但不仅仅是按累积距离排序。按累积距离加上可接受的启发式排序。但理论家们在说什么？你肯定在抱怨。排序很昂贵。我们真的需要排序吗？不，我们实际上不需要排序。我们该怎么做？观众：我们只需要跟踪最小值。
        </p>
        <p>So what you want to do is you want to extend the first path and sort. But not just by accumulated distance.
            Sort by accumulated distance plus admissible heuristic. But what are the theoreticians? You must be
            complaining. Sort’s expensive. Do we actually need to sort? No, we don’t actually need to sort. What do we
            to do? AUDIENCE: We just need to keep track of what’s the minimum.</p>
        <p>教授：我们只需要跟踪最小值。所以我们实际上不需要进行排序。这是不必要的计算。因此，我们可以测试最短路径而不是第一条路径。现在您已经知道了。现在您已经知道了 A 星的全部。</p>
        <p>PROFESSOR: We just need to keep track of what’s the minimum. So we don’t need to, actually, do that sort.
            That’s an unnecessary computation. So instead, we can test, not the first path but the shortest path. And
            now you have it. Now you have the whole of A star.</p>
        <p>现在你可以回家了，但我认为你还不应该回家，因为我即将向你展示，这种可采纳性的想法实际上会导致我们在考试中非常喜欢问的某些错误案例。因此，在某些情况下，可采纳启发式可能会给你带来麻烦。看起来不太可能，因为从逻辑上讲，我说的一切都并不奇怪或值得怀疑。但那是因为我一直在用地图工作。
        </p>
        <p>And now you can go home, but I don’t think you should because I’m about to show you that this idea of
            admissibility, actually, leads to certain screw cases that we’re very fond of asking about on exams. So it
            turns out that the admissible heuristic, in certain circumstances, could get you into trouble. It doesn’t
            look like it could because, logically, nothing I’ve said seems strange or questionable. But that’s because
            I’ve been working with a map.</p>
        <p>事实证明，如果你使用地图，那么可接纳性是进行最佳搜索的完美方式。但是，Travis，搜索只是关于地图吗？不，搜索不仅仅是关于地图。所以我们可能会有非欧几里得的安排，这会给我们带来麻烦。所以我想用下面的例子来说明这一点。它不会是一张大地图或一张大图。
        </p>
        <p>And it turns out that if you work with a map then admissibility is a perfectly sound way of doing an optimal
            search. But, Travis, is search just about maps? No, search is not just about maps. So we may have non
            Euclidean arrangements that will cause us trouble. So I’d like to illustrate that with the following
            example. It’s not going to be a large map or a large graph.</p>
        <p>S，然后从这里向上到 A，或者从这里向下到 B。然后它们在 C 处汇合。然后它们从这里出发到达目的地 G。实际距离分别为 1、1.1 和 10。在这里，我们将其设为
            100。因此，这是一种构造奇怪的地图，但它存在是因为我们需要一个病理案例来说明这个想法。现在，这就是实际距离。</p>
        <p>S, then go up here to A or down here to B. Then they merge at C. And then they go out here to the goal, G.
            And the actual distances are 1,1.1, and 10. And over here, we’ll make that 100. So it’s a kind of oddly
            constructed map, but it’s there because we need a pathological case to illustrate the idea. Now that’s the
            actual distances.</p>
        <p>如果我们使用扩展列表进行分支和向下搜索，一切都会顺利进行。但我们没有这样做。我们将使用可接受的启发式方法。我们将说这个人距离目标的估计距离是 100。这个人是 0。这个人是 0。现在，0
            总是低估了距离目标的实际距离，对吧？所以我总是可以自由使用 0。100 可以吗？</p>
        <p>And if we did branch and down with an extended list, everything would work just fine. But we’re not. We’re
            going to use an admissible heuristic. And we’re going to say that this guy has an estimated distance to the
            goal of 100. This guy is 0. And this guy is 0. Now, 0 is always an underestimate of the actual distance to
            the goal, right? So I’m always free to use 0. Is that 100 OK?</p>
        <p>是的，因为实际距离是 101，所以它小于实际距离。所以作为可接受的启发式方法，它是可以接受的。所以我在这里列出的这些数字，一起构成了一组可接受的启发式估计值，可以实现目标。</p>
        <p>Yeah because the actual distances is 101, so it’s less than that the actual distance. So it’s OK as an
            admissible heuristic. So these numbers that I put up here, together, constitute an admissible heuristic set
            of estimates to the goal.</p>
        <h2 id="simulating-a-star">模拟星星</h2>
        <h2>Simulating a star</h2>
        <p>现在，我们来模拟 A 星，看看会发生什么。首先，从 S 开始，然后可以转到 A 或 B。</p>
        <p>So now, let’s just simulate A star and see what happens. So first of all, you start with S, and that can
            either go to A or B.</p>
        <p>实际距离是 1 加上对剩余距离的估计。这样我们就得到了 100 加 100。等于 101。如果我们改为去 B，实际距离是 1 加上启发式距离 0，所以等于
            1。好的，很好。所以现在我们知道我们总是将最短路径延伸到目前的位置。是我搞错了吗，还是你在问问题？</p>
        <p>The actual distance is 1 plus an estimate on the remaining distance. That gives us 100 plus 100. That’s equal
            to 101. If we go to B instead, the actual distance is 1 plus the heuristic’s distance is 0, so that’s equal
            to 1. OK, good. So now we know that we always extend the shortest path so far. Did I goof this, or are you
            asking a question?</p>
        <p>听众：教授：是的，我说的实际距离是指你实际走过的距离。听众：但那是教授：等一下。如果我从 S 走到 A，我实际走过的距离是
            1。听众：我的意思是，地图上是这样的。教授：所以现在我要把实际距离加上估计的距离相加。听众：好的。</p>
        <p>AUDIENCE: PROFESSOR: Yeah, when I say actual, it’s the actual distance that you’ve traveled. AUDIENCE: But
            that’s PROFESSOR: So wait a second. If I go from S to A, the actual distance I’ve traveled is 1. AUDIENCE: I
            meant like, does the map. PROFESSOR: So now I’m taking the sum of the actual distance, plus the estimated
            distance to go. AUDIENCE: All right.</p>
        <p>我只是想知道原始地图是否必须是教授：看，这不是地图。她问地图是否必须在几何上准确。看，这可能是某种非地图的模型。因此，我可以自由地在这些链接上放置任何我想要的数字，包括估算值，只要它们是沿长度方向的距离的低估值。
        </p>
        <p>I’m just wondering if the original map has to be PROFESSOR: See this is not a map. She was asking if the map
            has to be geometrically accurate. See, this could be a model of something that’s not a map. And so, I’m free
            to put any numbers on those links that I want, including estimates, as long as they’re underestimates of the
            distance along the lengths.</p>
        <p>所以这告诉我，到目前为止，我估计的距离是 1。所以我肯定会从这里走到 C。如果我走到 C，那么我的累计距离就是 11。我估计的剩余距离是 0。所以总共是
            11。所以现在我再次遵循我的启发式方法，并说出根据累计距离加上估计距离得出的最短路径是什么？</p>
        <p>So this tells me that my estimated distance here, so far, is 1. So I’ll, surely, go down here to C. And if I
            go to C, then my accumulated distance is 11. And my estimate of the remaining distance is 0. So that’s a
            total of 11. So now I’m following my heuristic again and saying what’s the shortest path on a base of the
            accumulated distance plus the estimated distance?</p>
        <p>这里，累计距离加上估计距离是 101。这里只有 11。所以简单地说，我延长了这条路。这样就到达了目标。现在总累计距离是 111 加 0 等于 111。这不是最短路径，但是等一下。我还得检查一下，对吧？我必须延长
            A。当我延长 A 时，我就到达了 B。</p>
        <p>Here, the accumulated distance plus the estimated distance is 101. Here, it’s only 11. So plainly, I extend
            this guy. And that gets me to the goal. And the total accumulated distance is now 111 plus 0 equals 111. And
            that’s not the shortest path, but wait. I still have to do my checking, right? I have to extend A. I when I
            extend A, I get to B.</p>
        <p>现在，当我以这种方式到达 B 时，我的累计距离是 2 加上我的。哦，抱歉。S、A、C。我的累计距离是 2。我的估计距离是 0，所以等于
            2。所以我没问题，因为我仍然会延伸到这个人，对吗？错了。我已经延伸了那个人。所以我很沮丧。我找不到最短路径，因为我要停在那里。</p>
        <p>And now, when I get to B that way, my accumulated distance is 2 plus my. oh, sorry. S, A, C. My accumulate
            distance it 2. My estimated distance is 0, so that’s equal to 2. So I’m OK because I’m still going to extend
            to this guy, right? Wrong. I’ve already extended that guy. So I’m hosed. I won’t find the shortest path
            because I’m going to stop there.</p>
        <p>我就到此为止了，因为这是一个可接受的启发式方法，除非它是一张地图，否则它还不够好。对于这个特殊情况来说，它不够好，因为这不是几何的。这不能作为平面上的地图来完成。所以在这种情况下，到目前为止，我和你讨论过的方法是使用分支定界法。使用分支定界法加上扩展列表。但当我们添加可接受的启发式方法时，它就不起作用了。
        </p>
        <p>And I’m going to stop there because this is an admissible heuristic and that’s not good enough unless it’s a
            map. It’s not good enough for this particular case because this is not geometric. This cannot be done as a
            map on a plane. So that’s a situation where what I’ve talked to you about, so far, works with branch and
            bound. Works with branch and bound plus an extended list. But doesn’t work when we added an admissible
            heuristic.</p>
        <p>因此，如果我们要普遍地做到这一点，我们需要比可接受性更强大的东西，可接受性只适用于地图。因此，今天讲座的最后几秒钟，我将告诉你们一个巧妙之处，即添加如下改进。到目前为止，我们已经获得了可接受性。</p>
        <p>So if we’re going to do this in general, we need something stronger than admissibility, which works only on
            maps. And so the flourish that I’ll tell you about here in the last few seconds of today’s lecture is to add
            a refinement as follows. So far, we’ve got admissibility.</p>
        <p>如果我们想用某种数学符号来表示这一点，我们可以说，如果任何节点 X 和目标之间的估计距离小于或等于 X
            和目标之间的实际距离，那么它是可接受的。这就是可接受的定义。只要启发式算法能做到这一点，它就是可接受的。如果是地图，星号就是有效的。</p>
        <p>And if we want to write this down in a kind of mathematical notation, we could say that it’s admissible if
            the estimated distance between any node X and the goal is less than or equal to the actual distance between
            X and the goal. That’s the definition of admissible. As long as heuristic does that it’s admissible. And A
            star works if it’s a map.</p>
        <p>但是对于不是地图的那种情况，我们需要一个更强的条件，即一致性。这意味着 X 和目标之间的距离减去目标中其他节点 Y 之间的距离。取其绝对值。这必须小于或等于 X 和 Y
            之间的实际距离。那么这个启发式方法满足一致性条件吗？好吧，让我们看看。</p>
        <p>But for that kind of situation where it’s not a map we need a stronger condition, which is called
            consistency. And what that says is that the distance between X and the goal minus the distance between some
            other node in the goal, Y. Take the absolute value of that. That has to be less than or equal to the actual
            distance between X and Y. So this heuristic satisfy the consistency condition? Well, let’s see.</p>
        <p>这里的猜测是 100。这里是 0。所以绝对差是 100。但实际距离只有
            2。所以它满足可接受性，但不满足一致性。而且它不起作用。所以你几乎可以保证我们会给你一个情况，如果你使用可接受的启发式方法，你会输。而如果你使用一致的启发式方法，你仍然会赢。那么我们如何才能将其重新纳入正轨呢？
        </p>
        <p>Here the guess is 100. Here it’s 0. So the absolute difference is 100. But the actual distance is only 2. So
            it satisfies admissibility, but it doesn’t satisfy consistency. And it doesn’t work. So you can almost be
            guaranteed we’ll give you a situation where if you use an admissible heuristic you’ll lose. And if you use a
            consistent heuristic, you’ll still win. So how can we bring this back into the fold?</p>
        <p>好吧，我们不能使用那个启发式方法。它没有用。但是，如果这个启发式目标估计值为
            2，那么我们就没问题了，因为那时它仍然是可以接受的。但它也是一致的。所以最重要的是，你现在知道了两堂课前你刚开始学习时不知道的东西。你现在知道了 MapQuest 及其所有后代的工作原理。</p>
        <p>Well, we can’t use that heuristic. It’s no good. But if this heuristic estimate of the goal were 2, then we’d
            be OK because then it would still be admissible. But it would also be consistent. So the bottom line is that
            you now know something you didn’t know when you started out two lectures ago. You now know how MapQuest and
            all of its’ descendents work.</p>
        <p>现在，您可以找到一条最佳路径，以及一条启发式的良好路径。您会发现，如果您除了分支定界之外不做任何事情，那么它的成本可能会非常高。您甚至可以发明一些病态的情况，其中它呈指数增长，并且距离目标的距离也呈指数增长。因此，由于它在计算上可能非常糟糕，因此您需要利用所有可能的优势，这通常涉及使用扩展列表。还有。请不要使用笔记本电脑。
        </p>
        <p>Now you can find an optimal path, as well as a heuristically good path. You see that if you don’t do anything
            other than branch and bound it can be extremely expensive. And you can even invent pathological cases where
            it’s exponential and the distance to the goal. So because it can be so computationally horrible, you want to
            use every advantage you can, which, generally, involves using an extended list. As well as. no laptops,
            please.</p>
        <p>它仍然有效。禁止吸烟、禁止饮酒、禁止使用笔记本电脑。所以你要使用你能用到的所有技能。这些技能包括使用扩展列表和可接受或一致的启发式方法，具体取决于具体情况。所以，我想我们就到此为止了，因为我们的时间到了。艾略特，你可以在课后提问。你为什么不现在就上来问呢？
        </p>
        <p>It still holds. No smoking, no drinking, and no laptops. So you’re going to use all the muscles you can. And
            those muscles include using an extended list and an admissible or consistent heuristic, depending on the
            circumstances. And so, I think we’ll conclude there since our time is up. And Elliot, you can ask a question
            after class. Why don’t you come up and ask it now?</p>
        <h1 id="search-games-minimax-and-alpha-beta">6. 搜索：游戏、Minimax 和 Alpha-Beta</h1>
        <h1>6. Search: Games, Minimax, and Alpha-Beta</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 48:17 <a
                href="https://youtube.com/watch?v=STjW3eH0Cik">https://youtube.com/watch?v=STjW3eH0Cik</a></p>
        <p> 11 years ago (Jan 11, 2014) — 48:17 <a
                href="https://youtube.com/watch?v=STjW3eH0Cik">https://youtube.com/watch?v=STjW3eH0Cik</a></p>
        <h2 id="unknown-60">未知</h2>
        <h2>Unknown</h2>
        <p>发言者 1：大约在 1963 年，麻省理工学院的一位著名哲学家，名叫 Hubert Dreyfus。Hubert Dreyfus 在 1963
            年左右写了一篇论文，其中有一个标题是“计算机不能下棋”。当然，他后来被邀请到人工智能实验室与 Greenblatt 的国际象棋机下棋。当然，他输了。</p>
        <p>SPEAKER 1: It was about 1963 when a noted philosopher here at MIT, named Hubert Dreyfus. Hubert Dreyfus wrote
            a paper in about 1963 in which he had a heading titled, “Computers Can’t Play Chess.” Of course, he was
            subsequently invited over to the artificial intelligence laboratory to play the Greenblatt chess machine.
            And, of course, he lost.</p>
        <p>于是西摩·帕维特 (Seymour Pavitt) 写了一篇反驳德雷福斯著名论文的文章，该文章的主题是“德雷福斯也不会下棋”。但奇怪的是，如果德雷福斯说计算机还不能像人类那样下棋，那么他可能是对的，而且是正确的。
        </p>
        <p>Whereupon Seymour Pavitt wrote a rebuttal to Dreyfus’ famous paper, which had a subject heading, “Dreyfus
            Can’t Play Chess Either.” But in a strange sense, Dreyfus might have been right and would have been right if
            he were to have said computers can’t play chess the way humans play chess yet.</p>
        <p>无论如何，大约在 1968
            年，一位名叫大卫·利维的国际象棋大师与著名的人工智能创始人约翰·麦卡锡打赌说，十年内没有计算机能够打败世界冠军。五年后，麦卡锡放弃了，因为已经很明显，没有计算机能够以麦卡锡希望的方式获胜，也就是说，没有计算机能够像人类下棋一样下棋。
        </p>
        <p>In any case, around about 1968 a chess master named David Levy bet noted founder of artificial intelligence
            John McCarthy that no computer would beat the world champion within 10 years. And five years later, McCarthy
            gave up, because it had already become clear that no computer would win in a way that McCarthy wanted it to
            win, that is to say by playing chess the way humans play chess.</p>
        <h2 id="unknown-61">未知</h2>
        <h2>Unknown</h2>
        <p>但 20 年后的 1997 年，深蓝击败了世界冠军，国际象棋突然变得无趣起来。但我们今天要讨论的是游戏，因为游戏中的某些元素确实模拟了我们头脑中发生的事情。即使它们不能模拟我们头脑中发生的事情，它们也能模拟某种智力。
        </p>
        <p>But then 20 years after that in 1997, Deep Blue beat the world champion, and chess suddenly became
            uninteresting. But we’re going to talk about games today, because there are elements of game play that do
            model some of the things that go on in our head. And if they don’t model things that go on in our head, they
            do model some kind of intelligence.</p>
        <p>如果我们要对智能有一个总体的了解，那么我们也必须了解这种智能。因此，我们将首先讨论设计计算机程序来玩国际象棋等游戏的各种方法。最后，我们将讨论深蓝除了极快的速度之外还能为智能带来什么。这就是我们的议程。</p>
        <p>And if we’re to have a general understanding of what intelligence is all about, we have to understand that
            kind of intelligence, too. So, we’ll start out by talking about various ways that we might design a computer
            program to play a game like chess. And we’ll conclude by talking a little bit about what Deep Blue adds to
            the mix other than tremendous speed. So, that’s our agenda.</p>
        <p>到这一小时结束时，如果您愿意，您将能够理解并编写自己的“深蓝”。首先，我们想谈谈计算机如何下棋。让我们讨论几种可能的方法。</p>
        <p>By the end of the hour, you’ll understand and be able to write your own Deep Blue if you feel like it. First,
            we want to talk about how it might be possible for a computer to play chess. Let’s talk about several
            approaches that might be possible.</p>
        <h2 id="unknown-62">未知</h2>
        <h2>Unknown</h2>
        <p>方法一是，机器可能会像人类一样描述棋盘；谈论兵的结构、王的安全、是否是王车易位的好时机，诸如此类的事情。所以，这将是分析，也许是某种策略与某种战术的混合。所有这些都会混合在一起，最终导致某种举动。</p>
        <p>Approach number one is that the machine might make a description of the board the same way a human would;
            talk about pawn structure, King safety, whether it’s a good time to castle, that sort of thing. So, it would
            be analysis and perhaps some strategy mixed up with some tactics. And all that would get mixed up and,
            finally, result in some kind of move.</p>
        <p>如果这是游戏板，下一步要做的事情将由类似的过程决定。问题是没有人知道怎么做。从这个意义上说，德雷福斯是对的。当今的游戏程序都没有包含任何这类内容。既然没有人知道如何做到这一点，我们就不能谈论它。所以我们可以讨论我们可能尝试的其他方法。
        </p>
        <p>If this is the game board, the next thing to do would be determined by some process like that. And the
            trouble is no one knows how to do it. And so in that sense, Dreyfus is right. None the game playing programs
            today incorporate any of that kind of stuff. And since nobody knows how to do that, we can’t talk about it.
            So we can talk about other ways, though, that we might try.</p>
        <p>例如，我们可以有 if then
            规则。这将如何工作？它将按以下方式工作。您查看由此处的节点表示的棋盘，然后说，好吧，如果可以将皇后棋子向前移动一个，那么就这样做。因此，它不会对棋盘进行任何评估。它不会尝试任何事情。它只是说让我看看棋盘并在此基础上选择一个动作。
        </p>
        <p>For example, we can have if then rules. How would that work? That would work this way. You look at the board,
            represented by this node here, and you say, well, if it’s possible to move the Queen pawn forward by one,
            then do that. So, it doesn’t do any of evaluation of the board. It doesn’t try anything. It just says let me
            look at the board and select a move on that basis.</p>
        <h2 id="unknown-63">未知</h2>
        <h2>Unknown</h2>
        <p>所以，这就是处理此类游戏情况的方法。情况如下。以下是可能的走法。然后根据“如果然后”规则选择一个走法，就像这样。没有人能用这种方式制造出非常强大的国际象棋选手。奇怪的是，有人制作了一个相当不错的跳棋程序，它就是这样运行的。
        </p>
        <p>So, that would be a way of approaching a game situation like this. Here’s the situation. Here are the
            possible moves. And one is selected on the basis of an if then rule like so. And nobody can make a very
            strong chess player that works like that. Curiously enough, someone has made a pretty good checkers playing
            program that works like that.</p>
        <p>它会检查棋盘上有哪些可用的走法，对它们进行排序，然后选择可用的最高走法。但总的来说，这不是一个很好的方法。它不是很强大。你做不到。好吧，当我说“做不到”时，意思是我想不出任何方法可以用这种方式制作一个强大的国际象棋程序。所以，第三种方法是展望未来并进行评估。
        </p>
        <p>It checks to see what moves are available on the board, ranks them, and picks the highest one available. But,
            in general, that’s not a very good approach. It’s not very powerful. You couldn’t make it. well, when I say,
            couldn’t, it means I can’t think of any way that you could make a strong chess playing program that way. So,
            the third way to do this is to look ahead and evaluate.</p>
        <p>这意味着你要像这样向前看。你看到了所有可能采取的行动的后果，然后你说，这些棋盘局势中哪一个对我最好？所以，这种方法就是这样，然后说，这三种情况中哪一个最好？要做到这一点，我们必须有某种方法来评估情况，决定其中哪一个是最好的。
        </p>
        <p>What that means is you look ahead like so. You see all the possible consequences of moves, and you say, which
            of these board situations is best for me? So, that would be an approach that comes in here like so and says,
            which one of those three situations is best? And to do that, we have to have some way of evaluating the
            situation deciding which of those is best.</p>
        <h2 id="unknown-64">未知</h2>
        <h2>Unknown</h2>
        <p>现在，我想简单说一下，因为我想谈谈通常用于进行这种评估的机制。最后，棋盘有很多特征。我们把它们称为 f1、f2
            等等。我们可能会形成这些特征的一些函数。总的来说，这被称为静态值。所以，它是静态的，因为你没有探索可能发生的任何后果。</p>
        <p>Now, I want to do a little, brief aside, because I want to talk about the mechanisms that are popularly used
            to do that kind of evaluation. In the end, there are lots of features of the chessboard. Let’s call them f1,
            f2, and so on. And we might form some function of those features. And that, overall, is called the static
            value. So, it’s static because you’re not exploring any consequences of what might happen.</p>
        <p>你只需查看棋盘，检查国王的安全性，检查棋子的结构。这些都会产生一个数字，输入到这个函数中，输出一个值。这就是从你的角度来看棋盘的值。现在，通常情况下，这个函数 g 被简化为一个线性多项式。</p>
        <p>You’re just looking at the board as it is, checking the King’s safety, checking the pawn structure. Each of
            those produces a number fed into this function, out comes a value. And that is a value of the board seen
            from your perspective. Now, normally, this function, g, is reduced to a linear polynomial.</p>
        <p>所以，最后，形成静态值的最流行方法是取 f1，将其乘以某个常数 c1，加上 c2，再将其乘以 f2。这是一个线性得分多项式。因此，我们可以使用该函数从每个事物中生成数字，然后选择最高的数字。这就是玩游戏的一种方式。
        </p>
        <p>So, in the end, the most popular kind of way of forming a static value is to take f1, multiply it times some
            constant, c1, add c2, multiply it times f2. And that is a linear scoring polynomial. So, we could use that
            function to produce numbers from each of these things and then pick the highest number. And that would be a
            way of playing the game.</p>
        <h2 id="unknown-65">未知</h2>
        <h2>Unknown</h2>
        <p>实际上，评分多项式比我们所需的要多一点。因为我们真正需要的是一种查看这三个板并说“我最喜欢这个”的方法。它不必对它们进行排名。它不必给它们编号。它所要做的就是说出它最喜欢哪一个。因此，一种方法是使用线性评分多项式。
        </p>
        <p>Actually, a scoring polynomial is a little bit more than we need. Because all we really need is a method that
            looks at those three boards and says, I like this one best. It doesn’t have to rank them. It doesn’t have to
            give them numbers. All it has to do is say which one it likes best. So, one way of doing that is to use a
            linear scoring polynomial.</p>
        <p>但这不是唯一的方法。所以，这是第二点和第三点。但现在我们还能做什么呢？好吧，如果我们回顾一下我们讨论过的一些搜索，那么其他所有搜索的基本情况是什么呢？这些搜索不需要任何智能，只需要蛮力。</p>
        <p>But it’s not the only way of doing that. So, that’s number two and number three. But now what else might we
            do? Well, if we reflect back on some of the searches we talked about, what’s the base case against which
            everything else is compared much the way of doing search that doesn’t require any intelligence, just brute
            force?</p>
        <p>我们可以使用大英博物馆算法，简单地评估整个可能性树；我移动，你移动，我移动，你移动，一直到……什么？也许是 100，50 步。你做 50 件事。我做 50
            件事。所以，在我们决定这是否是个好主意之前，我们可能应该开发一些词汇。所以，考虑一下这棵移动树。每一级都会考虑一些选择。</p>
        <p>We could use the British Museum algorithm and simply evaluate the entire tree of possibilities; I move, you
            move, I move, you move, all the way down to. what? maybe 100,50 moves. You do 50 things. I do 50 things. So,
            before we can decide if that’s a good idea or not, we probably ought to develop some vocabulary. So,
            consider this tree of moves. There will be some number of choices considered at each level.</p>
        <h2 id="unknown-66">未知</h2>
        <h2>Unknown</h2>
        <p>并且会存在一定数量的层级。因此，我们将其称为分支因子。在这种特殊情况下，b 等于 3。这是树的深度。在这种情况下，d 为 2。因此，现在会产生一定数量的终端节点或叶节点。其中有多少个？嗯，这是一个非常简单的计算。它只是
            b 除以 d。&nbsp;</p>
        <p>And there will be some number of levels. So, the standard language for this as we call this the branching
            factor. And in this particular case, b is equal to 3. This is the depth of the tree. And, in this case, d is
            two. So, now that produces a certain number of terminal or leaf nodes. How many of those are there? Well,
            that’s pretty simple computation. It’s just b to the d.&nbsp;</p>
        <p>对吧，克里斯托弗，b 到 d 的乘积？所以，如果你在这个层次上有 b 到 d 的乘积，你就得到了 1。在这个层次上有 b 到 d 的乘积，你就得到了 b。在这个层次上有 b 到 d 的乘积，你就得到了平方。</p>
        <p>Right, Christopher, b to the d? So, if you have b to the d at this level, you have one. b to the d at this
            level, you have b. b to the d at this level, you have squared.</p>
        <p>因此，在这个特定情况下，b 到 d 的乘积是 9。所以，现在我们可以使用我们开发的词汇来讨论是否合理地只执行大英博物馆算法，然后就此结束，忘掉国际象棋，回家。好吧，让我们看看。这里面有相当深奥的东西。</p>
        <p>So, b to the d, in this particular case, is 9. So, now we can use this vocabulary that we’ve developed to
            talk about whether it’s reasonable to just do the British Museum algorithm, be done with it, forget about
            chess, and go home. Well, let’s see. It’s pretty deep down there.</p>
        <h2 id="unknown-67">未知</h2>
        <h2>Unknown</h2>
        <p>如果我们以国际象棋为例，我们考虑一个标准的游戏，每个人做 50 件事，那么广告大约有 100 个。如果你考虑国际象棋中的分支因子，它通常被认为是，根据游戏的阶段等等，它会有所不同，但平均可能在 14 或 15
            左右。如果它只是 10，那就是 10 的 100 次方。</p>
        <p>If we think about chess, and we think about a standard game which each person does 50 things, that gives a d
            about 100. And if you think about the branching factor in chess, it’s generally presumed to be, depending on
            the stage of the game and so on and so forth, it varies, but it might average around 14 or 15. If it were
            just 10, that would be 10 to the 100th.</p>
        <p>但实际上比这稍微多一点，因为分支因子大于 10。所以，最终，根据克劳德·香农的说法，那里大约有 10 的 120 次方个叶节点。</p>
        <p>But it’s a little more than that, because the branching factor is more than 10. So, in the end, it looks
            like, according to Claude Shannon, there are about 10 to the 120th leaf nodes down there.</p>
        <p>如果你要去大英博物馆看看这棵树，你就必须在底部进行 10 的 120
            次静态评估，才能知道顶部哪个步骤最好。这个数字合理吗？以前这似乎不可行。以前这似乎是不可能的。但现在我们有了云计算等一切。也许我们真的可以做到这一点，对吧？</p>
        <p>And if you’re going to go to a British Museum treatment of this tree, you’d have to do 10 to the 120th static
            evaluations down there at the bottom if you’re going to see which one of the moves is best at the top. Is
            that a reasonable number? It didn’t used to seem practicable. It used to seem impossible. But now we’ve got
            cloud computing and everything. And maybe we could actually do that, right?</p>
        <h2 id="unknown-68">未知</h2>
        <h2>Unknown</h2>
        <p>瓦妮莎，你觉得怎么样？你能做到吗？让足够多的计算机运行在云端？不能？你不确定？我们应该算出来吗？我们算出来吧。我需要一些帮助，尤其是你们中任何一位正在研究宇宙学的人。那么，我们先从宇宙中有多少个原子开始吧？志愿者？10
            的 38 次方。说话人 2。10 的 38 次方？说话人 1：不，不，10 的 38 次方已经有人答应了。</p>
        <p>What do you think, Vanessa, can you do that, get enough computers going in the cloud? No? You’re not sure?
            Should we work it out? Let’s work it out. I’ll need some help, especially from any of you who are studying
            cosmology. So, we’ll start with how many atoms are there in the universe? Volunteers? 10 to the. SPEAKER
            2.10 to the 38th? SPEAKER 1: No, no, 10 to the 38th has been offered.</p>
        <p>这就是它太低的原因。上次我查看时，它大约是宇宙中的 10 的 80 次方个原子。接下来我想知道的是一年有多少秒？这是一个我记住的好数字。这个数字大约是 π 乘以 10 的 7 次方。那么，一秒有多少纳秒？这给了我们
            10 的 9 次方。</p>
        <p>That’s why it’s way too low. The last time I looked, it was about 10 to the 80th atoms in the universe. The
            next thing I’d like to know is how many seconds are there in a year? It’s a good number have memorized. That
            number is approximately pi times 10 to the seventh. So, how many nanoseconds in a second? That gives us 10
            to the ninth.</p>
        <p>最后，宇宙的历史有多少年？31.47 亿。 1 号发言者：她给出的答案是 100 亿，也许是 140 亿。但为了计算方便，我们就说 100 亿吧。也就是 10 的 10 次方年。如果我们把 80、90 加起来，再加上
            16，那就是宇宙历史的 10 的 106 次方纳秒。把它乘以宇宙中原子的数量。</p>
        <p>At last, how many years are there in the history of the universe? SPEAKER 3.14.7 billion. SPEAKER 1: She
            offers something on the order of 10 billion, maybe 14 billion. But we’ll say 10 billion to make our
            calculation simple. That’s 10 to the 10th years. If we will add that up, 80,90, plus 16, that’s 10 to the
            106th nanoseconds in the history of the universe. Multiply it times the number of atoms in the universe.</p>
        <h2 id="unknown-69">未知</h2>
        <h2>Unknown</h2>
        <p>因此，如果宇宙中的所有原子自大爆炸开始以来都以纳秒的速度进行静态评估，我们仍然会缺少 14 个数量级。所以，这将是一个相当不错的云。它必须将许多宇宙整合在一起。所以，大英博物馆的算法不会奏效。不好。</p>
        <p>So, if all of the atoms in the universe were doing static evaluations at nanosecond speeds since the
            beginning of the Big Bang, we’d still be 14 orders of magnitudes short. So, it’d be a pretty good cloud. It
            would have to harness together lots of universes. So, the British Museum algorithm is not going to work. No
            good.</p>
        <p>所以，我们要做的就是把一些事情整合起来，并希望取得最好的结果。所以，第五种方法就是我们实际要采用的方法。我们要做的就是展望未来，不只是展望一个层面，而是尽可能地展望远方。</p>
        <p>So, what we’re going to have to do is we’re going to have to put some things together and hope for the best.
            So, the fifth way is the way we’re actually going to do it. And what we’re going to do is we’re going to
            look ahead, not just one level, but as far as possible.</p>
        <p>我们不仅要考虑这里的情况，还要尽可能地将其推广开来，查看下面叶节点的静态值，并以某种方式将其用作玩游戏的方式。所以，这是第五个。第四个就是一直往下走。最终，这就是我们所能做的一切。</p>
        <p>We consider, not only the situation that we’ve developed here, but we’ll try to push that out as far as we
            can and look at these static values of the leaf nodes down here and somehow use that as a way of playing the
            game. So, that is number five. And number four is going all the way down there. And this, in the end, is all
            that we can do.</p>
        <h2 id="unknown-70">未知</h2>
        <h2>Unknown</h2>
        <p>这个想法是由克劳德·香农和艾伦·图灵共同发明的，我从一位朋友那里得知，他们经常在午餐时间聊天，讨论计算机如何在未来下棋。所以，唐纳德、米奇和艾伦·图灵也在午餐时间发明了这个，当时他们正在休息，没有时间破解德国密码。那么，方法是什么呢？
        </p>
        <p>This idea is multiply invented most notably by Claude Shannon and also by Alan Turing, who, I found out from
            a friend of mine, spent a lot a lunch time conversations talking with each other about how a computer might
            play chess against the future when there would be computers. So, Donald, Mickey and Alan Turing also
            invented this over lunch while they were taking some time off from cracking the German codes. Well, what is
            the method?</p>
        <p>我想用最简单的树来说明这个方法。因此，我们将分支因子设为 2，而不是 14。我们将深度设为 2，这不是什么大问题。这是游戏树。底部会有一些数字。这些数字是从顶部玩家的角度来看棋盘的价值。</p>
        <p>I want to illustrate the method with the simplest possible tree. So, we’re going to have a branching factor
            of 2 not 14. And we’re going to have a depth of 2 not something highly serious. Here’s the game tree. And
            there are going to be some numbers down here at the bottom. And these are going to be the value of the board
            from the perspective of the player at the top.</p>
        <p>假设最上面的玩家希望尽可能地将游戏推向大数字。所以我们将称该玩家为最大化玩家。他希望达到 8，因为这是最大的数字。还有另一个玩家，他的对手，我们将其称为最小化玩家。他希望游戏能够尽可能地达到最小的棋盘局面。</p>
        <p>Let us say that the player at the top would like to drive the play as much as possible toward the big
            numbers. So, we’re going to call that player the maximizing player. He would like to get over here to the 8,
            because that’s the biggest number. There’s another player, his opponent, which we’ll call the minimizing
            player. And he’s hoping that the play will go down to the board situation that’s as small as possible.</p>
        <h2 id="unknown-71">未知</h2>
        <h2>Unknown</h2>
        <p>因为他的观点与追求最大化的玩家相反，因此得名极小极大。但它是如何运作的呢？你知道比赛会朝哪个方向发展吗？你如何决定比赛会朝哪个方向发展？嗯，乍一看并不明显。你知道比赛会朝哪个方向发展吗？乍一看并不明显。</p>
        <p>Because his view is the opposite of the maximizing player, hence the name minimax. But how does it work? Do
            you see which way the play is going to go? How do you decide which way the play is going to go? Well, it’s
            not obvious at a glance. Do you see which way it’s going to go? It’s not obvious to the glance.</p>
        <p>但如果我们不仅仅只是看一下，如果我们从中间层次的最小化玩家的角度来看待这种情况，那么很明显，如果最小化玩家发现自己处于这种情况，他会选择那样做。</p>
        <p>But if we do more than a glance, if we look at the situation from the perspective of the minimizing player
            here at the middle level, it’s pretty clear that if the minimizing player finds himself in that situation,
            he’s going to choose to go that way.</p>
        <p>因此，从最小化玩家的角度来看，这种情况的价值是 2。他永远不会走向 7。同样，如果最小化玩家在这里选择走向 1 还是 8，他显然会走向 1。因此，从最小化玩家的角度来看，该棋盘情况的价值是
            1。现在，我们将分数放在树的底部，并将它们向上移动一级。</p>
        <p>And so the value of this situation, from the perspective of the minimizing player, is 2. He’d never go over
            there to the 7. Similarly, if the minimizing player is over here with a choice between going toward a 1 or
            toward an 8, he’ll obviously go toward a 1. And so the value of that board situation, from the perspective
            of the minimizing player, is 1. Now, we’ve taken the scores down here at the bottom of the tree, and we back
            them up one level.</p>
        <h2 id="unknown-72">未知</h2>
        <h2>Unknown</h2>
        <p>你看我们怎么能一直这样做？现在最大化玩家可以看到，如果他向左走，他的分数是 2。如果他向右走，他的分数只有 1。所以，他会向左走。所以，总的来说，最大化玩家将得到 2 作为顶部情况的感知值。这就是极小极大算法。</p>
        <p>And you see how we can just keep doing this? Now the maximizing player can see that if he goes to the left,
            he gets a score of 2. If he goes to the right, he only gets a score of 1. So, he’s going to go to the left.
            So, overall, then, the maximizing player is going to have a 2 as the perceived value of that situation there
            at the top. That’s the minimax algorithm.</p>
        <p>这很简单。你一直走到树的底部，计算静态值，逐级备份它们，然后决定去哪里。在这种特殊情况下，最大化者会向左移动。最小化者也会向左移动，所以游戏就在这里结束，远远低于最大化者想要的 8，也低于最小化者想要的
            1。但这是一场对抗游戏。</p>
        <p>It’s very simple. You go down to the bottom of the tree, you compute static values, you back them up level by
            level, and then you decide where to go. And in this particular situation, the maximizer goes to the left.
            And the minimizer goes to the left, too, so the play ends up here, far short of the 8 that the maximizer
            wanted and less than the 1 that the minimizer wanted. But this is an adversarial game.</p>
        <p>你们在相互竞争。所以，你不会指望得到你想要的东西，对吧？所以，也许我们应该看看我们能否做到这一点。这是一棵博弈树。你知道它是怎么回事吗？让我们看看系统能否弄清楚。它就这样爬过树。这是一个分支因子为 2
            的博弈树，就像我们的样本一样，但现在有四个级别。</p>
        <p>You’re competing with each other. So, you don’t expect to get what you want, right? So, maybe we ought to see
            if we can make that work. There’s a game tree. Do you see how it goes? Let’s see if the system can figure it
            out. There it goes, crawling its way through the tree. This is a branching factor of 2, just like our
            sample, but now four levels.</p>
        <h2 id="unknown-73">未知</h2>
        <h2>Unknown</h2>
        <p>你可以看到它有很多工作要做。也就是 2 的 4 次方、1、2、3、4、2 的 4 次方，总共 16
            个静态求值。所以，它找到了答案。但这需要做很多工作。我们可以得到一棵新树并重新启动它，也许可以加快速度。这样做可以得到一棵新树。这些只是随机数。</p>
        <p>You can see that it’s got quite a lot of work to do. That’s 2 to the fourth, one, two, three, four, 2 to the
            fourth, 16 static evaluations to do. So, it found the answer. But it’s a lot of work. We could get a new
            tree and restart it, maybe speed it up. There is goes down that way, get a new tree. Those are just random
            numbers.</p>
        <p>因此，每次它都会根据生成的数字在树中找到不同的路径。现在，16 还不错。但是如果你深入到 10 层左右，分支因子是 14，那么我们知道这些数字会变得非常糟糕，因为在底层要进行的静态评估数量会以 b 到 d
            的倍数增加。它是指数级的。</p>
        <p>So, each time it’s going to find a different path through the tree according to the numbers that it’s
            generated. Now, 16 isn’t bad. But if you get down there around 10 levels deep and your branching factor is
            14, well, we know those numbers get pretty awful pretty bad, because the number of static evaluations to do
            down there at the bottom goes as b to the d.&nbsp;It’s exponential.</p>
        <p>时间已经证明，如果你下降了七八级，你就是个混蛋。如果你下降了 15 或 16 级，你就打败了世界冠军。所以，你会想尽可能地在树上走得更远。因为当你尽可能地深入树中时，这些粗糙的质量衡量标准开始变得清晰起来。</p>
        <p>And time has shown, if you get down about seven or eight levels, you’re a jerk. And if you get down about 15
            or 16 levels, you beat the world champion. So, you’d like to get as far down in the tree as possible.
            Because when you get as far down into the tree as possible, what happens is as these that these crude
            measures of bored quality begin to clarify.</p>
        <h2 id="unknown-74">未知</h2>
        <h2>Unknown</h2>
        <p>事实上，当你走得足够远时，唯一真正重要的事情就是零件数量，这是其中一个功能。如果你走得足够远，零件数量和其他一些事情会让你很好地了解如果走得足够远该怎么做。但走得足够远可能会成为一个问题。所以，我们希望尽一切努力尽可能走得更远。
        </p>
        <p>And, in fact, when you get far enough, the only thing that really counts is piece count, one of those
            features. If you get far enough, piece count and a few other things will give you a pretty good idea of what
            to do if you get far enough. But getting far enough can be a problem. So, we want to do everything we can to
            get as far as possible.</p>
        <p>我们希望使出浑身解数，尽可能走得更远。现在，您还记得，当我们谈到分支时，我们知道有些事情可以切断整个搜索树。</p>
        <p>We want to pull out every trick we can find to get as far as possible. Now, you remember when we talked about
            branching down, we knew that there were some things that we could do that would cut off whole portions of
            the search tree.</p>
        <p>因此，我们想要做的是找到与游戏世界类似的东西，所以我们切断了整个搜索树，这样我们就不必查看那些静态值了。我想要做的是，我想回来重做这件事。但这次，我要一次计算一个静态值。我在树中得到了相同的结构。</p>
        <p>So, what we’d like to do is find something analogous to this world of games, so we cut off whole portions of
            this search tree, so we don’t have to look at those static values. What I want to do is I want to come back
            and redo this thing. But this time, I’m going to compute the static values one at a time. I’ve got the same
            structure in the tree.</p>
        <h2 id="unknown-75">未知</h2>
        <h2>Unknown</h2>
        <p>和之前一样，我假设最上面的玩家想要达到最大值，而下一个玩家想要达到最小值。但是还没有计算出任何静态值。所以我最好开始计算它们。</p>
        <p>And just as before, I’m going to assume that the top player wants to go toward the maximum values, and the
            next player wants to go toward the minimum values. But none of the static values have been computed yet. So,
            I better start computing them.</p>
        <p>这是我找到的第一个值，2。现在，只要我看到 2，只要最小化器看到 2，最小化器就知道这个节点的值不能大于 2。因为如果这个分支产生更大的数字，他总是会选择沿着这条路走下去。</p>
        <p>That’s the first one I find, 2. Now, as soon as I see that 2, as soon as the minimizer sees that 2, the
            minimizer knows that the value of this node can’t be any greater than 2. Because he’ll always choose to go
            down this way if this branch produces a bigger number.</p>
        <p>因此，我们可以说，最小化器已经确保那里的分数等于或小于 2。现在，我们继续计算下一个数字。</p>
        <p>So, we can say that the minimizer is assured already that the score there will be equal to or less than 2.
            Now, we go over and compute the next number.</p>
        <h2 id="unknown-76">未知</h2>
        <h2>Unknown</h2>
        <p>有 7。现在，我知道它正好等于 2，因为他永远不会往下到 7。一旦最小化器说等于 2，最大化器就会说，好的，我可以等于或大于 2。首先，最小化器说等于或小于 1。现在怎么办？你准备好那 2 个数字了吗？</p>
        <p>There’s a 7. Now, I know this is exactly equal to 2, because he’ll never go down toward a 7. As soon as the
            minimizer says equal to 2, the maximizer says, OK, I can do equal to or greater than 2. One, minimizer says
            equal to or less than 1. Now what? Did you prepare those 2 numbers?</p>
        <p>最大化者知道如果他往下走，他不可能做得比 1 更好。他已经知道如果他往上走，他会得到 2。就好像这个分支根本不存在一样。因为最大化者永远不会选择往下走。所以，你必须明白这一点。</p>
        <p>The maximizer knows that if he goes down here, he can’t do better than 1. He already knows if he goes over
            here, he an get a 2. It’s as if this branch doesn’t even exist. Because the maximizer would never choose to
            go down there. So, you have to see that.</p>
        <p>这是 alpha beta 算法概念的重要本质，它是 minimax 之上的一层，可以切断搜索树的大部分。所以，再说一遍。</p>
        <p>This is the important essence of the notion the alpha beta algorithm, which is a layering on top of minimax
            that cuts off large sections of the search tree. So, one more time.</p>
        <h2 id="unknown-77">未知</h2>
        <h2>Unknown</h2>
        <p>我们已经开发出一种情况，因此我们知道最大化者向左走会得到 2，并且他看到如果向右走，他不可能做得比 1 更好。所以，他对自己说，就好像那个分支不存在，总分是 2。而且那个静态值是什么并不重要。它可以是
            8，就像以前一样，也可以是加上 1,000。这并不重要。</p>
        <p>We’ve developed a situation so we know that the maximizer gets a 2 going down to the left, and he sees that
            if he goes down to the right, he can’t do better than 1. So, he says to himself, it’s as if that branch
            doesn’t exist and the overall score is 2. And it doesn’t matter what that static value is. It can be 8, as
            it was, it can be plus 1,000. It doesn’t matter.</p>
        <p>它可以是负 1,000。也可以是正无穷或负无穷。这无关紧要，因为最大化器总是会选择另一个方向。这就是 alpha beta 算法。你能猜出它为什么被称为 alpha beta 算法吗？因为算法中有两个参数，alpha
            和 beta。因此，重要的是要理解 alpha beta 不是 minimax 的替代品。它是 minimax 的升级版。</p>
        <p>It can be minus 1,000. Or it could be plus infinity or minus infinity. It doesn’t matter, because the
            maximizer will always go the other way. So, that’s the alpha beta algorithm. Can you guess why it’s called
            the alpha beta algorithm? Well, because in the algorithm there are two parameters, alpha and beta. So, it’s
            important to understand that alpha beta is not an alternative to minimax. It’s minimax with a flourish.</p>
        <p>它是某种分层的东西，就像我们在分支定界法之上分层东西以使其更有效率一样。我们在极小极大法之上分层东西以使其更有效率。正如你对我说的，这是一个非常简单的例子。确实如此。所以，让我们尝试一个稍微复杂一点的例子。这只是为了看看我是否能在不搞砸的情况下做到这一点。
        </p>
        <p>It’s something layered on top like we layered things on top of branch and bound to make it more efficient. We
            layer stuff on top of minimax to make it more efficient. As you say to me, well, that’s a pretty easy
            example. And it is. So, let’s try a little bit more complex one. This is just to see if I can do it without
            screwing up.</p>
        <h2 id="unknown-78">未知</h2>
        <h2>Unknown</h2>
        <p>我之所以做这么复杂的示例，不仅仅是为了在众多观众面前展示我的实力。而是因为某些兴趣点只会出现在深度为 4
            或更大的树中。这就是我举这个例子的原因。但请和我一起努力，看看我们能否解决这个问题。我要做的是圈出我们实际上必须计算的数字。</p>
        <p>The reason I do one that’s complex is not just to show how tough I am in front of a large audience. But,
            rather, there’s certain points of interest that only occur in a tree of depth four or greater. That’s the
            reason for this example. But work with me and let’s see if we can work our way through it. What I’m going to
            do is I’ll circle the numbers that we actually have to compute.</p>
        <p>因此，我们实际上必须计算 8。一旦我们这样做，最小化器就知道该节点的得分将等于或小于 8，而无需查看其他任何内容。然后，他查看 7。因此，它等于 7。因为最小化器显然会向右移动。</p>
        <p>So, we actually have to compute 8. As soon as we do that, the minimizer knows that node is going to have a
            score of equal to or less than 8 without looking at anything else. Then, he looks at 7. So, that’s equal to
            7. Because the minimizer will clearly go to the right.</p>
        <p>一旦确定了这一点，最大化器就知道这里的分数等于或大于 8。现在，我们评估 3。最小化器知道等于或小于 3。发言人 4：发言人 1：哦，对不起，最小化器在
            7，是的。好的，现在发生了什么？好吧，让我们看看，最大化器以这种方式得到 7。</p>
        <p>As soon as that is determined, then the maximizer knows that the score here is equal to or greater than 8.
            Now, we evaluate the 3. The minimizer knows equal to or less than 3. SPEAKER 4: SPEAKER 1: Oh, sorry, the
            minimizer at 7, yeah. OK, now what happens? Well, let’s see, the maximizer gets a 7 going that way.</p>
        <h2 id="unknown-79">未知</h2>
        <h2>Unknown</h2>
        <p>这样一来，他得到的值不可能比 3 更好，所以我们又得到了一个这样的截止情况。就好像这个分支根本不存在一样。所以，不需要进行这种静态评估。现在我们知道这不仅仅是等于或大于 7，而是恰好等于
            7。我们可以把这个数字推回去。这就等于或小于 7。好的，到目前为止你明白了吗？</p>
        <p>He can’t do better than 3 going that way, so we got another one of these cut off situations. It’s as if this
            branch doesn’t even exist. So, this static evaluation need not be made. And now we know that’s not merely
            equal to or greater than 7, but exactly equal to 7. And we can push that number back up. That becomes equal
            to or less than 7. OK, are you with me so far?</p>
        <p>让我们尽快到达树的另一边。所以，有一个 9，等于或小于 9，8 等于 8，将 8 向上推，等于或大于 8。最小化者可以这样下去得到 7。他绝对不会走最大化者可以得到 8 的路。再一次，我们得到了截止点。</p>
        <p>Let’s get over to the other side of the tree as quickly as possible. So, there’s a 9, equal to or less than
            9,8 equal to 8, push the 8 up equal or greater than 8. The minimizer can go down this way and get a 7. He’ll
            certainly never go that way where the maximizer can get an 8. Once again, we’ve got a cut off.</p>
        <p>如果这个分支不存在，那么就意味着这些静态评估不必进行。这个值现在正好是 7。但这里还有一点需要注意。那就是我们不仅不必在这里进行这些静态评估，甚至不必生成这些动作。因此，我们在静态评估和动作生成方面都节省了两种方法。
        </p>
        <p>And if this branch didn’t exist, then that means that these static evaluations don’t have to be made. And
            this value is now exactly 7. But there’s one more thing to note here. And that is that not only do we not
            have to make these static evaluations down here, but we don’t even have to generate these moves. So, we save
            two ways, both on static evaluation and on move generation.</p>
        <h2 id="unknown-80">未知</h2>
        <h2>Unknown</h2>
        <p>这是真正的赢家，这个 alpha beta 东西，因为它节省了大量的计算。好吧，我们现在正在路上。这里的最大化器保证等于或大于
            7。有人找到获胜的媒体动作了吗？是在左边吗？我知道我们最好继续下去，因为我们想要信任任何预言。所以，让我们看看。有一个 1。我们已经计算过了。</p>
        <p>This is a real winner, this alpha beta thing, because it saves as enormous amount of computation. Well, we’re
            on the way now. The maximizer up here is guaranteed equal to or greater than 7. Has anyone found the winning
            media move yet? Is it to the left? I know that we better keep going, because we want to trust any oracles.
            So, let’s see. There’s a 1. We’ve calculated that.</p>
        <p>可以保证最小化器在特定点等于或小于 1。想一想。在顶部，最大化器知道他可以向左走并得到 7。如果比赛进行到这里，最小化器可以确保他将把情况推向棋盘数字
            1。那么，问题是最大化器是否会允许这种情况发生？答案肯定是不允许的。</p>
        <p>The minimizer can be guaranteed equal to or less than 1 at that particular point. Think about that for a
            while. At the top, the maximizer knows he can go left and get a 7. the minimizer, if the play ever gets
            here, can ensure that he’s going to drive the situation to a board number that’s 1. So, the question is will
            the maximizer ever permit that to happen? And the answer is surely not.</p>
        <p>因此，在树的这一侧的开发过程中，我们总是比较树中相邻级别的数字。但在这种情况下，我们比较树中彼此分开的数字。我们仍然得出结论，进一步检查这个节点毫无意义。这称为深度切断。</p>
        <p>So, over here in the development of this side of the tree, we’re always comparing numbers at adjacent levels
            in the tree. But here’s a situation where we’re comparing numbers that are separated from each other in the
            tree. And we still concluded that no further examination of this node makes any sense at all. This is called
            deep cut off.</p>
        <h2 id="unknown-81">未知</h2>
        <h2>Unknown</h2>
        <p>这意味着这里的整个分支可能都不存在，我们不必计算该静态值。好吗？所以，看起来。你有一种难以置信的表情，这完全正常。每次我都必须说服自己这确实有效。但是当你仔细思考时，很明显，我已经计算出的这些计算是不需要进行的。
        </p>
        <p>And that means that this whole branch here might as well not exist, and we won’t have to compute that static
            value. All right? So, it looks. you have this stare of disbelief, which is perfectly normal. I have to
            reconvince myself every time that this actually works. But when you think your way through it, it is clear
            that these computations that I’ve x ed out don’t have to be made.</p>
        <p>那么，让我们继续，看看我们是否能完成这个等于或小于 8，等于 8，等于 8。因为另一个分支根本不存在。等于或小于 8。我们比较这两个数字，我们继续吗？是的，我们继续。因为也许最大化器可以向右移动并真正得到那个
            8。所以，我们必须从这里继续努力。</p>
        <p>So, let’s carry on and see if we can complete this equal to or less than 8, equal to 8, equal to 8. because
            the other branch doesn’t even exist. equal to or less than 8. And we compare these two numbers, do we keep
            going? Yes, we keep going. Because maybe the maximizer can go to the right and actually get to that 8. So,
            we have to go over here and keep working away.</p>
        <p>有一个 9，等于或小于 9，另一个 9 等于 9。将该数字向上推至等于或大于 9。最小化者以这种方式获得 8。最大化者确保以这种方式获得
            9。因此，我们再次遇到了截止情况。就好像这不存在一样。不会进行那些静态评估。不会进行此移动生成，从而节省了计算。</p>
        <p>There’s a nine, equal to or less than 9, another 9 equal to 9. Push that number up equal to or greater than
            9. The minimizer gets an 8 going this way. The maximizer is insured of getting a 9 going that way. So, once
            again, we’ve got a cut off situation. It’s as if this doesn’t exist. Those static evaluations are not made.
            This move generation is not made and computation is saved.</p>
        <h2 id="unknown-82">未知</h2>
        <h2>Unknown</h2>
        <p>那么，让我们看看我们能否使用这个 alpha beta 想法在这个例子中做得更好。我会稍微放慢一点，将搜索类型更改为带有 alpha beta 的
            minimax。我们现在在每个节点上看到两个数字，猜猜它们叫什么。我们已经知道了。它们是 alpha 和 beta。因此，将要发生的是，算法通过树进行，这些数字将收缩并围绕情况进行。</p>
        <p>So, let’s see if we can do better on this very example using this alpha beta idea. I’ll slow it down a little
            bit and change the search type to minimax with alpha beta. We see two numbers on each of those nodes now,
            guess what they’re called. We already know. They’re alpha and beta. So, what’s going to happen is the
            algorithm proceeds through trees that those numbers are going to shrink wrap themselves around the
            situation.</p>
        <p>所以，我们开始吧。没有进行两次静态评估。让我们尝试一棵新树。没有制作两个不同的树。一棵新树，同样，没有制作两个不同的树。让我们看看当我们使用教室示例时会发生什么，我在上面做的那个。让我们确保我没有搞砸它。我会将其放慢到
            1.2，答案相同。所以，你可能一开始没有意识到这一点。谁能呢？</p>
        <p>So, we’ll start that up. Two static evaluations were not made. Let’s try a new tree. Two different ones were
            not made. A new tree, still again, two different ones not made. Let’s see what happens when we use the
            classroom example, the one I did up there. Let’s make sure that I didn’t screw it up. I’ll slow that down to
            1.2, same answer. So, you probably didn’t realize it at the start. Who could?</p>
        <p>事实上，比赛是这样进行的，从这个方向开始，再从那个方向开始，最后到了 8，这不是最大的数字，也不是最小的数字。这是在对抗的情况下得出的折衷数字。所以，你问我，通过这样做，你实际上节省了多少精力、多少工作？</p>
        <p>In fact, the play goes down that way, over this way, down that way, and ultimately to the 8, which is not the
            biggest number. And it’s not the smallest number. It’s the compromised number that’s arrived at virtue of
            the fact that this is an adversarial situation. So, you say to me, how much energy, how much work do you
            actually saved by doing this?</p>
        <h2 id="unknown-83">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，在最佳情况下，如果一切都井然有序，如果上帝降临并以正确的方式安排了你的树，那么你需要做的大致工作量，即执行的静态评估的大致次数，大约等于 2 乘以 b 的 d 除以 2。我们不关心这个 2。我们非常关心那个
            2。这就是完成的工作量。</p>
        <p>Well, it is the case that in the optimal situation, if everything is ordered right, if God has come down and
            arranged your tree in just the right way, then the approximate amount of work you need to do, the
            approximate number of static evaluations performed, is approximately equal to 2 times b to the d over 2. We
            don’t care about this 2. We care a whole lot about that 2. That’s the amount of work that’s done.</p>
        <p>是 b 到 d 除以 2，而不是 b 到 d。这是什么意思？假设没有这个想法，我可以下降 7 个级别。有了这个想法，我能下降多少？14
            个级别。所以，这就是混蛋和世界冠军之间的区别。所以，但这只有在上帝安排得恰到好处的最佳情况下才会发生。</p>
        <p>It’s b to the d over 2, instead of b to d.&nbsp;What’s that mean? Suppose that without this idea, I can go
            down seven levels. How far can I go down with this idea? 14 levels. So, it’s the difference between a jerk
            and a world champion. So, that, however, is only in the optimal case when God has arranged things just
            right.</p>
        <p>但在实际情况下，实际的游戏情况下，实验表明，实际数字接近最佳安排的近似值。所以，你永远不想使用 alpha
            beta。它节省了大量的时间。你可以换个角度看。假设你走同样的层数，你需要做的工作会少多少？嗯，相当多。平方根对吧？</p>
        <p>But in practical situations, practical game situations, it appears to be the case, experimentally, that the
            actual number is close to this approximation for optimal arrangements. So, you’d never not want to use alpha
            beta. It saves an amazing amount of time. You could look at it another way. Suppose you go down the same
            number of levels, how much less work do you have to do? Well, quite a bit. The square root right?</p>
        <h2 id="unknown-84">未知</h2>
        <h2>Unknown</h2>
        <p>这是另一种看待它如何工作的方式。所以，我们现在可以回家了，除了一个问题，那就是我们假装分支因子总是相同的。但事实上，分支因子会随着游戏状态而变化，也会随着游戏而变化。所以，你可以计算出你在两分钟内可以做多少计算，或者你平均有多少时间进行一次移动。
        </p>
        <p>That’s another way of looking at how it works. So, we could go home at this point except for one problem, and
            that is that we pretended that the branching factor is always the same. But, in fact, the branching factor
            will vary with the game state and will vary with the game. So, you can calculate how much computing you can
            do in two minutes, or however much time you have for an average move.</p>
        <p>然后你可能会问，我能走多远？但你无法确定，因为这取决于游戏。所以，在游戏程序的早期，游戏程序会留下大量的计算，因为它会在三秒钟内做出决定。如果它利用所有可用的竞争对手，它可能会做出截然不同的举动。</p>
        <p>And then you could say, how deep can I go? And you won’t know for sure, because it depends on the game. So,
            in the earlier days of game playing programs, the game playing program left a lot of computation on the
            table, because it would make a decision in three seconds. And it might have made a much different move if it
            used all the competition it had available.</p>
        <p>或者，它可能在不停地磨合，两分钟后就被消耗掉了。它没有移动，只是做了一些随机的事情。这不是很好。但这就是早期游戏程序所做的，因为没有人知道它们能走多远。所以，让我们看看这里的情况，然后说，好吧，这是一棵游戏树。它是一棵二叉游戏树。
        </p>
        <p>Alternatively, it might be grinding away, and after two minutes was consumed. It had no move and just did
            something random. That’s not very good. But that’s what the early game playing program’s did, because no one
            knew how deep they could go. So, let’s have a look at the situation here and say, well, here’s a game tree.
            It’s a binary game tree.</p>
        <h2 id="unknown-85">未知</h2>
        <h2>Unknown</h2>
        <p>这是 0 级。这是 1 级。这是 d 级减 1。这是 d 级。所以，下面的情况看起来是这样的。我把中间的所有博弈树都去掉了。那么，下面有多少个叶节点？b 到 d，对吧？哦，我暂时忘掉 alpha beta。</p>
        <p>That’s level 0. That’s level 1. This is level d minus 1. And this is level d.&nbsp;So, down here you have a
            situation that looks like this. And I left all the game tree out in between So, how many leaf nodes are
            there down here? b to the d, right? Oh, I’m going to forget about alpha beta for a moment.</p>
        <p>正如我们在查看某些最优搜索时所做的那样，我们将逐个添加这些内容。因此，忘记 alpha beta，假设我们只是直接进行 minimax。在这种情况下，我们必须计算底部的所有静态值。其中有 b 到 d
            个。下一级有多少个？</p>
        <p>As we did when we looked at some of those optimal searches, we’re going to add these things one at a time.
            So, forget about alpha beta, assume we’re just doing straight minimax. In that case, we would have to
            calculate all the static values down here at the bottom. And there are b to d of those. How many are there
            at this next level up?</p>
        <p>那么，那一定是 b 到 d 减 1。相对于最后一层，倒数第二层（即倒数第二层）的节点数少了多少？嗯，1 除以 b，对吧？因此，如果我担心无法完成 d 层的所有计算，我可以为自己买一份保险，计算出如果我只进行到 d 减
            1 层，答案会是什么。</p>
        <p>Well, that must be b to the d minus 1. How many fewer nodes are there at the second to the last, the
            penultimate level, relative to the final level? Well, 1 over b, right? So, if I’m concerned about not
            getting all the way through these calculations at the d level, I can give myself an insurance policy by
            calculating out what the answer would be if I only went down to the d minus 1th level.</p>
        <h2 id="unknown-86">未知</h2>
        <h2>Unknown</h2>
        <p>你拿到那份保险单了吗？假设分支因子是 10，那份保险单要花多少钱？我竞争对手的 10%。因为我可以进行这项计算，并在 d 级减 1 时掌握一项行动，而这仅是计算量（如果我一路下降到基层，计算出我会做什么）的
            1/10。好，清楚了吗？</p>
        <p>Do you get that insurance policy? Let’s say the branching factor is 10, how much does that insurance policy
            cost me? 10% of my competition. Because I can do this calculation and have a move in hand here at level d
            minus 1 for only 1/10 of the amount of the computation that’s required to figure out what I would do if I go
            all the way down to the base level. OK, is that clear?</p>
        <p>因此，这个想法在一般情况下非常重要。但我们还没有完全做到这一点，因为如果分支因子真的很大，我们也无法通过这一关怎么办？我们应该怎么做才能确保我们仍然有好的举动？说话者 5：说话者 1：好的，我们可以在 b 减 2
            的级别上做到这一点。所以，这将在这里。</p>
        <p>So this idea is extremely important in its general form. But we haven’t quite got there yet, because what if
            the branching factor turns out to be really big and we can’t get through this level either? What should we
            do to make sure that we still have a good move? SPEAKER 5: SPEAKER 1: Right, we can do it at the b minus 2
            level. So, that would be up here.</p>
        <p>而在这个级别，计算量将是 b 到 d 减 2。所以，现在我们加上 10% 再加 10%。我们的膝跳反射开始形成了，对吧？我们最后要做什么才能确保无论发生什么，我们都能有所行动？克里斯托弗：从最开始。说话者
            1：正确，那是什么，克里斯托弗？克里斯托弗：从最开始的级别开始？</p>
        <p>And at that level, the amount of computation would be b to the d minus 2. So, now we’ve added 10% plus 10% of
            that. And our knee jerk is begin to form, right? What are we going to do in the end to make sure that no
            matter what we’ve got a move? CHRISTOPHER: Start from the very first. SPEAKER 1: Correct, what’s that,
            Christopher? CHRISTOPHER: Start from the very first level?</p>
        <h2 id="unknown-87">未知</h2>
        <h2>Unknown</h2>
        <p>发言者 1：从第一个层次开始，为我们试图计算的每个层次都买一份保险。但这可能真的很昂贵。所以，我们最好弄清楚这是否会是一笔难以承受的巨额费用。</p>
        <p>SPEAKER 1: Start from the very first level and give our self an insurance policy for every level we try to
            calculate. But that might be real costly. So, we better figure out if this is going to be too big of an
            expense to bear.</p>
        <p>那么，让我们看看，如果我们按照克里斯托弗的建议去做，那么我们的保险单所需的计算量将等于 1。我们将在这个层面上进行计算，2，即使我们不需要它，只是为了让所有事情都变得简单。1 加
            b，这就是在这个第一层级上得到的保险单。</p>
        <p>So, let’s see, if we do what Christopher suggests, then the amount of computation we need in our insurance
            policy is going to be equal 1. we’re going to do it up here at this level, 2, even though we don’t need it,
            just to make everything work out easy. 1 plus b, that’s getting or insurance policy down here at this first
            level.</p>
        <p>我们将把 b 平方一直加到 b 到 d 减 1。这就是我们在每个级别购买保险所要花费的金额。我希望有一些高中代数知识，对吧？让我们只是为了好玩而做吧。哦，变量名选择不当。bs 等于。哦，我们将所有这些乘以 b。</p>
        <p>And we’re going to add b squared all the way down to b to d minus 1. That’s how much we’re going to spend
            getting an insurance policy at every level. I wished that some of that high school algebra, right? Let’s
            just do it for fun. Oh, unfortunate choice of variable names. bs is equal to. oh, we’re going to multiply
            all those by b.</p>
        <h2 id="unknown-88">未知</h2>
        <h2>Unknown</h2>
        <p>现在，我们用第二个减去第一个，这样我们就知道我们的保险单所需的计算量等于 b 的 d 减 1 除以 b 减 1。这个数字大吗？我们可以对此进行一些代数运算，得出 b 的 d 是一个巨大的数字。所以，减一不算数。</p>
        <p>Now, we’ll subtract the first one from the second one, which tells us that the amount of calculation needed
            for our insurance policy is equal to b to the d minus 1 over b minus 1. Is that a big number? We could do a
            little algebra on that and say that b to the d is a huge number. So, that minus one doesn’t count.</p>
        <p>B 可能在 10 到 15 之间。因此，b 减 1 本质上等于 b。因此，这大约等于 b 减
            d。因此，考虑到近似值，在每一级执行保险单所需的计算量与仅在一个级别（即倒数第二级）获取保险单所需的计算量相差无几。因此，这个想法被称为渐进深化。</p>
        <p>And B is probably 10 to 15. So, b minus 1 is, essentially, equal to b. So, that’s approximately equal b to
            the d minus 1. So, with an approximation factored in, the amount of computation needed to do insurance
            policies at every level is not much different from the amount of computation needed to get an insurance
            policy at just one level, the penultimate one. So, this idea is called progressive deepening.</p>
        <p>现在我们可以查看我们的金星创意列表，看看这些内容如何与之匹配。首先，当我们谈论 alpha beta 时，死马原则就凸显出来了。因为我们知道使用 alpha
            beta，我们可以摆脱大量的树，而不必进行静态评估，甚至不必进行移动生成。这是我们不想打败的死马。</p>
        <p>And now we can visit our gold star idea list and see how these things match up with that. First of all, the
            dead horse principle comes to the fore when we talk about alpha beta. Because we know with alpha beta that
            we can get rid of a whole lot of the tree and not do static evaluation, not even do move generation. That’s
            the dead horse we don’t want to beat.</p>
        <h2 id="unknown-89">未知</h2>
        <h2>Unknown</h2>
        <p>做这种计算毫无意义，因为它无法解释答案。我喜欢用武术原理来思考渐进深化理念的发展，即利用敌人的特点来对付他们。由于这种指数级增长，我们拥有恰到好处的特点，可以在每个级别都使用一种动作，作为防止无法进入下一级别的保险政策。
        </p>
        <p>There’s no point in doing that calculation, because it can’t figure into the answer. The development of the
            progressive deepening idea, I like to think of in terms of the martial arts principle, we’re using the
            enemy’s characteristics against them. Because of this exponential blow up, we have exactly the right
            characteristics to have a move available at every level as an insurance policy against not getting through
            to the next level.</p>
        <p>最后，渐进深化这一整体理念可以看作是我们所说的随时算法的一个典型例子，这种算法总是在需要答案时随时准备好答案。因此，两分钟后，系统就会给出答案。考虑到游戏树目前的发展特点，这将是系统在可用时间内计算出的最佳答案。
        </p>
        <p>And, finally, this whole idea of progressive deepening can be viewed as a prime example of what we like to
            call anytime algorithms that always have an answer ready to go as soon as an answer is demanded. So, as soon
            as that clock runs out at two minutes, some answer is available. It’ll be the best one that the system can
            compute in the time available given the characteristics of the game tree as it’s developed so far.</p>
        <p>所以，还有其他类型的任意时间算法。这是一个例子。这就是所有游戏程序的工作方式，极小极大，加上 alpha beta，加上渐进深化。克里斯托弗，alpha beta 是极小极大的替代品吗？克里斯托弗：不是。说话者
            1：不是。它是你在极小极大之上分层的东西。alpha beta 给你的答案和极小极大不同吗？克里斯托弗：不。不是。说话者 1：让我们看看每个人都会摇头还是不摇头。</p>
        <p>So, there are other kinds of anytime algorithms. This is an example of one. That’s how all game playing
            programs work, minimax, plus alpha beta, plus progressive deepening. Christopher, is alpha beta a
            alternative to minimax? CHRISTOPHER: No.&nbsp;SPEAKER 1: No, it’s not. It’s something you layer on top of
            minimax. Does alpha beta give you a different answer from minimax? CHRISTOPHER: No.&nbsp;No, it doesn’t.
            SPEAKER 1: Let’s see everybody shake their head one way or the other.</p>
        <h2 id="unknown-90">未知</h2>
        <h2>Unknown</h2>
        <p>它不会给你与极小极大不同的答案。没错。它给你完全相同的答案，而不是不同的答案。这是一种加速。这不是近似值。这是一种加速。它砍掉了很多树。这是死马原则在起作用。你有问题吗，克里斯托弗？克里斯托弗：是的，因为所有的线都是渐进的，如果值是，是否可以保留一个临时值？说话者
            1：哦，很好的建议。</p>
        <p>It does not give you an answer different from minimax. That’s right. It gives you exactly the same answer,
            not a different answer. It’s a speed up. It’s not an approximation. It’s a speed up. It cuts off lots of the
            tree. It’s a dead horse principle at work. You got a question, Christopher? CHRISTOPHER: Yeah, since all of
            the lines progressively is it possible to keep a temporary value if the value SPEAKER 1: Oh, excellent
            suggestion.</p>
        <p>事实上，克里斯托弗刚刚……我想，如果我可以向前跳几步的话。克里斯托弗重新发明了一个非常重要的想法。渐进式深化不仅可以确保您随时都有答案，而且当您在其上分层 alpha beta 时，它实际上可以提高 alpha
            beta 的性能。因为这些在树的中间部分计算的值用于重新排序树下的节点，从而为您提供最大的 alpha beta 截止值。</p>
        <p>In fact, Christopher has just. I think, if I can jump ahead a couple steps. Christopher has reinvented a very
            important idea. Progressive deepening not only ensures you have an answer at any time, it actually improves
            the performance of alpha beta when you layer alpha beta on top of it. Because these values that are
            calculated at intermediate parts of the tree are used to reorder the nodes under the tree so as to give you
            maximum alpha beta cut off.</p>
        <p>我想这就是你说的，克里斯托弗。但如果不是，我们下课后再讨论你的想法。所以，这就是每个游戏程序所做的。深蓝有什么不同？没什么不同。所以，截至 1997 年，深蓝每秒进行约 2 亿次静态评估。使用 alpha
            beta，它下降了约 14,15.16 个级别。</p>
        <p>I think that’s what you said, Christopher. But if it isn’t, we’ll talk about your idea after class. So, this
            is what every game playing program does. How is Deep Blue different? Not much. So, Deep Blue, as of 1997,
            did about 200 million static evaluations per second. And it went down, using alpha beta, about 14,15.16
            levels.</p>
        <h2 id="unknown-91">未知</h2>
        <h2>Unknown</h2>
        <p>所以，深蓝是极小极大算法，加上 alpha
            beta，加上渐进深化，加上大量的并行计算，加上一本开局书，加上为游戏结束而准备的特殊东西，加上也许是最重要的东西。不均匀的树发展。到目前为止，我们一直假设树总是以均匀的方式上升到固定的水平。但没有什么特别的理由必须如此。
        </p>
        <p>So, Deep Blue was minimax, plus alpha beta, plus progressive deepening, plus a whole lot of parallel
            computing, plus an opening book, plus special purpose stuff for the end game, plus. perhaps the most
            important thing. uneven tree development. So far, we’ve pretended that the tree always goes up in an even
            way to a fixed level. But there’s no particular reason why that has to be so.</p>
        <p>树底部的一些情况可能特别动态。在下一步中，您可能能够捕获对手的皇后。因此，在这种情况下，您希望进行一些额外的搜索。因此，最终，您会意识到没有特别的理由将搜索降低到固定水平。</p>
        <p>Some situation down at the bottom of the tree may be particularly dynamic. In the very next move, you might
            be able to capture the opponent’s Queen. So, in circumstances like that, you want to blow out a little extra
            search. So, eventually, you get to the idea that there’s no particular reason to have the search go down to
            a fixed level.</p>
        <p>但是，相反，你可以用一种让你最有信心相信你备份的数字是正确的方式开发这棵树。这是深蓝在 1997
            年击败卡斯帕罗夫时添加的这些额外花样中最重要的。现在我们可以回过头来说，好吧，你了解深蓝。但这是我们自己头脑中发生的事情的模型吗？这是任何一种人类智能的模型吗？</p>
        <p>But, instead, you can develop the tree in a way that gives you the most confidence that your backed up
            numbers are correct. That’s the most important of these extra flourishes added by Deep Blue when it beat
            Kasparov in 1997. And now we can come back and say, well, you understand Deep Blue. But is this a model of
            anything that goes on in our own heads? Is this a model of any kind of human intelligence?</p>
        <h2 id="unknown-92">未知</h2>
        <h2>Unknown</h2>
        <p>或者说这是一种不同的智能？答案是混合的，对吧？因为我们经常处于玩游戏的境地。我们在与另一家制造商竞争。我们必须考虑其他制造商会如何应对我们在几个级别上所做的事情。另一方面，下降 14
            个级别是人类棋手在赢得世界冠军时会做的事情吗？</p>
        <p>Or is it a different kind of intelligence? And the answer is mixed, right? Because we are often in situations
            where we are playing a game. We’re competing with another manufacturer. We have to think what the other
            manufacturer will do in response to what we do down several levels. On the other hand, is going down 14
            levels what human chess players do when they win the world championship?</p>
        <p>甚至对他们来说，这似乎都是不可能的。他们必须做些不同的事情，因为他们没有那种计算能力。这就像推土机处理碎石一样进行计算。它用原始力量代替复杂性。因此，当人类象棋大师下棋时，他们的头脑中有大量象棋知识，并且能够识别模式。
        </p>
        <p>It doesn’t seem, even to them, like that’s even a remote possibility. They have to do something different,
            because they don’t have that kind of computational horsepower. This is doing computation in the same way
            that a bulldozer processes gravel. It’s substituting raw power for sophistication. So, when a human chess
            master plays the game, they have a great deal of chess knowledge in their head and they recognize patterns.
        </p>
        <p>顺便说一句，有一些著名的实验以以下方式证明了这一点。向象棋大师展示棋盘并要求他们记住它。只要是合法的棋盘，他们就非常擅长这一点。如果棋子是随机放置的，他们就根本不擅长。</p>
        <p>There are famous experiments, by the way, that demonstrate this in the following way. Show a chessboard to a
            chess master and ask them to memorize it. They’re very good at that, as long as it’s a legitimate
            chessboard. If the pieces are placed randomly, they’re no good at it at all.</p>
        <h2 id="unknown-93">未知</h2>
        <h2>Unknown</h2>
        <p>所以，很明显，他们已经掌握了一套国际象棋知识，这使得他们能够识别情况，并像上面的 1
            号选手一样下棋。所以，深蓝表现出了某种智慧。但这不是我们的智慧。这是推土机智慧。所以，了解这种智慧也很重要。但它不一定是我们头脑中的智慧。</p>
        <p>So, it’s very clear that they’ve developed a repertoire of chess knowledge that makes it possible for them to
            recognize situations and play the game much more like number 1 up there. So, Deep Blue is manifesting some
            kind of intelligence. But it’s not our intelligence. It’s bulldozer intelligence. So, it’s important to
            understand that kind of intelligence, too. But it’s not necessarily the same kind of intelligence that we
            have in our own head.</p>
        <p>今天我们要做的事情到此结束。正如你们所知，周三我们将举行学习庆祝活动，如果你参加 309.1 课程，你一定很熟悉这个活动。因此，我想周三我会和大家见面。</p>
        <p>So, that concludes what we’re going to do today. And, as you know, on Wednesday we have a celebration of
            learning, which is familiar to you if you take a 309.1. And, therefore, I will see you on Wednesday, all of
            you, I imagine.</p>
        <h1 id="constraints-interpreting-line-drawings">7. 约束：解读线条画</h1>
        <h1>7. Constraints: Interpreting Line Drawings</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:13 <a
                href="https://youtube.com/watch?v=l-tzjenXrvI">https://youtube.com/watch?v=l-tzjenXrvI</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:13 <a
                href="https://youtube.com/watch?v=l-tzjenXrvI">https://youtube.com/watch?v=l-tzjenXrvI</a></p>
        <h2 id="introduction-2">介绍</h2>
        <h2>Introduction</h2>
        <p>帕特里克·温斯顿：你们中的许多人，也许是大多数人，在你们的一生中，再也不用手工解决搜索问题了。其他人可能希望在期末考试时再试一次。我因为这种课堂评分方式而受到很多批评。</p>
        <p>PATRICK WINSTON: Many of you, maybe most of you, will never have to work another search problem by hand in
            your natural life. Others will want to take another run at it on the final. I’ve been much criticized for
            this way of doing grading in the class.</p>
        <p>但我认为，学生和教师之间的关系应该比我当学生时的美好时光少一些对抗性。我特别记得我们参加的一次考试。我们所有人都参加了。关于瑞利散射。我想那是 803 级。在过去，我们必须选修四门物理课程，而不仅仅是两门。</p>
        <p>But the way I look at it is that the relationship between students and instructors ought to be less
            adversarial than it used to be in the good old days when I was a student. I especially remember an
            examination we took. all of us took. on Rayleigh scattering. That was 803, I think. And back in the old
            days, we had to take four physics courses, not just two.</p>
        <p>而且我们必须选修四门数学课，而不是两门。在那个年代……好吧，我本来想说男人就是男人。但那时我们大多数人都是男人。我想我们班上只有 20
            名女生。无论如何，我们进行了一次关于瑞利散射的测验。我想，好吧，这很难。然后我拿回了我的测验。26。我想，好吧，我被发现了。</p>
        <p>And we had to take four math courses, not just two. Back in the days when. well, I was going to say men were
            men. But most of us were men in those days. I think we only had 20 women in our class. Anyway, we had a quiz
            on Rayleigh scattering. And I thought, well, this is pretty hard. And then I got my quiz back. 26. I
            thought, well, I’ve been found out.</p>
        <p>我就要被开除。我父亲会强迫我去读法学院。我永远都不会吸引任何人来和我结婚。可怕的事情会发生。然后老师宣布班级平均成绩是 18
            分。我比这个高出两个标准差。两周后他们给我们进行了同样的考试，结果各不相同。有人说班级平均成绩下降了。不管怎样，今天我们要学习一些东西。</p>
        <p>I’m going to flunk out. My father will make me go to law school. I’ll never attract anyone to marry. Horrible
            things will happen. Then the instructor announced the class average was 18. I was two standard deviations
            above that. They gave us the same exam two weeks later, and accounts vary. Some people say that the class
            average went down. Anyway, today we’re going to study some stuff.</p>
        <p>我们将研究一些东西，让你能够理解如何在几秒钟内完成计算，即使重绘会造成延迟。现在这个特定的程序。我不太确定，也没有证据，但我认为要找到美国大陆的合法颜色，需要的时间比宇宙的寿命还要长。</p>
        <p>We’re going to study some stuff that will make it possible for you to understand how you can do that
            computation in just a couple of seconds, even with the delays introduced by the redrawing. Now this
            particular program. I’m not real sure and I don’t have a proof, but I think it will take more than the
            lifetime of the universe to find a legitimate coloring of the continental United States.</p>
        <p>但是在下一节课结束时，您将会看到如何在短短几秒钟内完成这一快速操作，即使重新显示有延迟。</p>
        <p>But by the end of the next class, you’ll see how to do that lickety split in just couple of seconds, even
            with the re display delays.</p>
        <h2 id="two-ways">两种方式</h2>
        <h2>Two Ways</h2>
        <p>当然，现在我们可以用两种方式来做到这一点。一种方法是，我可以先说，让 C 成为约束满足问题，然后直接给出结果。任何人都可以做到这一点。这很棒。你需要通过这种方式学习一些东西。</p>
        <p>Now we could, of course, do this in two ways. One way is I could start off by saying, let C be a constraint
            satisfaction problem and just give you the result. And anybody can do that. That’s great. And you needed to
            learn some stuff that way.</p>
        <p>但我认为有些东西你们需要用不同的方式去学习。而这种不同的方式需要我告诉你们这一切是如何发生的。这是一个全新的领域，因此我认识这个领域的大多数人。我也认识所有从事我今天要告诉你们的工作的人。</p>
        <p>But there are some things that you needed to learn, I think, a different way. And that different way involves
            my telling you the story of how it all came to be. This is a new field, pretty much, and therefore I know
            most of the people in it. And I know all of the people who did the work that I’m going to tell you about
            today.</p>
        <p>我将向你简要讲述一下提出这些想法的过程，这些想法催生出你将在该主题中学习到的最强大的方法之一。</p>
        <p>I’m going to tell you a little bit about the struggle for coming up with the ideas that led to one of the
            most powerful methods you’ll learn about in the subject.</p>
        <h2 id="aldo-guzman">奥尔多·古兹曼</h2>
        <h2>Aldo Guzman</h2>
        <p>这一切最初都是为了尝试让计算机能够看东西。每个人都对自己说，好吧，让我们从看简单的东西开始，比如儿童积木。</p>
        <p>It all has to do, originally, with an attempt to make a computer capable of seeing. And everybody said to
            themselves, well, let’s start with seeing simple things, like children’s blocks.</p>
        <p>因此，马文·明斯基的研究生阿道夫·古兹曼负责了一个暑期项目，该项目最终促成了他的博士论文，即编写一个程序，可以查看线条图并确定线条图中有多少个物体。例如，在我的轮廓后面是一幅线条图。你立刻就会相信那里有两个物体，尽管从某种深层意义上讲，这是模棱两可的。
        </p>
        <p>And so Adolfo Guzman, a graduate student of Marvin Minsky’s, was charged with the summer project, which led
            to his Ph.D.&nbsp;thesis, of writing a program that could look at a line drawing and determine how many
            objects are in the line drawing. So for example, there behind my outline is a line drawing. And you believe
            instantly that there are two objects there, even though in some deep sense, it’s ambiguous.</p>
        <p>有各种各样的方法，你可以通过小技巧，将甚至七个看起来是那样的物体排列起来。但大多数人会说只有两个物体。所以古兹曼开始着手解决如何做到这一点的问题。后来，戴夫·霍夫曼 (Dave Huffman)
            跟进了他的工作。戴夫·华尔兹 (Dave Waltz) 跟进了他的工作。简 (Jane) 跟进了他的工作，但目前还没有列出。</p>
        <p>There are all sorts of ways that you could, through trickery, arrange something with even seven objects that
            look that way. But most people would say there are two objects. So Guzman set about the problem of figuring
            out how to do that. And then his work was followed by Dave Huffman. And his work was followed up by Dave
            Waltz. And his work was followed up by Jane who’s not listed there quite yet.</p>
        <p>我想给你讲一下这一切是怎么发生的。古兹曼拍了很多照片。他去了玩具反斗城的前身波士顿宝贝，根据政府合同购买了一套大型积木，然后拍了很多照片，以便对这个领域有个了解。</p>
        <p>And I want to tell you a little bit of story how that all happened. So Guzman took a lot of pictures. He went
            to Boston Baby, the precursor to Toys R Us, purchased a large block set on a government contract, and went
            about the business of taking a lot of pictures of them so he’d get a feel for the domain.</p>
        <p>最后他决定编写一个程序，利用这些线条作为证据量，判断物体中哪些面属于同一物体，从而确定这里有两个物体。经过长时间的研究，他说，这些图画往往有很多箭头型连接点和叉型连接点。</p>
        <p>And eventually he decided that he could build a program that could determine that there are two objects here
            by using the lines as a quanta of evidence about which faces belong together in an object. So he said, after
            studying these for a long time, that these drawings tend to have a lot of arrow type junctions and a lot of
            fork type junctions.</p>
        <p>当你看到箭头时，这有一点证据表明轴两侧的物体是相同的。轴两侧的面属于同一物体。在这里，叉形连接表明成对的三对面似乎属于同一物体。</p>
        <p>And when you see an arrow, it’s a little bit of evidence that the objects on either side of the shaft are the
            same. the face on either side of the shaft belong to the same object. And over here, the fork type junction
            suggests that pairwise, three pairs of faces seem to belong to the same object.</p>
        <p>因此，有了这个想法，他可以回到这样的一幅画上，开始用这些所谓的链接来装饰它，这些证据表明面孔属于同一物体。如果我做得对，你会得到类似这样的结果。很难看出图画本身在做什么。所以让我给它们编号。现在我有一张更容易处理的图片了。
        </p>
        <p>So with that idea, he could come back to a drawing like this and start decorating it with these so called
            links, these quanta of evidence that faces belong to the same object. And if I’ve done it right, you get
            something that looks like so. It’s a little hard to see what’s going on the drawing itself. So let me number
            these. Now I have an easier to deal with picture.</p>
        <p>1 和 2 以及 1 和 3 之间有两个链接。2 和 3 之间有一个链接。2 和 4 之间有一个链接。这里有两个。这里有两个。所有这些之间各有一个。</p>
        <p>There are two links between 1 and 2 and 1 and 3. One link between 2 and 3. One between 2 and 4. Two here. Two
            here. And one each from all of these.</p>
        <h2 id="two-link-theory">双环理论</h2>
        <h2>Two Link Theory</h2>
        <p>因此，古兹曼提出了如何解决问题的证据，然后他开始思考使用这些证据的各种方法。</p>
        <p>So Guzman produces this evidence for how the problem ought to be solved and then he begins to think about
            various ways of using the evidence.</p>
        <p>因此他可以而且确实决定，一个链接就足以假设这些脸属于同一个物体。你可以看出这有点太宽泛了。因为这意味着整个物体只是一个物体。所以古兹曼拒绝了单链接理论，转而采用双链接理论。双链接理论说，哦，好吧，让我们看看。</p>
        <p>So he could, and did, decide that one link is enough to presume that the faces belong to the same object. And
            you can see that’s a little bit too liberal. Because that says that the whole thing is just one object. So
            Guzman rejected the one link theory and went to the two link theory. And the two link theory says, oh, well,
            let’s see.</p>
        <p>如果我们坚持认为必须有两个链接才能决定它是同一个物体，那么这三个面就会被拉在一起形成一个物体。这三个面就会被拉在一起形成一个物体。但是，遗憾的是，7
            不与任何东西共享两个链接，所以它悬而未决。所以很明显，两个链接理论太保守了。</p>
        <p>If we insist that two links should be there before we are willing to decide that it’s the same object, then
            these three faces are pulled together into one object. These three are pulled together into one object. But
            alas, 7 doesn’t share two links with anything, so it’s left dangling out there. So plainly, the two link
            theory is too conservative.</p>
        <p>因此，正如您很快在任何项目中发现的那样，这将引出第三种理论，即两个长度重复。现在我们已经形成了这些超级区域，我们可以回过头来说，如果超级区域通过两个或更多个链接连接在一起，我们将合并它们。所以这里没有什么新事情发生。但是这个超级区域在
            7 处通过两个或更多个链接连接。所以它把所有东西都拉到了一起。所以这很有效。</p>
        <p>So that as you would soon discover in any project, would lead you to a third theory, which is two lengths
            repeated. So now that we’ve formed these super regions, we can come back and say we’ll merge super regions
            if they’re connected together by two or more links. So nothing new happens up here. But this super region is
            joined at 7 by two or more links. So it pulls everything together like so. So that worked fine.</p>
        <p>嗯，它并没有很好地发挥作用。有很多例子表明它没有发挥作用。在我们人类认为荒谬的情况下，它得出的结论看起来很愚蠢。所以当古兹曼在他的博士论文答辩中展示这项工作时，据说。谁知道这是不是真的。但据说马文·明斯基认为这是一项伟大的工作，我们应该让他当教授。
        </p>
        <p>Well, it didn’t work fine. There were lots of examples of situations where it didn’t work. in situations that
            were considered nonsensical by us humans because it seemed silly, the kind of conclusions that it reached.
            So when Guzman presented this work in his Ph.D.&nbsp;thesis defense, it’s said. and who knows if it’s true.
            but it’s said that Marvin Minsky thought it was great work and we should make him a professor.</p>
        <p>碰巧 Dave Huffman 也在委员会中，他说这是临时决定，论文应该被拒绝。所以我们对此有两种截然相反的看法。顺便说一句，Dave Huffman。我想你以前听说过这个名字。他是 Bob Fano
            教授的信息理论课程中发明 Huffman 涂层作为学期论文的人。据说他得了 A。所以 Huffman 不喜欢它。</p>
        <p>It happens that Dave Huffman was also on the committee and said that it was ad hoc and the thesis should be
            rejected. So we had two polar opposites of impressions there. Dave Huffman, by the way. you’ve heard that
            name before, I imagine. It’s the guy who invented Huffman coating as a term paper in a information theory
            course taught by Bob Fano. He got an A, they say. So Huffman didn’t like it.</p>
        <p>他认为这种方法有点太临时了，太过启发式，没有利用我们所知道的物理学知识。所以人们开始问，为什么这种方法有效？什么时候这种方法无效？这就是在这种情况下你能回答的最好的问题。</p>
        <p>He thought it was a little bit too ad hoc. It was too heuristic. It didn’t take advantage of anything we
            might know about physics. And so people began to say, well, why does it work? And when does it not work? And
            that’s just about the best question you can answer in a situation like this.</p>
        <p>通过对古兹曼的更多图片进行研究和反思，我发现这个方法之所以有效，是因为这个世界充满了三面交汇点。或者让我说是三面顶点，因为它们存在于这个世界中。我们将“交汇点”一词保留给其他东西。三面顶点通常投射成箭头或叉子。</p>
        <p>And by playing with some more of Guzman’s pictures and reflecting on them, it turned out that it worked
            because the world is full of three face junctions. Or let me say three face vertexes because they’re out
            there in the world. We’ll reserve the word junction for something else. And three face vertexes generally
            project into either an arrow or a fork.</p>
        <p>因此，古兹曼的整个程序都基于弱反向结论，即如果你看到其中一个，它可能来自其中一个。所以这是在图画中。那是在世界上。因此，通过既不是演绎也不是归纳，而是溯因的过程，你会看到其中一个。</p>
        <p>So Guzman’s whole program worked on the weak backward conclusion that if you see one of those, it probably
            came from one of these. So this is in the drawing. That’s in the world. So by a process that’s neither
            deduction or induction, but rather abduction, you see one of those guys.</p>
        <p>你会说，好吧，它们通常来自世界上的三个面顶点，所以如果你看到一个，它可能来自世界上的三个面顶点。这就是绑架。所以，一旦霍夫曼看到这一切，作为一名数学家，他开始思考如何发展一个不同且更好的理论。但我们必须认识到，所有这些工作都是从古兹曼的努力开始的，他是一名实验主义者。而霍夫曼是一名数学家。
        </p>
        <p>And you say, well, they often come from three face vertexes in the world, so if you see one, it probably came
            from a three face vertex in the world. That’s abduction. So once Huffman saw all that, being a
            mathematician, he began to think about how one might develop a different and better theory. But we have to
            recognize that all this work started off with the efforts of Guzman, who was an experimentalist. And Huffman
            was a mathematician.</p>
        <p>因此，他们自然而然地以不同的方式处理这个问题。霍夫曼说，我不会太关心古兹曼试图解决的实际问题。相反，我将在一个非常简单的世界里工作，我可以用数学来处理它。所以他决定在一个具有几个特征的世界里工作。第一个特征是世界将以一般的位置呈现。
        </p>
        <p>So naturally, they approached the problem differently. So Huffman says, I’m not going to concern myself too
            much with the actual problem that Guzman was trying to solve. Rather, I’m going to work in a very simple
            world, which I can deal with mathematically. So he decided that he was going to work in a world which had
            several characteristics. Characteristic number one was that the world would be presented in general
            position.</p>
        <p>也就是说，没有螺丝盒。所以如果你看到一个立方体，它看起来会像这样。它看起来不会像这样。所以那是出局。这是入局。这里的想法是，这不是一般立场，因为如果你稍微扰乱你的观点，你就会把那些 T
            形连接点变成叉子和箭头。所以我们知道它必须处理这些奇怪的情况。</p>
        <p>That is to say, no screw cases. So if you see a cube, it’s going to look like this. And it’s not going to
            look like this. So that’s out. And this is in. And the idea here is that’s not general position, because if
            you perturb your point of view a little bit, you’ll change those T junctions into forks and arrows. So we
            know it had to deal with those kinds of weird kinds of cases.</p>
        <p>这就是第一个假设。第二个假设是，我们将要面对的是一个三面体世界。这意味着那里的所有顶点都将由三个平面组成。所以你将得到三个平面的交点。那么，如果世界是这样组成的，你能看到多少种交点呢？这就是霍夫曼提出的问题。</p>
        <p>So that’s presumption number one. Presumption number two is that we’re going to be dealing with a world
            that’s trihedral. That means all vertexes out there are going to be formed from three planes. So you’re
            going to have the intersection of three planes. So how many kinds of junctions can you see if the world is
            composed that way is the question that Huffman sent himself upon.</p>
        <h2 id="four-kinds-of-lines">四种线条</h2>
        <h2>Four Kinds of Lines</h2>
        <p>下一个假设是，将有四种线。你看，古兹曼只承认两种线。有长度的线和没有连接的线。它们与物理世界没有太大关系。所以霍夫曼说，我想把约束从物理世界中去除。所以我要处理四种线。凹线、凸线和边界线。</p>
        <p>The next assumption was that there are going to be four kinds of lines. See, Guzman only recognized two kinds
            of lines. lines that have a length and lines that don’t have a link. And they don’t have very much to do
            with the physical world. So Huffman says, I want to get the constraint out of the physical world. So I’m
            going to deal with four kinds of lines. concave, convex, and boundary.</p>
        <p>因此，每一种线都有自己的符号。我们将凸线称为正线，凹线称为负线，边界线上会有一个箭头，具体取决于物体的哪一侧。如果你​​沿着这个小箭头的方向走，你会在哪一侧看到物体。学生：问题。哪个是凹的，哪个是凸的？你说的是相反的。
        </p>
        <p>So each of those kinds of lines is going to have its own notation. We’ll call the convex lines plus, the
            concave lines minus, and a boundary lines are going to get an arrow on them, depending on which side of the
            object. which side you would see the object if you walk along the direction of this little arrow. STUDENT:
            Question. Which is concave and which is convex? You said something and you wrote the opposite.</p>
        <p>帕特里克·温斯顿：是的，抱歉。谢谢。凹面、凸面和边界。谢谢。下面是一条凹线。那条线会标为减号。哦，我不知道。你在这里看到的这些线，如果后面不是我，而是有东西，那么它们都是凸线。很多时候你会看到一条边界线。例如，现在你看不到下面这条线的两侧。
        </p>
        <p>PATRICK WINSTON: Yeah, sorry. Thank you. So concave, convex, and boundary. Thank you. So this down here,
            that’s a concave line. And that would get a minus label. Oh, I don’t know. These lines you’re seeing here,
            if there were stuff behind them instead of me, then all those would be convex lines. Many times you see a
            boundary line. For example, now you don’t see both sides of that line down here at the bottom.</p>
        <p>这就是一条边界线。箭头指向那个方向，这样当你沿着某种数学惯例行走时，物体的内容就会保持在右侧。所以有三种线，四种标签。还有一些内容被省略了。这是因为霍夫曼想让他的问题保持简单，让他可以手动处理。省略了什么？省略了裂缝。省略了阴影。
        </p>
        <p>So that’s a boundary line. And the arrow would point in that direction, so as to keep the stuff of the object
            on the right as you walk along a kind of mathematical convention. So three kinds of lines, four kinds of
            labels. And there’s some things left out. That’s because Huffman wanted to keep his problem simple,
            something he could manage by hand. What’s left out? Cracks are left out. Shadows are left out.</p>
        <p>假设这里没有什么有趣的东西。所以在继续之前，让我们先了解一下词汇。我会尽量坚持这些词汇。但那里有一些东西，包括顶点和边。在黑板上，我们会有连接点和线。我会尽量坚持这些词汇。好吗？是的，克里斯托弗？学生：你不是说有四种类型的线吗？
        </p>
        <p>There’s a presumption that there’s nothing of interest here with respect to that. So let’s have a little
            vocabulary before I go on. And I’ll try to stick to it. But there’s the stuff out there, and that consists
            of vertexes and edges. And over here, on the blackboard, we’ll have junctions and lines. And I’ll try to
            stick to that vocabulary. All right? Yes, Christopher? STUDENT: Didn’t you say there were four types of
            lines?</p>
        <p>帕特里克·温斯顿：是的，有四种。问题是，我不是说过有四种线吗？有三种线，但是由于我们可以在任一方向标记边界线，所以我们有四个标签。好吗？这取决于物体在物体的哪一侧。我想，只要我举个例子，这一点就会很清楚。所以，一、二、三个假设。一些词汇。
        </p>
        <p>PATRICK WINSTON: Yeah, there are four kinds of. the question is, didn’t I say there were four kinds of lines?
            There are three kinds of lines, but since we can label a boundary line in either direction, we have four
            labels. OK? It depends on which side of the object the stuff is on. And that will be clear, I think, as soon
            as I do an example. So one, two, three assumptions. A little bit of vocabulary.</p>
        <p>因此，我们有可能制作一个完整的目录。这很简单。我们有可能制作一个完整的目录，列出所有可以汇聚在一起形成这四个标签的连接点的方式。现在，一开始你可能会说，天哪，这需要几年时间。但也许不需要几年时间。</p>
        <p>So we have the possibility of making a complete catalog. This is so simple. We have the possibility of making
            a complete catalog of all the ways that lines can come together to form a junction with respect to these
            four labels. Now at first you might say, oh my god, that will take a couple of years. But maybe it won’t
            take a couple of years.</p>
        <p>最后，也许你会惊讶地发现，在这个世界上，只有 18 种方法可以排列连接点周围的标签。其他所有方法都被排除在外。如果你有的东西不在我们现在要制作的集合中，它就不能用三面体顶点来构建。所以我列出了六个 L、五个叉、四个 T
            和三个箭头。</p>
        <p>And in the end, to perhaps your surprise, you discover that there are only 18 ways to arrange the labels
            around a junction in this world. Everything else is excluded. If you have something that’s not in the set
            we’re going to produce now, it can’t be built with trihedral vertexes. So I’ve listed up there six L’s, five
            forks, four T’s, and three arrows.</p>
        <p>让我们看看我是否能弄清楚为什么只有这 18 个，而没有其他的。好吧，如果我们有三个顶点汇聚在一起，那就意味着有八个八分圆，对吧？物体的内容可能填充 1、2、3、4、5、6、7
            或所有八个八分圆。当然，如果你填满了所有八个八分圆，就没有交汇点。所以我们不考虑这种情况。如果我们不填充任何八分圆，就没有交汇点。没有顶点。</p>
        <p>Let’s see if I can figure out why there are those 18 and nothing else. Well, if we have three vertexes coming
            together, that means there are eight octants, right? And the stuff of the object may fill 1,2.3,4.5,6.7 or
            all eight octants. Now of course, if you fill all eight octants, there’s no junction. So we don’t consider
            that case. If we don’t fill any the octants, there’s no junction. There’s no vertex.</p>
        <p>所以我们不考虑这种情况。但如果八个八分之一中只有一个充满了东西，那么我们可以从剩下的七个八分之一中查看它。所以现在，你是从剩下的七个八分之一中查看它。如果我没记错的话，你会看到那里有一个叉形交叉点，对吧？你会看到一个叉形交叉点，其中所有边缘都是凸的。
        </p>
        <p>So we don’t consider that case. But if just one of the eight octants is filled with stuff, then we can look
            at it from any of the seven remaining octants. So right now, you’re looking at it from one of the seven
            remaining octants. And if I’m not mistaken, you’re going to see a fork style junction there, right? And
            you’re going to see a fork style junction in which all of the edges are convex.</p>
        <p>名字的选择不太好，因为语言学家告诉我，我们用第一个名字来索引所有单词。而这些单词的第一个名字相同，所以很容易混淆。所以这是一个叉形连接。我们知道，一种合法的方法是使用四个加号。现在，这不是您能看到的唯一方法。这是另一种方法。这是
            L 形连接，对吗？</p>
        <p>An unfortunate selection of names, because the linguists tell me that we index all of our words by the first
            forename. And those have the same first forename, so they’re easily confused. So here’s a fork style
            junction. And we know that one way that’s legitimate for the lines to come in is with four pluses. Now
            that’s not the only way you can see that. And here’s another way. There’s an L style junction, right?</p>
        <p>这两条都是边界线。我们想在上面画出边界线指示器，这样如果我们沿着箭头走，物体的东西就会在右边。所以我认为，为了让我更容易画图，我应该这样看。然后我会说，好吧，这肯定是标记交叉点的合法方法。还有其他方法吗？</p>
        <p>And both of those are boundary lines. And we want to draw the boundary line indicators on there so that if we
            walk along the arrows, the stuff of the object would be on the right. So I suppose, to make it easier to me
            do my drawing, I should look at it this way. And then I would say, well, that has to be a legitimate way of
            labeling a junction. Are there any more?</p>
        <p>嗯，一共有七个，但其中很多都是一样的。不过还有一个稍微不同。我可以像这样举起这个家伙。如果我以正确的角度握住它，你会看到箭头式的交叉点，对吧？两个边界，倒钩是凸的。所以在这个特定情况下，我明白了。我明白了。我在那里有一个加号。
        </p>
        <p>Well, there’s seven altogether, but many of them are the same. There is one more that’s a little different,
            though. I can hold this guy up like so. And if I’m holding at the right angle for you, you see an arrow
            style junction, right? Two boundaries, and the barb is convex. So in this particular case, I’ve got that.
            I’ve got that. And I’ve got a plus there.</p>
        <p>而这正是一个八分圆充满东西的情况。不过，我碰巧能够将其反转。这是七个八分圆充满东西的情况。所以这告诉我们，在空间中可能存在一个顶点，当将其简化为板上的交叉点时，它应该有三个减号标签，因为您现在看到的所有这些都是凹的。所以另一个叉形交叉点看起来像这样。
        </p>
        <p>And that’s there happen to be for the one octant filled with stuff case. I happen to be able to reverse this,
            though. And here’s the seven octants filled case. So that tells us that it’s possible to have a vertex out
            there in a space that when reduced to a junction on the board deserves three minus labels, because all of
            these that you’re seeing now are concave. So another fork style junction looks like so.</p>
        <p>由于只有一个八分圆可供观察，因此我们对七个八分圆填充情况的分析就完成了。</p>
        <p>And since there’s only one octant to look at from, that completes our analysis of the seven octants filled
            case.</p>
        <h2 id="three-options">三种选择</h2>
        <h2>Three Options</h2>
        <p>现在我们还有其他几种可能。我们可能有五个八分圆内装满了东西。这意味着有三个八分圆可供我们观察。让我们假设这些小粉笔碎片是小人。他们正在看下面的这个连接点，这个白色物体与桌子融合在一起。</p>
        <p>Now we have a couple of other possibilities here. We might have five octants filled with stuff. So that means
            there are three octants that we can look from. So let’s suppose that these little chalk pieces are little
            people. And they’re looking at this junction down there, where this white object is fused with the table.
        </p>
        <p>我把它和桌子融合在一起是因为我想把它看作一个物体。我们可以从那三块小粉笔所代表的三个物体来观察它。问问自己，如果我们从这三个地方看它，我们会看到什么？如果我们从紫色粉笔的角度来看它。我必须四处走走，确保万无一失。
        </p>
        <p>I’m fusing it with the table because I want to consider it to be one object. We can view it from the three
            objects indicated by those three little chalk pieces. And ask ourselves, well, in the event that we look at
            it from those three places, what do we see? And if we look at it from the perspective of the piece of purple
            chalk. I’ll have to walk around and be sure.</p>
        <p>看起来像一个箭头连接点，有两个凹面和一个凸面。我说得对吗？所以我从这个角度看它。这是一支箭头。这是凸面，这两个是凹面，因为我把纸盒和桌子融合在一起了。从这个蓝色家伙的角度来看它。让我旋转它，这样你就能更好地理解这个蓝色家伙。它看起来像一条凹线和一条边界。
        </p>
        <p>Looks like an arrow junction with two concaves and a convex. Did I get that right? So I’m looking at it from
            this perspective. It’s an arrow. This is convex and these two are concave because I fused the paper box with
            the table. Looking at it from the perspective of this blue guy. let me rotate it so you can have a better
            understanding of the blue guy. it looks like a concave line and a boundary.</p>
        <p>所以它是一个 L。这个是边界。它是凹的。根据某种对称性，我们也将从另一侧得到那个，即三个八分圆中的第三个。好了，我们开始了。但我们还有很长的路要走。我们可以通过再次思考这个物体来处理它。</p>
        <p>So it’s an L. And this one is a boundary. And that’s concave. And by a kind of symmetry, we’re also going to
            get that one from the other side, the third of the three octants. Well, we’re off and running. But we still
            have an awful lot to go. And we could manage to deal with it by thinking about this object once again.</p>
        <p>但是，不要把这种情况放在一边，而是转过身来看看这个顶点。想想它能产生的连接点。我想我会帮你做到这一点。因为你真的必须玩弄它，稍微移动一下，才能看到事情会是什么样子。所以，让我想想这会如何运作。我知道其中一种可能性看起来会是这样的。
        </p>
        <p>But instead of this situation out here, to turn it around and look at this vertex. Think about the junctions
            that it can produce. I think I’ll do that for you. Because you really have to play with this and move it
            around a little bit to see how things are going to look. So let me think about how that’s going to work out.
            I know that one of the possibilities is going to look like so.</p>
        <p>我最好不要向你隐瞒这一点。当一个小个子男人抬头看着路口时，就会发生这种情况。这个将是负数。现在我们又有两个这样的。看起来像这样。你会说，哎呀。你会说，它们不就是彼此的旋转方差吗？答案是肯定的。</p>
        <p>I might as well not hide that from you. It’s going to be what happens when there’s a little man looking up at
            the junction. And this one’s going to be minus. And now we’ve got two more that are just like that. Look
            like so. And you say, oops. You say, aren’t those just a rotational variance of each other? And the answer
            is sure.</p>
        <p>我把它们都记下来了，因为如果你在空间中遇到一个叉形连接点，那么有三种不同的标记方法。这取决于你在哪条线上放置减号标签。这样就解决了这个问题。然后还有一个这样的叉形连接点。这就是从这个案例中衍生出来的加号、加号、减号。而且似乎还有三个这样的
            L 形连接点。</p>
        <p>I write them all down, because if you get a fork style junction in space, there are three different ways it
            could be labeled. Depending on which of the lines you put the minus label on. So that takes care of that.
            And then there’s one more of these fork style junctions. And that’s plus, plus, minus that derives from this
            case. And there appear to be three more of these L style junctions.</p>
        <p>它们看起来像，让我们看看，加号，然后是加号。我必须边走边思考。然后。就这样。那么，T 呢？好吧，在这个世界上，唯一能得到 T
            的方法是某个物体遮挡了另一个物体。如果一个物体遮挡了一条线，那它可能是任何一条线。所以这就是为什么剩下的四个很容易被描述和标记。</p>
        <p>And they look like, let’s see, plus, then plus. I’m having to think this through as I go. And then. and
            that’s it. Well, what about the T’s? Well, in this world, the only way you can get a T is if some object is
            obscuring another object. And if an object is obscuring a line, it can be any line at all. So that’s why the
            four remaining ones are easy beyond description to label.</p>
        <p>当然，T
            的交叉部分都是边界线，右侧是遮挡物体。现在让我们看看。我们现在已经处理了填充一、三、五和七个八分圆的情况。那么填充二、四和六个八分圆的情况呢？好吧，事实证明你也可以用这种方式制作顶点。但它们的面不止​​三个。它们有六个面。
        </p>
        <p>And of course, the cross pieces of the T are all boundary lines, with the obscuring object on the right. Now
            let’s see. We’ve taken care now of the one, three, five, and seven octants filled cases. What about two,
            four, and six? Well, it turns out you can have vertexes that are made that way too. But they will have more
            than three faces. They’ll have six faces.</p>
        <p>它们就像物体在某一点聚集时发生的情况，就像这样。就像那样。你可以稍微玩一下，看看如果你有两个、四个或六个装满东西的物体，那么它们的面就会超过三个。所以我们要忽略这些。所以我们的约束会比霍夫曼使用的术语所暗示的更严格一些。
        </p>
        <p>They’ll be like what happens when an object comes together at a point, like so. Like that. You can play with
            it a little bit and see that if you have two, four, or six objects filled with stuff, there are more than
            three faces. So we’re going to ignore those. So our constraint is going to be a little more severe than
            would be suggested by the terminology Huffman uses.</p>
        <p>它们确实是三面体，但它们也会是三个面。所以我们在这方面做了很多工作。但是我们发现了什么？我们发现在这个世界上，这是一个完整的、100%
            的目录，没有任何排除，没有其他东西可以存在，它包含了所有可能的方式，这些方式可以在交叉点周​​围排列线标签。这个世界上没有其他东西了。所以这是一个非常强大的约束。</p>
        <p>They’re going to be trihedral all right, but they’re also going to be three faces. So we went to a lot of
            work there. But what have we discovered? We’ve discovered that in this world, this is a complete, 100%
            percent, nothing excluded, nothing else can be there, catalog of all possible ways the junctions can have
            line labels arranged around them. There’s nothing else in this world. So that’s a very powerful constraint.
        </p>
        <p>现在让我们看看我们可以用它做什么。</p>
        <p>So now let’s see what we can do with it.</p>
        <h2 id="example">例子</h2>
        <h2>Example</h2>
        <p>这个例子通常在红袜队表现更好的时候更有趣，但事实并非如此。但我们还是会用到它。我们将从一个看起来像本垒板的物体开始。我会问你一个问题。你能造一个这样的物体吗？我不知道。我们来试一试。我们假设这个物体悬空，漂浮在太空中。
        </p>
        <p>This example is usually more fun when the Red Sox are doing better, but they’re not. Yet we’ll use it any
            way. We’re going to start with an object that looks like home plate. And I’ll ask you the question. Can you
            build one of those? I don’t know. Let’s give it a shot. We’re going to assume that this object is hanging,
            floating in space.</p>
        <p>因此，边界周围的所有这些线都是边界线，就像这样。现在，这让我们有了一个很好的开端。它只是悬挂在空间中，克里斯托弗，好吗？你看起来很困惑。它只是悬挂在空间中，所以边缘周围的所有线都是你只看到一侧有东西的地方。这样我们就可以快速绕着外围跑一圈，并在所有外线上贴上箭头标签。
        </p>
        <p>So therefore, all of these lines around the boundary are boundary lines, like so. Now that gets us off to a
            good. it’s just hanging in space, Christopher, all right? You look confused. It’s just hanging in space so
            that all the lines around the edges are places where you see only one side has stuff on it. So that enables
            us to just quickly run around the periphery and put arrow labels on all those outside lines.</p>
        <p>现在我们在边界上有很多箭头样式的连接点。这是常见的情况。因此，我们可以查看所有可能的标签目录，然后我们会看到，如果箭头的倒钩上有边界，则只有一个这样的标签。因此，我立即知道轴上一定有一个加号。因此，我们可以回到这里，把所有这些箭头都拿过来。
        </p>
        <p>Now we have a lot of arrow style junctions on the boundary. That’s commonly the case. So we can run over to
            our catalog of all possible labels, and we see that if we have an arrow with boundaries on its barbs,
            there’s only one of those. So I know instantly that there must be a plus on the shaft. So we can come back
            here and take all these arrows here.</p>
        <p>并在它们的轴上用正线标记它们。现在，一条线不能沿着它的长度改变它的性质。所以如果一端是正线，那么另一端也会是正线。好吗？那么我们还能做什么？这里深处是一个叉形连接点。这两条线上都有凸起的标记。</p>
        <p>And label them with plus lines on their shafts. Now a line can’t change its nature along its length. So if
            it’s a plus line on one end, it’s going to be a plus line on the other end. All right? So what else can we
            do? Here deep inside is a fork style junction. It’s got convex markers on both of those two lines.</p>
        <p>于是我们翻看目录，说，既然它的两条线都有加号，我们能说些什么呢？哇哦，这意味着第三条线也必须是加号。现在我们完成了。我们给所有东西都贴上了标签。除了。看看这个。那家伙呢？有一个 L
            型接头，两条线上都有加号。我的目录中有这样的产品吗？没有。&nbsp;</p>
        <p>So we go over to our catalog and say, what can we say about it, given that there are pluses on two of its
            lines? Whoop, that means that the third one has to be a plus as well. And now we’re done. We’ve labeled
            everything. Except. look at this. What about that guy? There’s an L style junction with pluses on both of
            its two lines. Is there one of those in my catalog? No.&nbsp;</p>
        <p>因此，我还没有通过我所创造的世界的可构建性必要条件。你无法创造它。你无法构建它。所以霍夫曼的想法为我们提供了一种测试某物的方法，看看它是否不可能存在于这个世界中。如果它通过了测试，是否意味着它是可能的？不。这是一个必要条件，但不是充分条件。就这一点而言。蓝领职业。
        </p>
        <p>Therefore, I haven’t passed a necessary condition for constructability in the world that I’ve made. You can’t
            make it. You can’t construct it. So Huffman’s ideas give us a way of testing something to see if it’s not
            possible for it to be in this world. If it passes the test, does that mean it’s possible? No.&nbsp;It’s a
            necessary but not sufficient condition. On this one. blue collar occupation.</p>
        <p>就这一点而言，如果我们再加一条线，也许会有所帮助。例如，我们可以像这样加一条线。现在你感觉好些了吗？我不知道。让我们看看。对于我们之前在其他几个箭头上使用的相同论点，这肯定是一个加号。这给了我们一个箭头样式的连接点，所有东西上都有加号。有这样的连接点标签吗？没有。我们输了。这没有帮助。
        </p>
        <p>On this one, maybe it will help if we put in another line. For example, we could put a line like so. You feel
            better about it now? I don’t know. Let’s see. This has to be a plus for the same argument we used on several
            other arrows before. That gives us an arrow style junction here with a plus on everything. Is there such a
            junction label? No.&nbsp;We lose. It doesn’t help.</p>
        <p>你认为你能做到，但你做不到。学生：但你实际上可以把它构造成一个 3D 物体。帕特里克·温斯顿：他认为你可以把它构造成一个 3D
            物体。克里斯托弗，让我给你看下一个例子。考虑一下这个例子。你能做到吗？你的直觉是可以的。所以让我们给它贴上标签。哦，我已经输了。我们只是稍微加强一下，让情况更清楚。</p>
        <p>You think you can make it, but you can’t. STUDENT: You could actually construct it as a 3 D object, though.
            PATRICK WINSTON: He thinks you can construct it as a 3 D object. Let me show you the next example,
            Christopher. Consider this example. Can you make that? Your intuition is yes. So let’s label it. Oh, I’ve
            already lost. We just boost that up a little bit to make the situation more clear.</p>
        <p>所以，我已经陷入了无法标记的境地。但你觉得你可以做到。那么问题出在哪里？问题是什么。什么，艾略特？学生：你有一个模糊的。或者，我们假设我们有一条模糊的小巷，从左上角到帕特里克·温斯顿：对艾略特所说的内容进行一些解释。如果你看看这里的情况，你会发现那里有一个四面交叉路口。所以你可以做到。
        </p>
        <p>So already, I’ve got myself in a situation where I can’t label that. But you feel like you can make it. So
            what’s wrong? What’s wrong is. what, Elliott? STUDENT: You have an obscured. or, we’re presuming that we
            have an obscured alley from the upper left corner to the PATRICK WINSTON: Putting a little interpretation on
            what Elliott has said. If you look at this situation back here, you get a four faced junction there. So you
            can make it.</p>
        <p>但不是三面。所以有些看起来可以。但它们不能贴标签，因为一个交叉点需要四个以上的面。我们可以把这个想法带回来。你可以这样做。但是这个交叉点，后面有两个，这里有两个。所以它有四个面。同样的想法。这就是古兹曼的贡献。</p>
        <p>But not with three faces. So some of these look like you could make it. But they can’t be labeled because you
            need more than four faces at a junction. And we can carry that idea back here. You can make this OK. But
            this junction, you’ve got two in the back and two here. So it has four faces. Same idea. So that’s Guzman’s
            contribution.</p>
        <h2 id="huffman-and-waltz">霍夫曼和华尔兹</h2>
        <h2>Huffman and Waltz</h2>
        <p>这就是哈夫曼的贡献。哈夫曼是一位数学家。</p>
        <p>That’s Huffman’s contribution. Huffman was a mathematician.</p>
        <p>但那时我们想制造机器人。但这两个人都没有解决处理自然物体的阴影、裂缝、空间中三面体顶点以上的问题，该怎么办呢？好吧，这是另一个研究生大卫·华尔兹着手解决的问题。所以华尔兹决定，除非他增加裂缝、阴影、非三面体顶点，否则他不会满足。嗯。是的，非三面体顶点。还有光。
        </p>
        <p>But we wanted to build robots back in those days. And neither one of these guys solved the problem of dealing
            with natural objects with shadows, with cracks, with more than trihedral vertexes in space, and what to do
            about that? Well, that was a problem that another graduate student, David Waltz, set about to solve. So
            Waltz decided that he would not be content unless he added cracks, shadows, non trihedral vertexes. Uh.
            yeah, non trihedral vertexes. And light.</p>
        <p>这些考虑促使 Waltz 将标签数量从 4 个增加到 50 多个。因为他必须在每条线路上贴的每个标签中都包含大量信息。右边是什么灯？左边是什么灯？也许是裂缝。也许是裂缝……各种各样的考虑。在这里，我们有 18
            种线路在交叉点处汇合的方式。</p>
        <p>These considerations led Waltz to go from four labels to 50 plus. Because he had to pack a lot of information
            into each of the labels he put on a line. What kind of light was on the right? What kind of light was on the
            left? Maybe it’s a crack. Maybe it’s a crack that. all sorts of considerations. Here we had 18 ways that
            lines can come together around junctions.</p>
        <p>这涉及到华尔兹世界中的数千个节点。下面是古兹曼。他编写了一个可以工作的程序。下面是霍夫曼。霍夫曼有一个理论，但解决了错误的问题。华尔兹来了，他试图用一个令人满意的理论来解决正确的问题，这个理论有一个可推广的原则。所以当我们完成这一切后，我们将讨论成功的标准。
        </p>
        <p>That went to thousands of junctions in Waltz’s world. So here’s Guzman. He writes a program that sort of
            works. Down below, we have Huffman. Huffman, who has a theory but solves the wrong problem. So here comes
            Waltz, and he’s trying to solve the right problem with a satisfying theory that has a generalizable
            principle. So when we get all through this, we’ll talk about criteria for success.</p>
        <p>我们得出的结论是，要想真正成功，首先你需要解决一个问题。你需要一个行之有效的方法。你必须证明它之所以有效是因为某些原理。所以古兹曼有这个问题和一些行之有效的方法。霍夫曼有一种方法，但该方法在错误的问题上行之有效。而沃尔兹负责把所有问题整合在一起。所以沃尔兹做了所有这些工作。
        </p>
        <p>And we’ll conclude that to have a really successful thing, you need a problem, to start with. You need a
            method that works. And you have to show that it works because of some principal. So Guzman had the problem
            and something that worked. Huffman had a method which worked on the wrong problem. And it’s left to Waltz to
            bring it all together. So Waltz does all this work.</p>
        <p>现在，他有的标签已经从 18 个增加到了数千个。标签数量也从 4 个增加到了 50 多个。因此，手工处理这些标签自然变得困难。我们能够手工处理这些 Huffman
            示例。我们从标记边界上的所有内容开始，然后逐步解决。这没有什么特别的方法。解决这个难题很容易。</p>
        <p>And now he has, instead at 18 labels, he has thousands. Instead of four line labels, he has more than 50. So
            naturally, it becomes difficult to work these by hand. We were able to work those Huffman examples by hand.
            We started with labeling everything on the boundary and worked our way in. There’s no particular method
            there. It was just easy to work out the puzzle.</p>
        <h2 id="depthfirst-search">深度优先搜索</h2>
        <h2>DepthFirst Search</h2>
        <p>但可怜的华尔兹却没有这样的奢侈。</p>
        <p>But poor Waltz, he didn’t have that luxury.</p>
        <p>因此，在典型的场景中，他可能有数十甚至数百个结点需要标记，而且没有简单的处理方法。因此，他自然会为自己编写一个深度优先搜索程序。因此，这是顶点，或者说是结点
            1。有很多选择可以为其使用哪种标签。对于每个选择，无论他决定结点 2 是什么，都有自己的一套可能性。</p>
        <p>So he might have, in a typical scene, he might have tens or even hundreds of junctions to label and no easy
            way of dealing with it. So naturally writes himself a depth first search program. So here is vertex, or
            rather junction number 1. There are many choices for which label can be used on it. And for each of those
            choices, whatever he’s decided junction number 2 is has its own suite of possibilities.</p>
        <p>所以它就变成了一个简单的深度优先搜索问题，对吧？所以实际上，华尔兹当时是我的办公室同事。我可以告诉你这是事实。他一写完这个程序，就一直盯着电脑看。那时候电脑很大。它们都有灯。所以你看着电脑，看看灯是否还在闪烁。</p>
        <p>And so it becomes a simple depth first search problem, right? So in actuality, as soon as Waltz. he was my
            office mate at the time. I can tell you this for a fact. As soon as he wrote this program, he kept looking
            over at the computer. they were big in those days. They all had lights. So you looked over at the computer
            to see if the lights were still blinking.</p>
        <p>因为他启动了这个深度优先搜索程序，但什么都没有发生。他认为计算机崩溃了。什么都没有发生。为什么什么都没有发生？因为搜索基数是指数级的，对于普通计算机甚至普通宇宙来说都太大了。所以华尔兹必须做点别的事。他必须想出一种新方法来使用他所有的这些标签。
        </p>
        <p>Because he’d start this depth first search program up and nothing would happen. He thought the computer had
            crashed. Nothing happened. Why did nothing happen? Because the search base is exponential and much too big
            for an ordinary computer, or maybe even an ordinary universe. So Waltz has to do something else. He has to
            come up with a new method for using all these labels that he’s.</p>
        <p>经过大约一年半的辛勤工作，他的办公桌上放满了小块纸张。经过一年半的辛勤工作，他弄清楚了所有这些连接标签，然后他必须想出一种方法来弄清楚如何使用它们。所以我们不知道他最大的贡献是那套标签还是他的方法。也许两者都应该得到同等的重视。
        </p>
        <p>After about a year and a half’s worth of hard work, with lots of paper on his desk in little blocks. After
            year and a half of hard work getting all these junction labels figured out, he then has to come up with a
            method for figuring out how to use them. And so we don’t know whether to think his biggest contribution was
            that label set or his method. And probably both deserve about equal billing.</p>
        <p>哦，我不知道如何解释华尔兹所做的事情。好吧，一种方法是举个例子。我想我会冒昧地举个例子。让我们看看。让我找点空间。我想我只能在这里了。但那会很方便，因为线标签在这里。这是我的例子。你会说，我怎么能只给你图片的一部分呢？好吧，你可以假设我是透过窗户看这个。
        </p>
        <p>Oh, I don’t know how to explain what Waltz did. Well, one way is to do an example. And I think I will hazard
            an example. Let’s see. Let me find some space. I think I’m reduced to going over here. But that will be
            convenient, since the line labels are here. Here’s my example. And you say, how can I give you just part of
            a picture? Well, you can assume I’m looking at this through a window.</p>
        <p>因此，窗户的边缘形成了边界线，它们对后面的东西没有任何限制。所以这是一幅值得思考的合法画作。</p>
        <p>So the edge of the window form boundary lines, and they exert no constraint whatsoever on what’s behind them.
            So this is a legitimate drawing to have to think about.</p>
        <h2 id="walters-algorithm">Walters 算法</h2>
        <h2>Walters Algorithm</h2>
        <p>顺便问一下，这是否含糊不清？或者您是否确信所有这些台词都有独特的解释？我认为所有这些台词都有独特的解释。</p>
        <p>By the way, is this ambiguous? Or do you get a firm sense that there’s a unique interpretation of all those
            lines? I think there’s a unique interpretation of all those lines.</p>
        <p>我实际上打算使用霍夫曼标签和华尔兹方法解决这个问题。因为我无法在黑板上模拟具有 50
            种线型和数千个线连接点的东西。所以我将使用霍夫曼集来演示华尔兹算法。华尔兹算法涉及首先将所有可能的标签放在某个连接点上，以便从中得出答案。</p>
        <p>What I’m going to do is I’m actually going to solve this problem using Huffman’s labels but Waltz’s method.
            Because I can’t simulate on the blackboard something with 50 line types and thousands of line junctions. So
            I’m going to use Huffman’s set to demonstrate Waltz’s algorithm. So Waltz’s algorithm involves starting out
            by plopping on some junction all of the possible labels that the answer has to be drawn from.</p>
        <p>让我按照我们要访问的顺序对它们进行编号。像这样。到目前为止，我刚刚写下了位于第一个交叉口的三个分叉选项。我必须记下它们对从交叉口出来的线路的具体作用。让我们看看。我把它们抄下来。一种可能是这个。另一种可能是这个。还有一种可能是加、加、减。
        </p>
        <p>So let me number these in the order that we’re going to visit them. Like so. And so far, I’ve just put down
            the three fork options that are resident on that first junction. And I have to take note of exactly what
            they do with the lines that come out of the junction. So let’s see. I’ll just copy them down. One
            possibility is this one. Another possibility is this one. And another possibility is plus, plus, minus.</p>
        <p>哎呀，我得到了加号，加号。不，对。那么到目前为止对吗？我所做的只是从我的库中复制了连接点标签。此时，Waltz
            算法表示除了继续进行第二个连接点之外没有其他事情可做。不幸的是，不幸的是，在第二个连接点上有很多标签需要考虑。其中六个。1、2、3、4、5、6。所以其中一个看起来像那样。另一个看起来像那样。</p>
        <p>Oops, I’ve got plus, plus. No, that’s right. So that right so far? All I’ve done is copy the junction
            labelings from my library. And at this point, Waltz’s algorithm says there’s nothing else to do but go on to
            junction number two. And unfortunately, sadly, there are lots of labelings that have to be considered on
            junction number 2. Six of them. 1,2.3,4.5,6. So one of them looks like that. Another one looks like that.
        </p>
        <p>其中一个是加号，箭头朝内。另一个是加号，箭头朝外。另一个是减号，箭头朝下。还有减号，箭头朝上。我想我已经复制了它们。但是现在，复制完它们后，Waltz
            算法会查看相邻的交叉点，并说，我刚刚在第二个交叉点上放置的任何东西是否被我已经在相邻交叉点上放置的东西所禁止？</p>
        <p>One of them is plus here, arrow in. Another one is plus here, arrow out. Another one is minus here, arrow
            down. And minus here, arrow up. I think I’ve copied those all right. But now, having copied those down,
            Waltz’s algorithm looks around at the neighboring junctions and says, are any of the things that I just
            placed on junction two disallowed by what I’ve already placed on a neighboring junction?</p>
        <p>因此，它在第二步中查看了这里。它发现这三个箭头要求连接 1 和 2 的线为负或正。因此，在六种可能性中，我只能保留那些同样满足于在连接两者的线上放置正号的那些。</p>
        <p>So it looks over here in step number two. And it sees that these three arrows require the line that joins
            junctions 1 and 2 to be either minus or plus. So of the six possibilities, I can only keep the ones that are
            likewise content to put a plus on that line that joins the two.</p>
        <p>所以这意味着从连接点 1 流出的影响会消除那个，消除那个，消除那个，消除那个。所以一半都消失了。所有试图在 1 和 2
            之间的那条线上划一条边界线的行为都是不允许的。好吗？现在同样，我们可以说，好吧，在剩下的那些中，它们会限制我在连接点 1 处可以做的事情吗？让我们看看。这是一个减号。</p>
        <p>So that means that the influence flowing from junction 1 eliminates that one, eliminates that one, eliminates
            that one, and eliminates that one. So half of them are gone. All the ones that try to put a boundary line on
            that line between 1 and 2 are disallowed. All right? Now likewise, we could say, well, of the remaining
            ones, do they restrict what I can do at junction 1? So let’s see. Here’s a minus.</p>
        <p>这里还有一个优点。所以这里所有这些可能性仍然存在。现在，继续，我们必须看看我们可以在 3 号路口做什么。这些又是箭头标签。所以我们必须复制与之前完全相同的标签集。现在我们看看 2
            号路口，然后说，好吧，这对我刚刚放置的三个路口有什么启示？</p>
        <p>And here’s a plus. So all these possibilities over here are still alive. So now, continuing on, we have to
            see what we can do at junction 3. These are arrow labels again. So we have to copy exactly the same labels
            set as we had before. And now we look down at junction 2 and say, well, what does that tell me about the
            three that I’ve just placed?</p>
        <p>如果我们从 2
            号交叉点往上看，看看它在这里施加了什么样的约束，我们可以看到这个还活着，这个还活着。我想我们已经排除了六个中的四个。所以我们还有这两个还活着。它们都是边界线。我想我一定是弄错了这条边界线，对吧？不，那是对的。哦，是的，我明白了。另外。这个可以。等一下。你让我做错了什么。
        </p>
        <p>If we look up from junction 2 to see what kind of constraints it puts on here, we have this one alive and
            this one alive. I guess we’ve eliminated four of the six. So we have these two alive. And they both but
            boundary lines. I think I must have had this boundary line wrong, right? No, that’s right. Oh yes, I see.
            Plus. This one goes. hang on a second. You let me do something wrong.</p>
        <p>所以加号被排除了。那一定是一个。减号上升。哦，是的。我太急了，不确定自己在做什么。那么让我们看看。这个家伙有一个向下的边界。这个家伙有一个向上的边界。所有其他的都被消除了。所以这意味着试图在那里画一条凹线的东西消失了。试图在那里画一条加号的东西也消失了。
        </p>
        <p>So plus is out. And that must be one that goes. this minus goes up. Oh, yes. I’m too hasty and uncertain
            about what I was doing. So let’s see. This guy has a boundary going down. And this guy has a boundary going
            up. All of the others have been eliminated. So that means that something that tries to put a concave line
            there is gone. And something that tries to put a plus line there is gone.</p>
        <p>因此，第三步中沿此方向向上流动的影响会消灭那个人，然后消灭那个人，只留下这个人。但是现在，我担心的是，此时你还必须下降到 2，看看根据这里 3
            号交叉口发生的情况，那里是否还有任何可以存活下来的限制。现在让我们看看。这个向上，与幸存者兼容。</p>
        <p>So the influence flowing up in this direction in the third step eliminates that guy and eliminates that guy,
            leaving only this guy. But now, the thing I was worried about is you have to also at this point go down to 2
            and see if there’s any further constraint on what can survive down there, based on what has happened over
            here at junction 3. Now let’s see. This one goes up, which is compatible with a survivor.</p>
        <p>但是这个倒下了，与幸存者不相容。所以当我在第三步把它放下来时，这个人就被淘汰了。所以现在我只剩下一个解释，即在顶点 2 上会发生什么。还有一种解释，即在顶点 3 上会发生什么。现在让我们看看。这可以传播。</p>
        <p>But this one goes down, which is not compatible with a survivor. So when I bring this down in step three,
            this guy is eliminated. So now I’m down to just one interpretation for what can be going on at vertex number
            2. And one interpretation for vertex number 3. Now let’s see. This can propagate.</p>
        <p>所以现在我已经对顶点 2 进行了更改，我还必须查看这是否会导致顶点 1
            发生变化。所以它会传播。现在我可以看到这里唯一的可能性是减号，即从我们的幸存者那里传下来的标签。所以这排除了这两个。呼！手动完成这项工作很难，但我已经标记了四个事物中的三个。</p>
        <p>So now that I’ve made a change on vertex number 2, I have to also see if that causes a change at vertex
            number 1. So it’s propagating through. And now I can see that the only possibility here is a minus, the
            label that’s coming down from our survivor. So that eliminates these two. Whew! It’s hard to do this by
            hand, but I’ve got three of the four things labeled.</p>
        <p>即使只标注了四个中的三个，我也能对所有交叉点及其之间的线做出单一解释。所以还剩一个。我们必须处理那个岔路顶点。我们最好处理它，因为据我们所知，这在世界上不是合法的绘图。我们有五个岔路顶点要放置。但你知道吗？</p>
        <p>And even with just three of the four labeled, I’m down to a single interpretation for all of the junctions
            and the lines between them. So there’s one left. We have to deal with that fork vertex. We better deal with
            it, because for all we know, this is not a legal drawing in this world. We have five fork vertexes to place.
            But you know what?</p>
        <p>我不需要在这里画太多，因为我知道现在这个必须为正数。现在这个必须为正数。只有一个分叉顶点带有正数。所以现在我可以说，好吧，唯一可能的幸存者就是这个。这些都消失了。现在我对所有交叉点都有了解释。我看到获胜者就是这个。
        </p>
        <p>I don’t have to draw much here, because I know this is forced to be a plus now. And this is forced to be a
            plus now. And there’s only one fork vertex with any pluses on it at all. So now I can come through and say,
            well, the only possible survivor is this one. These are gone. And now I have an interpretation for all of
            the junctions. And I see that the winners are this one.</p>
        <p>还有这个。还有这个。还有这个。所以我有一个独特的解释。这条线是凸的。这条是凹的。这是边界。那是边界。这条线是凸的，那是凸的。现在这需要做很多工作。所以我最好检查一下，确保我做对了。你想看看这个演示，以确保我没有犯错。我很确定。让我们看看。就是这样？
        </p>
        <p>And this one. And this one. And this one. So I’ve got a unique interpretation. This line is convex. This one
            is concave. This is a boundary. That’s a boundary. And this line is convex, and that’s convex. Now that’s a
            lot of work. So I better check and make sure I got it right. You’d like to see this demonstrated to make
            sure I haven’t made a mistake. I’m sure of that. Let’s see. That it?</p>
        <p>因此，每条线被遮挡的地方都有四种可能性，标记为 E。箭头交叉点标记为 A。分叉路口有五个。在分叉路口
            5。所以让我们一步步走过去看看会发生什么。砰。我成功了。我做对了。让我们再试几次。你认为这个会发生什么？独特的解决方案？它停止了。我的程序中有错误？不可想象。发生了什么？</p>
        <p>So each of the places where a line is obscured has four possibilities, labeled E. The arrow junctions are
            labeled A. The forks. there are five of them. at the fork junction 5. So let’s just step through here and
            see what happens. Boom. I’ve got it. I did do it right. So let’s try some more. What do you think will
            happen with this one? Unique solution? It stopped. Bug in my program? Unthinkable. What’s happened?</p>
        <p>它确实很模糊。它可以是从天花板垂下来的东西。或者它可以是我们可以想象成从左到右向上的台阶。让我们看一些更复杂的东西。你认为它会起作用吗？没有足够的约束让我们弄清楚。它同样模糊，但例子要大一些。这个怎么样？</p>
        <p>It is genuinely ambiguous. It can be something hanging down from the ceiling. Or it could be something that
            we can think of as a step going up from left to right. Let’s look at something more complicated. You think
            it’ll work? Not enough constraint for us to figure that one out. It’s equally ambiguous, but a little bit
            larger example. What about this one?</p>
        <p>是的，但东西从左下角向上爬到右上角。是的，宾果。成功了。它很明确。它是我们之前讨论过的同一主题的变奏。但让我，只是为了好玩，把这两行去掉。你认为现在会发生什么？似乎一切顺利，直到它到达右上角并发现它无法标记东西。所以它又向下传播。
        </p>
        <p>Yeah, but the stuff is creeping up from the lower left up to the upper right. Yeah, bingo. It worked. It’s
            unambiguous. It’s variation on the same theme we had before. But let me, just for fun, take these two lines
            out. What do you think will happen now? Seems to be doing just fine until it hits the upper right hand
            corner and discovers it can’t label stuff. So it propagates back down.</p>
        <p>左下角看起来还行，但其实并不好。所以这些结果与我们人类看到这类东西时的反应一致。所以很可能我们的大脑中确实有一些我们在视觉中使用的约束传播装置。</p>
        <p>And what looked OK in the lower left is no good after all. So these results are kind of consistent with what
            we humans do when we look at these kinds of things. So it’s very likely that we, in our heads, do have some
            constraint propagation apparatus that we use in vision.</p>
        <p>但撇开这一点不谈，我们可以考虑其他与人类不同的智能，它们可能会使用这种机制来解决在寻找解决方案时涉及很多约束的问题。所以在这里，我们看到了约束传播活动在线条绘制分析中的应用。但下次，我们将要讨论的是它在地图着色中的应用。谁会关心地图着色？做调度的人会关心，因为那也是同样的问题。
        </p>
        <p>But putting that aside, we can think about other kinds of intelligence different from human, that might use
            this kind of mechanism to solve problems that involve a lot of constraint in finding a solution. So here, we
            saw the constraint propagation activity at work on line drawing analysis. But next time, what we’re going do
            is we’re going to see at work in map coloring. And who cares about map coloring? People who do scheduling,
            because that turns out to be the same problem.</p>
        <h1 id="constraints-search-domain-reduction">8. 约束：搜索、领域缩减</h1>
        <h1>8. Constraints: Search, Domain Reduction</h1>
        <p><img
                src="data:image/jpeg;base64,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">3
            年前 (2021 年 4 月 24 日) — 45:06 <a
                href="https://youtube.com/watch?v=dARl_gGrS4o">https://youtube.com/watch?v=dARl_gGrS4o</a></p>
        <p> 3 years ago (Apr 24, 2021) — 45:06 <a
                href="https://youtube.com/watch?v=dARl_gGrS4o">https://youtube.com/watch?v=dARl_gGrS4o</a></p>
        <h2 id="unknown-94">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：从某种意义上说，我们不能把所有东西都涂成黑色，这太糟糕了，因为这个地图着色问题肯定会容易得多。所以我不知道我们该怎么做。这要花多长时间？这就是我们要做的。我们要等到所有笔记本电脑都关闭，或者这个程序终止，以先发生者为准。所以这要花多长时间？我真的不知道。
        </p>
        <p>PATRICK WINSTON: It’s too bad, in a way, that we can’t paint everything black, because this map coloring
            problem sure would be a lot easier. So I don’t know what we’re going to do about that. How long is this
            going to take? Here’s what we’re going to do. We’re going to wait till either all the laptops are closed, or
            this program terminates, whichever comes first. So how long is this going to take? I actually don’t know.
        </p>
        <p>我认为它需要的时间比宇宙的寿命还要长，但我也不确定。所以让我们看一个稍微简单一点的地图着色问题。我们将停止这个问题，将地图改为我所说的“有 26
            个州，每个州代表字母表中的每一个字母”。我们要做的是对这张地图进行深度优先搜索，以找到合适的着色方法。</p>
        <p>I think it’ll take more than the life time of the universe, but I’m not sure. So let’s take a look at a
            slightly easier map coloring problem. We’ll stop this one and change the map to something I call There’s 26
            states, one for each letter in the alphabet. And what we’re going to do is we’re going to do a depth first
            search for a suitable coloring of this map.</p>
        <p>我们将按顺序进行，A、B、C、D、E。正如我在这里建议的那样，我们将在每个级别上轮换颜色选择，这样我们就不会过度使用任何一种颜色。因此，如果我们启动这个特定的搜索，这个深度优先尝试为这张地图着色。就这样。也许我们应该等到它终止。或者也许我们应该让它运行，也许它会在讲座期间的某个时间终止。
        </p>
        <p>We’re going to go in order, A, B, C, D, E. And as I’ve suggested here, at each level we’re going to rotate
            the color choices so we don’t over use any one color. So if we launch this particular search, this depth
            first attempt to color this map. There it goes. And maybe we should wait until it terminates. Or maybe we
            should just let it run and it’ll terminate, perhaps, sometime within the lecture.</p>
        <h2 id="unknown-95">未知</h2>
        <h2>Unknown</h2>
        <p>或者我们应该让它在周末运行。或者你认为如果我们想回来观看它终止，我们需要等待多长时间？大约每秒 30 帧。我正在脑子里计算这一切，我不想拿我的生命打赌。但我认为大约需要 5,000 年。</p>
        <p>Or maybe we should just let it run over the weekend. Or how long do you think we would have to wait, if we
            want to come back and watch it terminate? At roughly 30 frames a second. I’m calculating it all in my head,
            I don’t want to bet my life on it. but I think about 5,000 years.</p>
        <p>如果我们想使用美国所有州，那么在这个演示中，你会得到像 10 的 17
            倍这样的数字。17、18、16，我不太确定。我做了一个粗略的计算。当然，你可以做一些类比，这并不像国际象棋那么糟糕，你需要宇宙中的所有原子，但你仍然做不到，诸如此类。</p>
        <p>And if we want to use as many states as there are in the United States, in this demonstration, you get up to
            numbers like 10 to the 17th years. 17th, 18th, 16th, I’m not exactly sure. I did a rough calculation. And,
            of course, you could do some parallels into that, and it’s not as bad as chess, where you need all the atoms
            in the universe and you still can’t do it, and things like that.</p>
        <p>充当计算机。但这仍然非常可怕。问题是，这个问题可以通过我放在我身后的这张图来说明。如果你进行深度优先搜索，就会遇到像德克萨斯这样的问题。如果你标记。</p>
        <p>Acting as computers. But still, it’s pretty horrendous. The problem is, well the problem is illustrated by
            this diagram I put in back of me. If you do a depth first search and you have a problem like Texas. If you
            label.</p>
        <h2 id="unknown-96">未知</h2>
        <h2>Unknown</h2>
        <p>如果你先为亚利桑那州、俄克拉荷马州、阿肯色州和路易斯安那州分配颜色，然后最后才为德克萨斯州分配颜色，那么你就会在第四个选择时陷入困境，直到第 48
            个选择你才会发现这一点。那么接下来会发生什么呢？你开始像这样开发这棵树，你会到达德克萨斯州周围的那些州。德克萨斯州是最后一个被分配颜色的州，而且已经没有颜色了。</p>
        <p>If you assign a color to Arizona, Oklahoma, Arkansas, and Louisiana first, and then wait around to Texas
            last, then you get yourself into a bind by your fourth choice, that you don’t discover until you’re 48th
            choice. So what happens then, is that you start developing this tree like this, you get to those states
            surrounding Texas. Texas is last state assigned a color and there’s nothing left for it.</p>
        <p>而这个问题出现在第四个选择上，而你直到第五十个选择才发现它。所以你进行了一次可怕的、不可能的搜索。所以你根本不能用那种方式做。但现在不用担心。我已经掌握了约束传播的概念。所以我们可以取一个有四个州的国家，就像这样。每个州都可以标记为红色、绿色、蓝色和黄色。
        </p>
        <p>And that problem was there on the fourth choice, and you don’t discover it until your 50th choice. So you
            develop a horrendous, impossible, impossible search. So you simply can’t do it that way. But now, not to
            worry. I’ve come equipped with the idea of constraint propagation. So we could just take a country with four
            states, like this. Each can be labeled red, green, blue and yellow.</p>
        <p>因此，就像在画线图时一样，我们会将所有可能的值堆积起来。红色、绿色、蓝色和黄色。红色、绿色、蓝色和黄色。然后我们开始约束传播。因此，对于左上角的状态，我们说，有任何理由相信 R
            是不可能的吗？好吧，我们看看我们的邻居，看看从他们那里流入了什么样的约束。</p>
        <p>So just like in a case of line drawings, we’ll pile up all the possible things that the value can be. red,
            green, blue and yellow. Red, green, blue and yellow. And we start up constraint propagation. So we say for
            the upper left hand corner state, is there any reason to believe that R is impossible? Well we look at our
            neighbors and see what kind of constraint flows in from them.</p>
        <h2 id="unknown-97">未知</h2>
        <h2>Unknown</h2>
        <p>当然，这个人可能是绿色的，这个人可能是蓝色的。它们不一定是红色的，所以没有什么可以排除红色。也没有任何东西可以排除蓝色。也没有任何东西可以排除黄色。也没有任何东西可以排除绿色。所以约束传播只是坐在那里，什么也不做。所以看起来我们既不能使用深度优先搜索，也不能使用约束传播。
        </p>
        <p>And sure, this guy could be green, and this guy could be blue. They don’t have to be red, so there’s nothing
            that rules out red. And there’s nothing the rules out blue. And there’s nothing that rules out yellow. And
            there’s nothing that rules out green. So constraint propagation just sits there with its finger up its nose,
            doing nothing. So it doesn’t look like we can use either depth first search, or constraint propagation.</p>
        <p>所以，我们只能放弃，然后哭诉。但也许还有其他方法会有所帮助。那么，让我来实际解决一下德克萨斯州的问题。对不起休斯顿和泰勒，这是德克萨斯州的地图。我们在那里。这里，大致来说，亚利桑那州就在这边的某个地方。俄克拉荷马州也在那里。还有老比尔·克林顿的州阿肯色州。然后路易斯安那州在那里稍微突出一点。
        </p>
        <p>So we could just give up and cry. But maybe there’s some other approach that will help. So let me actually
            work the Texas problem. So with apologies to Houston and Tyler, here’s a map of Texas. There we are. And
            here’s, roughly speaking, Arizona is over here somewhere. And we’ve got Oklahoma in there. And old Bill
            Clinton’s state, Arkansas. And then Louisiana sticks out there a little bit.</p>
        <p>这是该国特定地区的地图。这里有亚利桑那州、俄克拉荷马州、阿肯色州、路易斯安那州和德克萨斯州。我们已选择按顺序为这些州分配颜色。所以这是一、二、三、四。我们将这样做。我们将轮换颜色选择，这样我们就不会过度使用任何一种颜色。
        </p>
        <p>So there’s our map of that particular part of the country. So we’ve got Arizona here, Oklahoma here, Arkansas
            here, and Louisiana here, and Texas here. And we have elected to assign colors to these states, in that
            order. So this is one, two, three, four. And we’re going to do that. We’re going to rotate our color
            choices, just so we don’t over use any one color.</p>
        <h2 id="unknown-98">未知</h2>
        <h2>Unknown</h2>
        <p>但是我们还要看看德克萨斯州。因为德克萨斯州是与他们选择颜色的州接壤的州。所以德克萨斯州唯一可能的颜色是红色、绿色、蓝色和黄色。所以当我们在这里做出选择时，我们会说。我们不必遵循任何特定的风格。我们可以说亚利桑那州将获得红色，R。
        </p>
        <p>But we’re going to also have a look at Texas as we go around. Because Texas is a state that borders on the
            States they were choosing colors for. So the only possible colors that Texas could be are red, green, blue,
            and yellow. So as we make our choices around here, we’ll say that. we don’t have to adhere to any particular
            style. we can say that Arizona is going to get the colored red, R.</p>
        <p>这将排除德克萨斯州的 R
            色，因为相邻的州不能有相同的颜色。然后我们转到俄克拉荷马州，我们轮换颜色选择，所以我们会说它可以是绿色。这很好，因为它与这里的红色一致，但它排除了德克萨斯州可能是绿色的可能性。然后我们转到阿肯色州，红色、绿色、蓝色。
        </p>
        <p>That’s going to rule out R over here for Texas, because no adjacent states can have the same color. Then we
            go over to Oklahoma, and we’re rotating our color choices, so we’ll say that can be green. And that’s fine,
            because it’s consistent with the red here, but it rules out the possibility that Texas could be green. And
            then we go over here to Arkansas, red, green, blue.</p>
        <p>没问题，这与俄克拉荷马州的绿色一致，但如果我们看看它的邻居，我们就知道德克萨斯州现在永远不能使用蓝色。所以现在我们来看看路易斯安那州，记住，我们正在轮换我们的颜色选择，因为我们不想过度使用它们。所以这意味着我们在这里为路易斯安那州做出的第一个选择是黄色。
        </p>
        <p>That’s fine, that’s consistent with the green on Oklahoma, but if we look at its neighbors we know that Texas
            is forever forbidden to be blue, now. So now we go over to Louisiana, and remember, we’re rotating our color
            choices because we don’t want to overuse them. So this means that the first choice we’re going to make here,
            for Louisiana, is yellow.</p>
        <h2 id="unknown-99">未知</h2>
        <h2>Unknown</h2>
        <p>这很好，因为它与阿肯色州一致，但这并不好，因为现在已经排除了德克萨斯州的最后可能性。因此，尽管德克萨斯州将成为我们着色的第 48
            个州，但我们会说，目前没有必要继续下去。我们最好后退。因为当我们开始着色时，德克萨斯州已经没有剩余的了。所以这意味着这里的黄色被排除了。</p>
        <p>And that’s fine because it’s consistent with Arkansas, but it’s not so fine because it’s now ruled out the
            last possibility for Texas. So even though Texas is going to be the 48th state that we color, we’re going to
            say, at this point, there’s no need in going on. We’d better back up. Because there’s nothing left for Texas
            when we get around to coloring it. So that means that this yellow is ruled out here.</p>
        <p>黄色再次出现。我们选择路易斯安那州的下一种颜色，它恰好是红色。现在它与德克萨斯州剩下的黄色一致。它也与阿肯色州的蓝色一致。这很酷。我想知道我们是否可以用它制作一个算法来解决这样的问题。你明白我们在做什么的直觉吗？
        </p>
        <p>This yellow reappears. We select the next color in my line for Louisiana, which happens to be red. And now
            that’s consistent with this yellow that’s still left for Texas. And it’s also consistent with the blue
            that’s up here for Arkansas. So that’s cool. I wonder if maybe we could make an algorithm out of that, and
            solve problems like this. Do you see, sort of, the intuition of what we’re doing?</p>
        <p>我们实际上又运用了武术原理。因为整个问题在于局部约束，未被发现的局部约束，正在导致下游问题。所以我们将利用敌人的力量来对付他，我们将在行动过程中审视这些局部约束，确保它们不会顺流而下，不会在以后困扰我们。</p>
        <p>We’re actually using the martial arts principle, again. Because the whole problem is that local constraints,
            undiscovered local constraints, are causing downstream problems. So we’re going to use the enemy’s powers
            against him, and we’re going to look at those local constraints as we go and make sure they’re not going
            downstream, not going to get us later on.</p>
        <h2 id="unknown-100">未知</h2>
        <h2>Unknown</h2>
        <p>现在我看起来有点正式了，但其实我更正式一点。因为我想用一些语言来描述正在发生的事情，这样选择就一目了然了。所以首先，我们必须有一些词汇。让我们从这里开始我们的词汇。我们将有一个变量 v 的概念。</p>
        <p>So now I’m going to look like I’m getting a little formal, but I’m just getting a little bit more formal.
            Because I want to have some language that I can use to describe what’s going on, so that it’s clear what the
            choices are. So to start off with, we’re going to have to have some vocabulary. So let’s start up our
            vocabulary here. We’re going to have a notion of a variable v.</p>
        <p>那是可以赋值的。这没什么复杂的。还有一个值。值 x 是可以赋值的。这有点循环，但我们都是计算机科学专业的，所以你知道我的意思。所以下一个东西有点不太明显，那就是域 d
            的概念。这将是一个值包。好的。还有一件事。一个约束。</p>
        <p>And that’s something that can have an assignment. There’s nothing complicated about that. And a value. A
            value x is something that can be in assignment. It’s a little bit circular, but we’re all in computer
            science so you know what I mean. So the next thing is a little slightly less obvious, and that’s the notion
            of a domain, d.&nbsp;And that’s going to be a bag of values. OK. So one more thing. A constraint.</p>
        <p>这是约束 c，是限制。在我们的例子中，它主要是变量对，变量值对。但一般来说，它可能是变量值。所以如果我们回到德克萨斯州，我们可以说，好吧，我们的词汇表如何覆盖该配置？答案是，州充当变量的角色，颜色充当值的角色。</p>
        <p>That’s a constraint c, is a limit on. in our examples it’s mostly going to be pairs of variables, pairs of
            variable values. But in general, it could be variable values. So if we go back here to Texas we could say,
            OK, how does our vocabulary drape itself over that configuration? And the answer is, the states have the
            role of variables, the colors have the role of values.</p>
        <h2 id="unknown-101">未知</h2>
        <h2>Unknown</h2>
        <p>域是我们仍可在特定州使用的剩余颜色可能性。在这种情况下，约束是简单的地图着色约束，即共享边界的州不能具有相同的颜色。因此，州是变量，颜色是值，域是颜色包，约束只有一个。相邻州不能具有相同的颜色。这就是它与这个词汇表的契合之处。
        </p>
        <p>And the domains are the remaining color possibilities that we can still use on a particular state. And the
            constraint, in this case, is the simple map coloring constraint that no states that share a boundary can
            have the same color. So states are variables, colors are values, domains are bags of colors, and
            constraints. there’s only one. adjacent states can’t have the same color. So that’s how it fits with this
            vocabulary.</p>
        <p>那么现在，我在这里到底做了什么？好吧，我在这里到底做了什么，我现在要通过将其以伪代码写下来，稍微形式化一下。所以在这里，我们将看看我们在这里凭直觉做了什么，并将其简化为一个过程。这是程序。记住，我们正在对这些东西进行深度优先搜索。我做了深度优先搜索。
        </p>
        <p>So now, what did I actually do here? Well what I actually did here, I’m going to now formalize a little by
            writing it down in pseudo code. So here we are, we’re going to have a look at what we did here with our
            intuition, and we’re going to reduce it to a procedure. And here’s the procedure. Remember, we’re doing
            depth first search on this stuff. I did a depth first search.</p>
        <p>我们将进行深度优先搜索，对于每个深度优先搜索任务。好的，我在这里，我标记了亚利桑那州，然后是俄克拉荷马州，然后是阿肯色州，然后是路易斯安那州。当我给每个州贴上标签、颜色时，我将执行此过程。每次我进行这些任务之一时。最后一个引起麻烦的是将路易斯安那州涂成黄色。
        </p>
        <p>We’re going to do a depth first search, and for each depth first search assignment. OK, so here I am, I’m
            labeling Arizona, and then Oklahoma, and then Arkansas, and then Louisiana. When I give each one of those a
            label, a color, I’m going to do this procedure. Every time I make one of those assignments. The last one
            that caused trouble was coloring Louisiana yellow.</p>
        <h2 id="unknown-102">未知</h2>
        <h2>Unknown</h2>
        <p>每次我把其中一种颜色放下来，每次我分配任务时，我都会执行此过程。因此，对于每个深度优先搜索任务，对于每个变量
            v，我都会考虑。现在你不知道我所说的考虑是什么意思。但是当我贴上标签，当我设置一个值，将某物显示为路易斯安那州的颜色时，我想到的是德克萨斯州。所以当我为路易斯安那州分配任务时，我正在考虑德克萨斯州这个变量。</p>
        <p>Each time I put one of those colors down, each time I make an assignment, I’m going to do this procedure. So
            for each depth first search assignment, for each variable v, considered. Now you don’t know what I mean by
            considered. But when I put a label, when I put up a value, show something as a color for Louisiana, I
            thought about Texas. So I was considering the variable, Texas, when I made the assignment for Louisiana.</p>
        <p>现在，我要稍微模糊一下我所说的考虑的含义。因为对于实际考虑的内容，有很多选择。所以我只说考虑，然后我们稍后再讨论选项，因此对于考虑的每个变量 v。让我们将该变量称为 v sub i。</p>
        <p>Now I’m going to be a little bit vague about what I mean by considered, right now. Because there are lots of
            choices about how much stuff you actually consider. So let me just say consider, and then we’ll open that up
            and talk about the options in a moment, so for each variable v considered for. let’s call that variable v
            sub i.</p>
        <p>对于每个 x sub i，对于该变量域中的每个值，考虑每个仍然存在的事物。对于其中每个事物，对于每个约束 c，它位于 x sub i 和某个 x sub j 之间，其中 x sub j 是 j 域的一个元素。</p>
        <p>For each x sub i, for each value in the domain of that variable, consider each of the things that still
            surviving. For each of those, for each constraint c, that’s between x sub i, and some x sub j, where x sub j
            is an element of the domain of j.</p>
        <h2 id="unknown-103">未知</h2>
        <h2>Unknown</h2>
        <p>这听起来很花哨，但这只是说，就德克萨斯州而言，每当我考虑德克萨斯州的剩余选择值之一时，我都想考虑该变量与相邻州之间的所有约束。我想确保我留在域中的任何内容对于其他州的某些选择、其他州的某些剩余选择都是可行的。</p>
        <p>Now that sounds awfully fancy, but this just says, in the case of Texas up there, whenever I consider one of
            the values that’s still remaining as a choice for Texas, I want to consider all of the constraints between
            that variable and the adjacent states. And I want to be sure that anything I leave in the domain is OK for
            some selection in the other states, some remaining choices in the other states.</p>
        <p>这就是为什么我们在这里嵌套得很好。但我们在进行深度优先搜索。我们正在考虑某个变量集合中的变量。对于每个变量，考虑仍然保留在这些变量域中的所有值。然后对于每个值，我们检查它是否满足某些约束，是否满足对其施加的约束。
        </p>
        <p>So that’s why we’re getting pretty nested here. But we’re doing depth first search. We are considering the
            variables in a certain collection of variables. For each one of those where considering all the values that
            still remain in the domains of those variables. And then for each of those values, we’re checking to see if
            it satisfies this some constraint, satisfies the constraint that are placed upon it.</p>
        <p>因此，对于每个约束，如果不存在 x sub j，使得 x sub I 和 x sub j
            之间的约束得到满足，那么如果在那个相邻位置没有与值一致的东西，那么我们必须摆脱它。如果这是真的。如果相邻变量中不存在某个值，使得约束得到满足，我们就完蛋了。</p>
        <p>So for each of these constraints if there does not exist an x sub j, such that, the constraint between x sub
            I and x sub j is satisfied, well if in that adjacent place there’s nothing that is consistent with a value,
            then we’ve got to get rid of it. If that’s true. If there does not exist some value in an adjacent variable
            such that constraint is satisfied, we’re hosed.</p>
        <h2 id="unknown-104">未知</h2>
        <h2>Unknown</h2>
        <p>我们必须去掉那个值。所以我们要从 d sub i 中删除 x sub
            I。好的。现在，没问题。这有点像我对德克萨斯州所做的。当我为路易斯安那州输入一个值时，我说，那么德克萨斯州的可能值是什么？红色、绿色、蓝色和黄色。让我们考虑红色。让我们考虑德克萨斯州和所有相邻州之间的约束。</p>
        <p>We’ve got to get rid of that value. So we’re going to remove x sub I from d sub i. OK. Now, that’s fine.
            That’s sort of what I did with Texas. As soon as I plopped down a value for Louisiana I said, well what are
            the possible values in Texas? Red, green, blue and yellow. Let’s consider red. Let’s consider the
            constraints between Texas and all adjacent states.</p>
        <p>其中一个约束规定它不能与亚利桑那州的颜色相同。由于我已经分配了亚利桑那州的颜色，所以我唯一可以为亚利桑那州选择的颜色是红色。红色与红色不一致，所以我必须去掉它。所以这看起来很复杂，但这只是直觉。那么如果我们遇到域为空的情况，我们该怎么办？
        </p>
        <p>One of those constraints says it can’t be the same color as Arizona. The only color I’ve got available for
            Arizona, since I’ve already made the assignment is red. Red is not consistent with red, so I’ve got to get
            rid of it. So it looks complicated, but it’s just intuition. So what do we do if we get to a situation where
            the domain is empty?</p>
        <p>这意味着，无论何时我们开始对它进行分配，都不会剩下任何东西。所以，如果发生这种情况，如果域变为空，那么我们该怎么办？我们必须备份。所以直觉很清楚。这就是算法。当你研究算法时，想想它是否有意义，什么没有意义。它如何与德克萨斯州相适应。是的，它确实如此。
        </p>
        <p>That means whenever we get around to making assignment to it, there won’t be anything left. So if that ever
            happens, if the domain ever becomes empty, then what do we do? We’ve got to back up. So the intuition is
            clear. This is the algorithm. The algorithm when you work through it, think about whether it makes sense and
            what not. How it fits with Texas. Yeah, it sure does.</p>
        <h2 id="unknown-105">未知</h2>
        <h2>Unknown</h2>
        <p>我们所做的就是进行这些深度优先分配。在这些深度优先分配的邻域中，我们四处查看可能的值是否包含某些内容。如果它们不包含任何内容，我们就知道我们犯了不可挽回的错误，我们必须后退。这就是这个想法的本质。现在，它的效果如何？嗯，这有点取决于我们选择考虑什么。
        </p>
        <p>All we’re doing is we’re making these death first assignments. And in the neighborhood of those depth first
            assignments we’re looking around to see if the values that are possible include something. And if they don’t
            include anything, we know we made and irrevocable blunder, and we have to back up. So that’s the essence of
            the idea. Now, how well does it work? Well a little bit depends on what we choose for considered.</p>
        <p>我们考虑的选项有很多。所以让我列举一些选项，然后我们来看看它们的作用。哦，我想有一种可能性就是什么都不考虑。让我们试试看。所以我们的搜索类型将是不检查。你认为会发生什么？我们甚至不检查作业。这相当快。</p>
        <p>There are lots of choices for what we consider. So let me enumerate some of those choices and then we’ll have
            a look and see what they do. Oh I guess one possibility is to consider nothing. Let’s try it out. So our
            type of search is going to be no checks. What do you think is going to happen? We’re not even checking the
            assignment. That’s pretty fast.</p>
        <p>不幸的是，由于我们还没有检查最近的分配，我们得到很多地方，其中相邻的州具有相同的颜色。这不好。所以我们可以做的另一件事是考虑一切。这不好，因为这意味着，一旦我们为第一个州着色，我们就会检查以确保所有其他 47
            个州都可以着色。</p>
        <p>Unfortunately, since we haven’t even check the most recent assignment, we get lots of places where there are
            states that are adjacent to each other that have the same color. That’s no good. So another thing we can do
            is consider everything. That’s no good, because that would say, as soon as we color our first state, we
            check to make sure that all 47 other states can be colored.</p>
        <h2 id="unknown-106">未知</h2>
        <h2>Unknown</h2>
        <p>这似乎有点过了。但无论如何，至少我们要检查一下分配。所以，如果我们回到这里检查分配，让我们看看会发生什么。键入分配，只键入分配。砰。天哪，这就是我之前遇到麻烦的地方。这个东西将以纳秒或类似的速度运行 170 亿年。
        </p>
        <p>That seems like it over doing it a little bit. But in any case, at least we want to check the assignment. So
            if we go back here and check the assignment, let’s see what happens. Type assignments, assignments only.
            Boom. Aw, gees, that’s where I got in trouble before. This is the thing is going to run for 17 billion years
            at nanosecond or something like that.</p>
        <p>如果以纳秒级的速度运行，则只需要 10
            亿年，所以我想也许你可以这样做。有一台快速的计算机。但是，由于我们不幸地选择了德克萨斯州作为最后一个要考虑的州，并且不幸地将四个周围州的颜色放在了最前面，因此这行不通。而且我们不幸地决定旋转颜色，以避免过度使用任何一种颜色。所以这行不通。
        </p>
        <p>It’s only a billion years if you run it at nanosecond speed, so I guess maybe you could do that. Have a fast
            computer. But this isn’t going to work because of our unfortunate choice of Texas as the last state to be
            considered, and the unfortunate coloring of the four surrounding states right up front. And our unfortunate
            decision to rotate the color so as to avoid overdoing any one color. So this doesn’t work.</p>
        <p>我们知道，我们费了很大劲才用域缩减算法手工解决了与德克萨斯州的业务问题。最好记下这是域缩减算法。我们要做的就是检查分配的邻居。就像我们在这里做的一样。</p>
        <p>We know we went to the trouble of working out the business with Texas by hand, using the domain reduction
            algorithm. Better make a note that this is the domain reduction algorithm. And what we’re going to do is
            we’re going to check the neighbors of the assignments. Just like we did here.</p>
        <h2 id="unknown-107">未知</h2>
        <h2>Unknown</h2>
        <p>每次我们做出这四个选择之一时，我们都会检查德克萨斯州，因为它是我们做出的所有选择的州的邻居。所以要做的一件事是检查邻居。这是一、二、三，现在让我们看看会发生什么。只检查邻居，开始吧。我不知道。德克萨斯州没问题，对吧？因为它没有用所有四种颜色选择来给德克萨斯州周围的州上色。
        </p>
        <p>We checked Texas each time we made one of those four choices, because it’s a neighbor of all of the choices
            of the states that we made. So one thing to do is to check neighbors. This is one, two, three, now let’s see
            what happens. Check neighbors only, go. Shoot, I don’t know. It’s OK with Texas, right? Because it didn’t
            color the states around Texas with all of the four color choices.</p>
        <p>但它在其他地方仍然遇到麻烦。比如密苏里州、肯塔基州、弗吉尼亚州、田纳西州，这些州有很多边界。所以我不知道这是否会成功。哦，它终于成功了。不过，它付出了很多努力。为了比较，我们可以记下它遇到了 9,139
            个死胡同。但它确实做了一些好事。</p>
        <p>But it’s still getting into trouble in other places. Like the states like Missouri, Kentucky, Virginia,
            Tennessee, states with lots of boundaries. So I don’t know whether this is going to. oh, there it finally
            worked. It went through a lot of effort, though. For the sake of comparison, we might make a note that it
            ran into 9,139 dead ends. But it did do some good.</p>
        <p>它所花的时间并不比宇宙剩余部分更长。但如果检查邻居是个好主意，如果我们对邻居进行更改，这可能意味着我们还需要做什么？好吧，这可能意味着如果我们对邻居进行更改，我们也要检查它的邻居，确保它们都没有问题。所以另一个选择是传播。所以通过具有减少域的变量进行传播。
        </p>
        <p>It didn’t take a length of time longer than the remaining part of the universe. But if it’s a good idea to
            check the neighbors, if we make a change to the neighbors, what might that suggests that we do in addition?
            Well it might suggest that if we make a change to a neighbor, that we check its neighbors, too, make sure
            they’re all right. So another choice is to propagate. So propagate through variables with reduced domains.
        </p>
        <h2 id="unknown-108">未知</h2>
        <h2>Unknown</h2>
        <p>让我们看看它是如何工作的。等一下。我肯定犯了一个错误。让我们再试一次。哦，也许我们最好放慢速度。所有那些灰色的东西都表明了传播的极限。伙计，让我们看看 40 秒中的 4 秒，大约是 10
            秒。砰。不错。零死角。而且速度快了很多。</p>
        <p>Let’s see how that works. Wait a minute. I must have made a mistake. Let’s try that again. Oh, maybe we
            better slow it down. All that grey stuff is showing the limit of the propagation. Man it’s, let’s see at
            four second of 40, that’s about 10 seconds. Boom. Not bad. Zero dead ends. And it was a lot faster.</p>
        <p>我没注意到检查了其他东西的多少个约束，我认为大约是 20,000 个左右。这个少了很多。所以这看起来是个好主意。但我为什么把它标记为第五个？因为这个和第三个之间有一些东西。所以第四个是通过 v 将 d 减少到一个值。
        </p>
        <p>I didn’t happen to notice how many constraints were checked on that other thing, I think it was around 20,000
            or so. This is a lot less. So this looks like a good idea. But why did I label it number five? Well because
            there’s something between this and number three. So number four is, through v with d reduced to one value.
        </p>
        <p>因此，我们不会传播所有域缩小的变量。我们只会传播那些缩小幅度较大的变量，直到缩小到单个值。让我们看看这是如何工作的。有人想打赌吗？让我们看看。上次我们检查了 2,623 个约束。让我们看看这次会发生什么。</p>
        <p>So we’re not going to propagate through all of the variables which have their domains shrunk a little bit.
            We’re only going to propagate through those that have the greater shrinkage, all the way down to a single
            value. So let’s see how that might work. Anybody want to place any bets on this one? Let’s see. We checked
            2,623 constraints last time. Let’s see what happens this time.</p>
        <h2 id="unknown-109">未知</h2>
        <h2>Unknown</h2>
        <p>您可以看到灰色的范围较小，因为它没有传播到这么远。当我们屏息等待答案时，我想说我们已经找到了赢家。因为这确实有几个死胡同，但检查的约束数量少于
            1,000。所以一般来说，对于这样的问题，您可以考虑所有这些选择。您不想什么都不考虑，因为这样您就不符合您的约束条件。</p>
        <p>You can see that the extent of the gray is less, because it’s not propagating so far. And as we breathily
            await the answer, I’d say we’ve found our winner. As this does a couple of dead ends, but the number of
            constraint checked is less than 1,000. So in general, with problems this, you have all of these choices for
            what you consider. You don’t want to consider nothing, because then you’re not honoring your constraints.
        </p>
        <p>您当然要考虑刚刚分配的内容，否则您将构建一个违反约束的解决方案。您不想做所有事情，因为那太麻烦了。因此，检查邻居是个好主意，但在实践中总是更好。在实践中，不可避免地，最好通过您更改的内容进行一些传播。传播多少？</p>
        <p>You’ll certainly want to consider the things you just made assignments for, because otherwise you’ll
            construct a solution that violates a constraint. You don’t want to do everything, because that’s excessive
            work. And so checking the neighbors is a good idea, but it’s always better in practice. In practice,
            inevitably it’s the case that it’s better to do some propagation through the things that you’ve changed. How
            much propagation?</p>
        <p>传播刚刚改变的事物似乎没有多大用处。但传播已改变并被简化为单个值的事物似乎确实有好处。因此，只要您获得刚刚完成的某个分配的邻居，并且该分配的范围已简化为单个值，那么您也可以检查其邻居。</p>
        <p>It doesn’t seem to do much good to propagate through things are just changed. But it does seem to do some
            good to propagate through the things that have changed and been reduced to a single value. So as soon as you
            get a neighbor of some assignment you just made that has its domain reduced to a single value, then you
            check its neighbors, too.</p>
        <h2 id="unknown-110">未知</h2>
        <h2>Unknown</h2>
        <p>因此，只要您发现一个域正在减少，您就会检查邻居、邻居的邻居、邻居的邻居，等等等等。不仅要减少，还要减少到信号值。好吗？这就是需求减少算法。我保证您有这样的问题。我知道您还不知道如何解决这些问题，因为这有点抽象。</p>
        <p>So you check the neighbors, of the neighbors, of the neighbors, of the neighbors, on and on and on, as long
            as you’ve found a domain being reduced. And not only being reduced, but reduced to a signal value. All
            right? So that’s the demand reduction algorithm. And I guarantee you a problem like that. And I know you
            don’t know how to work those problems yet, because this is a little bit abstract.</p>
        <p>要在考试中解决这些问题，您需要了解一些如何跟踪域中剩余的变量值之类的知识。您将在复习、大型复习和教程中学到更多相关知识。所以我们可以回家了，但这里还有一些小花招需要处理。这些花招包括一些肮脏的小秘密。</p>
        <p>And to work these problems in the exam setting you need to know a little bit about how to keep track of the
            variable values that remain in the domain, and that sort of thing. And you’ll learn more about that in your
            recitations, and in this mega recitation, and in the tutorials. So we could go home except that there are
            few little flourishes to deal with here. And those flourishes, include some dirty, filthy little secrets.
        </p>
        <p>例如，作为课堂示例，我选择了德克萨斯州。并将这种情况安排得特别糟糕。所以我可以用不同的方式来安排各州。我们的州受到很大限制，周围有很多接壤的州。我们还有其他州，比如缅因州，只与另一个州接壤。所以我不知道。威尔，你怎么看？
        </p>
        <p>For example, I’ve chosen, as my classroom example, to pick on Texas. And arranged for this situation to be
            especially ugly. So I could arrange the states in a different way. We have highly constrained states, that
            have a lot of bordering states around them. And we have other states, like Maine, up there, that only
            borders on one other state. So I don’t know. Will, what do you think?</p>
        <h2 id="unknown-111">未知</h2>
        <h2>Unknown</h2>
        <p>我们应该按照从最不受限到最受限的顺序排列各州，还是按照从最受限到最受限的顺序排列各州？换句话说，我们应该从密苏里州、田纳西州、肯塔基州或类似的州开始吗？或者我们应该从缅因州开始？您觉得呢？您有 50%
            的机会回答正确，只需按点数即可。威尔：先从最受限的州开始。教授。</p>
        <p>Should we arrange the states for our death first search in the order of least constrained to most
            constrained, or most constrained to least constrained? In other words, should we start with Missouri, or
            Tennessee, or Kentucky, or something like that? Or should we start with Maine? What do you think? You have a
            50% chance of getting it right, just by at points. WILL: Start with the most first. PROF.</p>
        <p>帕特里克·温斯顿：他认为我们应该先从限制最多的开始。有没有志愿者想建议我们先从限制最少的开始？这就是我做事情的方式。我正在写一本书或类似的东西，我有 500
            件事情要解决，我总是选择先处理最简单的事情，这样我感觉我的清单就小了很多。把最难的事情留到最后。</p>
        <p>PATRICK WINSTON: He thinks we should start with the most constraint first. Do we have a volunteer who wants
            to suggest that we start with the least constraint first? That’s the way I always work on stuff. I’m working
            on a book or something, I have 500 things to fix, I’ll always choose to work on the easiest stuff first, so
            that I feel like I’m making the list a lot smaller. Leave the hardest things to last.</p>
        <p>但是我们没有任何志愿者愿意押注于最小约束优先的想法？好的。杰森想建议我们应该首先研究最小约束。好吧，我们有基本事实，因为我们可以尝试一下。我想我们会坚持在这里缩小到一个值。但我们会重新排序，以便我们首先得到最小约束。所以马上，我们就得到了缅因州的颜色。
        </p>
        <p>But we don’t have any volunteers who want to bet on that idea of least constraint first? OK. Jason wants to
            suggest that we should work on least constraint first. Well we have ground truth, because we can just try it
            out. I guess we’ll stick with our shrinking to one value thing, here. But we will reorder things so that we
            have the least constrained first. So right away, we got a color for Maine.</p>
        <h2 id="unknown-112">未知</h2>
        <h2>Unknown</h2>
        <p>也许我们应该稍微加快一点速度。嗯，这是个好主意。杰森建议这样做，我们只有 1,732 个约束，我们有 59
            个死胡同。所以让我们反过来试试，我们将回到每秒四帧。所以我们从这个国家的中部开始工作。我想，我们最后要处理缅因州。哪个更好。</p>
        <p>Maybe we ought to speed this up a little bit. Well, that’s a good idea. Jason suggested this and we only
            1,732 constraints and we had 59 dead ends. So let’s try the other way around, and we’ll go back to four
            frames a second. So we’re working, kind of from the middle of the country out, with this one. We’re going to
            deal with Maine, I guess, last. Which is better.</p>
        <p>太糟糕了，我觉得这样更好。事实上，我们不要太过激进地使用约束传播。我们只需检查分配即可。如果我们回到最不具约束的安排，我们就会加快速度。实际上，我们必须将其提高很多，因为密苏里州、田纳西州、肯塔基州等州将像德克萨斯州一样。
        </p>
        <p>Too bad, I think it looks like this is better. In fact, let’s not be so aggressive with the use of constraint
            propagation. Let’s just check the assignments only. If we go back to an arrangement where we have least
            constrained first, and we’ll crank up the speed. Well actually, we would have to crank it up pretty big,
            because the states like Missouri, Tennessee, Kentucky, they’re going to be like Texas.</p>
        <p>所以我们又回到了宇宙长度类型的问题。首先检查约束最少的项，除了检查当前分配之外，不使用任何约束。所以让我们停止这样做，首先检查约束最严格的项，只检查分配。我不知道这要花多长时间。这就是肮脏的小秘密。</p>
        <p>And so were kind of back to the length of the universe type problem, here. With the least constraint first,
            and no use of constraints, other than to check the current assignment. So let’s stop that, though, and check
            the most constrained first, assignments only. I don’t know how long’s this going to take. That’s the dirty
            little secret.</p>
        <h2 id="unknown-113">未知</h2>
        <h2>Unknown</h2>
        <p>如果我们把状态从最受约束到最不受约束地排列起来，普通的深度优先搜索（不包含我们今天讨论的内容）就可以正常工作。好吧。这有点像游戏。您使用渐进式深化，还是使用 alpha beta？答案是两者兼有。您使用一切来解决问题。
        </p>
        <p>If we had arranged our states from most constrained to least constrained, ordinary depth first search with
            none of this stuff we talked about today would work just fine. All right. So it’s a little bit like games.
            Do you use progressive deepening, or do you use alpha beta? And the answer is both. You use everything
            you’ve got to deal with the problems.</p>
        <p>根据问题的不同，如果你足够幸运，你的方法中融入的一些东西会非常有效。所以现在，我保证这不仅对想要给地图上色的人有用。天啊，谁想这么做？我们知道可以用四种颜色来完成。但它对于解决各种资源规划问题也很有用。</p>
        <p>And depending on the problem, one or another of the things you incorporate into your approach will work just
            great, if you’re lucky. So now, I promise that this is useful not only for people who want to color maps.
            God, who wants to do that? We know it can be done with four colors. But it’s also useful for doing all kinds
            of resource planning problems.</p>
        <p>所以我想向你们展示一个资源规划问题，我希望你们思考一下。在我做这件事的时候，想想它是否实际上类似于地图着色问题。好吗？事情是这样的。你刚刚在一家新航空公司 Jet Green 找到了一份暑期工作。Jet Green
            是一家低成本、没有多余装饰、几乎不需要维护的航空公司。他们主要在波士顿和纽约之间飞行。</p>
        <p>So I want to show you a resource planning problem, and I want you think about. while I’m doing it. think
            about whether it’s actually analogous to the map coloring problem. All right? So here’s the deal. You have
            just landed a summer job with the Jet Green, a new airline. And Jet Green is a low cost, no frills, hardly
            any maintenance type of airline. And they want to fly mostly between Boston and New York.</p>
        <h2 id="unknown-114">未知</h2>
        <h2>Unknown</h2>
        <p>他们偶尔会想飞往洛杉矶。他们想用最少的飞机来满足需求。这就是为什么我们与 Jet Green 之间存在某种资源分配问题。所以我要记下他们的时间表。他们有一班航班
            F1，从波士顿飞往肯尼迪机场，就像这样。这是一班早班航班。然后他们想再有一班航班 F2，从肯尼迪机场飞往波士顿。</p>
        <p>Occasionally they want to fly to Los Angeles. And they’re trying to get by with the smallest number of
            airplanes. So that’s why we have a kind of resource allocation problem with Jet Green. So I’m going to write
            down what their schedule looks like. They have one flight, F1, that goes from Boston to JFK, like so. It’s
            an early in the day flight. Then they want to have another one, F2, that flies from JFK to Boston.</p>
        <p>然后他们想在当天晚些时候再有一班从波士顿飞往肯尼迪机场的航班。再晚一点，他们想再有一班从肯尼迪机场飞往波士顿的航班。他们一开始主要以穿梭航班的形式运营。所以有 F1、F2、F3 和 F4。他们还有第五班航班
            F5，从波士顿飞往洛杉矶，飞行时间较长。</p>
        <p>And then they want to have another flight a little later in the day that flies from Boston to JFK. And a
            little later than that, they want to have another flight that goes from JFK to Boston. They’re going to
            start off mostly as a shuttle airline in the beginning. So that’s F1, F2, F3, and F4. And they have a fifth
            flight, F5, that goes from Boston to Los Angeles, that takes a long time.</p>
        <p>所以航班时刻表是这样的。当然我们有时间去那边。这就是从波士顿飞往洛杉矶的航班。现在你的工作是确定他们是否可以用四架飞机按这个时间表飞行？当然，你不想过度使用任何一架飞机，因为你希望它们磨损均匀。对吧？所以当你做出选择时，你会轮换飞机。所以你将把这架飞机分配给
            A1，即一号飞机。</p>
        <p>So it looks like this on the schedule. Of course we have time going that way. So that’s Boston to LAX. Now
            your job is to determine if they can fly this schedule with four aircraft? And naturally you don’t want to
            over use any one aircraft, because you would like to have even wear on them. Right? So as you make your
            choices, you’ll rotate the aircraft. So you’ll assign to this one to A1, aircraft number one.</p>
        <h2 id="unknown-115">未知</h2>
        <h2>Unknown</h2>
        <p>这架飞机将是 A2。这架飞机将是 A3。这架飞机将是 A4。哎呀，飞往洛杉矶的航班已经没有飞机了，因为你只有四架飞机。所以，这很明显，对吧？这 100%
            就是地图着色问题，甚至精确到四个选择。因为你有约束，没有一架物理飞机可以同时飞行两个航班。</p>
        <p>This one will be A2. This one will be A3. And this one will be A4. And, oops, there’s no aircraft left for
            the flight to Los Angeles, because you only have four. So, it’s obvious, right? This is 100%, exactly, the
            map coloring problem, even down to the four choices. Because, you have the constraint, the no single
            physical aircraft can fly two flights at the same time.</p>
        <p>就像没有两个相邻的州可以涂成相同的颜色。所以没有相同的时间限制，就像这样。所以如果你为这些航班分配飞机，你会下到 F4，即第四个航班，你会说，好吧，让我们看看，这家伙可以是 A1、A2、A3 或
            A4。但是如果我为第四个航班选择 A4，那么它的域中将什么都没有了。</p>
        <p>Just like no two adjacent states can be colored the same. So there’s a no same time constraint, like so. So
            if you were assigning aircraft to these flights, you would get down to F4, the fourth flight, and you would
            say, well, let’s see, this guy down here can be A1, A2, A3, or A4. But if I choose A4 for that fourth
            flight, then there would be nothing left in its domain.</p>
        <p>因此，您已将问题设置为与地图着色问题相同。当然，您可以用其他类型的约束来丰富它。例如，您可能有。这不是同一时间的约束。这些，我的意思是，这是在肯尼迪机场。它从肯尼迪机场起飞，所以也许您可以使用同一架飞机来处理这些。
        </p>
        <p>So you’ve thus set the problem up to be identical to the map coloring problem. And, of course, you can enrich
            it with other kinds of constraints. So, for example, you might have. this is a not same time constraint. and
            these, I mean, this is at JFK. And it flies out of JFK, so maybe you can use the same aircraft for those.
        </p>
        <h2 id="unknown-116">未知</h2>
        <h2>Unknown</h2>
        <p>但如果它们彼此紧邻，那就不行，因为有最低地面时间规则。所以这里有最低地面时间限制。这里有最低地面时间限制。这里有最低地面时间限制。如果它们在同一个城市，那么你必须留出足够的时间让它们在所涉及的两个城市之间飞行。</p>
        <p>But not if they’re right up against each other, because you have a minimum ground time rule. So there’s a
            minimum ground time constraint here. And there’s a minimum ground time constraint here. There’s a minimum
            ground time constraint here. And if these are at the same city, then you’ve got to allow enough time for
            them to fly between the two cities that are involved.</p>
        <p>因此约束可能会变得稍微复杂一些，但想法是一样的。所以你对我说，我不相信你，你给我看看。好的。那就让我给你看看。哦，顺便说一下，还有一种方法可以让这个地图着色问题变得更容易，对吧？让我们看看。排列将按字母顺序排列，类型将仅为分配，我们知道这是失败的。
        </p>
        <p>So the constraints can get a little bit more complicated, but the idea is the same. So you say to me, I don’t
            trust you, show me. OK. So let me show you. Oh, by the way, there’s one more way to make this map coloring
            problem easier, right? Let’s see. The arrangement is going to be alphabetical, the type is going to be
            assignments only, and we know that’s a loser.</p>
        <p>但如果我们使用大量颜色，情况就不一样了。所以，使用四种颜色给我们带来了麻烦。现在这是一个题外话，但一会儿还会再说。调度，这就是问题所在。砰。你几乎可以看到它的工作原理就像地图着色一样。但这可能太简单了。让我们来做这个。
        </p>
        <p>But not if we use a whole lot of colors. So it’s the use of four colors that got us into trouble. Now that’s
            an aside, but it’ll be coming back in a moment or two. Scheduling, here’s that problem. Boom. You can almost
            see it working just like the map coloring thing. But that’s maybe too easy. Let’s do this one.</p>
        <h2 id="unknown-117">未知</h2>
        <h2>Unknown</h2>
        <p>看，这就像我们这里有一个愚蠢的安排，由于顶部的选择，底部肯定会失败。但没关系。我们可以停止这种情况，我们可以改为只检查邻居。然后砰的一声，就这样了。或者，让我看看我是否可以这样做。首先是最受约束的，类型将仅为分配。砰的一声。这也很好用。
        </p>
        <p>See this is just like we’ve kind of got a goofy arrangement here that’s guaranteed to lose at the bottom
            because of choices made at the top. But that’s OK. We can that we can stop this, and we can change to check
            neighbors only. And boom, there it goes. Or alternatively, let me see if I can do it this way. Most
            constrained first, type will be assignments only. Boom. That worked fine, too.</p>
        <p>所以你可能会选择一个稍微难一点的问题，就像这样。我们可以搜索一下。实际上我不知道这是否会完成。这是一个随机生成的例子。但是如果你不能弄清楚你是否能做到这一点，你就会失去你的暑期工作。那么你打算怎么做？你有什么想法要如何保住你的工作吗？所以这就是你被问到的问题。
        </p>
        <p>So you might choose to have a slightly harder problem, like this. And we can search away. Actually I don’t
            know if this will complete or not. This is a randomly generated example. But you’re going to lose your
            summer job if you can’t figure out whether you can do this, or not. So what are you going to do? you got any
            thoughts about how you’re going to save your job? So here’s the question you’ve been asked.</p>
        <p>Jet Green
            需要多少架飞机？你决定，嗯，以前四架似乎可行，所以我们就试试四架。你不确定它是否会终止，我的意思是，在你的一生中，更不用说在你的暑期工作中。让我给你一个提示。看看大纲。这里的大纲，在黑板上，最后一项。帕特里克·温斯顿教授：是的，那是什么意思？我们最多需要多少架飞机？
        </p>
        <p>How many airplanes does Jet Green need? And you decide, well, four seemed to work before, so we’ll try four
            here. You’re not sure if it’s going to terminate or not, I mean, in your lifetime, let alone in your summer
            job. let me give you a hint. Look at the outline. The outline up here, on the board, the last item. PROF.
            PATRICK WINSTON: Yeah, what’s that mean? What’s the maximum number of airplanes we’re going to need?</p>
        <h2 id="unknown-118">未知</h2>
        <h2>Unknown</h2>
        <p>假设我们有五个航班，我们最多需要多少架飞机？五架。我们最少需要多少架飞机？一架。那么让我们用少量飞机和大量飞机来试试。这很快向我们表明，我们无法用一架飞机做到这一点。这很快向我们表明，我们可以用十架飞机做到这一点。说话者
            1：重叠量从上方开始 帕特里克·温斯顿教授：志愿者？</p>
        <p>Suppose we’ve got five flights, what’s the maximum number of airplanes we would ever need? Five. What’s the
            minimum number of airplanes we’ll need? One. So let’s try it with a small number of airplanes and a large
            number of airplanes. So that showed us very fast that we can’t do it with one airplane. That showed us very
            fast we can do it with ten airplanes. SPEAKER 1: amount of overlappage from up PROF. PATRICK WINSTON:
            Volunteer?</p>
        <p>我们要怎么做才能尽快找到实际数字？至少要给我们的老板一个合理的答案，即使不一定是准确的数字，也要非常快？这很容易，对吧？我们将从一台计算机开始，再从另一台计算机开始，看看会发生什么。让我们看看我们能否用九台计算机做到这一点。让我们看看我们能否用八台计算机做到这一点。
        </p>
        <p>What are we going to do to find the actual number, as fast as possible. And at least give our boss a
            reasonable answer, even if not necessarily the exact number very fast? It’s easy, right? We’re going to
            start up here with one computer, and we’re going to start down here with another computer, and see what
            happens. So let’s see if we can do it with nine. Let’s see if we can do it with eight.</p>
        <p>让我们看看我们能否用 7 来做。这些几乎不花费时间，对吧？因为它们受到约束。哇，这很好，7。让我们试试 6。实际上，让我们试试 2。它很快就失败了。让我们试试 3。我不知道。也许如果你让它运行 1 足够长的时间，3
            就会起作用。我对此表示怀疑。不过，既然我们已经这样做了，我们不妨回到这里，用 6 来试试。记住，7 真的很快见效。</p>
        <p>Let’s see if we can do it with seven. These take almost zero time, right? Because they’re under constraint.
            Wow, that’s good, seven. Let’s try six. Actually let’s try two. It loses fast. Let’s try three. I don’t
            know. Maybe if you let it run one long enough three will work. I doubt it. While we’re at it though, we
            might as well go back here and try it with six. Remember seven worked real fast.</p>
        <h2 id="unknown-119">未知</h2>
        <h2>Unknown</h2>
        <p>天哪，六，就是六，对吧？是的。那么让我们用五试试。好的。五的时候运行得非常快。二的时候它很快就终止了，所以我们剩下三和四。所以我们可以告诉我们的老板，就像任何时间算法一样，你不太确定，但你知道它会是三或四。然后，你有两台计算机。你可以让两台都运行，看看是否有其中一台终止。
        </p>
        <p>Gees, six, that was six, right? Yeah. So let’s try it with five. OK. So it runs real fast with five. It
            terminates real quick with two, so we got three and four left. So we could tell our boss, a la any time
            algorithm, that you’re not real sure, but you know it’s going to be either three or four. And then, you got
            two computers. You can let both run and see if either one terminate.</p>
        <p>所以你有三和四。我猜三最终会放弃。但当然，这里还有另一个小问题。我们没有首先使用最大的约束。如果我们这样做，我们可能会做得更快。实际上，我认为如果不获得另一个随机分配，我无法进行这种切换，但让我们看看会发生什么。也许会这样。说话者
            2：帕特里克·温斯顿教授：哦，我已经首先有了最大的约束？好的。</p>
        <p>So you have three and four. My guess is that three will eventually give up. But of course, there’s another
            little problem here. We haven’t used the most constraint first. If we did that, we might be able to do it
            even faster. Actually, I don’t think I can make that switch without getting another random assignment, but
            let’s see what happens. Maybe so. SPEAKER 2: PROF. PATRICK WINSTON: Oh, I already had most constraint first?
            OK.</p>
        <p>所以实际上切换并没有帮助。我想我有一个新的工作时间表。故事就到此结束。如果你一次做几件事，你就可以进行良好的资源分配。第一，你总是想先使用最多的约束。第二，你想通过单一算法产生的域进行传播。</p>
        <p>So it didn’t help to actually switch. And I think I’ve got a new schedule to work here. So that’s the end of
            the story. You can do good resource allocation if you do several things are once. Number one, you always
            want to use most constraint first. Number two you want to propagate through domains produced to a single
            algorithm.</p>
        <h2 id="unknown-120">未知</h2>
        <h2>Unknown</h2>
        <p>第三，如果你真的想弄清楚所需的最少资源数量，那么你可以在业务量过大的情况下进行，这样你就可以快速收敛到一个狭窄的范围，在这个范围内搜索需要很长时间，并确保它在这个狭窄的范围内。因为当你的资源过多时，它很快就会完成，而当你的资源不足时，它很快就会终止。
        </p>
        <p>And number three, if you really try to figure out what the minimum number of resources needed is, you do this
            under over business and you can quickly converge on a narrow range where the search is taking a long time,
            and be sure that it lies within that narrow range. Because when you over resource, it’s fast to complete,
            and when you under resource, it’s fast to terminate.</p>
        <p>所以你可以把它压缩到一个很小的范围内。故事就到此结束。祝你周一假期愉快。正如广告所说，下周我们将在周三和周五开设两节课。</p>
        <p>So you can just squeeze it right down into a very small range. And that is the end of the story. Enjoy your
            holiday on Monday. We’ll have two classes next week on Wednesday and Friday, as advertised.</p>
        <h1 id="constraints-visual-object-recognition">9.约束：视觉物体识别</h1>
        <h1>9. Constraints: Visual Object Recognition</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 51:32 <a
                href="https://youtube.com/watch?v=gvmfbePC2pc">https://youtube.com/watch?v=gvmfbePC2pc</a></p>
        <p> 11 years ago (Jan 11, 2014) — 51:32 <a
                href="https://youtube.com/watch?v=gvmfbePC2pc">https://youtube.com/watch?v=gvmfbePC2pc</a></p>
        <h2 id="unknown-121">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：你知道，政治变得如此严肃，这很不幸。小时候，政治有趣得多。你可以取笑政客。有些人可能认识这位政客。但可以改变这位政客的形象，这很方便。例如，我们可以从饼干烘焙师变成激进分子。帕特里克·温斯顿：我们可以从女超人变成荡妇。帕特里克·温斯顿：社交名媛。我把社交名媛放进去了。
        </p>
        <p>PATRICK WINSTON: You know, it’s unfortunate that politics has become so serious. Back when you were little it
            was a lot more fun. You could make fun of politicians. Here’s a politician some of you may recognize. But
            it’s convenient to be able to vary what this particular politician looks like. For example, we can go from a
            cookie baker to radical. PATRICK WINSTON: We can go from superwoman to bimbo. PATRICK WINSTON: Socialite. I
            put socialite into this.</p>
        <p>她就在那里。或者我们可以将滑块移到流浪女的另一侧。警觉、睡着、悲伤、快乐。这是怎么回事？我不知道。但我敢打赌，到这一个小时结束时，你会知道这是怎么回事。不仅如此，你还会了解一些识别人脸所需的条件。事实证明，一些人脸识别理论所基于的原理与这个程序所基于的原理相同。
        </p>
        <p>There she is. Or we can move the slider over the other way to bag lady. Alert, asleep, sad, happy. How does
            that work? I don’t know. But I bet by the end of this hour you’ll know how that works. And not only that,
            you’ll understand something about what it takes to recognize faces. It turns out to some theories of face
            recognition are based on the same principles that this program is based on.</p>
        <p>但你可以猜到这里发生了什么。有很多存储的图像，当我移动这些滑块时，它会在它们之间进行插值。这就是它的工作原理。但今天的主要主题是识别物体。脸部可以是物体，但不一定非得是。这可能是您想要识别的物体。</p>
        <p>But you can kind of guess what’s happening here. There are many stored images and when I move those sliders
            it’s interpolating amongst them. So that’s how that works. But the main subject of today is this matter of
            recognizing objects. Faces could be the objects, but they don’t have to be. This could be an object that you
            might want to recognize.</p>
        <h2 id="unknown-122">未知</h2>
        <h2>Unknown</h2>
        <p>我想跟你们简单谈谈这个问题的历史以及它现在的状况。它仍然没有得到解决。但看看过去 30 年来解决问题的尝试是如何缓慢发展的，这是一件有趣的事情。事实上，进展如此缓慢，以至于如果有人在 30
            年前告诉我要花多长时间才能达到我们现在的水平，我想我会上吊自杀。</p>
        <p>And I want to talk to you a little bit about the history of this problem and where it stands today. It’s
            still not solved. But it’s an interesting exercise to see how the attempts at solution have evolved slowly
            over the past 30 years. So slowly, in fact, that I think if someone told me how long it would take to get to
            where we are 30 years ago I think I would have hung myself.</p>
        <p>但事情确实进展缓慢。重要的是要看到它们进展得有多慢。因为它们将来会继续缓慢地发展。你必须明白，有时事情就是这样运作的。</p>
        <p>But things do move slowly. And it’s important to see how slowly they move. Because they will continue to move
            slowly in the future. And you have to understand that’s the way things work sometimes.</p>
        <p>首先，我们必须回顾传奇人物大卫马尔的思想，他于 1980 年左右因白血病去世。我说这是马尔所说的福音，因为他是一位如此强大和重要的人物，几乎他说的任何话都会被一大批信徒相信。</p>
        <p>So to start this all off, we have to go back to the ideas of the legendary David Marr, who dropped dead from
            leukemia in about 1980. I say, the gospel according to Marr, because he was such a powerful and central
            figure that almost anything he said was believed by a large collection of devotees.</p>
        <h2 id="unknown-123">未知</h2>
        <h2>Unknown</h2>
        <p>但马尔阐述了一套关于计算机视觉如何运作的想法，这些想法首先建议利用来自摄像头的输入来寻找边缘。然后你会发现边缘碎片。通常它们甚至不会像我现在画得这么好。或者像我现在画得这么糟糕。但视觉识别的第一步是形成这种基于边缘的描述，描述世界上存在的事物。
        </p>
        <p>But Marr articulated a set of ideas about how computer vision would work that started off by suggesting that
            with the input from the camera, you look for edges. And you find edge fragments. And normally they wouldn’t
            be even as well drawn as I’ve done them now. Or as badly drawn as I’ve done them now. But the first step,
            then, in visual recognition would be to form this edge based description of what’s out there in the world.
        </p>
        <p>Marr 将其称为原始草图。从原始草图开始，下一步就是用一些向量、一些表面法线来修饰原始草图，以显示物体表面的方向。他将其称为 2.5D 草图。那么为什么是 2.5D？嗯，它有点像
            2D，因为它在呈现信息的方式上仍然以相机为中心。</p>
        <p>And Marr called that the primal sketch. And from the primal sketch, the next step was to decorate the primal
            sketch with some vectors, some surface normals, showing where the faces on the object were oriented. He
            called that the two and a half D sketch. Now why is it two and a half D? Well, it’s sort of 2D in the sense
            that it’s still camera centric in its way of presenting information.</p>
        <p>但与此同时，它试图表达一些关于脸部三维排列的内容。因此推测你无法一步到位。所以你需要几个步骤才能从图像变成你可以识别的东西。第三步是将 2.5D 草图转换为通用圆柱体。这个想法是这样的。</p>
        <p>But at same time, it attempts to say something about the three dimensional arrangement of the faces. So the
            speculation was that you couldn’t get to where you wanted to go in one step. So you needed several steps to
            get from the image to something you could recognize. And the third step was to convert the two and a half D
            sketch into generalized cylinders. And the idea is this.</p>
        <h2 id="unknown-124">未知</h2>
        <h2>Unknown</h2>
        <p>如果你有一个普通的圆柱体，你可以把它想象成一个沿着轴线移动的圆形区域，就像这样。这就是圆柱体的描述。一个沿着轴线移动的圆形区域。如果你沿着相同的轴线移动，但允许圆的大小随移动而变化，那么你就可以得到不同类型的圆柱体。例如，如果你要描述一个酒瓶。
        </p>
        <p>If you have a regular cylinder, you can think of it as a circular area moving along an axis like so. So
            that’s the description of a cylinder. A circular area moving along an axis. You can get a different kind of
            cylinder if you go along the same axis but you allow the size of the circle to change as you go. So for
            example, if you were to describe a wine bottle.</p>
        <p>它将是沿轴的距离函数，可适当缩小圆以匹配酒瓶的尺寸。我看，这是上好的勃艮第酒。无论如何，一旦将其转换为通用圆柱体，与此类描述的库进行匹配，就会识别出来。</p>
        <p>It would be a function of distance along the axis that would shrink the circle appropriately to match the
            dimensions of a wine bottle. A fine burgundy, I perceive. In any case, this one once converted into a
            generalized cylinder, when matched against a library of such descriptions, results in recognition.</p>
        <p>伟大的理论，基于这样的想法：你从观察边缘开始，经过几步变换，最终得到可以在描述库中查找的东西。好主意。问题是，没有人能让它发挥作用。这太难了。那也太难了。而且，即使有，产生的广义圆柱也太粗糙了。</p>
        <p>Great theory, based on the idea that you start off by looking at edges and you end up, in several steps of
            transformation, producing something that you could look up in a library of descriptions. Great idea. Trouble
            is, no one could make it work. It was too hard to do this. It was too hard to do that. And the generalized
            cylinders produced, if any, were too coarse.</p>
        <h2 id="unknown-125">未知</h2>
        <h2>Unknown</h2>
        <p>你无法区分福特和雪佛兰，也无法区分大众和凯迪拉克。因为它们太粗糙了。所以，尽管这是一个很棒的想法，它基于你必须在表征装置的几次转换中进行识别的想法，但它就是行不通。很久以后，大概 15 年后，我们才进入故事的下一部分。
        </p>
        <p>You couldn’t tell the difference between a Ford and a Chevrolet or between a Volkswagen and a Cadillac.
            Because they were just too coarse. So although it was a great idea based on the idea that you have to do
            recognition in several transformations of representational apparatus, it just didn’t work. So much later,
            maybe 15 years later or so, we get to the next part of our story.</p>
        <p>也就是对齐理论，最著名的是马尔的学生之一西蒙·乌尔曼提出的理论。因此，识别对齐理论基于一个非常奇怪和奇特的想法。对于机械工程师来说，这暂时并不奇怪和奇特，因为他们习惯于机械图纸。但这里有一个奇怪而神奇的想法。想象一下这个物体。你拍了三张照片。你可以重建该物体的任何视图。
        </p>
        <p>Which is the alignment theories, most notably the one produced by Shimon Ullman, one of Marr’s students. So
            the alignment theory of recognition is based on a very strange and exotic idea. It doesn’t seem strange and
            exotic to mechanical engineers for a while, because they’re used to mechanical drawings. But here’s the
            strange and miraculous idea. Imagine this object. You take three pictures of it. You can reconstruct any
            view of that object.</p>
        <p>现在，我必须小心一点。首先，有些顶点在视图中不可见。所以，当然，你无法对它们做任何事情。假设我们有一个透明物体，你可以看到它的所有顶点。如果你有三张该物体的图片，你就可以重建该物体的任何视图。</p>
        <p>Now, I have to be a little bit careful about how I say that. First of all, some of the vertexes are not
            visible in the views that you have. So, of course, you can’t do anything with those. So let’s say that we
            have a transparent object where you can see all the vertexes. If you have three pictures of that, you can
            reconstruct any view of that object.</p>
        <h2 id="unknown-126">未知</h2>
        <h2>Unknown</h2>
        <p>现在我必须小心一点，因为这不是真的。事实是，你可以在正交投影中产生任何视图。所以如果你离物体足够近，以至于你得到透视，它就不起作用了。但在大多数情况下，当你距离物体大约两倍半远时，你可以忽略透视。</p>
        <p>Now I have to be a little careful about how I say that, because it’s not true. What’s true is, you can
            produce any view of that in orthographic projection. So if you’re close enough to the object that you get
            perspective, it doesn’t work. But for the most part, you can neglect perspective after you get about two and
            a half times as far away as the object is big.</p>
        <p>你可以假设你得到了正交投影。这是一个奇怪而奇特的想法。但是你如何从中得出一个识别理论呢？让我给你演示一下。好吧，这是物体的一张图，我还需要两张。让我们看看。我们先看这张。也许还有一张稍微向上倾斜的。重要的是，这些图片不仅仅是围绕一个轴的旋转。
        </p>
        <p>And you can presume that you’ve got orthographic projection. So that’s a strange and exotic idea. But how can
            you make a recognition theory out of that? So let me show you. Well, here’s one drawing of the object, I
            need two more. Let’s see. Let’s have this one. And maybe one that’s tilted up a little bit. It’s important
            that these pictures not be just rotations on one axis.</p>
        <p>因为它们不会形成你可能认为的某种基组。所以有三幅图。我们将它们称为 a、b 和 c。然后我们需要第四幅图。它看起来会像这样。它不必太精确。我们将其称为未知数。我们真正想知道的是未知数是否是这三幅图所基于的同一物体。
        </p>
        <p>Because they wouldn’t form what you might think of as a kind of basis set. So there are three pictures. We’ll
            call them a, b, and c.&nbsp;And then we want a fourth picture. Which will look like this. It doesn’t have to
            be too precise. And we’ll call that the unknown. And what we really want to know is if the unknown is the
            same object that these three pictures were made from.</p>
        <h2 id="unknown-127">未知</h2>
        <h2>Unknown</h2>
        <p>那么让我先来提出一个断言。我需要四种颜色的粉笔来做出这个断言。我想要做的是选择物体上的一个特定位置，比如这个。也许这个物体上的相同位置也是如此。那些是对应位置，对吧？</p>
        <p>So let me begin with an assertion. I’ll need four colors of chalk to make this assertion. What I want to do
            is I want to pick a particular place on the object, like this one. And maybe the same place on this object
            over here. Those are corresponding places, right?</p>
        <p>所以我现在可以写一个方程，这个未知物体的 x 坐标等于，哦，我不知道，alpha x sub a 加上 beta x sub b 加上 gamma x sub c 加上某个常数
            tau。当然，这显然是正确的。因为我让你把这些 alpha、beta、gamma 和 tau 变成你想要的任何东西。所以虽然这显然是正确的，但它并不有趣。所以让我再说一点。</p>
        <p>So I can now write an equation that the x coordinate of that unknown object is equal to, oh, I don’t know,
            alpha x sub a plus beta x sub b plus gamma x sub c plus some constant, tau. Well, of course, that’s
            obviously true. Because I’m letting you take those alpha, beta, gamma, and tau and make them anything you
            want. So although that’s conspicuously obviously true, it’s not interesting. So let me take another point.
        </p>
        <p>当然，我可以为这个紫色点写下相同的方程式。现在我正处于顺势，并且对此非常感兴趣，我可以以此点创建一个蓝色方程式。你知道我注定要这样做，所以我还有一种颜色。我不妨使用它。让我们确保我在这里得到一些有用的东西。就是这个，就是这个。
        </p>
        <p>And of course, I can write the same equation down for this purple point. And now that I’m on a roll and
            having a great deal of fun with this, I can take this point and make a blue equation. And you know I’m
            destined to do it, so I’ve got one more color. I might as well use it. Let’s just make sure I get something
            that works here. That’s this one, that’s this one.</p>
        <h2 id="unknown-128">未知</h2>
        <h2>Unknown</h2>
        <p>我希望我理解这些对应关系。学生：帕特里克·温斯顿：我弄错了吗？学生：帕特里克·温斯顿：哪种颜色？学生：蓝色。帕特里克·温斯顿：好的。所以这个和这个配，和这个配。那个错了吗？学生：是的。帕特里克·温斯顿：哦，哦，哦。当然，对不起，这个应该放在这儿。对吧？然后这个也弄错了。
        </p>
        <p>I hope I’ve got these correspondences right. STUDENT: PATRICK WINSTON: Have I got one off? STUDENT: PATRICK
            WINSTON: Which color? STUDENT: Blue. PATRICK WINSTON: OK. So this one goes with this one, goes with this
            one. Is that one wrong? STUDENTS: Yeah. PATRICK WINSTON: Oh, oh, oh. Of course this one, excuse me, goes
            down here. Right? And then this one is off as well.</p>
        <p>如果我不能正确计算这些对应关系，我就无法得到很好的识别方案。这是今天的课程之一。好的。现在我把它们都算对了。现在那个等式是正确的。我想我已经算对了。所以现在我可以把它写下来。我正在努力，我只是抄下这个。所以那些是一堆方程式。
        </p>
        <p>I wouldn’t get a very good recognition scheme if I can’t get those correspondences right. Which is one of the
            lessons of today. OK. Now I’ve got them right. And now that equation is correct. I think I’ve got this one
            right already. So now I can just write that down. I’m on a roll, I’m just copying this. So those are a bunch
            of equations.</p>
        <p>现在令人惊奇的是，我可以选择 alpha、beta、gamma 和 tau 都相同。也就是说，有一组 alpha、beta、gamma 和 tau
            适用于所有事物，适用于所有四个点。所以你看着它很困惑。困惑也没关系。因为我当然还没有证明这一点。我只是断言这一点。</p>
        <p>And now the astonishing part is that I can choose alpha, beta, gamma, and tau to be all the same. That is,
            there’s one set of alpha, beta, gamma, and tau that works for everything, for all four points. So you look
            at that puzzled. And that’s OK to be puzzled. Because I certainly haven’t proved it. I’m asserting it.</p>
        <h2 id="unknown-129">未知</h2>
        <h2>Unknown</h2>
        <p>但马上，这其中就有一个有趣的地方，那就是未知物体上的点和这个存储的图像库中的点之间的关系是线性相关的。这是真的，因为它是正交投影。线性相关。所以我可以用线性运算从三个样本物体中的点生成第四个物体中的点。克里斯托弗？学生：那是
            x 坐标吗？帕特里克·温斯顿：是 x 坐标。克里斯托弗问的是 x 坐标。</p>
        <p>But right away, there’s something interesting about this and that is that the relationship between the points
            on the unknown object and the points in this stored library of images are related linearly. That’s true
            because it’s orthographic projection. Linearly related. So I can generate the points in some fourth object
            from the points in three sample objects with linear operations. Christopher? STUDENT: Is that the x
            coordinate of. PATRICK WINSTON: It’s the x coordinate. Christopher asked about the x coordinates.</p>
        <p>这些 x 坐标中的每一个都应该用颜色编码。符号和其他东西会变得有点复杂。这就是我为坐标进行颜色编码的原因。所以橙色 x sub u 是该特定点的 x 坐标。学生：在 3D 空间中？帕特里克·温斯顿：不。不在 3D
            空间中。在图像中。学生：所以它是它的 2D 投影？帕特里克·温斯顿：它是它的 2D 投影，正交投影。好吗？</p>
        <p>Each of these x coordinates are meant to be color coded. It gets a little complicated with notation and
            stuff. So that’s the reason I’m color coding the coordinates. So the orange x sub u is the x coordinate of
            that particular point. STUDENT: In 3D space? PATRICK WINSTON: No.&nbsp;Not in 3D space. In the image.
            STUDENT: So it’s a 2D projection of it? PATRICK WINSTON: It’s a 2D projection of it, an orthographic
            projection. OK?</p>
        <p>所以我们在看图画。那边的坐标是图画中的二维坐标。就像在你的视网膜上一样。学生：3D
            投影上的顶点，还是曲面也可以？帕特里克·温斯顿：所以他问的是曲面。答案是你必须找到物体上的对应点。所以如果你有一个完全弯曲的表面，而你无法识别任何对应点，你就输了。</p>
        <p>So we’re looking at drawings. And those coordinates over there are the two dimensional coordinates in the
            drawing. Just as if it were on your retina. STUDENT: vertexes on the 3D projection or can curved surfaces
            also? PATRICK WINSTON: So he asked about curved surfaces. And the answer is that you have to find
            corresponding points on the object. So if you have a totally curved surface and you can’t identify any
            corresponding points, you lose.</p>
        <h2 id="unknown-130">未知</h2>
        <h2>Unknown</h2>
        <p>但是如果你考虑一下我们的脸，就会发现一些明显的点，尽管我们的脸不像这些物体那样平坦。我们有鼻尖和眼球中心等等。如果这是真的，那么恢复 alpha、beta、gamma 和 tau
            意味着什么？我们能找到它们吗？你怎么看？我们该如何找到它们？你点头表示方向正确。</p>
        <p>But if you consider our faces, there are some obvious points, even though our face are not by any means flat
            like these objects. We have the tip of our nose and the center of our eyeballs and so on. So if that’s true,
            what does that mean about recovering alpha, beta, gamma, and tau? Can we find them? what do you think? How
            do we go about finding them? You’re nodding your head in the right direction.</p>
        <p>学生：它有四个方程和。帕特里克·温斯顿：太棒了。它有四个方程和四个未知数。四个线性方程和四个未知数。显然，如果你知道这些方程是正确的，你就可以解出 alpha、beta、gamma 和
            tau。那么这对我们识别有什么帮助呢？</p>
        <p>STUDENT: It’s four equations and. PATRICK WINSTON: Splendid. It’s four equations and four unknowns. Four
            linear equations and four unknowns. So obviously, you can solve for alpha, beta, gamma, and tau if you know
            that these equations are correct. So how does that help us with recognition?</p>
        <p>它有助于我们识别，因为我们可以取另一个点，比如说这里的这个方形点，这里对应的方形点，这里对应的方形点，我们现在可以用这三个点做什么？我们有 alpha、beta、gamma 和
            tau，所以我们可以预测它在第四幅图像中的位置。所以我们可以预测那个方形点就在那里。</p>
        <p>It helps us with recognition because we can take another point, let me say this square point here and this
            corresponding square point here and this corresponding square point here, and what can we do with those
            three points now? We’ve got alpha, beta, gamma, and tau, so we can predict where it’s going to be in the
            fourth image. So we can predict that square point is going to be right there.</p>
        <h2 id="unknown-131">未知</h2>
        <h2>Unknown</h2>
        <p>如果不是，我们就会高度怀疑这个物体是否是我们所认为的那种物体。所以你难以置信地看着我。我想你希望我演示一下。学生：是的。帕特里克·温斯顿：好的。让我看看我能否演示一下。所以我将以稍微简化的版本来演示。我只允许绕垂直轴旋转。
        </p>
        <p>And if it isn’t, we’re highly suspicious about whether this object is the kind of object we think it is. So
            you look at me in disbelief. You’d like me to demonstrate this, I imagine. STUDENT: Yeah. PATRICK WINSTON:
            OK. Let me see if I can demonstrate this. So I’m going to do this in a slightly simplified version. I’m only
            going to allow rotation around the vertical axis.</p>
        <p>为了让你知道我没有作弊，这里有一个小滑块可以旋转第三个物体。让我们看看，为什么只有两个已知物体和一个未知物体？那是因为我将运动限制为围绕垂直轴旋转和一些平移。现在我把它旋转了一点，让我选一些相应的点。哎呀。发生了什么事？哇。让我再运行一遍。好的。
        </p>
        <p>And just so you know I’m not cheating, there’s a little slider here that rotates that third object. Let’s
            see, why are there just two known objects and one unknown? Well that’s because I’ve restricted the motion to
            rotation around the vertical axis and some translation. So now that I’ve spun that around a little bit, let
            me pick some corresponding points. Oops. What’s happened? Wow. Let me run that by again. OK.</p>
        <p>我从模型对象中选了一个点。未知点上的对应点就在那里。我可能有点偏。但没关系。所以我就选那个点，然后它对应这个点。克里希纳，你想指定一个点，这样人们就知道我没有作弊。选一个点。选一个点，克里希纳。学生：哦，右边？帕特里克·温斯顿：右边？学生：是的。
        </p>
        <p>So there’s one point I’ve selected from the model objects. The corresponding point over here on the unknown
            is right there. I’m going to be a little off. But that’s OK. So let me just pick that one and then that
            corresponds to this one. Krishna, would you like to specify a point so people know I’m not cheating. Pick a
            point. Pick a point, Krishna. STUDENT: Oh, the right? PATRICK WINSTON:. The right? STUDENT: Yeah.</p>
        <h2 id="unknown-132">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：这个？ 学生：是的。
            帕特里克·温斯顿：哎呀。好的，我们先在模型上选出来。现在在这里选。砰。所以所有的点都在它们应该在的地方。这不是很酷吗？好吧，我们假设未知数是别的东西。这是一个精心挑选的对象。因为这些点在垂直方向上都是正确的位置，但它们在其他两个维度上不一定是正确的位置。
        </p>
        <p>PATRICK WINSTON: This one? STUDENT: Yep. PATRICK WINSTON: Oops. OK, let’s pick it out on the model first. Now
            pick it over here. Boom. So all the points are where they’re supposed to be. Isn’t that cool? Well, let’s
            suppose that the unknown is something else. This is a carefully selected object. Because the points are all
            the correct positions vertically, but they’re not necessarily the correct positions in the other two
            dimensions.</p>
        <p>所以让我选这个点，这个点，这个点，这个点。Krishna
            让我选这个点。所以让我选这个点。所以如果它认为未知物体是这些方尖碑物体之一，那么我们希望看到所有对应的点都被正确识别。但是突然间，所有点都偏离了。所以它似乎在这个特定例子中起作用了。我使用两个图像找到了 alpha
            和 beta。</p>
        <p>So let me pick this point, and this point, and this point, and this point. And Krishna had me pick this
            point. So let me pick this point. So if it thinks that the unknown is one of these obelisk objects, then we
            would expect to see all of the corresponding points correctly identified. But boom. All the points are off.
            So it seems to work in this particular example. I find the alpha and beta using two images.</p>
        <p>然后我预测其他点的位置。然后我确定这些位置是否正确。如果它们是正确的，那么我就可以很好地确定我实际上已经将右侧的物体识别为方尖碑或器官，这取决于我选择的模型选项和未知选项。</p>
        <p>And I predict the locations of the other points. And I determine whether those positions are correct. And if
            they are correct, then I have a pretty good idea that I have in fact identified the object on the right as
            either an obelisk or an organ, depending on which of the model choices and the unknown choices I’ve
            selected.</p>
        <h2 id="unknown-133">未知</h2>
        <h2>Unknown</h2>
        <p>所以我唯一要做的就是证明我所说的是正确的。所以我将使用本演示中的配置来实际证明我所说的是正确的。因为我很难记住三维广义旋转的矩阵变换。所以它是这样工作的。z 轴向上。或者，它将指向你。</p>
        <p>So the only thing I have left to do is to demonstrate that what I said about this is true. So I’m going to
            actually demonstrate that what I said about this is true using the configuration in this demonstration.
            Because it’s much too hard for me to remember matrix transformations for generalized rotation in three
            dimensions. So here’s how it’s going to work. The z axis is going up that way. Or, it’s going to be pointing
            toward you.</p>
        <p>我要做的是围绕这个轴旋转。我想要做的是找出在旋转过程中点图像中的 x 坐标如何移动。这是 x 轴。这是您可以看到的坐标。这是 y 轴。这是深度，所以您无法判断它有多远。这是 z 轴。</p>
        <p>And what I’m going to do is I’m going to rotate around this axis. And what I want to do is I want to find out
            how the x coordinate in the image of the points move as I do that rotation. So here’s the x axis. This is
            the coordinate that you can see. Here is the y axis. That’s in depth, so you can’t tell how far away it is.
            And the z axis.</p>
        <p>X、y、z 轴必须指向你。所以现在我要考虑物体上的一个点，看看会发生什么。所以我会对自己说，让我们把物体放在某种标准位置。我不在乎它是什么。它可以是随机的，只要旋转它就可以了。某个位置，我们称之为标准位置，S。</p>
        <p>X, y, z axis must be pointing out that way toward you. So now I’m going to consider just a single point on
            the object and see what happens to it. So I’m going to say to myself, let’s put the object in some kind of
            standard position. I don’t care what it is. It can be just random, just spin it around. Some position, we’ll
            call that the standard position, S.</p>
        <h2 id="unknown-134">未知</h2>
        <h2>Unknown</h2>
        <p>这意味着标准位置的 x 坐标是 x sub s。标准位置的 y 坐标是 y sub s。现在我要旋转物体三次。一次形成 a 图片，一次形成 b 图片，一次形成 c
            图片。你可以做出这些选择。这些可以是任何东西，对吧？假设 a 图片在这里。</p>
        <p>And that means that the x coordinate of the standard position is x sub s. And the y coordinate of the
            standard position is y sub s. And now I’m going to rotate the object three times. Once to form the a
            picture, once to form the b picture, and once to form the c picture. And you can make those choices. Those
            can be anything, right? So let’s say that the a picture is out here.</p>
        <p>这就是 a 图。B 图在这里。未知数在那边。所以我想知道的是取决于这些向量。我们将其称为 theta sub a，这是 theta sub b。这是 theta sub u。所以我想知道 x sub a 如何依赖于 x
            sub s 和 y sub s。</p>
        <p>So that’s the a picture. The B picture is out here. And the unknown is up that way. And so what I want to
            know depends on these vectors. We’ll call that theta sub a, and this is theta sub b. And this one is theta
            sub u. So I would like to know how x sub a depends on x sub s and y sub s.</p>
        <p>我总是记不住怎么做，因为我永远记不住旋转的变换方程。所以我每次都要自己想出来。这次也不例外。所以我要说的是，这个到 S 的向量由两部分组成。有 x 部分和 y 部分。</p>
        <p>And I can never remember how to do that, because I can never remember the transformation equations for
            rotation. So I have to figure it out every time. And this is no exception. So what I’m going to say is that
            this vector that goes out to S consists of two pieces. There’s the x part and the y part.</p>
        <h2 id="unknown-135">未知</h2>
        <h2>Unknown</h2>
        <p>我知道我可以通过旋转这个向量和旋转那个向量并将结果相加来旋转这个向量 alpha sub a。因此，如果我将这个向量旋转 alpha sub a，那么它对 a 的 x 坐标的贡献将由 theta sub a 的余弦乘以
            x sub s 给出。</p>
        <p>And I know that I can rotate this vector by alpha sub a by rotating this vector and rotating that vector and
            adding up the results. So if I rotate this vector by alpha sub a, then the contribution of that to the x
            coordinate of a is going to be given by the cosine of theta sub a multiplied by x sub s.</p>
        <p>因此，您可以夸大该运动，比如说，如果我以那种方式向上倾斜，则在 x 轴上的投影将是该向量的长度乘以角度的余弦。现在，还会依赖于 y 子 s。让我们弄清楚那会是什么。我这里有这个向量。我也将按 theta sub a
            旋转它。</p>
        <p>So you can just exaggerate that motion, say, well if I pitch it up that way then the projection down on the x
            axis is going to be this length of the vector times the cosine of the angle. Now there’s also going to be a
            dependence on y sub s. Let’s figure out what that’s going to be. I’ve got this vector here. And I’m going to
            rotate it by theta sub a as well.</p>
        <p>如果我将其旋转 theta sub a 并查看 x 轴上的投影，它将由角度的正弦给出。但它的方向不对，所以我必须减去它。这样我就不必记住这些方程式的符号了。嗯，这很好。现在我可以开始运行了，我可以做我之前做过的事情了。
        </p>
        <p>If I rotate that by theta sub a and see what the projection is on the x axis, that’s going to be given by the
            sine of the angle. But it’s going the wrong way, so I have to subtract it off. So that’s how I don’t have to
            remember what the signs are on these equations. Well, that was good. And now that I’m off and running I can
            do what I did before.</p>
        <h2 id="unknown-136">未知</h2>
        <h2>Unknown</h2>
        <p>这样讲课就容易多了。因为 x sub b 等于 x sub s 乘以 theta sub b 的余弦减去 y sub s 乘以 theta
            的余弦。哦，你让我犯错误了。真丢脸。我一般都能从所有不安的表情中看出来。但也应该有人大喊大叫。那是正弦，那是正弦。再说一遍。</p>
        <p>It makes it easy to give the lecture. Because this is going to be x sub b is equal to x sub s times the
            cosine of theta sub b minus y sub s times the cosine of theta. oh, you’re letting me make mistakes. Shame. I
            can generally tell by all the troubled looks. But there should be some shouting as well. That’s the sine and
            that’s the sine. And one more time.</p>
        <p>X sub u 等于 x sub s 乘以 theta sub u 的余弦减去 y sub s 乘以 theta sub u 的正弦。我忘了上面的
            b。所以有一些方程。我们不知道我们在做什么。我们只是盯着他们看一会儿，看看他们是否为我们唱了一首歌。所以让我们看看他们是否为我们唱了一首歌。</p>
        <p>X sub u is equal to x sub s times the cosine of theta sub u minus y sub s times the sine of theta sub u. And
            I forgot the b up there. So there are some equations. And we don’t know what we’re doing. We’re just going
            to stare at them awhile and see if they sing us a song. So let’s see if they sing us a song.</p>
        <p>那么 x sub a 和 x sub b 呢？这些是我们在图像中看到的东西。这些是我们可以测量的东西。那么所有那些 theta a 和 theta b
            的余弦和正弦呢？好吧，我们不知道它们是什么。但有一点很清楚。它们对物体上的所有点都适用。</p>
        <p>What about x sub a and x sub b? These are things that we see in the image. These are things that we can
            measure. What about all those cosines and sines of theta a’s and theta b’s. Well, we have no idea what they
            are. But one thing is clear. They’re true for all of the points on the object.</p>
        <h2 id="unknown-137">未知</h2>
        <h2>Unknown</h2>
        <p>因为当我们将物体旋转角度 θ 时，我们会将所有点旋转相同的角度，对吗？因此，对于物体的任何特定视图。这里我们处于标准位置。这里我们处于位置 a。当我们从标准位置移到位置 a
            时，指向物体上所有点的向量都会旋转相同的角度。</p>
        <p>Because when we rotate the object around by angle theta, we’re rotating all of the points through the same
            angle, right? So with respect to any particular view of the object. here we are in the standard position.
            Here we are in position a. The vectors to all of the points on the object are rotated by the same angle when
            we go from the standard position to the a position.</p>
        <p>因此，这意味着对于此特定渲染中的所有图像，通过特定的旋转，θ a、θ b 和 θ u，这些都是常数。现在请记住，这是针对特定的θ a、特定的θ be 和特定的θ
            u。只要我们讨论的是每个旋转的所有点，这些角度和余弦对于物体上的所有可能点都将是相同的。好的。</p>
        <p>So that means that for all of the images in this particular rendering, with a particular rotation by theta a,
            theta b, and theta u, those are constants. Now remember this is for a particular theta a, a particular theta
            be, and a particular theta u. As long as we’re talking about all of the points for each of those rotations,
            those angles and cosines are going to be the same for all possible points on the object. OK.</p>
        <p>现在我们回到高中代数专家那里，我们说，看看前两个方程，我们有两个方程，现在可以将其解释为两个未知数。剩下的未知数是什么？我们可以测量 a 和
            b。无论余弦是多少，它们对于所有图片都是相同的。所以如果我们将它们视为常数，那么我们就可以解出 x sub s 和 y sub s。对吗？</p>
        <p>So now we go back to our high school algebra experts and we say, look at these first two equations, We’ve got
            two equations and what we can now construe to be two unknowns. What are the unknowns that are left? We can
            measure a and b. Whatever the cosines are, they’re the same for all the pictures. So if we treat those as
            constants, then we can solve for x sub s and y sub s. Right?</p>
        <h2 id="unknown-138">未知</h2>
        <h2>Unknown</h2>
        <p>我们可以用 x sub a 和 x sub b 以及一大堆常数来解出 x sub s 和 y sub
            s。但是，我不知道，一大堆常数，让我们看看。我们可以收集所有这些余弦和正弦与余弦的比率以及所有这些东西，然后将它们放在一起。因为它们都是常数。然后我们就可以这样做了。</p>
        <p>We can solve for x sub s and y sub s in terms of x sub a and x sub b and a whole bunch of constants. But, I
            don’t know, a whole bunch of constants, let’s see. We can gather up all of those cosines and ratios of sines
            and cosines and all that stuff and put them all together. Because they’re all constants. And then we can do
            this.</p>
        <p>我们可以说 x sub u 等于。嗯，这取决于 x sub a 和 x sub b。当我们洗牌或操纵或摆弄所有这些余弦时，我们可以说 x sub a 的乘数是某个常数 alpha，而 x sub b 的乘数是某个常数
            beta。所以这不是小技巧。这只是对这些方程的线性操作。</p>
        <p>We can say x sub u is equal to. well, it’s going to depend on x sub a and x sub b. And by the time we wash or
            manipulate or screw around with all those cosines, we can say that the multiplier for x sub a is some
            constant alpha and the multiplier for x sub b is some constant beta. So that’s not a slight of hand. That’s
            just linear manipulation of those equations.</p>
        <p>这就是我们想要展示的，对于正交投影，这里不涉及透视，我们只是沿 x
            轴进行投影。我们可以证明，对于这种简化的情况，该方程必须成立。现在我想给你几个难题。因为这个东西很简单。假设我允许平移和旋转。会发生什么？学生：你只得到了 tau。基本上，你得到了一个常数。</p>
        <p>And that’s what we wanted to show, that for orthographic projection, which this is. there is no perspective
            involved here, we’re just taking the projection along the x axis. we can demonstrate for this simplified
            situation that equation must hold. Now I want to give you a few puzzles. Because this stuff is so simple.
            Suppose I allow translation as well as rotation. What’s going to happen? STUDENT: You just get the tau.
            Basically, you get a constant.</p>
        <h2 id="unknown-139">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：是的，你加了一个常数 tau。但是我们需要怎么做才能解出它呢？ 学生：减去它们 你减去两个方程式，然后 帕特里克·温斯顿：让我们看看，现在我们有三个未知数，对吧？ 我不知道 tau。我不知道 x
            子数。我也不知道 y 子数。所以我需要另一个方程式。我从哪里得到另一个方程式？ 学生：帕特里克·温斯顿：从另一张图片中。</p>
        <p>PATRICK WINSTON: Yeah, you add a constant, tau. But what do we need to do in order to solve it? STUDENT:
            Subtract them You subtract two equations and then PATRICK WINSTON: Let’s see, now we’ve got three unknowns,
            right? I don’t know tau. I don’t know x sub s. And I don’t know y sub s. So I need another equation. Where
            do I get the other equation. STUDENT: PATRICK WINSTON: From another picture.</p>
        <p>这就是为什么上面我需要四个点。这涵盖了旋转和平移三度的情况。在这个插图中，我只用了两张图片。那一个涉及 tau
            平移元素，所以我需要三张图片。而这个有完整的旋转，所以我需要四张图片。这个想法很棒，效果很好。问题是它在自然物体上效果不太好。</p>
        <p>That’s why up there I needed four points. That covers a situation where I’ve got three degrees of rotation
            and translation. Here I got by with just two pictures in this illustration. That one involved a tau
            translational element, so I needed three pictures. And this one’s got full rotation, so I needed four. So
            great idea, works fine. The trouble is it doesn’t work so fine on natural objects.</p>
        <p>它对制造出来的物品很有效，因为它们的尺寸都相同。所以如果我在工厂里制造了一百万个这样的物品，我也能毫不费力地认出它们。因为我所要做的就是拍三张照片，记录下一些点的坐标，然后就大功告成了。问题是自然界不是这样的。你也不是这样的。
        </p>
        <p>It works fine on things that are manufactured because they all have identical dimensions. So if I made a
            million of these in a factory, I’d have no trouble recognizing them. Because all I’d have to do is take
            three pictures, record the coordinates of some of the points, and I’d be done. The trouble is the natural
            world isn’t like this. And you aren’t like this either.</p>
        <h2 id="unknown-140">未知</h2>
        <h2>Unknown</h2>
        <p>我不知道，如果我试图识别人脸，那么做这些事情就没那么容易了。首先，找到准确的点，准确对应的点有点困难。我自己做的时候犯了一个错误。如果计算机犯了错误，它肯定会出错。因为它会使用不对应的点来进行预测。所以它会偏离得很远。
        </p>
        <p>I don’t know, if I’m trying to recognize faces, it’s not that easy to do all this. First of all, it’s a
            little difficult to find the exact point, the exactly corresponding points. I made a mistake in doing it
            myself. And if the computer made a mistake it would certainly make an error. Because it would be using non
            corresponding points to make the prediction. So it would be way off.</p>
        <p>但这仍然是从物体的局部特征开始进行识别的传统。因此，在研究了该理论之后，我们也发现它有些不足。在某些情况下，它很有效，但似乎无法解决整个识别问题。多年过去了。Shimon Ullman
            提出了另一种理论，它不是基于边缘碎片或特定特征的位置，而是基于相关性。</p>
        <p>But this is still in the tradition of working from local features in the objects toward recognition. So
            having looked at that theory, we also find it a little wanting. It works great it some circumstances,
            doesn’t seem to solve the whole recognition problem. Years pass. Shimon Ullman comes up with another theory
            that’s not so much based on edge fragments or the location of particular features but rather on correlation.
        </p>
        <p>比如说，拍一张克里希纳的脸，拍一张全班的照片，然后使用这种相关性蒙版，在整个班级的照片上移动它，看看它在哪里最大化。现在这很模糊。我稍后在谈论相关性时会解释。但它基本上是说，如果我有一张克里希纳的照片，我在哪里可以找到他？我会在一个地方找到他。但你知道吗？
        </p>
        <p>Taking a picture of, say, Krishna’s face, taking a picture of the whole class, and then using that as a kind
            of correlation mask, running it all over the picture of the class, seeing where it maximizes out. Now that’s
            vague. I’ll explain when I’m talking about correlation in a minute. But it’s basically saying, if I have a
            picture of Krishna, where do I find him? I’ll find him in one place. But you know what?</p>
        <h2 id="unknown-141">未知</h2>
        <h2>Unknown</h2>
        <p>克里希纳看起来和其他人都不一样。所以我可能找不到其他面孔。如果我的目标是找到所有的面孔，那么这个想法可能也行不通。或者，再举一个例子，这是一张一美元的钞票。我们的工资涨得不是很好，所以这是我的最后一张了。上面有一张乔治·华盛顿的照片。我可以看看整个班级。
        </p>
        <p>Krishna doesn’t look like anybody else. So I might not find any other faces. And if my objective is to find
            all the faces, then maybe that idea won’t work either. Or, to take another example, here’s a dollar bill. We
            haven’t had raises in quite well, so this is my last one. It’s got a picture of George Washington on it. And
            I can look all over the class.</p>
        <p>如果我将其用作人脸检测器，我会非常失望。因为我找不到任何脸。因为感谢上帝，没有人长得和乔治·华盛顿一模一样。所以相关性不会很好。所以这个想法是失败的。但等一下。我不必寻找整张脸。我可以只寻找眼睛。然后我可以寻找鼻子和嘴巴。
        </p>
        <p>And if I use this is as a face detector, I’d be sorely disappointed. Because I wouldn’t find any faces.
            Because thank God, nobody looks exactly like George Washington. So the correlation wouldn’t work very well.
            So that idea’s a loser. But wait a minute. I don’t have to look for the whole face. I could just look for
            eyes. And then I could look for noses and maybe mouths.</p>
        <p>也许我可以建立一个包含 10 只不同眼睛、10 个不同鼻子和 10
            个不同嘴巴的图库。这个想法可行吗？可能不太好。这个方法的问题是，我会在每个门把手上找到眼球。那里的东西不够多，无法给我提供可靠的关联。所以让我们通过画一些图来使这一点更具体一些。万圣节快到了。这是一张脸。好吗？这是另一张脸。
        </p>
        <p>And maybe I could have a library of 10 different eyes and 10 different noses and 10 different mouths. Would
            that idea work? Probably not so well. The trouble with that one is, I’d find eyeballs in every doorknob.
            There’s just not enough stuff there to give me a reliable correlation. So let’s make this a little more
            concrete by drawing some pictures. Halloween is approaching. So here’s a face. All right? Here’s another
            face.</p>
        <h2 id="unknown-142">未知</h2>
        <h2>Unknown</h2>
        <p>所以这些可能是我预先录制的南瓜脸库中的脸。现在出现了这张脸。会发生什么？好吧，我不知道。让我们再画一张脸。我不知道，我想那可能是一张非常奇怪的南瓜脸。但我的意思是它看起来不太像脸。</p>
        <p>So those might be faces in my pre recorded library of pumpkin faces. Now along comes this face. What’s going
            to happen? Well, I don’t know. Let’s draw yet another face. I don’t know, that could be a pretty weird
            pumpkin face, I suppose. But I mean it to be something that doesn’t look very much like a face.</p>
        <p>因此，如果我要对库中的这两张脸中的任何一张进行完全关联，那么没有一张脸与这张脸非常匹配。如果我要寻找像眼睛这样的精细特征，那么到处都有这些眼睛。所以这没什么帮助。所以你明白我的意思。你可以重新发明乌尔曼的伟大想法。它是什么？我们不寻找大的特征，比如整个脸。我们不寻找小的特征，比如单个眼睛。
        </p>
        <p>So if I’m doing a complete correlation with either of these faces in my library, neither one will match this
            one very well. If I’m looking for fine features like eyes, then I’ve got these eyes everywhere. So it
            doesn’t help very much. So you can see where I’m going. And you can reinvent Ullman’s great idea. What is
            it? We don’t look for big features, like whole faces. We don’t look for small features, like individual
            eyes.</p>
        <p>我们寻找中间特征，比如两只眼睛和一个鼻子，或者一张嘴和一个鼻子。所以当我们这样做时，我们可以说，现在，这里有两只眼睛和一个鼻子。好吧，这张图片中找到了。那么鼻子和嘴巴的组合呢？哦，那是在这里。但这两个特征在第四张图片中都找不到。这就是金发姑娘原则。
        </p>
        <p>We look for intermediate features, like two eyes and a nose, or a mouth and a nose. So when we do that, then
            we can say, now, here are two eyes and a nose. Well, that’s found in this one. And what about the
            combination of that nose and that mouth? Oh, that’s over here. But neither of those features can be found in
            the fourth image. So that’s the Goldilocks principle.</p>
        <h2 id="unknown-143">未知</h2>
        <h2>Unknown</h2>
        <p>当你做这种事情时，你希望事物既不太小也不太大。顺便说一下，我上面也提到了 Rumpelstiltskin 原则。因为我原本想说 Marr
            是命名事物的天才。尽管他的许多理论已经消失，但他仍然以这些名字而闻名，例如原始草图和 2.5 D 草图，因为他非常擅长命名概念。</p>
        <p>When you’re doing this sort of thing, you want things that are not too small and not too big. I’ve got the
            Rumpelstiltskin principle up there, too, by the way. Because I meant to mention that Marr was a genius at
            naming things. And even though many of his theories have faded, he’s still known for these names like primal
            sketch and two and a half D sketch because he was such an artist at naming the concepts.</p>
        <p>他甚至因为很多他没有做过的事情而获得赞誉。不是因为他故意试图不恰当地获得赞誉，而是因为他非常擅长命名事物。所以我们当时有 Rumpelstiltskin 原则。现在我们有 Goldilocks
            原则。不要太大，也不要太小。</p>
        <p>He even got credit for a lot of stuff that he didn’t do. Not because he was deliberately trying to get it
            inappropriately, but just because he was so good at naming stuff. So we had the Rumpelstiltskin principle
            back then. And now we have the Goldilocks principle. Not too big, not too small.</p>
        <p>但这给我们留下了最后一个问题，即如果我们想要寻找中等大小的特征，那么我们如何在众多面孔中找到它们？你看，我可能有一个图书馆，我可能会从你们当中挑选 10 个人，记录你们的眼睛。再挑选 10
            个人，记录你们的嘴巴。然后，它们可能会以独特的方式组合在一起，适合你们每个人。</p>
        <p>But that leaves us with the final question, which is, so if what we want to do is look for intermediate size
            features, how do we actually find them in a sea of faces out there? See, I might have a library, I might
            take 10 of you and record your eyes. Take another ten, record your mouths. And they may be put together in a
            unique way for each of you out there.</p>
        <h2 id="unknown-144">未知</h2>
        <h2>Unknown</h2>
        <p>但我很可能会在人群中的其他地方找到拉娜的眼睛。尼古拉的嘴巴也在人群中的其他地方。那么我们实际上该如何找到它们呢？我已经多次提到过“相关性”这个术语。让我来具体说明一下。让我们考虑一张像这样的一维脸。这是一个信号。我将考虑一张一维图像。
        </p>
        <p>But it’s likely that I’ll fin Lana’s eyes somewhere else in a crowd. And Nicola’s mouth somewhere else in a
            crowd. So how do we in fact go about finding them? And I mentioned the term correlation a couple of times
            now. Let me make that concrete. So let’s consider a one dimensional face that looks like this. Which is a
            signal. And I’m going to consider a one dimensional image.</p>
        <p>在那张一维图像中，我得到了脸部的复制品。问题是，我可以使用哪种算法来确定脸部在图像中的偏移量？因此，您可以看到一种可能性是，您只需对脸部信号和此处脸部范围内的信号进行积分，然后查看其如何相乘。</p>
        <p>And in that one dimensional image I’ve got a facsimile of the face. And the question is, what kind of
            algorithm could I use to determine the offset in the image where the face occurs? So you can see that one
            possibility is you just do an integral of the signal in the face and the signal out here over the extent of
            the face and see how it multiplies out.</p>
        <p>或者，为了使其不那么像律师，而更像 MIT，我们假设我们要做的就是最大化某个参数 x 上某个面在 x 上的积分，它是 x 和图像 g 的函数，它是 x 减去该偏移量的函数。因此，当偏移量 t
            等于该偏移量时，我们本质上就是将该值乘以自身，然后对面的范围进行积分。</p>
        <p>Or, to make it less lawyerly and more MITish, let’s say that what we’re going to do is we’re going to
            maximize over some parameter x the integral over x of some face, which is a function of x and the image g,
            which is a function of x minus that offset. So when the offset, t, is equal to this offset, then we’re
            essentially multiplying the thing by itself and integrating over the extent of the face.</p>
        <h2 id="unknown-145">未知</h2>
        <h2>Unknown</h2>
        <p>如果它们排列整齐，结果会非常大，如果它们排列不整齐，结果会非常小。如果我们给图像添加大量噪声，结果也会非常大。但这些是图像，不是一维的。不过没关系。在这里做修改很容易。我们将最大化平移参数 x 和 y。这些不再只是 x
            的函数，也是 y 的函数。</p>
        <p>And that gives you a very big number if they’re lined up and a very small number if they’re not. And it’s
            even true if we add a whole lot of noise to the images. But these are images. They’re not one dimensional.
            But that’s OK. It’s easy enough to make a modification here. We’re going to maximize over translation
            parameters x and y. And these are no longer functions of just x, they’re also functions of y.</p>
        <p>就像这样。所以基本上就是这样的。我们不会详细介绍标准化和所有类似的东西，因为这些东西仍由其他课程负责。所以你想看一个演示吗？好的。好吧。所以在不知不觉中，Nicola 和 Erica
            借给了我们他们的照片。它们嵌入在巨大的噪音场中。很容易就能认出 Erica 和 Nicola，对吧？</p>
        <p>Like so. So that’s basically how it works. We won’t go into details about normalization and all that sort of
            thing because that’s the stuff of which other courses remain the custodians. So would you like to see a
            demonstration? OK. All right. So without realizing it, Nicola and Erica have loaned us their pictures. And
            they are embedded in that big field of noise. And it’s pretty easy to pick out Erica and Nicola, right?</p>
        <p>因为我们实际上非常擅长从这些图像中挑选出人脸。所以让我们添加一些噪音。现在有点难了。我要做的是，我将使用 Nicola
            的脸作为蒙版，对整个图像运行这个关联程序，看看尽管里面有很多噪音，关联在哪里达到峰值。砰，他在那里。我不知道，也许我们也能找到 Erica。我忘了她在哪里了。</p>
        <p>Because we are actually pretty good at picking faces out of these images. So let’s add some noise. It’s a
            little harder now. What I’m going to is I’m going to run this correlation program over this whole image
            using Nicola’s face as a mask and seeing where the correlation peaks up, in spite of all the noise that’s in
            there. Boom, there he is. I don’t know, maybe we can find Erica too. I forgot where she was.</p>
        <h2 id="unknown-146">未知</h2>
        <h2>Unknown</h2>
        <p>我找不到她。她就在那里。不幸的是，这里的参数不太好。你看到了吗？让我再来一个版本。我会做一些实时编程。我一直在尝试重置参数，以便演示中的图像清晰地显示在那里。让我们看看这是否能更好地工作。好的，让我们添加一些噪音。让我们找到艾丽卡。她就在那里。
        </p>
        <p>I can’t find her. There she is. Unfortunately the parameters aren’t very good here. Do you see that? Let me
            get another version of this. I’ll just do some real time programming. I’ve been trying to reset the
            parameters so that the images in the demonstration comes out clearly up there. Let’s see if this works a
            little better. OK, so let’s add some noise. And let’s find Erica. There she is.</p>
        <p>还有一些其他的东西看起来有点像 Erica。但没有什么东西看起来和 Erica 一模一样。所以让我们试试 Nicola 的眼睛。所以它们在背景中非常显眼。让我们看看我们能不能找到 Erica
            的眼睛。所以它们在背景中非常显眼。请注意，它也得到了 Nicola 的眼睛。所以两只眼睛是一个中等大小的限制。它足够宽松，可以匹配不止一个人。</p>
        <p>There are some other things that look a little bit like Erica. But nothing looks quite exactly like Erica. So
            let’s try Nicola’s eyes. So they stand out pretty clearly against the background. Let’s see if we can find
            Erica’s eyes. So they stand out pretty clearly against the background. Notice that it also gets Nicola’s
            eyes. So two eyes is an intermediate size constraint. It’s loose enough that it will match more than one
            person.</p>
        <p>但它并不像寻找一只眼睛那么松散。你看，它们到处都是。所以两只眼睛和一个鼻子，一张嘴和一个鼻子，作为中间特征会更好。但最好的特征是什么并不重要，因为你可以通过实验来解决这个问题。这就是相关性的工作原理。令人惊讶的是，你可以添加多少噪音，它仍然会选出正确的东西。
        </p>
        <p>But it’s not so loose that it’s as bad as looking for one eye. See, they’re all over the place. So two eyes
            and a nose, a mouth and a nose, that would be even better as an intermediate feature. But it doesn’t matter
            what the best ones are, because you can work that out experimentally. So that’s how correlation works. And
            it’s just amazing how much noise you can add and it’ll still pick out the right stuff.</p>
        <h2 id="unknown-147">未知</h2>
        <h2>Unknown</h2>
        <p>这就是 Nicola。嘭。非常清晰。想再加点噪音吗？我不知道，我能看到，但那是因为我也是一名非常优秀的相关器。嘭。我不知道，让我们再加点噪音吧。很难去除它。它挑选噪音的能力真是令人惊讶。这很好。这很酷。现在，但 30
            年过去了，我们还没有完成，因为还有一些问题。</p>
        <p>There’s Nicola. Boom. Very clear. Want to add some more noise? I don’t know, I can see it, but that’s because
            I’m a pretty good correlator, too. Boom. I don’t know, let’s add some more noise. It’s just hard to get rid
            of it. It’s just amazing how well it picks it out. That’s good. That’s cool. Now, but the reason that this
            is 30 years and we’re still not done is there are still some questions.</p>
        <p>这是直接识别事物。我怎么能从侧面认出你在走廊里呢？没人知道。一种可能是你有能力做出这些转变。如果是这样，那么对齐理论就会发挥作用。另一种理论是，嗯，在我见过你一次之后，我就能看着你转过头，并不断记录你在所有可能的角度的样子。那会起作用。
        </p>
        <p>This is recognizing stuff straight on. How is it I can recognize you in the hall from the side? Nobody knows.
            One possibility is that you have an ability to make those transformations. If so, then that alignment theory
            has a role to play. Another theory is that, well, after I’ve seen you once I can watch you turn your head
            and keep recording what you look like at all possible angles. That would work.</p>
        <p>问题是，里面的东西够多吗？也许够多。我们不知道。现在要怎样才能打破这个机制？好吧，我不知道。我们看看能不能打破这个机制。我们看看你能不能认出一些熟悉的面孔。那是谁？快。学生：奥巴马。帕特里克·温斯顿：哦，这太简单了。我们看看能不能弄点难点的。是的，那是奥巴马。那是谁？学生：布什。帕特里克·温斯顿：天哪。
        </p>
        <p>The trouble is, is there enough stuff in there? Maybe. We don’t know. Now what would it take to break this
            mechanism? Well, I don’t know. Let’s just see if we can break the mechanism. Let’s see if you can recognize
            some well known faces. Who’s that? Quick. STUDENT: Obama. PATRICK WINSTON: Oh, that’s too easy. We’ll see if
            we can make some harder ones. Yeah, that’s Obama. Who’s this? STUDENT: Bush. PATRICK WINSTON: Oh boy.</p>
        <h2 id="unknown-148">未知</h2>
        <h2>Unknown</h2>
        <p>你在这方面真的很擅长。那是布什。这个人怎么样？学生：克里。帕特里克·温斯顿：好的。现在我明白了。有些人开始转过头。这不公平。帕特里克·温斯顿：这不公平。因为你知道发生了什么，如果这种南瓜在理论上是正确的，那么当你把面倒过来时，你就会失去那些具有垂直分量的特征的相关性。
        </p>
        <p>You’re really good at this. That’s Bush. How about this guy? STUDENT: Kerry. PATRICK WINSTON: OK. Now I’ve
            got it. Some people are starting to turn their heads. And that’s not fair. PATRICK WINSTON: That’s not fair.
            Because you see what’s happened is that if this kind of pumpkin in theory is correct, then when you turn the
            face upside down you lose the correlation of those features that have vertical components.</p>
        <p>所以如果你有两只眼睛和一个鼻子，当它们倒过来时，它们不会匹配。好吧，让我们看看。我们再试几次。那是谁？学生：戈尔巴乔夫。帕特里克·温斯顿：戈尔巴乔夫。谁说的？列昂尼德，你在哪里？这是戈尔巴乔夫，对吧？你可以根据他头顶上的小胎记认出他。再问一个。谁。哦，这很简单。是谁？那是克林顿。这个怎么样？
        </p>
        <p>So if you have two eyes and a nose, they won’t match two eyes and a nose when they’re turned upside down.
            Well, let’s see. We’ll try some more. Who’s that? STUDENT: Gorbachev. PATRICK WINSTON: Gorbachev. Who said
            that? Leonid, where are you? This is Gorbachev, right? You can recognize him because of the little birthmark
            on the top of his head. One more. Who’s. oh, that’s easy. Who is it? That’s Clinton. How about this one?</p>
        <p>你知道在麻省理工学院工作是多么丢脸吗？那就是我。帕特里克·温斯顿：你甚至都不知道。哦，天哪。所以这可能是相关理论的证据。但当然，把脸倒过来也会让对齐变得非常困难。所以它也会打破对齐理论。下课后我再说，是不是搞错了？学生：不，不是。
        </p>
        <p>Do you see how insulting it is to be at MIT? That’s me. PATRICK WINSTON: And you didn’t even know. Oh, god.
            So this might be evidence for the correlation theory. But of course, turning the face upside down would make
            it very difficult to do alignment, too. So it would break out alignment theory, as well. Let me get that
            after class, Was there a mistake, or? STUDENT: No, no.</p>
        <h2 id="unknown-149">未知</h2>
        <h2>Unknown</h2>
        <p>我只是好奇拉伸是否会破坏相关性。帕特里克·温斯顿：什么会破坏结构？什么？拉伸？学生：帕特里克·温斯顿：艾略特问拉伸是否会破坏相关性。答案是，我认为，垂直方向的拉伸比水平方向的拉伸更糟糕。因为当你转动头部时，水平方向会产生一定程度的拉伸。
        </p>
        <p>I was just curious stretching would break the correlation. PATRICK WINSTON: If what would break the
            structure? What? Stretching? STUDENT: PATRICK WINSTON: Elliot asked if stretching would break the
            correlation. And the answer is, I think, stretching in the vertical dimension is worse than stretching in
            the horizontal dimension. Because you get a certain amount of stretching in the horizontal dimension when
            you just turn your head.</p>
        <p>顺便说一句，因为我们的脸基本上是装在圆柱体上的，这种变换实际上可能有效。这是对你问题的回答的补充，艾略特。但现在你说，好吧，所以这个问题还没有完全解决。你可以解决这个问题。但如果你真的想解决这个问题，让我告诉你计算机视觉领域目前存在的问题是什么。
        </p>
        <p>By the way, since our faces are basically mounted on a cylinder, this kind of transformation might actually
            work. That’s a sidebar to the answer to your question, Elliot. But now you say, well, OK, so this is not
            completely solved. You can work this out. But if you really want to work something out, let me tell you what
            the current questions are in computer vision.</p>
        <p>人们在识别方面已经研究了很长时间，但在我看来，他们忽视了更严肃的问题。更严肃的问题是，你如何从视觉上判断发生了什么？如果你能编写一个程序，可靠地确定这些动词何时在你的视野中发生，我明天就会签署你的博士论文。那里有 48
            个动词。这就是今天的挑战。</p>
        <p>People have worked for an awful long time on this recognition stuff and, to my mind, have neglected the more
            serious questions. It’s more serious questions are, how do you visually determine what’s happening? If you
            could write a program that would reliably determine when these verbs are happening in your field of view, I
            will sign your Ph.D.&nbsp;thesis tomorrow. There are 48 of them there. And that is today’s challenge.</p>
        <h2 id="unknown-150">未知</h2>
        <h2>Unknown</h2>
        <p>但是因为时间不多了，我想略过这个，对你做一个实验。我要你告诉我我在做什么。学生：帕特里克·温斯顿：那么最好的单词答案是？学生：喝酒。帕特里克·温斯顿：好的，这不是一个陷阱问题。好的，最好的单词答案。克里斯托弗，你怎么看？学生：敬酒。帕特里克·温斯顿：克里斯托弗。嗯，你。你。学生：敬酒。帕特里克·温斯顿：什么？敬酒。好的。这不是陷阱问题。这是怎么回事？
        </p>
        <p>But since we’re short on time, I want to skip over that and perform an experiment on you. I want you to tell
            me what I’m doing. STUDENT: PATRICK WINSTON: So the best single word answer is? STUDENT: Drinking. PATRICK
            WINSTON: OK, this is not a trick question. OK, the best single word answer. Christopher, what do you think?
            STUDENT: Toasting. PATRICK WINSTON: Christopher. Well, you. You. STUDENT: Toasting. PATRICK WINSTON: What?
            Toasting. OK. Not a trick question. What’s happening here?</p>
        <p>最佳单词答案？学生：喝水。帕特里克·温斯顿：喝水。哪一对看起来更像？帕特里克·温斯顿：所以那只猫在喝水，没人能认出它。我相信这是因为你在讲故事。所以我们讲故事的能力甚至延伸到我们的视觉器官。所以这里的故事是，某种动物显然有想找点东西喝的冲动，水正从那只动物的嘴里流过。这就是喝水的故事。
        </p>
        <p>Best single word answer? STUDENT: Drinking. PATRICK WINSTON: Is drinking. Which pair look more alike? PATRICK
            WINSTON: So that cat is drinking and nobody has any trouble recognizing that. And I believe it’s because
            you’re telling a story. So our power of storytelling even reaches down into our visual apparatus. So the
            story here is that some animal has evidently had an urge to find something to drink and water is passing
            through that animal’s mouth. That’s the drinking story.</p>
        <p>因此，尽管它们在视觉上看起来差别很大，但我们视觉系统底部的东西为我们的故事装置提供了足够的证据，让我们可以给左边的猫和右边的猫不同的标签，并识别出这只猫在喝水。这就是故事的结尾。</p>
        <p>So even though they look enormously different visually, the stuff at the bottom of our vision system provides
            enough evidence for our story apparatus so that we can give the left one and the right one different labels
            and recognize the cat is drinking. And that’s the end of the story.</p>
        <h1 id="introduction-to-learning-nearest-neighbors">10. 学习简介、最近邻</h1>
        <h1>10. Introduction to Learning, Nearest Neighbors</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:56 <a
                href="https://youtube.com/watch?v=09mb78oiPkA">https://youtube.com/watch?v=09mb78oiPkA</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:56 <a
                href="https://youtube.com/watch?v=09mb78oiPkA">https://youtube.com/watch?v=09mb78oiPkA</a></p>
        <h2 id="unknown-151">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿教授：好吧，这就是鼓童鼓手。他们是一群约 30 或 40 名日本人，居住在日本沿海某岛的一个村庄里，保留了日本传统音乐。这是一个不寻常的半社区团体。他们通常在早餐前跑 10 公里左右，早餐在早上
            5:00 供应。奇怪的团体。</p>
        <p>PROF. PATRICK WINSTON: Well that’s the Kodo Drummers. They’re a group of about 30 or 40 Japanese people who
            live in a village on some island off the coast of Japan, and preserve traditional Japanese music. It’s an
            unusual semi communal group. They generally run about 10 kilometers before breakfast, which is served at
            5:00 AM. Strange group.</p>
        <p>不会错过一场世界级的音乐会，尽管他们似乎不会很快来到波士顿地区。如果你去听 Kodo Drummers
            的音乐会，你应该去。如果你不再年轻，你会想带上耳塞。因为，随着人类年龄的增长，我们内耳的动态范围控制往往会变得不那么有效。</p>
        <p>Wouldn’t miss a concert for the world, although they, alas, don’t seem to be coming down to the Boston area
            very soon. If you go to a concert from the Kodo Drummers. and you should. and if you’re no longer young,
            you’ll want to bring earplugs. Because, as we humans get older the dynamic range control in our inner ear
            tends to be less effective.</p>
        <p>所以这就是为什么像我这个年纪的人可能会觉得某些音乐太吵，而你却觉得没问题。因为你有更好的自动增益控制。就像任何一种通讯设备一样，声音的强度也可以控制。啊，但我要说点别的了。当我喝咖啡的时候，你们中的许多人都惊讶地看着我。
        </p>
        <p>So that’s why a person of my age might find some piece of music excruciatingly loud, whereas you’ll think
            it’s just fine. Because you have better automatic gain control. Just like in any kind of communication
            device there’s a control on how intense the sound gets. Ah, but I go off on a sidebar. Many of you have
            looked at me in astonishment as I drink my coffee.</p>
        <h2 id="unknown-152">未知</h2>
        <h2>Unknown</h2>
        <p>你肯定一直在对自己说，你知道，温斯顿看起来不像职业运动员，但他喝咖啡似乎没有问题。所以今天的材料会相当简单。所以我想给你一个附带问题，让你思考一个人怎么可能做到这一点。怎么可能呢？</p>
        <p>And you have undoubtedly have been saying to yourself, you know, Winston doesn’t look like a professional
            athlete, but he seemed to have no trouble drinking his coffee. So today’s material is going to be pretty
            easy. So I want to give you the side problem of thinking about how it’s possible for somebody to do that.
            How is it possible?</p>
        <p>如果计算机程序想要一杯咖啡，你会如何让它伸手去喝？这就是我希望你解决的一个难题。还有另一个难题。这个难题与减肥饮料有关。这就是所谓的健怡可乐。是的，它已经成熟了。如果你拿一瓶健怡可乐问自己，狗会认为健怡可乐是用来做什么的？
        </p>
        <p>How would you make a computer program that could reach out and drink a cup of coffee, if it wanted a cup of
            coffee? So that’s one puzzle I’d like you to work on. There’s another puzzle, too. And that puzzle concerns
            diet drinks. This is a so called Diet Coke. Yeah, it’s ripe. If you take a Diet Coke and ask yourself, what
            would a dog think a Diet Coke is for?</p>
        <p>这是另一个谜题，你可以在我们学习当天的内容时解答。这是我们关于学习的第一堂课，我想在开始时花一两分钟介绍一下情况。然后我们将快速浏览一些关于最近邻学习的材料。然后我们将以宣传的关于睡眠的讨论结束。</p>
        <p>That’s another puzzle that you can work on while we go through the material of the day. So this is our first
            lecture on learning, and I want to spend a minute or two in the beginning talking about the lay of the land.
            And then we’ll race through some material on nearest neighbor learning. And then we’ll finish up with the
            advertised discussion of sleep.</p>
        <h2 id="unknown-153">未知</h2>
        <h2>Unknown</h2>
        <p>因为我知道你们中的许多人认为，麻省理工学院的学生很坚强，不需要睡觉之类的。我们需要在学期结束前解决这个问题，否则就太晚了，无法回到正轨。好吧。故事是这样的。现在我们要看待学习的方式有两种。有这种，还有那种。</p>
        <p>Because I know many of you think that because your MIT students you’re pretty tough, and you don’t need to
            sleep and stuff. And we need to address that question before it’s too late in the semester to get back on
            track. All right. So here’s the story. Now the way we’re going to look at learning is there are two kinds.
            There’s this kind, and there’s that kind.</p>
        <p>我们将对这两种类型进行一些讨论。正确的类型是基于对规律性观察的学习。计算机在这方面尤其擅长。在与基于规律性的学习相关的内容中，我们将讨论今天的主题，即最近邻。然后，稍后我们将讨论神经网络。然后，在本节的末尾，我们将讨论增强。
        </p>
        <p>And we’re going to talk a little bit about both kinds. The kind of the right is learning based on
            observations of regularity. And computers are particularly good at this stuff. And amongst the things that
            we’ll talk about in connection with regularity based learning are today’s topic, which is nearest neighbors.
            Then a little bit downstream we’ll talk about neural nets. And then somewhere near the end of the segment,
            we’ll talk about boosting.</p>
        <p>这些想法来自四面八方。特别是我们今天讨论的东西，最近邻，是模式识别领域的东西。它是模式识别期刊中充斥的东西。这个东西已经存在很长时间了。这是否意味着它不好？</p>
        <p>And these ideas come from all over the place. In particular, the stuff we’re talking about today, nearest
            neighbors, is the stuff of which the field of pattern recognition. it’s the stuff of which pattern
            recognition journals are filled. This stuff has been around a long time. Does that mean it’s not good?</p>
        <h2 id="unknown-154">未知</h2>
        <h2>Unknown</h2>
        <p>我希望不是，因为那意味着你在 1801 年学到的一切都不好，因为 1910
            年教授的是同样的课程。所以它已经存在了一段时间，但它非常有用。当你遇到学习问题时，它是你首先要尝试的东西，因为它是最简单的东西。你总是想先尝试最简单的东西，然后再尝试一些你不太可能理解的更复杂的东西。</p>
        <p>I hope not, because that would mean that everything you learned in 1801 is not good, because the same course
            was taught 1910. So it has been around a while, but it’s extremely useful. And it’s the first thing to try
            when you have a learning problem, because it’s the simplest thing. And you always want to try the simplest
            thing before you try something more complex that you will be less likely to understand.</p>
        <p>这就是最近邻和模式识别。神经网络知识的保管者，这有点像是模仿生物学的尝试。当我们深入讨论这个问题时，我会对此进行大量的诽谤。最后，这是理论家的礼物。所以在人工智能领域，我们发明了一些东西，我们借用了一些东西，我们窃取了一些东西，我们支持了一些东西，我们改进了一些东西。
        </p>
        <p>So that’s nearest neighbors and pattern recognitions. And the custodians of knowledge about neural nets, well
            this is sort of an attempt to mimic biology. And I’ll cast a lot of calumny on that when we get down there
            to talk about it. And finally, this is the gift of the theoreticians. So we in AI have invented some stuff,
            we’ve borrowed some stuff, we’ve stolen some stuff, we’ve championed some stuff, and we’ve improved some
            stuff.</p>
        <p>这就是为什么我们对学习的讨论会涉及所有这些主题。这就是基于规律的学习。你可以把它看作推土机计算的一个分支。因为，在做这些事情时，计算机处理信息就像推土机处理碎石一样。现在，这不一定是人类进行的所有学习的好模型。毕竟，学习是我们认为表征人类智能的事物之一。
        </p>
        <p>That’s why our discussion of learning will reach around all of these topics. So that’s regularity based
            learning. And you can think of this as the branch of bulldozer computing. Because, when doing these kinds of
            things, a computer’s processing information like a bulldozer processes gravel. Now that’s not necessarily a
            good model for all the kinds of learning that humans do. And after all, learning is one of the things that
            we think characterizes human intelligence.</p>
        <h2 id="unknown-155">未知</h2>
        <h2>Unknown</h2>
        <p>因此，如果我们要建立模型并理解这一点，我们也必须沿着另一个分支走下去。沿着另一个分支，我们会发现基于约束的学习理念。我们称之为人类的一面。我们将讨论实现一次性学习的理念，例如，从每次经历中学习到一些确定的东西。我们将讨论基于解释的学习。
        </p>
        <p>So if we were to build models of it and understand that we have to go down this other branch, too. And down
            this other branch we find learning ideas that are based on constraint. And let’s call this the human like
            side of the picture. And we’ll talk about ideas that enable, for example, one shot learning, where you learn
            something definite from each experience. And we’ll talk about explanation based learning.</p>
        <p>顺便问一下，你是通过自我解释来学习的吗？我想是的。我曾经有一个学生，他只拿了 A 和 F。我问他，你在哪些科目上拿了 A？为什么你不是在所有科目上都拿 A？他说，哦，当我说服自己材料是真实的时，我就能在这些科目上拿
            A。所以学习是自我解释的副产品，是一种重要的学习方式。</p>
        <p>By the way, do you learn by self explanation? I think so. I had an advisee once, who got nothing but A’s and
            F’s. And I said, what are the subjects that you get A’s in? And why don’t you get A’s in all of your
            subjects? And he said, oh, I get A’s in the subjects when I convince myself the material is true. So the
            learning was a byproduct of self explanation, an important kind of learning.</p>
        <p>但可惜，那是下游。我们今天要讨论的是这条穿过树的路径，即最近邻学习。一般来说，这就是它的工作原理。这只是我们正在讨论的内容的一般情况。当你想到模式识别或基于最近邻的学习时，你会得到某种机制来生成特征向量。所以我们将其称为特征检测器。然后输出一个值向量。
        </p>
        <p>But alas, that’s downstream. And what we’re going to talk about today is this path through the tree, nearest
            neighbor learning. And here’s how it works, in general. Here’s just a general picture of what we’re talking
            about. When you think of pattern recognition, or nearest neighbor based learning, you’ve got some sort of
            mechanism that generates a vector of features. So we’ll call this the feature detector. And out comes a
            vector of values.</p>
        <h2 id="unknown-156">未知</h2>
        <h2>Unknown</h2>
        <p>然后，该值向量进入某种比较器。该比较器将特征向量与来自可能性库的特征向量进行比较。通过找到最接近的匹配，比较器确定某个对象是什么。它进行识别。让我用这些电气盖来演示一下。假设它们到达装配线上，某个机器人想对它们进行分类。它会怎么做呢？
        </p>
        <p>And that vector of values goes into a comparator of some sort. And that comparator compares the feature
            vector with feature vectors coming from a library of possibilities. And by finding the closest match the
            comparator determines what some object is. It does recognition. So let me demonstrate that with these
            electrical covers. Suppose they arrived on an assembly line and some robot wants to sort them. How would it
            go about doing that?</p>
        <p>嗯，它可以轻松使用最近邻排序机制。那么它将如何工作呢？嗯，它的工作方式如下。您将进行一些测量。我们将在二维空间中进行一些测量。其中一个测量值可能是总面积，包括这些电气盖板孔的面积。为了让您可以不用伸长脖子就能跟上我的做法，让我看看我是否能找到电气盖板。
        </p>
        <p>Well it could easily use the nearest neighbor sorting mechanism. So how would that work? Well here’s how if
            would work. You would make some measurements. And it we’ll just make some measurements in two dimensions.
            And one of those measurements might be the total area, including the area of the holes of these electrical
            covers. Just so you can follow what I’m doing without craning your neck, let me see if I can find the
            electrical covers.</p>
        <p>是的，它们就在那里。所以我们有一个大的空白的，还有其他几个。所以我们也可以测量孔的面积。而这个，这个家伙，这个大的白色没有孔的面积，它的总面积最大。所以它会在这个特征空间中找到自己。然后我们得到了这个家伙，里面有四个插座的空间。
        </p>
        <p>Yes, there they are. So we’ve got one big blank one, and several others. So we might also measure the hole
            area. And this one here, this guy here, this big white one has no hole area, and its got the maximum amount
            of total area. So it will find itself at that point in this space of features. Then we’ve got the guy here,
            with room for four sockets in it.</p>
        <h2 id="unknown-157">未知</h2>
        <h2>Unknown</h2>
        <p>这是最大孔面积，也是最大面积。所以它会直接向上，可能在这里。然后，除了这两个之外，我们还有一个空白封面，像这样，它有大约任何封面总面积的
            1/2，所以我们就把它放在这里。最后，我们还有另一个这样的人。哦，对了，就是这个。1/2 的孔面积，1/2 的总面积。</p>
        <p>That’s got the maximum amount of hole area, as well as the maximum amount of area. So it will be right
            straight up, maybe up here. Then we have, in addition to those two, a blank cover, like this, that’s got
            about 1/2 the total area that any cover can have, so we’ll put it right here. And finally, we’ve got one
            more of these guys. Oh yes, this one. 1/2 the hole area, and 1/2 the total area.</p>
        <p>所以我不知道，让我们看看。它会去哪里？也许就在这儿。现在我们的机器人正在观察装配线，它看到有东西经过，并测量面积。当然，还有噪音。制造过程中存在差异。所以它不会精确地位于任何东西的上方。但假设它就在那里。</p>
        <p>So I don’t know, let’s see. Where will that go? Maybe about right here. So now our robot is looking on the
            assembly line and it sees something coming along, and it measures the area. And of course, there’s noise.
            There’s manufacturing variability. So it won’t be precisely on top of anything. But suppose it’s right
            there.</p>
        <p>好吧，不需要任何天才，无论是人还是计算机，就能弄清楚这一定是那些面积最大和洞面积最大的物体之一。但现在让我们问一些其他问题。那会是什么？或者这是什么？等等。好吧，我们必须弄清楚这些新近看到的物体最接近什么，以便进行识别。但这很容易。
        </p>
        <p>Well it doesn’t take any genius human, human or computer, to figure out that this must be one of those guys
            with maximum area and maximum hole area. But now let’s ask some other questions. Where would what would that
            be? Or what would this be? and so on. Well we have to figure out what those newly viewed objects are closest
            to in order to do an identification. But that’s easy.</p>
        <h2 id="unknown-158">未知</h2>
        <h2>Unknown</h2>
        <p>我们只需计算与所有那些标准、柏拉图式、理想事物描述的距离，然后找出哪个距离最近。但一般来说，考虑在这些不同的理想位置之间建立一些边界会更容易一些，这样我们就可以说出，物体在哪个区域？然后我们就会立即知道它属于哪个类别。
        </p>
        <p>We just calculate the distance to all of those standard, platonic, ideal descriptions of things, and we find
            out which is nearest. But in general, it’s a little easier to think about producing some boundaries between
            these various idealize places, so that we can just say, well which area is the object in? And then we’ll
            know instantaneously to what category it belongs.</p>
        <p>所以如果我们只有两条线，比如紫色线和黄色线，那就简单了。因为我们只需在两者之间画一条线，紫色和黄色之间的线作为垂直平分线。所以，如果只有两条线，就画出来而不是谈论它，那就是边界线。虚线以南的任何东西都是紫色，以北的任何东西都是黄色。
        </p>
        <p>So if we only had two, like the purple one and the yellow one, it would be easy. Because, we would just
            construct a line between the two, with a line between the purple and yellow as a perpendicular bisector. And
            so drawing it out instead of talking about it, if there were only two, that would be the boundary line.
            Anything south of the dotted line would be purple, and anything north would be yellow.</p>
        <p>现在我们可以对所有点进行此操作，对吧？所以我们可以算出来。哦，皮埃尔，你能把笔记本电脑关上吗？所以如果我们想对所有这些人都这样做，它会像这样。我最好去掉这些带点的 x，以免它们让我感到困惑。</p>
        <p>And now we can do this with all the points, right? So we can figure out. oh could you, Pierre, could you just
            close the lap top please? So if we want to do this with all these guys it would go something like this. I
            better get rid of these dotted x’s before they confuse me.</p>
        <h2 id="unknown-159">未知</h2>
        <h2>Unknown</h2>
        <p>让我们看看，如果只有这两个点，那么我们会想在连接它们的线之间构造一条垂直平分线。如果只有这两个点，我会想构造这条垂直平分线。如果只有这两个点，我会想构造一条垂直平分线。如果只有这两个点，我会想构造一条垂直平分线。哦，你明白我在做什么吗？
        </p>
        <p>Let’s see, if these were the only two points, then we would want to construct a perpendicular bisector
            between the line joining them. And if these two were the only points, I would want to construct this
            perpendicular bisector. And if these two were the only points, I would want to construct a perpendicular
            bisector. And if these two points were the only ones involved I’d want to construct. oh, you see what I’m
            doing?</p>
        <p>我正在构造垂直平分线，这些线正是我划分这个空间所需要的线。它会像这样划分。我不会说我们会在考试中给你这样的问题，但过去十年我们每年都有。划分一个空间并产生。我们想给它起个名字。</p>
        <p>I’m constructing perpendicular bisectors, and those are exactly the lines that I need in order to divide up
            this space. And it’s going to divide up like this. And I won’t say we’ll give you a problem like this on an
            examination, but we have every year in the past ten. To divide up a space and produce. something we would
            like to give a name.</p>
        <p>你知道，侏儒怪效应，当你有名字的时候，你就拥有了控制权。所以我们把这些称为决策边界。好吧，这些是简单的决策边界，由一个简单的想法在样本空间中产生。但关于这一点，还有一点要说。因为，我谈论这个的时候，就好像我们在试图识别一些东西。还有另一种思考方式，这非常重要。那就是这个。
        </p>
        <p>You know, Rumpelstiltskin effect, when you have a name you get power over it. So we’re going to call these
            decision boundaries. OK so those are the simple decision boundaries, produced in a sample space, by a simple
            idea. But there is a little bit more to say about this. Because, I’ve talked about this as if we’re trying
            to identify something. There’s another way of thinking about it that’s extremely important. And that is
            this.</p>
        <h2 id="unknown-160">未知</h2>
        <h2>Unknown</h2>
        <p>假设我拿来一个全新的盖子，以前从未见过。我只测量，好吧，假设我只测量洞的面积。洞的面积就是这个值。最可能的总面积是多少？我不知道。但有一种弱原理，如果某样东西在某些方面相似，那么它在其他方面也可能相似。</p>
        <p>Suppose I come in with a brand new cover, never before seen. And I only measure, well let’s say I only
            measure the hole area. And the hole area has that value. What is the most likely total area? Well I don’t
            know. But there’s a kind of weak principle of, if something is similar in some respects, it’s likely to be
            similar in other respects.</p>
        <p>所以我猜，如果你拿把刀架在我的喉咙上，把我逼到墙角，它的总面积将和整个橙色覆盖物的总面积差不多。所以这是一个虚构的例子，我不会过多地解释它。但我确实想多解释一下第一原则。</p>
        <p>So I’m going to guess, if you hold a knife to my throat and back me into a corner, that it’s total area is
            going to be something like that orange cover whole, total area. So this is a contrived example, and I don’t
            make too much of it. But I do want to make a lot of that first principal, over there.</p>
        <p>这个想法就是，如果某件事在某些方面相似，那么它在其他方面也可能相似。因为教育的大部分内容都是如此。童话故事、法律案例、医疗案例、商业案例。如果你发现它们在某些方面与你现在的情况相似，那么它们在其他方面也可能相似。
        </p>
        <p>And that is the idea that, if something is similar in some respects, it’s likely to be similar in other
            respects. Because that’s what most of education is about. Fairy tales, legal cases, medical cases, business
            cases. if you can see that there are similar in some respects to a situation you’ve got now, then it’s
            likely that they’re going to be similar in other respects, as well.</p>
        <h2 id="unknown-161">未知</h2>
        <h2>Unknown</h2>
        <p>所以当我们学习的时候，我们不仅仅是在学习识别一个类别，我们学习是因为我们试图应用某种先例。这就是这个故事。好吧，这是一个简单的想法，但它有任何应用吗？答案是肯定的。这是一个例子。我的第二个例子，细胞识别的例子。假设你有一些白细胞，你会怎么做？你可以测量细胞的总面积。
        </p>
        <p>So when we’re learning, we’re not just learning to recognize a category, we’re learning because we’re
            attempting to apply some kind of precedent. That’s the story on that. Well that’s a simple idea but does it
            have any application? The answer is sure. Here’s an example. My second example, the example of cell
            identification. Suppose you have some white blood cells, what might you do? You might measure the total area
            of the cell.</p>
        <p>不是空洞面积，可能是核面积。也许你可以测量四五个其他的东西，然后把这个东西放在高维空间中。你仍然可以测量高维空间中的接近度。所以你可以用这个想法来做这件事。它效果很好。我的一个朋友曾经根据这个想法创办了一家公司。当然，他破产了，但这不是他的错。
        </p>
        <p>And not the hole area, but maybe the nucleus area. And maybe you might measure four or five other things, and
            put this thing in a high dimensional space. You can still measure the nearness in a high dimensional space.
            So you can use the idea to do that. It works pretty well. A friend of mine once started a company based on
            this idea. He got wiped out, of course, but it wasn’t his fault.</p>
        <p>事情是这样的，有人发明了一种更好的染色剂，这样就更容易通过蛮力进行识别了。让我们看看，这是两个例子。一个是电气盖板孔的介绍性例子，另一个是电池的例子。我现在想做的是向你们展示这个想法是如何以伪装的形式出现在你们可能意想不到的地方的。所以考虑下面的问题。
        </p>
        <p>What happened is that somebody invented a better stain and it became much easier to just do the recognition
            by brute force. So let’s see, that’s two examples. the introductory example of the holes of the electrical
            covers, and the example of cells. And what I want to do now is show you how the idea can reappear in
            disguised forms in areas where you might not expect to see it. So consider the following problem.</p>
        <h2 id="unknown-162">未知</h2>
        <h2>Unknown</h2>
        <p>你有一系列杂志文章。你对如何解决某个问题感兴趣。你如何找到与你的问题相关的文章？这是一个几十年来一直被对信息检索感兴趣的人研究的难题。下面是一个简单的方法。我将再次从两个维度来说明。</p>
        <p>You have a collection of articles from magazines. And you’re interested in learning something about how to
            address a particular question. How do you go about finding the articles that are relevant to your question?
            So this is a puzzle that has been studied for decades by people interested in information retrieval. And
            here’s the simple way to do it. I’m going to illustrate, once again, in just two dimensions.</p>
        <p>但它必须应用在很多方面。这个想法是，你计算图书馆文章中的字数，然后将字数与探索性问题的字数进行比较。所以你可能对 100
            个字感兴趣。为了举例说明，我只会在黑板上写两个。所以我们要考虑两本杂志上的文章。首先，我们要用什么词？</p>
        <p>But it has to be applied in many, many dimensions. The idea is you count up the words in the articles in your
            library, and you compare the word counts to the word counts in your probing question. So you might be
            interested in 100 words. I’m only going to write two on the board for illustration. So we’re going to think
            about articles from two magazines. Well first of all, what words are we going to use?</p>
        <p>其中一个词是 hack，它包括所有 hack 的派生词，如 hacker、hacking 等等。另一个词是 computer。因此，你看到《连线》杂志的文章出现在这样的地方并不奇怪。它们会大量使用 computer
            这个词，也会大量使用 hack 这个词。</p>
        <p>One word is going to be hack, and that will include all derivatives of hack. hacker, hacking, and so on. And
            the other word is going to be computer. And so it would not be surprising for you to see that articles from
            Wired Magazine might appear in places like this. They would involve lots of uses of the word computer, and
            lots of uses of the word hack.</p>
        <h2 id="unknown-163">未知</h2>
        <h2>Unknown</h2>
        <p>现在为了便于说明，我们要从第二本杂志中选取文章，那就是《城镇与乡村》。这是一本非常时髦的杂志，读《城镇与乡村》的人往往是社会寄生虫。他们仍然使用 hack 这个词。因为你可以谈论
            hacking，所以在处理马匹时有某种专门的术语。</p>
        <p>And now for the sake of illustration, the second magazine from which we are going to draw articles is Town
            and Country. It’s a very tony magazine, and the people who read out Town and Country tend to be social
            parasites. And they still use the word hack. Because you can talk about hacking, there’s some sort of
            specialize term of art in dealing with horses.</p>
        <p>因此，所有《城镇与乡村》文章很可能都在这里。也许他们会像这样谈论雇用一些计算机专家来跟踪结果，以便进行每周搜索，或者其他什么。现在，你带着你的调查来了。当然，你的调查问题会相对较小。它不会包含很多文字。</p>
        <p>So all the Town and Country articles would be likely to be down here somewhere. And maybe they would be one
            like that when they talk about hiring some computer expert to keep track of the results so the weekly hunt,
            or something. And now, in you come with your probe. And of course your probe question is going to be
            relatively small. It’s not going to have a lot of words in it.</p>
        <p>所以这是你的探索问题。这是你的未知数。哪篇文章最接近？哪些文章最接近？唉，所有这些《城镇与乡村》文章都是最接近的。所以你似乎不能使用最近邻居的想法。有人能建议我们如何摆脱这种困境吗？是的，克里斯托弗。</p>
        <p>So here’s your here’s your probe question. Here’s your unknown. Which article’s going to be closest? Which
            articles are going to be closest? Well, alas, all those Town and Country articles are closest. So you can’t
            use the nearest neighbor idea, it would seem. Anybody got a suggestion for how we might get out of this
            dilemma? Yes, Christopher.</p>
        <h2 id="unknown-164">未知</h2>
        <h2>Unknown</h2>
        <p>克里斯托弗：如果您正在寻找字数统计，并且想要包含一些计算机术语，那么您不想将其用作阈值，而不是最近邻吗？帕特里克·温斯顿教授：我不知道，这是一个好主意。它可能会起作用，谁知道呢。道格？道格：如果您将 Wired 和
            Town and Country 视为目标，而不是使用垂直平分线作为决策边界。它们看起来就像这里的一些。</p>
        <p>CHRISTOPHER: If you’re looking for word counts and you want to include some terms of computer, then wouldn’t
            you want to use that as a threshold, rather than the nearest neighbor? PROF. PATRICK WINSTON: I don’t know,
            it’s a good idea. It might work, who knows. Doug? DOUG: Instead of using decision boundaries that are
            perpendicular bisectors, if you treated Wired and Town and Country as sort of this like, targets. And they
            would look like some here.</p>
        <p>我想，曲线周围的一些半径。如果它在某个半径内，那么。帕特里克·温斯顿教授：是的？必然，是否使用某种度量？帕特里克·温斯顿教授：哦，我们开始吧。我们不打算使用任何度量。我们将使用其他度量。说话者
            1：比如算法，还是什么？帕特里克·温斯顿教授：嗯，算法，天哪，我不知道。帕特里克·温斯顿教授：让我给你一个提示。让我给你一个提示。</p>
        <p>I guess, some radius around curves. If it’s within a certain radius then. PROF. PATRICK WINSTON: Yes?
            necessarily, have it done with some sort of a metric? PROF. PATRICK WINSTON: Oh, here we go. We’re not going
            to use any metric. We’re going to use some other metric. SPEAKER 1: Like alogrithmic, or whatnot? PROF.
            PATRICK WINSTON: Well, algorithmic, gees, I don’t know. PROF. PATRICK WINSTON: Let me give you a hint. Let
            me give you a hint.</p>
        <p>上面有很多文章，比如，上面有很多。这里是《城镇与乡村》的文章。比如，上面有很多。现在我们的未知数也出来了。现在有人有想法了吗？嘿，布雷特，你觉得怎么样？布雷特：所以你想要比例。或者在这种情况下，你可以取角度。帕特里克·温斯顿教授：让我们开始吧。
        </p>
        <p>There are all those articles up there, out there, and out there, just for example. And here are the Town and
            Country articles. They’re out there, and out there, for example. And now our unknown is out there. Anybody
            got an idea now? Hey Brett, what do you think? BRETT: So you sort of want the ratio. Or in this case, you
            can take the angle. PROF. PATRICK WINSTON: Let’s be.</p>
        <h2 id="unknown-165">未知</h2>
        <h2>Unknown</h2>
        <p>啊，我们开始讲了，现在有点复杂了。什么之间的角度？布雷特：。向量之间的角度。帕特里克·温斯顿教授：。向量。很好。所以我们要使用不同的度量标准。我们要做的就是，我们将忘记距离，我们将测量向量之间的角度。所以向量之间的角度，好吧，我们实际上测量向量之间角度的余弦。
        </p>
        <p>Ah, there we go, we’re getting a little more sophisticated. The angle between what? BRETT:. The angle between
            the vectors. PROF. PATRICK WINSTON:. The vectors. Good. So we’re going to use a different metric. What we’re
            going to do is, we’re going to forget including a distance, and we’re going to measure the angle between the
            vectors. So the angle between the vectors, well let’s actually measure the cosine of the angle between the
            vectors.</p>
        <p>让我们看看如何计算。所以我们取向量之间夹角的余弦，我们称之为 theta。它将等于未知值乘以文章值的总和。这些只是各个维度的值。然后我们将其除以其他向量的大小。所以我们将除以 u 的大小，再除以艺术向量到文章的大小。
        </p>
        <p>Let’s see how we can calculate that. So we’ll take the cosine of the angle between the vectors, we’ll call it
            theta. That’s going to be equal to the sum of the unknown values times the article values. Those are just
            the values in various dimensions. And then we’ll divide that by the magnitude of the other vectors. So we’ll
            divide by the magnitude of u, and we’ll divide by the magnitude of the art vector to the article.</p>
        <p>所以这只是点积，对吧？这是一个非常快的计算。所以通过非常快的计算，你可以看到这些东西是否会朝着同一个方向。顺便问一下，如果这个向量实际上与其中一篇文章相同，那么它的值会是多少？那么余弦将为
            0，我们将得到余弦的最大值，即 1。是的，这样就可以了。</p>
        <p>So that’s just the dot product right? That’s a very fast computation. So with a very fast computation you can
            see if these things are going to be in the same direction. By the way, if this vector here is actually
            identical to one of those articles, what will the value be? Well then a cosine will be 0 and we’ll get the
            maximum die of the cosine, which is 1. Yeah, that will do it.</p>
        <h2 id="unknown-166">未知</h2>
        <h2>Unknown</h2>
        <p>因此，如果我们使用任何一篇文章来探测文章空间，它们都会找到自己，这是有机制的好事。好的。这只是这两个向量的点积。它工作得很好。这不是做这些事情的最复杂的方法。有棘手的方法。你可以用一些新的和复杂的方法做这类事情来获得博士学位。
        </p>
        <p>So if we use any of the articles to probe the article space, they’ll find themselves, which is a good thing
            to have a mechanism do. OK. So that’s just the dot product of those two vectors. And it works like a charm.
            It’s not the most sophisticated way of doing these things. There are hairy ways. You can get a Ph.D.&nbsp;by
            doing this sort of stuff in some new and sophisticated way.</p>
        <p>但这是一种简单的方法。它效果很好。而且你不需要太费力就能实现它。所以这很酷。这是一个我们有一个非常非标准度量的例子。现在让我们看看，我们还能做什么？机械臂控制怎么样？我们开始吧。我们只需要一个简单的手臂。</p>
        <p>But this is a simple way. It works pretty well. And you don’t have to strain yourself, much, to implement it.
            So that’s cool. That’s an example where we have a very non standard metric. Now let’s see, what else can we
            do? How about a robotic arm control? Here we go. We’re going to just have a simple arm.</p>
        <p>我们想要做的是，让这只手臂以我们确定的速度、速率和加速度沿着某个轨迹移动球。所以我们这里有两个问题。好吧，让我们看看，我们有两个问题，因为首先，我们有角度，θ 1 和 θ 2。这是手臂的 2 度 3，所以只有两个角度。
        </p>
        <p>And what we want to do is, we want to get this arm to move that ball along some trajectory at a speed,
            velocity, and acceleration that we have determined. So we’ve got two problems here. Well let’s see, we’ve
            got two problems because, first of all, we’ve got angles, theta 1 and theta 2. It’s a 2 degree of 3 of arm,
            so there are only two angles.</p>
        <h2 id="unknown-167">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们面临的第一个问题是将球的 (x,y) 坐标（即所需的坐标）平移到 theta 1、theta 2 空间的运动学问题。这是简单的运动学问题。其中没有 f 等于
            ma。它不涉及力、时间或加速度等任何东西。非常简单。但接下来我们面临的问题是让它沿着该轨迹以我们想要的位置、速度和加速度行进。</p>
        <p>So the first problem we have is the kinematic problem of translating the (x,y) cordinates of the ball, the
            desired ones, into the theta 1, theta 2 space. That’s simple kinematic problem. No f equals ma there. It
            Doesn’t involve forces, or time, or acceleration, anything. Pretty simple. But then we’ve got the problem of
            getting it to go along that trajectory with positions, speeds, and accelerations that we desire.</p>
        <p>现在你对我说，好吧，我有 801，我可以做到。没错，你可以。因为这是牛顿力学。你所要做的就是解方程。这就是方程。祝你好运。为什么它们这么复杂？因为几何形状复杂。你注意到我们在那里得到了一些 θ 1 和 θ 2
            的乘积，我想是的？你有 θ 2。我看到加速度的平方。</p>
        <p>And now you say to me, well I’ve got 801, I can do that. And that’s true, you can. Because, it’s Newtonian
            mechanics. All you have to do is solve the equations. There are the equations. Good luck. Why are they so
            complicated? Well because of the complicated geometry. You notice we’ve got some products of theta 1 and
            theta 2 in there, somewhere, I think? You’ve got theta 2’s. I see an acceleration squared.</p>
        <p>是的，有一个 θ 1 点乘以 θ 2 点。一个速度乘以一个速度。这到底是从哪里来的？我的意思是它应该是 f 等于 ma，对吧？由于几何形状复杂，这些是科里奥利力。好的。所以你雇佣了 Berthold Horn
            或其他人来为你解这些方程。他想出了类似的东西。你试了一下，它不起作用。为什么它不起作用？</p>
        <p>And yeah, there’s a theta 1 dot times a theta 2 dot. A velocity times a velocity. Where the hell did that
            come from? I mean it’s supposed to be f equals ma, right? Those are Coriolis forces, because of the
            complicated geometry. OK. So you hire Berthold Horn, or somebody, to work these equations out for you. And
            he comes up with something like this. And you try it out and it doesn’t work. Why doesn’t it work?</p>
        <h2 id="unknown-168">未知</h2>
        <h2>Unknown</h2>
        <p>我说，这是牛顿力学。它不起作用，因为我们忘了告诉 Berthold
            所有关节都有摩擦。我们忘了告诉他从昨天开始它们已经磨损了一点。我们还忘了我们在实验台上进行的测量并不十分精确。所以人们试图这样做。但根本行不通。一旦你得到一个不同重量的球，你就必须重新开始。这太糟糕了。</p>
        <p>It’s Newtonian mechanics, I said. It doesn’t work because we forgot to tell Berthold that there’s friction in
            all the joints. And we forgot to tell him that they’ve worn a little bit since yesterday. And we forgot that
            the measurements we make on the lab table are not quite precise. So people try to do this. It just doesn’t
            work. As soon as you get a ball of a different weight you have to start over. It’s gross.</p>
        <p>所以我不知道。我可以毫不费力地做这种事，但我无法开始解决这些方程式。让我们看看。我们要做的就是暂时忘掉这个问题。我们将讨论如何为自己构建一个巨大的表格。这就是表格上的内容。Theta 1、theta 2、theta
            3，哦，只有两个。</p>
        <p>So I don’t know. I can do this sort of thing effortlessly, and I couldn’t begin to solve those equations. So
            let’s see. What we’re going to do is we’re going to forget about the problem for a minute. And we’re going
            to talk about building ourselves a gigantic table. And here’s what’s going to be on the table. Theta 1,
            theta 2, theta 3, oops, there are only two.</p>
        <p>这就是 θ 1，但它是速度，角速度。然后我们有加速度。所以我们会有一个包含这些东西的大表格。我们要做的就是给这条手臂一个童年。我们会写下我们见过的所有组合，每 100 毫秒，或者其他。手臂会像孩子在摇篮里一样挥动。
        </p>
        <p>So that’s theta 1 again, but it’s the velocity, angular velocity. And then we have the accelerations. So
            we’re going to have a big table of these things. And what we’re going to do, is we’re going to give this arm
            a childhood. And we’re going to write down all the combinations we ever see, every 100 milliseconds, or
            something. And the arm is just going to wave around like a kid does in the cradle.</p>
        <h2 id="unknown-169">未知</h2>
        <h2>Unknown</h2>
        <p>然后，我们还没完成。因为我们还要记录另外两件事。你能猜出它们是什么吗？它们是第一个电机的扭矩和第二个电机的扭矩。现在，我们得到了一大堆这样的记录。问题是，我们要用它们做什么？好吧，这就是我们要做的事情。</p>
        <p>And then, we’re not quite done. Because there are two other things we’re going to record. Can you guess what
            they are? There are going to be the torque on the first motor, and the torque on the second motor. And so
            now, we’ve got a whole bunch of those records. The question is, what do we got to do with it? Well here’s
            what we’re going to do it.</p>
        <p>我们要把我们希望实现的轨迹分成小块。有一小块。在这小块中，没有什么会发生太大的变化。会有加速度、速度、位置。因此，我们可以在小时候制作的表格中查找这些。</p>
        <p>We’re going to divide this trajectory that we’re hoping to achieve, up into little pieces. And there’s a
            little piece. And in that little piece nothing is going to change much. There’s going to be an acceleration,
            velocity, position. And so we can look those up in the table that we made in the childhood.</p>
        <p>我们会四处寻找最接近的匹配，这将是与该特定运动相关的位置、速度和加速度的一组值。猜猜我们现在能做什么？我们可以说，在过去，与该特定小部分运动相关的扭矩就在那里。所以我们可以查找它。现在这种方法被提出并被拒绝了，因为计算机不够强大。
        </p>
        <p>And we’ll look around and find the closest match, and this will be the set of values for the positions,
            velocities, and accelerations that are associated with that particular movement. And guess what we can do
            now? We can say, in the past, the torques associated with that particular little piece of movement lie right
            there. So we can just look it up. Now this method was thought up and rejected, because computers weren’t
            powerful enough.</p>
        <h2 id="unknown-170">未知</h2>
        <h2>Unknown</h2>
        <p>然后，这是回收利用的时代，对吧？所以当计算机变得足够强大时，这个想法就被回收利用了。对于这样的事情，它工作得很好。但你可能会问我，它能做我们人类能做的事情吗？答案是，让我们看看。所以这是一个训练阶段，它正在经历它的童年。你看发生了什么。初始表不会很好。
        </p>
        <p>And then, this is the age of recycling, right? So the idea got recycled when computers got strong enough. And
            it works pretty well, for things like this. But you might say to me, well can it do the stuff that we humans
            can do, like this? And the answer is, let’s look. So this is a training phase, it’s going through its
            childhood. You see what’s happening is this. The initial table won’t be very good.</p>
        <p>但那没关系。因为只有少数事情对你来说是重要的。所以当你尝试那些事情时，它仍然会写入表格中。所以下次你尝试那个特定的动作时，它会做得更好，因为它有更好的东西可以插入到那个表格中。这就是为什么这个东西会变得越来越好。
        </p>
        <p>But that’s OK. Because there are only a small number of things that it’s important for you to be able to do.
            So when you try those things it’s still writing into the table. So the next time you try that particular
            motion, it’s going to be better at it, because its got better stuff to interpolate in that table. So that’s
            why this thing is getting better and better as it goes on.</p>
        <p>这和我做的一样好。非常好，你不觉得吗？为了好玩，我只想在这个片段的最后展示一件事。也许你看过一些老佐罗电影？这里有一个小场景，这个东西已经学会了使用睫毛。这是睫毛，下面有一根蜡烛。所以看这个。非常好，你不觉得吗？那么学习的速度有多快？
        </p>
        <p>That’s as good as I was doing. Pretty good, don’t you think? There’s just one thing I want to show at the end
            of this clip just for fun. Maybe you’ve seen some old Zorro movies? So here’s a little set up where this
            thing has learned to use a lash. So here’s the lash, and there’s a candle down there. So watch this. Pretty
            good, don’t you think? So how fast does the learning take place?</p>
        <h2 id="unknown-171">未知</h2>
        <h2>Unknown</h2>
        <p>让我回到另一张幻灯片并向你们展示。这里有一些图表，向你们展示机器人手臂移动的速度有多快。这些图表给出了机器人手臂在没有任何练习的情况下沿直线移动的曲线，只是在内存中记录了一些内容。然后通过几次练习，给它更好的值，在其中进行插值。所以我认为这很酷。非常简单，但非常有效。
        </p>
        <p>Let me go back to that other slides and show you. So here’s some graphs to show you how fast goes, boom. That
            gives you the curves of how well the robot arm can go along a straight line, after no practice with just
            some stuff recorded in the memory. And then with a couple of practice runs do give it better values amongst
            which to interpolate. So I think that’s pretty cool. So simple, but yet so effective.</p>
        <p>但你还是会说，好吧，我不知道，这可能是在特殊情况下可以做的事情。我想知道老温斯顿喝咖啡时会不会用类似的东西？好吧，我们应该算一下，看看是否可行。但我不想用咖啡，现在是棒球赛季。我们即将迎来世界大赛。我们不妨谈谈职业运动员。所以让我们假设这是一位棒球投手。
        </p>
        <p>But you still might say, well, I don’t know, it might be something that can be done in special cases. I
            wonder if old Winston uses something like that when he drinks his coffee? Well we’ ought to do the numbers
            and see if it’s possible. But I don’t want to use coffee, it’s the baseball season. We’re approaching the
            World Series. We might as well talk about professional athletes. So let’s suppose that this is a baseball
            pitcher.</p>
        <p>我想知道录制大量投球需要多少内存。现在有优秀的投手吗？红袜队很烂，所以我不做红袜队。我猜是克莱·布赫霍尔兹。我不知道，有些投手。我们要做的就是，对于每个小片段，我们将记录每个关节 100 个字节。我们到处都有关节。
        </p>
        <p>And I want to know how much memory I’ll need to record a whole lot of pitches. Is there a good pitcher these
            days? The Red Socks suck so I don’t do Red Socks. Clay Buchholz, I guess. I don’t know, some pitcher. And
            what we’re going to do, is we’re going to say for each of these little segments were going to record 100
            bytes per joint. And we’ve got joints all over the place.</p>
        <h2 id="unknown-172">未知</h2>
        <h2>Unknown</h2>
        <p>我不知道投一个棒球球需要多少个关节，但我们假设有 100 个关节。然后我们必须将投球分成多个部分。因此，为了便于讨论，我们假设有 100 个部分。投手一天投多少球？多少？ 发言者 2：一天？
            帕特里克·温斯顿教授：是的，一天。</p>
        <p>I don’t know how many are involved in doing a baseball pitch, but let’s just say we have had 100 joints. And
            then we have to divide the pitch up into a bunch of segments. So let’s just say for sake of argument that
            there are 100 segments. And how many pitches does a pitcher throw in a day? What? SPEAKER 2: In a day? PROF.
            PATRICK WINSTON: In a day, yeah.</p>
        <p>我们都知道，这个大约是 100 次。大家都知道，投球后大约 100 次，他们就会把球取出来。所以我想知道我们需要多少内存来记录投手职业生涯中投出的所有球。所以我们还需要在这方面再努力一点。投手一年投球几天？</p>
        <p>This, we all know, is about 100. Everybody knows that they take them out after about 100 pitches. So what I
            want to know is how much memory we need to record all the pitches a pitcher pitches in his career. So we
            still have to work on this little bit more. How many days a year does a pitcher pitch?</p>
        <p>好吧，他们有冬季舞会之类的东西，所以我们就把它近似为 100。我不知道，其中一些可能有点高，其他一些可能有点低。当然，职业生涯。只是为了简单起见。是 100 年。所以那是一、二、三、四、五、六。所以我们有 10 的
            12 次方字节。在这里存储这个数字是不是太大了？</p>
        <p>Well, they’ve got winter ball, and that sort of thing, so let’s just approximate it as 100. I don’t know,
            some of these may be a little high, some of the others may be a low. And of course, the career. just to make
            things easy. is 100 years. So that’s one, two, three, four, five, six. So we have 10 to the 12th bytes. Is
            that the hopelessly big to store in here?</p>
        <h2 id="unknown-173">未知</h2>
        <h2>Unknown</h2>
        <p>克里斯托弗：10 到 100 次还是仅仅投掷 100 次？帕特里克·温斯顿教授：一天投掷 100
            次。克里斯托弗问了一些细节。我们要做的是记录关于一次投球的所有信息，然后看看他一生投了多少次球。我们会记录所有这些。相信我。相信我。好的。所以我们想知道这是否真的是一个实用的量表。</p>
        <p>CHRISTOPHER: 10 to 100 or just 100 times throwing? PROF. PATRICK WINSTON: 100 pitches in a day. Christopher’s
            asking some detail. and what we’re gong to do is we’re going to record everything there is to know about one
            pitch, and then we’re going to see how many pitches, he pitches in his lifetime. And we’re going to record
            all that. Trust me. Trust me. OK. so we want to know if this is actually a practical scale.</p>
        <p>顺便说一句，这是鸡尾酒会上的谈话，谁知道呢，对吧？但计算出这些数字并了解其中一些数字很有用。所以我们要问的问题是，这里面有多少计算量？与此相关的第一个问题是，我们的大脑中有多少个神经元？志愿者？神经科学？没有人自愿？好吧。
        </p>
        <p>And this, by the way, is cocktail conversation, who knows, right? But it’s useful to work out these numbers,
            and know some of these numbers. So the question we have to ask is, how much computation is in there? And the
            first question relevant to that is, how many neurons do we have in our brain? Volunteer? Neuroscience? No
            one to volunteer? All right.</p>
        <p>嗯，这个数字你应该知道，因为这就是你脑子里有的数字。大脑中有 10 的 10 次方个神经元，其中 10 的 11
            次方个神经元只存在于小脑中。我这到底是什么意思？我的意思是，你的小脑充满了神经元，以至于它使大脑的其他部分相形见绌。所以，如果你排除小脑，你就有大约 10 的 10 次方个神经元。</p>
        <p>Well this is a number you should know, because this is what you’ve got in there. There are 10 to the 10th
            neurons in the brain, of which 10 to the 11th are in the cerebellum, alone. What the devil do I mean by
            that? I mean that your cerebellum is so full of neurons that it dwarfs the rest of the brain. So if you
            exclude the cerebellum, you’ve got about 10 to 10th neurons.</p>
        <h2 id="unknown-174">未知</h2>
        <h2>Unknown</h2>
        <p>仅小脑就有大约 10 到 11
            个神经元。小脑是用来做什么的？运动控制。很有趣。所以我们有点短。哦，但我们忘了，这只是神经元的数量。我们必须计算突触的数量。因为可以想象，我们可能能够调整这些突触，对吧？那么一个神经元有多少个突触？答案是，这取决于情况。但小脑中的那些。
        </p>
        <p>And there about 10 to the 11th neurons in the cerebellum, alone. What’s the cerebellum for? Motor control.
            Interesting. So we’re a little short. Oh, but we forget, that’s just the number of neurons. We have to count
            up the number of synapses. Because conceivably, we might be able to adjust those synapses, right? So how
            many synapses does a neuron have? The answer is, it depends. But the ones in the cerebellum.</p>
        <p>我想我应该指着那里。10 的 5 次方。所以如果我们把所有这些加起来，我们得到 10 的 16 次方。没问题。只是存在证明你不必太担心有存储空间。所以也许我们的小脑在某种程度上就像一张巨大的桌子。</p>
        <p>I should be pointing back there, I guess. 10 to the 5th. So if we add all that up we have 10 to the 16th. No
            problem. It’s just that existence proves that you don’t have to worry too much about having storage. So
            maybe our cerebellum functions, in some way, as a gigantic table.</p>
        <p>这也许就是我们学习运动技能的方式，从摇篮里出来时跑来跑去，填满桌子，学习如何在前进的过程中操纵自己。这就是手臂控制的故事。现在这一切都很简单，很容易理解。当然，也存在一些问题。第一个问题，如果样本空间看起来像这样会怎样？在这种情况下会发生什么？
        </p>
        <p>And that’s maybe how we learn motor skills, by filling up that table as we run around emerging from the
            cradle, learning how to manipulate ourselves as we go on. So that’s the story on arm control. Now all this
            is pretty straightforward, easy to understand. And of course, there are some problems. Problem number one,
            what if the space of samples looks like this? What’s going to happen in that case?</p>
        <h2 id="unknown-175">未知</h2>
        <h2>Unknown</h2>
        <p>那么，在这种情况下会发生什么呢？让我们看看，哪些值更重要？x 值，对吧？y 值分散在各处。所以您希望数据的分布在所有维度上都相同。那么我们可以做些什么来让这一点成为现实？当然，我们可以对数据进行标准化。</p>
        <p>Well what’s going to happen in that case is that the. let’s see, which values are going to be more important?
            The x values, right? The y values are spread out all over the place. So you’d like the spread of the data to
            sort of be the same in all the dimensions. So is there anything we can do to arrange for that to be true?
            Sure, we can just normalize the data.</p>
        <p>因此，我们可以借用统计学课程中的知识，比如，让我们看看，我们对 x 感兴趣。我们知道 x 的方差等于 1/n 乘以值的总和减去平均值的平方。这是数据分散程度的度量。因此，现在，我们可以使用 x prime，而不是使用
            x，它等于 x 除以 sigma。它的方差是多少？</p>
        <p>So we can borrow from our statistics course and say, well, let’s see, we’re interested in x. And we know that
            the variance of x is equal to 1 over n times the sum of the values, minus the mean value squared. That’s a
            measure of how much the data spreads out. So now, instead of using x, we can use x prime, which is equal to
            x over sigma. What’s the variance of that going to be?</p>
        <p>X 除以 sigma sub x。有人能立刻看出它的方差是多少吗？还是我们必须算出来？它将是 1，帮我算一下代数。很明显，很简单。只需将 x prime
            代入这个方差公式，然后进行高中代数运算。你会发现方差不是这个新变量，不是你想要的这个变换变量。</p>
        <p>X over sigma sub x. Anybody see, instantaneously, what the variance of that’s going be? Or do we have to work
            it out? It’s going to be 1, Work out the algebra for me. It’s obvious, it’s simple. Just substitute x prime
            into this formula for variance, and do the algebraic high school manipulation. And you’ll see that the
            variance turns out not to be of this new variable, this transformed variable you want.</p>
        <h2 id="unknown-176">未知</h2>
        <h2>Unknown</h2>
        <p>所以那个问题，非均匀性问题，扩散问题，很容易处理。那另一个问题呢？没有面粉就没有蛋糕？如果事实证明数据有两个维度，而答案实际上根本不依赖于
            y，那会怎么样？会发生什么？那么你通常会得到奇怪的结果，因为它会测量一个只会混淆答案的距离。所以第二个问题是“什么才是最重要的”问题。</p>
        <p>So that problem, the non uniformity problem, the spread problem, is easy to handle. What about that other
            problem? No cake without flour? What if it turns out that the data. you have two dimensions and the answer,
            actually, doesn’t depend on y at all. What will happen? Then you’re often going to get screwy results,
            because it’ll be measuring a distance that is merely confusing the answer. So problem number two is the what
            matters problem.</p>
        <p>把重要的事情写下来。第三个问题是，如果答案根本不依赖于数据怎么办？那么你就得试着做一个没有面粉的蛋糕。曾经有人问我。我的一个同学，后来成为一家重要信用卡公司的重要高管。问我我们是否可以使用人工智能来确定某人何时会破产？答案是，不能。
        </p>
        <p>Write it down, what matters. Problem number three is, what if the answer doesn’t depend on the data at all?
            Then you’ve got the trying to build a cake without flour. Once somebody asked me. a classmate of mine, who
            went on to become an important executive in an important credit card company. asked me if we could use
            artificial intelligence to determine when somebody was going to go bankrupt? And the answer was, no.</p>
        <p>因为可用的数据是与该问题无关的数据。所以他试图在不使用面粉的情况下制作蛋糕，而你做不到。所以这就是我想说的关于最近邻居的内容。不，我想谈谈睡眠。在那边的左侧分支上，现在已经消失了，我们讨论了学习的人性方面。我谈到了一次基于升级的学习。
        </p>
        <p>Because the data available was data that was independent of that question. So he was trying to make a cake
            without flour, and you can’t do that. So that concludes what I want to say about nearest neighbors. No I
            want to talk a little bit about sleep. Over there on that left side branch, now disappeared, we talked about
            the human side of learning. And I said something about one shot, an escalation based learning.</p>
        <h2 id="unknown-177">未知</h2>
        <h2>Unknown</h2>
        <p>这意味着，不解决问题就无法学习。问题是，解决问题与睡眠时间有什么关系？要回答这样的问题，当然，你需要咨询那些掌握你感兴趣的知识的人。所以你会问，谁知道你需要多少睡眠？如果你没有得到睡眠会发生什么？</p>
        <p>And what that means is, you don’t learn without problem solving. And the question is, how is problem solving
            related to how much sleep you get? And to answer questions like that, of course, you want to go to the
            people who are the custodians of the kind of knowledge you are interested in. And so you would say, who are
            the custodians of knowledge about how much sleep you need? And what happens if you don’t get it?</p>
        <p>答案是美国陆军。因为他们对跨越 10 或 12
            个时区、不睡觉、必须执行任务时会发生什么非常感兴趣。所以他们对这个问题非常感兴趣。第一次海湾战争是历史上研究最多的战争，在第一次海湾战争之后，他们对这个问题更加感兴趣。因为战后报告中充满了这样的例子。</p>
        <p>And the answer is the United States Army. Because they’re extremely interested in what happens when you cross
            10 or 12 times zones, and have no sleep, and have to perform. So they’re very interested in that question.
            And they got even more interested after the first Gulf War, which was the most studied war in history, up to
            that time. Because, there were after action reports they were full of examples like this.</p>
        <p>美军在战场的某个地方整装待发。那里是布雷德利战车，后面是艾布拉姆斯坦克。他们都准备好好睡一觉。顺便说一句，他们已经连续 36
            小时没有睡觉了。突然，令他们惊讶的是，一队伊拉克车辆出现在他们的视野中。双方都非常惊讶。一场枪战爆发了。</p>
        <p>The US Forces, in a certain part of the battlefield, and drawn up for the night. And those are Bradley
            fighting vehicles, there, and back here Abrams tanks. And they’re all just kind of settling down for good
            night’s sleep. They’ve been up for about 36 hours straight, by the way. When, much to their amazement,
            across their field of view came a column of Iraqi vehicles. And both sides were enormously surprised. A
            firefight broke out.</p>
        <h2 id="unknown-178">未知</h2>
        <h2>Unknown</h2>
        <p>伊拉克这边的领头车辆着火了。于是，这些布雷德利战车的士兵们四处查看情况，然后开始疯狂射击，进行自相残杀。有趣的是，所有这些士兵在战后报告中都发誓说，他们当时是直接朝前方射击。而他们向目标发射炮弹的能力丝毫没有受到影响。
        </p>
        <p>The lead vehicle, over here, on the Iraqi side caught on fire. So these guys, in the Bradley fighting
            vehicles, went around to investigate, whereupon, these guys started blasting away, in acts of fratricidal
            fire. And the interesting thing is that all these folks here swore in the after action reports that they
            were firing straight ahead. And what happened was their ability to put ordnance on target was not impaired
            at all.</p>
        <p>但是他们对目标在哪里、目标是什么、是否是目标的认识全都搞砸了。因此，这导致了大量实验，在这些实验中，人们被剥夺了睡眠。顺便说一句，你认为自己是一名坚强的麻省理工学院学生，对吧？他们是陆军游骑兵。相信我，没有比这更艰难的了。以下是进行的一项实验。
        </p>
        <p>But their idea of where the target was, what the target was, whether it was a target, was all screwed up. So
            this led to a lot of experiments in which people were sleep deprived. And by the way, you think you’re a
            tough MIT student, right? These are Army Rangers. It doesn’t get any tougher than this, believe me. So
            here’s one of the experiments that was performed.</p>
        <p>那时他们有所谓的火控小组。他们的工作是从这边的观察员那里获取有关这边目标的信息。然后告诉这边的炮兵，向哪里开火。所以他们让这些人连续值班 36 个小时。36
            小时后，他们都说，我们做得很好。当时他们向医院、清真寺、教堂、学校和他们自己开火。</p>
        <p>In those days they had what they called fire control teams. And their job is to take information from an
            observer, over here, about a target, over here. And tell the artillery, over here, where to fire. So they
            kept some of these folks up for 36 hours straight. And after 36 hours they all said, we’re doing great. And
            at that time they were bringing fire down on hospitals, mosques, churches, schools, and themselves.</p>
        <h2 id="unknown-179">未知</h2>
        <h2>Unknown</h2>
        <p>因为，36 小时没睡觉后，他们无法再进行计算。现在你对我说，我是麻省理工学院的学生，我想看数据。那么让我们看看数据。好的。就是这样。这就是 72
            小时没睡觉后你的情况。这些都是很简单的事情。你必须在脑子里进行非常简单的计算，比如加数字、拼写单词等等。</p>
        <p>Because, they couldn’t do the calculations anymore, after 36 hours without sleep. And now you say to me, well
            I’m a MIT student, I want to see the data. So let’s have a look at the data. OK. So there it goes. That’s
            what happens to you after 72 hours without sleep. These are simple things to do. Very simple calculations
            you have to do in your head, like adding numbers, spelling words, and things like that.</p>
        <p>因此，在 72 小时不睡觉之后，你的表现会比刚开始时下降约 30%。因此，睡眠不足会破坏能力。睡眠不足会累积。所以你会说，好吧，我需要八个小时的睡眠。顺便说一句，你需要的时间各不相同。但我可以睡七个小时。因此，在 20
            天的睡眠不足（每次一小时）之后，你的表现会下降约 25%。</p>
        <p>So after 72 hours without sleep, your performance relative to what you were at the beginning is about 30%. So
            loss of sleep destroys ability. Sleep loss accumulates. So you say, well I need eight hours of sleep. and
            what you need, by the way, varies. but I’m going to get by was seven hours of sleep. So after 20 days of one
            hour’s worth of sleep deprivation, you’re down about 25%.</p>
        <p>如果你说，好吧，我需要八小时的睡眠，但我只能睡六个小时，那么 20 天后，你的睡眠能力就会下降到原来的 25% 左右。所以你可能会说，咖啡因有帮助吗？或者小睡，在这种情况下是小睡。答案是，是的，有一点。</p>
        <p>If you say, well I need eight hours of sleep, but I’m going to have to get by with just six, after 20 days of
            that, you’re down to about 25% of your original capability. So you might say, well does caffeine help? Or
            naps, naps in this case. And the answer is, yes, a little bit.</p>
        <h2 id="unknown-180">未知</h2>
        <h2>Unknown</h2>
        <p>有些人认为，如果你将睡眠时间分成两部分，效果会更好。温斯顿·丘吉尔总是在下午小睡三个小时。他说这样他每天就能完成一天半的工作。他睡了足够多的觉。但他将睡眠时间分成两部分。这是咖啡因那部分。所以咖啡因确实有帮助。</p>
        <p>Some people argue that you get the more affect out of the sleep that you do get if you divide it into two.
            Winston Churchill always took a three hour nap in the afternoon. He said that way he got a day and a half’s
            worth of work out of every day. He got the full amount of sleep. But he divided it into two pieces. Here’s
            the caffeine one. So caffeine does help.</p>
        <p>现在你说，好吧，我想我这个学期会轻松一点。我会在期末考试前一周努力学习。也许在 6034 期末考试前的 24 小时内我甚至懒得睡觉。没关系。让我们看看会发生什么。让我们计算一下数字。这是 24 小时。这就是 24
            小时后你的效率。</p>
        <p>And now you say, well, shoot, I think I’m going to take it kind of easy this semester. And I’ll just work
            hard during the week before finals. Maybe I won’t even bother sleeping for the 24 hours before the 6034
            final. That’s OK. Well let’s see what will happen. So let’s work the numbers. Here is 24 hours. And that’s
            where your effectiveness is after 24 hours.</p>
        <p>现在让我们来看看血液酒精含量曲线的相同效果。它大约相当于法律上规定的醉酒水平。所以我想我们应该做的是，在每个人参加 6034 期末考试时对他们进行检查，如果你 24
            小时没有睡觉，就会被逮捕。一年内不让你再参加任何期末考试。</p>
        <p>Now let’s go over to the same amount of effectiveness on the blood alcohol curve. And it’s about the level at
            which you would be legally drunk. So I guess what we ought to do is to check everybody as they come in for
            the 6034 final, and arrest you if you’ve been 24 hours without sleep. And not let you take any finals again,
            for a year.</p>
        <h2 id="unknown-181">未知</h2>
        <h2>Unknown</h2>
        <p>所以如果你做了所有这些，你还不如喝醉。现在我们今天只剩一件事要做。那就是回答最初的问题：为什么世界上的狗和猫认为减肥饮料会让人发胖？答案是什么？因为只有像我这样的胖子才会喝这种垃圾。</p>
        <p>So if you do all that, you might as well get drunk. And now we have one thing left to do today. And that is
            address the original question of, why it is that the dogs and cats in the world think that the diet drink
            makes people fat? What’s the answer? It’s because only fat guys like me drink this crap.</p>
        <p>因此，由于狗和猫没有能力自己讲故事，没有能力将事件串联成叙述，它们无法说，这是不想变胖的结果。不是变胖的结果。它们没有那个故事。所以它们的行为是你必须非常小心的事情。</p>
        <p>So since the dogs and cats don’t have the ability to tell themselves stories, don’t have that capacity to
            string together events into narratives, they don’t have any way of saying, well this is a consequence of
            desiring not to be fat. Not a consequence of being fat. They don’t have that story. And so what they’re
            doing is something you have to be very careful about.</p>
        <p>你必须非常小心，不要把相关性和原因混淆。他们看到了相关性，却不明白原因，所以才会犯错误。</p>
        <p>And that thing you have to be very careful about is the confusion of correlation with cause. They see the
            correlation, but they don’t understand the cause, so that’s why they make a mistake.</p>
        <h1 id="learning-identification-trees-disorder">11. 学习：识别树、障碍</h1>
        <h1>11. Learning: Identification Trees, Disorder</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:37 <a
                href="https://youtube.com/watch?v=SXBG3RGr_Rc">https://youtube.com/watch?v=SXBG3RGr_Rc</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:37 <a
                href="https://youtube.com/watch?v=SXBG3RGr_Rc">https://youtube.com/watch?v=SXBG3RGr_Rc</a></p>
        <h2 id="introduction-3">介绍</h2>
        <h2>Introduction</h2>
        <p>帕特里克·温斯顿：女士们，先生们，这是罗马尼亚国歌。我没有要求你们起立，因为我演奏它不是为了象征罗马尼亚的民族认同。而是为了庆祝冷战的结束，冷战发生在你们出生的时候。在那之前，没有一个人从东欧来到麻省理工学院。</p>
        <p>PATRICK WINSTON: Ladies and gentlemen, the Romanian national anthem. I did not ask you to stand, because I
            didn’t play it as a symbol of Romanian national identity. But rather, to celebrate the end of the Cold War,
            which occurred about the time that you were born. Before that, no one came to MIT from Eastern Europe.</p>
        <p>但从那时起，我们很幸运，因为我们中间有立陶宛人、爱沙尼亚人、波兰人、捷克人、斯洛伐克人、保加利亚人、罗马尼亚人、斯洛文尼亚人、塞尔维亚人，以及来自世界各地被正式排除在我们之外的人。相信我，你们所有人都欢迎我们。也就是说，几乎所有人。因为你可能还记得，罗马尼亚是吸血鬼的传统故乡。
        </p>
        <p>But since that time, we’ve been blessed by having in our midst Lithuanians, Estonians, Poles, Czecs, Slovaks,
            Bulgarians, Romanians, Slovenians, Serbs, and all sorts of people from regions of the world formally
            excluded to us. Believe me, you are all welcome in our house. Almost all, that is to say. Because you may
            recall that Romania is the traditional home of vampires.</p>
        <p>自冷战结束以来，吸血鬼有了新的媒介，它们从传统的地方出现，渗透到整个世界。你的套房或地板上可能就有吸血鬼。知道如何识别它们并采取必要的预防措施很重要。所以如果你有这种担忧，我希望你要做的第一件事就是查看一些有关吸血鬼特征的数据。
        </p>
        <p>And since the end of the Cold War, vampires have had new vectors for emerging from their traditional places
            and penetrating into the world at large. You may have vampire in your suite, or on your floor. And it’s
            important to know how to recognize them, and take the necessary precautions. So if you have this concern, I
            would expect that the first thing you would do would be to look at some data concerning the characteristics
            of vampires.</p>
        <p>所以，有一个小型数据库，里面有被确定为吸血鬼和非吸血鬼的个体样本。我们今天的任务，以及你在本小时结束时会明白该怎么做，就是使用这样的数据来构建一个识别机制，帮助你识别某人是吸血鬼还是普通人。所以这与我们用神经网络处理的问题有点不同。对吧？
        </p>
        <p>So there’s a little database of samples of individuals who have been determined to be vampires and not
            vampires. And our task today. and what you’ll understand how to do by the end of the hour. is to use data
            like this to build a recognition mechanism that would help you to identify whether someone is a vampire or
            an ordinary person. So this is a little different from the kind of problem we worked with neural nets.
            Right?</p>
        <p>那么，这个数据集和我们上次研究过的最近邻算法之间最显著的区别是什么？凯蒂，你认为为什么使用最近邻算法处理这样的数据会很困难吗？之所以有问号，是因为这是麻省理工学院，很多人都是夜间活动的人。所以你无法判断他们是否投下了阴影。
        </p>
        <p>So what’s the most conspicuous difference between this data set and anything you could think to work on with
            nearest neighbors, which we studied last time. Katie, do you have any thoughts about why it would be
            difficult to use nearest neighbors with data like this? The question mark is there because this is MIT, and
            a lot of people are completely nocturnal. So you can’t tell whether they cast a shadow or not.</p>
        <p>我们想把这一点考虑进去。那么，这与电气覆盖数据集有什么不同？学生：帕特里克·温斯顿：你能用最近邻技术通过这些数据识别吸血鬼吗？学生：帕特里克·温斯顿：很明显。是的，拉娜？学生：学生：你无法真正量化。帕特里克·温斯顿：哦，这就是问题所在。这不是数字数据。这是符号数据。
        </p>
        <p>We want to take that into account. So what’s different about this from the electrical cover data set?
            STUDENT: PATRICK WINSTON: Could you use the nearest neighbor technique to identify vampires with this data?
            STUDENT: PATRICK WINSTON: So obviously. Yes, Lana? STUDENT: STUDENT: You cannot really quantify. PATRICK
            WINSTON: Oh, that’s the problem. This is not numerical data. This is symbolic.</p>
        <p>所以我们不是说你投射阴影的能力是 0.7。你要么投射阴影，要么向下投射阴影，要么我们就说不出来。这是一个象征性的结果。</p>
        <p>So we’re not saying that your ability to cast a shadow is 0.7. You either cast a shadow, down cast a shadow,
            or we can’t tell. It’s a symbolic result.</p>
        <h2 id="identification-trees">识别树</h2>
        <h2>Identification Trees</h2>
        <p>因此，面对此类数据，我们面临的第一个问题是它不是数字。此外，它还具有其他特征。例如，尚不清楚所有这些特征是否真的很重要。因此，有些特征并不重要。</p>
        <p>So problem number one we have to face with data of this kind is that it’s not numeric. And there are other
            characteristics, as well. For example, it’s not clear that all of these characteristics actually matter. So
            some characteristics don’t matter.</p>
        <p>由此得出的推论是，某些特征确实很重要，但它们只在部分时间里很重要。最后，还有成本问题。其中一些测试可能比其他测试更昂贵。例如，如果你想确定某人是否投下阴影，你就得费尽心思在白天起床。这对你来说可能是一项昂贵的操作。
        </p>
        <p>And a corollary to that is that some characteristics do matter, but they only matter part of the time. And
            finally, there’s the matter of cost. Some of these tests may be more expensive to perform than others. For
            example, if you wanted to determine whether someone casts a shadow, you’d have to go to the trouble of
            getting up during daylight. That might be an expensive operation for you.</p>
        <p>你必须去找一些大蒜，让他们吃。这可能很贵。所以其中一些测试可能相对于其他测试来说很昂贵。但一旦你意识到我们谈论的是测试，而不是实值向量，那么你要做的事情就很清楚了。你为自己建立一个小小的测试树。所以谁知道这个问题会如何发展呢？
        </p>
        <p>You’d have to go find some garlic and ask them to eat it. That might be expensive. So some of these tests
            might be expensive relative to other tests. But once you realize that we are talking in terms of tests, and
            not a vector of real values, then what you do is clear. You build yourself a little tree of tests. So who
            knows how this problem will turn out?</p>
        <p>但是你可以想象这样一种情况，你在这里有一个测试，它可能有三个结果。其中只有一个结果可能需要你执行另一个测试。只有当你创建了看起来像这样的测试树时，你才算完成。</p>
        <p>But you can imagine a situation where you have one test up here which might have three outcomes. And one but
            only one of those outcomes might require you to perform another test. And only when you’ve created the tree
            of tests that look like this are you finished.</p>
        <p>因此，给定这组测试和一组样本，问题就变成了，如何将测试排列成这样的树，以便进行您想要的识别？既然我们在谈论识别，那么这种树被称为识别树也就不足为奇了。有一种趋势……我自己也可能陷入其中……将其称为决策树。</p>
        <p>So given this set of tests and a set of samples, the question becomes, how do you arrange the tests in a tree
            like that so as to do the identification that you want to do? So since we’re talking about identification,
            it’s not surprising that this kind of tree is called an identification tree. And there’s a tendency. and I
            may slip into it myself. to call this a decision tree.</p>
        <p>但是决策树是其他东西的标签。这是一棵识别树。任务是创建一棵好的树。那么什么是好的识别树，什么是不太好的识别树？如果你要称其为好的识别树，你希望决策树具有什么特征？克里希纳，你认为什么特征好？什么是好的特征？学生：也许是最低层数？
        </p>
        <p>But a decision tree is a label for something else. This is an identification tree. And the task is to create
            a good one. So what is a good one versus a not so good one? What characteristic would you like for a
            decision tree. for an identification trade to have, if you’re going to call it good identification tree?
            What do you think, Krishna? What would be a good characteristic? STUDENT: Maybe the minimum number of
            levels?</p>
        <p>帕特里克·温斯顿：是的。他说的是最低层级数。还有什么其他方式可以说明什么是好的层级？每次测试都会花费一些钱，对吧？那么，还有什么其他方式可以说明好的树是什么样的呢？学生：最低成本。帕特里克·温斯顿：最低成本。如果所有树的成本都相同，那么就是测试次数。所以总的来说，你喜欢的是小树，而不是大树。
        </p>
        <p>PATRICK WINSTON: Yeah. He said minimum number of levels. What’s another way you could say what a good one is?
            Each test costs something, right? So what’s another way of thinking about what a good tree would look like?
            STUDENT: Minimum cost. PATRICK WINSTON:. The minimum cost. And if they all have the same cost, then it’s the
            number of tests. So overall, what you like is a small tree rather than a big one.</p>
        <p>因此，您可以提取样本数据并将其划分，这样在树的底部，即叶子部分，测试生成的所有集合都是均匀的、同质的。我们希望这棵树是您可以找到的最简单的树，而不是某个将所有数据划分为均匀子集的复杂大树。通过均匀子集。</p>
        <p>So you might be able to take your sample data and divide it up, so that at the bottom of the tree, at the
            leaves, all of the sets that are produced by the tests are uniform, homogeneous. We’d like that tree to be
            the simplest possible tree you can find, not some big complicated one that also divides up all the data into
            uniform subsets. By uniform subset.</p>
        <p>在树的底部，所有吸血鬼都聚集在一起，所有非吸血鬼都聚集在一起。所以你想要一棵小树。那么为什么不直接去大英博物馆，计算所有可能的树呢？好吧，你可以这样做，但这是那些 NP 问题之一。正如你所知，NP
            问题通常很糟糕。所以你不想这样做。</p>
        <p>At the bottom of the tree, you have all of the vampires together, and all the non vampires together. So you’d
            like a small tree. So why not just go all the way and do British Museum, and calculate all possible trees?
            Well, you can do that, but it’s one of those NP problems. And as you know, NP problems suck in general. And
            so you don’t want to do that.</p>
        <p>您希望有某种启发式机制来构建一棵小树。我们想要一棵小树是因为。我们为什么想要一棵小树？因为成本。但是我们想要一棵小树还有另一个更重要的原因。让我给你一个提示。那就是奥卡姆剃刀。最简单的解释往往是最好的解释。所以如果你有一个大而复杂的解释，它可能不如一个简单、小的解释好。
        </p>
        <p>You want to have some kind of heuristic mechanism for building a small tree. And we want a small tree
            because. Why do we want a small tree? Because of the cost. but there’s another, more important reason why we
            want a small tree. Let me give you a hint. It’s Occam’s Razor. The simplest explanation is often the best
            explanation. So if you have a big, complicated explanation, that’s probably less good than a simple, small
            explanation.</p>
        <p>奥卡姆剃刀原理。它有多种拼写方式，我怎么拼写都无所谓。这很好，因为我不会拼写。那么我们如何在这样的树中找到这四个测试的最佳排列呢？好吧，第一步是查看每个测试对数据的作用。</p>
        <p>Occam’s Razor. Spelled so many ways it doesn’t matter how I spell it. And that’s good, because I can’t spell.
            So how are we going to go about finding the best possible arrangement of those four tests in a tree like
            that? Well, step one will be to see what each test does with the data.</p>
        <p>顺便说一句，在我进一步阐述之前，你我都知道这是一个非常小的样本数据集，适合课堂操作。你绝不会拿这么小的数据集来打赌。我们只用它来做课堂演示。但想象一下，将这些行乘以 10。那么，你就得到了 80 个样本，而不是 8
            个。然后，你可能会开始相信产生的结果。</p>
        <p>And by the way, before I go a step further, you know and I know that this is a sample data set that’s very
            small, suitable for classroom manipulation. You’d never bet your life on a data set this small. We use it
            only for classroom illustration. But imagine that these rows are multiplied by 10. So instead of eight
            samples, you’ve got 80. Then you might begin to believe the results that are produced.</p>
        <p>因此我只是假装其中的每一个都代表着另外 10 个我没有费心展示的样本。</p>
        <p>So I’m just going to pretend that each one of those represents 10 other samples that I haven’t bothered to
            show.</p>
        <h2 id="shadow-test">阴影测试</h2>
        <h2>Shadow Test</h2>
        <p>但是我们可以在课堂上进行这个实验，因为它的规模很小。我们可以说，这个影子测试有什么作用？影子测试将样本人群分为三组。一组是“我不知道”组，他们是夜行性人群。</p>
        <p>But we can work with this one in the classroom, because it’s pretty small. And we can say, well, what does
            this shadow test do? Well, the shadow test divides the sample population into three groups. There’s the I
            Don’t Know group of people who are nocturnal.</p>
        <p>有些人会投下阴影，他们是“是”的人。有些人不会投下阴影，他们是“否”的人。所以如果我看看上面的那些行，看看哪些是吸血鬼，在我看来，如果没有投下阴影，只有一个人不会投下阴影，那就是吸血鬼。所以那边是个加分项。吸血鬼。
        </p>
        <p>There are the people who do cast the shadow, the Yes people. And the people who do not cast a shadow, the No
            people. So if I look at those rows up there and see which ones are vampires, it looks to me that if there’s
            no shadow cast. there’s only one that doesn’t cast a shadow. and that is a vampire. So that’s a plus over
            there. Vampire.</p>
        <p>现在，如果我们看看那些确实会投下阴影的人，他们全都不是吸血鬼。他们都没事。现在有 8
            个。三个是吸血鬼。所以这意味着其中两个一定是吸血鬼。到目前为止，我已经有三、四、五、六了。所以还剩下两个。这就是阴影测试划分数据的方式。现在我们来看看大蒜。传统上，吸血鬼不吃大蒜。我不知道为什么。</p>
        <p>Now, if we look at the ones who do cast a shadow, all those are not vampires. They’re all OK. And now
            there’re 8. Three are vampires. So that means that two of these must be vampires. And I’ve got three, four,
            five, six so far. So there must be two left. So that’s the way the shadow test divides up the data. Now
            let’s do garlic. Vampires traditionally don’t eat garlic. I don’t know why.</p>
        <p>所以我们看一下大蒜测试，我们发现所有的否定答案都是三个肯定答案，它们都得出否定答案。所以如果有人吃大蒜，他们就不是吸血鬼。这意味着这三个吸血鬼一定在这里。然后还剩下两个。这就是大蒜测试的作用。明白我们想做什么了吗？
        </p>
        <p>So we look at the garlic test, and we see that all of the Nos. well, there’re three Yeses, and they all
            produce a No answer. So if somebody eats garlic, they’re not vampires. That means the three vampires must be
            over here. Then there are two left. So that’s what the garlic test does. See what we’re trying to do?</p>
        <p>我们正在尝试查看所有这些测试，以根据其数据划分方式找出我们最喜欢哪一个。现在我们有了肤色。对此有三种选择。你可以拥有普通的肤色。但根据我的经验，很多吸血鬼的肤色都比较苍白。所以苍白是可能的。</p>
        <p>We’re trying to look at all these tests to see which one we like best on the basis of how it divides up the
            data. So now we’ve got complexion. And there are three choices for this. You can have an average complexion.
            But a lot of vampires, in my experience, are rather pale. So pale is a possibility.</p>
        <p>另一种情况是，在吸完血后，他们的脸会变得有点红。所以我们这里会有一个红润的脸。我们再次回到我们的数据集，看看这个测试如何划分事物。所以有三个红润的脸，一个是“否”，一个是“否”，一个是“是”。所以两个“否”和一个“是”。两个“否”和一个“是”。
        </p>
        <p>And then the other option is that just after gorging themselves with blood, they tend to get a little red in
            the face. So we’ll have a ruddy over here. Once again, we have to go back to our data set to see how this
            test divides things up. So there are three ruddies, and one’s a No, one’s a No, and one’s a Yes. So two Nos
            and a Yes. Two Nos and a Yes.</p>
        <p>现在我们可以试试肤色苍白的人。只有两种。一个是“否”，一个是“否”。这肯定意味着这里有两个加号，因为总共有三个吸血鬼。二、四、六、七、八、九。八个，对不起。八个。只有八个。只剩一个了，那就是口音。从历史上看，吸血鬼会竭尽全力保护他们的口音，不暴露他们的血统。
        </p>
        <p>Now we can try for pale complexion people. There are only two of those. A No and a No.&nbsp;That must mean
            that there are two pluses over here, because there are three vampires altogether. Two, four, six, seven,
            eight, nine. Eight, sorry. Eight. Only eight. Just one more to go, and that’s the accent. Historically,
            vampires go to great length to protect their accent and not betray their origins.</p>
        <p>但尽管如此，我们可以预料到，如果他们刚到。如果他们刚从罗马尼亚的特兰西瓦尼亚来。他们可能还会有口音。所以，口音很正常，有些人口音很重，有些人口音很奇怪。让我们看看。口音。其中四个，就在最上面，没有口音。两个没有口音，一个有口音。口音很重。其中三个。一个有口音，两个没有口音。这意味着我们这里必须有一个加分项。
        </p>
        <p>But nevertheless, we can expect that if they’ve just arrived. if they’re just in from Transylvania, part of
            Romania. they may still have an accent. So there’s a normal, some still have a heavy accent, and some
            persist in having odd accents. So let’s see. Accent. Four of them, right at the top, have no accent. Two Nos
            and a Yes. Heavy accent. Three of those. A Yes and two Nos. That means we must have a plus here.</p>
        <p>3、6，加上一个减号。所以我们可以看看这些数据，然后说，那么，最好的测试是什么？最好的测试肯定是在这里，在分支的底部，产生与测试结果相对应的集合的测试。我们正在寻找一种能产生同质组的测试。</p>
        <p>3,6, plus and a minus. So we can look at this data and say, well, what will be the best test to use? And the
            best test to use would surely be the one that produces sets here, at the bottom of the branches, that
            correspond to the outcomes of the test. We’re looking for a test that produces homogeneous groups.</p>
        <p>为了便于说明，我假设我们将通过测试中放入同质样本的数量来判断测试的质量。因此，理想情况下，我们希望测试能够将所有吸血鬼归为一组，将所有普通人归为另一组。但是没有这样的测试。</p>
        <p>So just for the sake of illustration, I’m going to suppose that we’re going to judge the quality of the test
            by how many sample individuals it put into a homogeneous set. So ideally, we’d like a test that will put all
            the vampires in one group and all the ordinary people in another group right off the bat. But there are no
            such tests.</p>
        <p>但是我们可以将至少被放入同质集合的样本个体数量加起来。当我们这样做时，这个人有 3 个在同质集合中。第四个。但这些不是同质集合。所以这个人的总分是 4。这个，嗯，不太好。它只把 3 个个体放在同质集合中。这个，有 2
            个个体放在同质集合中。</p>
        <p>But we can add up the number of sample individuals who are put in to at least homogeneous sets. So when we do
            that, this guy has 3 in a homogeneous set here. A fourth. But these are not a homogeneous set. So the
            overall score for this guy will be 4. This one, well, not quite as good. It only puts 3 individuals in a
            homogeneous set. This one here, 2 individuals into a homogeneous set.</p>
        <p>其他人都和其他人混在一起了。在这里，同质集合中有多少样本？0。因此，根据这一分析，您可以得出结论，测试的顺序（就其质量而言）是从左到右。因此，最佳测试一定是影子测试。因此，让我们首先选择影子测试，看看我们能用它做什么。
        </p>
        <p>Everybody else is all mixed up with some other kind of person. And over here, how many samples are in a
            homogeneous set? 0. So on the basis of this analysis, you would conclude that the ordering of the test with
            respect to their quality is left to right. So the best test must be the shadow test. So let’s pick the
            shadow test first, see what we can do with that.</p>
        <p>如果我们先选择阴影测试，那么我们就会得到这样的安排。我们有问号，有是，会投阴影，没有，不会。这里有 3
            个减号。这里有一个加号。不幸的是，这里我们有加号、加号、减号、减号。所以我们需要另一个测试来划分该组。是的。学生：你是怎么在阴影测试中再次得到 4 分的？为什么是 4 分？</p>
        <p>If we pick the shadow test first, then we have this arrangement. We have question mark, and we have Yes,
            casts a shadow, and No, doesn’t. We have 3 minuses here. We have a plus here. And unfortunately, over here,
            we have plus, plus, minus, minus. So we need another test to divide that group up. Yes. STUDENT: How did you
            get the 4 on the shadow test again? Why was it 4?</p>
        <p>帕特里克·温斯顿：嗯，如果我查看数据，就会知道是谁。问题是，影子测试怎么样？如果你查看影子测试，你会说，嗯，有 4 个问号。如果我们查看并查看哪种人属于这 4 个问号，就会发现有 2 个吸血鬼和 2
            个非吸血鬼。这就是为什么有 2 个加号和 2 个减号。学生：不，我明白。</p>
        <p>PATRICK WINSTON: Well, if I look at the data and I see who. the question is, what about that shadow test? If
            you look at the shadow test, and you say, well, there are 4 question marks. And if we look and see what kind
            of people belong to those 4 question marks, there are 2 vampires and 2 non vampires. That’s why it’s 2
            pluses and 2 minuses. STUDENT: No, I understand that.</p>
        <p>问题是，你是怎么得到 4 分的？帕特里克·温斯顿：哦，是的。问题是我是怎么得到这个数字 4
            的？这与此无关，因为这是一个混合集。事实上，我在这里有三个人处于同质集，在这里有一个人处于同质集，我只是把他们加起来。学生：好的。帕特里克·温斯顿：非常简单的课堂说明。在实践中行不通。是的。</p>
        <p>The question is, how did you get to the score of 4? PATRICK WINSTON: Oh, yeah. The question is how did I get
            this number 4? It has nothing to do this, because this is a mixed set. In fact, I’ve got three guys in a
            homogeneous set here, and one guy in a homogeneous set here, and I’m just adding them up. STUDENT: OK.
            PATRICK WINSTON: So very simple classroom illustration. Wouldn’t work in practice. Yes.</p>
        <p>学生：你如何调整这种方法以适应更大的数据集，因为不太可能有任何变化。帕特里克·温斯顿：问题是，我如何调整这种方法以适应更大的数据集？你领先一步。相信我，我马上就要处理大型数据集了。我只是想让大家明白我的想法。我不希望有人认为我们用于大型数据集的方法有什么神奇之处。
        </p>
        <p>STUDENT: How do you adjust this for larger data sets where it’s unlikely you’re going to have any PATRICK
            WINSTON:. The question is, how do I adjust this for larger data sets? You’re one step ahead. Trust me, I’ll
            be doing large data sets in a moment. I just want to get the idea across. And I don’t want there to be any
            thought that the method we use for larger data sets has got anything magic about it.</p>
        <p>好的，我们开始行动吧。现在我们必须选择一个测试来划分这四个人。所以我们必须更加努力，重复我们在那里做的分析。但至少它会更简单，因为现在我们只考虑 4 个样本，而不是 8 个。只有我们仍然需要划分的 4
            个样本已经从左边的分支下来了。所以我有阴影测试。</p>
        <p>OK, so we’re off and running. And now we have to pick a test that will divide those four guys up. So we’re
            going to have to work this a little harder, and repeat the analysis we did there. But at least it’ll be
            simpler, because now we’re only considering 4 samples, not 8. Just the 4 samples that we still have to
            divide up that have come down that left branch. So I have the shadow test.</p>
        <p>它有 3 种结果。我们有大蒜测试。它有 2 种结果。是和否。我们有肤色测试。有 3 种结果。普通、苍白和红润。最后我们还有口音测试。结果是正常、浓重或奇怪。现在，弄清楚如图所示的这组数据集的结果有点困难。所以让我划掉。
        </p>
        <p>It has 3 outcomes. We have the garlic test. It has 2 outcomes. Yes and No.&nbsp;We have the complexion test.
            There’s 3 outcomes. Average, pale, and ruddy. And we have finally the accent test. And that comes out to be
            either normal, heavy, or odd. And now, it’s a little awkward to figure out what the results are for this
            data set as shown. So let me just strike out.</p>
        <p>我们不再关心那些，并将我们的分析限制在阴影测试结果为问号的样本上。这正是我们仍然需要分开的四个人，对吧？所以切换颜色，保持颜色不变。我们实际上不想再做阴影测试了，对吧？因为我们已经做过了。再做一次没有意义。我们不必看那个。
        </p>
        <p>The ones that we’re no longer concerned with, and limit our analysis to the samples for which the outcome of
            the shadow test is a question mark. This is exactly the four people we still need to separate, right? So
            switching colors, keeping the color the same. We actually don’t want to do the shadow test anymore, right?
            Because we’ve already done that. There’s no point in doing that again. We don’t have to look at that.</p>
        <p>它已经完成了所有可以进行的数据划分。那么大蒜测试。好吧，让我们看看。大蒜。2 个“是”，2
            个“否”。“是”产生“否”，“否”产生“是”。所以，如果这个人吃了大蒜，他们就没事了。如果他们不吃大蒜，那就糟糕了。他们是吸血鬼。好吧，这看起来是个不错的测试。但只是为了弄清楚这一切，让我们试试其他的。肤色。</p>
        <p>It’s already done all the division of data that it can. So the garlic test. Well, let’s see. Garlic. 2 Yeses,
            2 Nos. The Yeses produce Nos and the Nos produce Yeses. So if the person does eat garlic, they’re OK. And if
            they don’t eat garlic, bad news. they’re vampires. Well, that looks like a pretty good test. But just for
            the sake of working it all out, let’s try the others.complexion.</p>
        <p>2 个 Ruddies，一个是，一个 No. 1 淡色，一个 No. 1 淡色，一个 No. 1 淡色，一个 No. 1 。我们必须有 1 个平均值，果然，这是一个 Yes。现在我们可以做
            Accent，最右边的那个，看看它与仍然被视为样本的人相比如何。Accent。让我们看看。2 个 Nones，一个 Yes 和一个 No。No Heavies。2 个 Odds，一个 Yes 和一个
            No。&nbsp;</p>
        <p>2 Ruddies, a Yes, and a No.&nbsp;1 pale, and that’s a No.&nbsp;1 pale, and that’s a No.&nbsp;And we must have
            1 average, and sure enough, that’s a Yes. Now we can do accent, the one on the far right, and look at how
            that measures up against the people who are still under consideration as samples. Accent. Let’s see. 2
            Nones, a Yes and a No.&nbsp;No Heavies. 2 Odds, a Yes and a No.&nbsp;</p>
        <p>好的。现在我们可以做之前做过的事情，为了便于课堂演示，我们只需说明有多少个人被放入同质集合中。这里我们有 4 个。这里我们有 2 个。这里我们有 0
            个。所以很明显，大蒜测试是首选测试。所以我们回到这里，我们已经完成了需要做的工作。这就是大蒜测试。</p>
        <p>All right. So now we can do the same thing we did before, and just say, for sake of classroom illustration,
            how many individuals are put into a homogeneous sets. And here we have 4. And here we have 2. And here we
            have 0. So plainly, the garlic test is the test of choice. So we go back over here, and we’ve completed the
            work that we needed to do. So that’s the garlic test.</p>
        <p>这会产生 2 个加号。让我们看看。吃大蒜，是。吃大蒜，否。我想加号在这里是这样的。这两个是普通人。我们的任务完成了。现在你可以快速跑开，把这个放进你的 PDA
            里，永远免受其中一个吸血鬼在从东欧涌入的人群中逃走的可能性。</p>
        <p>And that produces 2 pluses. Let’s see. Eats garlic, Yes. Eats garlic, No.&nbsp;I guess the pluses go over
            here like so. And these are the two ordinary people. And we’re done with our task. And now you can quickly
            run off and put this into your PDA, and forever be protected against the possibility that one of those
            vampires got out in the flood of people that came in from Eastern Europe.</p>
        <p>但是我们该如何处理大型数据集呢？问题是，大型数据集不太可能产生结果。如果你有一个大型数据集，任何测试都不可能立即将任何同质集组合在一起。所以你永远无法开始。一切都会是
            0。每个测试都会说，哦，它没有把任何人放入同质集。所以你完蛋了。你需要其他更复杂的方法来测量这些数据的无序程度。</p>
        <p>Except what do we do a large data set? Well, the trouble is, a large data set’s not likely to produce. if you
            have a large data set, no test is likely to put together any homogeneous set right off. So you never get
            started. Everything would be 0. Every test would say, oh it doesn’t put anybody into homogeneous sets. So
            you’re screwed. You need some other, more sophisticated way of measuring how disordered this data is.</p>
        <p>或者你在树枝底部发现的这些集合有多无序。这就是你所需要的。你需要一种方法来测量你在这些树枝底部发现的这些集合的无序性，这样你就可以根据对无序性的测量找到一种总体质量来进行测试。现在，美好生活的第一个启发式方法是，当你有问题需要解决时，向知道答案的人请教。
        </p>
        <p>Or how disordered these sets are that you find at the bottom of the tree branches. That’s what you need. You
            need a way of measuring disorder of these sets that you find at the bottom of these branches, so you can
            find a kind of overall quality to the test based on your measurement of disorder. Now, the first heuristic
            of a good life is, when you have a problem to solve, ask somebody who knows the answer.</p>
        <p>这是最省事的方法。甚至去 Google
            也不难。那么你会问谁有关测量集合无序性的方法呢？有两种可能的答案。学生：你可以只研究熵。帕特里克·温斯顿：什么？学生：求集合的熵。帕特里克·温斯顿：谁研究熵？学生：概率。帕特里克·温斯顿：什么课程？学生：物理。学生：热力学。帕特里克·温斯顿：热力学！
        </p>
        <p>It’s the least amount of work. It’s not even as hard going to Google. So who would you ask about ways of
            measuring disorder in sets? There are two possible answers. STUDENT: You could just do entropy. PATRICK
            WINSTON: What? STUDENT: Find the entropy of the set. PATRICK WINSTON: Who studies entropy? STUDENT:
            Probability. PATRICK WINSTON: What kind of classes? STUDENT: Physics. STUDENT: Thermodynamics. PATRICK
            WINSTON: Thermodynamics!</p>
        <p>热力学家擅长测量无序性，因为这就是热力学的全部内容。熵随时间增加，诸如此类。还有另一个同样好的答案。学生：统计学家？帕特里克·温斯顿：统计学家。也许吧，但这不是第二好的答案。实际上，这甚至不是最好的答案。这是最好的答案。你叫什么名字？学生：利奥。帕特里克·温斯顿：哦，是的。帕特里克·温斯顿：列奥纳多已经找到了答案。
        </p>
        <p>The thermodynamicists are good at measuring disorder, because that’s what thermodynamics is all about.
            Entropy increasing over time, and all that sort of stuff. There’s another equally good answer. STUDENT:
            Statisticians? PATRICK WINSTON: Statisticians. Perhaps, but it’s not the second best answer. It’s actually
            not even the best answer. That’s the best answer. What’s your name? STUDENT: Leo. PATRICK WINSTON: Oh, yeah.
            PATRICK WINSTON: Leonardo has got his finger on it.</p>
        <p>信息理论家非常擅长测量无序性，因为这也是信息的全部内容。</p>
        <p>The information theorists are pretty good at measuring disorder, because that’s what information is all
            about, too.</p>
        <h2 id="measuring-disorder">测量障碍</h2>
        <h2>Measuring Disorder</h2>
        <p>因此，我们不妨从那些信息论者那里借用一种测量集合无序性的机制。我们要做的就是这个。我们把它放在这里，这样当我们想要测量这些东西时，它就很方便了。</p>
        <p>So we might as well borrow a mechanism for measuring the disorder of a set from those information theory
            guys. So what we’re going to do is exactly that. Let’s put it over here, so we’ll have it handy when we want
            to try to measure those things.</p>
        <p>信息理论家的福音是无序度 D 或某个集合等于。现在让我们假设这是一组二进制值。所以我们有正数，然后有负数。正数和负数。但是正数在代数方程中不太适用，因为它们可能与加法混淆。所以我要说 P 和 N。然后就是总数，即 P
            加 N。</p>
        <p>The gospel according to information theorists is that the disorder, D, or some set is equal to. now let’s
            suppose that this is a set of binary values. So we have positives and then we have negatives. Pluses and
            minuses. But pluses, they don’t go very well in an algebraic equation, because they might be confused with
            adding. So I’m going to say P and N. And then it’ll be the total, which is P plus N.</p>
        <p>我们只有两种选择，正数和负数。所以，根据那些人的说法，集合的无序性等于正数减去总数，乘以正数除以总数的底数 2 的对数，再减去负数除以总数，乘以负数除以总数的对数
            2。这些负数看起来有点令人担忧，因为你会想，好吧，也许这个东西会变成负数。</p>
        <p>We only have two choices, positive and negative. So the disorder of set, according those guys, is equal to
            minus the number of positives over the total number, times the log to the base 2 of the positives over the
            total, minus the negatives over the total, times the log 2 of the negatives over the total. Those negatives
            look a little worrisome, because you think, well, maybe this thing can go negative.</p>
        <p>但事实并非如此，对吧？因为这些比率都小于 1，而小于 1 的对数为负数。所以没问题。这是测量混乱程度的好方法。然后我们应该画出该曲​​线的图形。我们要绘制的是阳性数与总数的比率。</p>
        <p>But that’s not going to be true, right? Because these ratios are all less than 1, and the logarithm of
            something that’s less than 1 is negative. So we’re OK. So that’s a lovely way of measuring disorder. And
            then we ought to draw a graph of what that curve looks like. And what we’re going to graph it against is the
            ratio of positives to the total number.</p>
        <p>所以这将是一个从 0 到 1
            的轴。让我们找到几个有用的值。顺便说一句，注意这些曲线是值得的，因为如果你注意这些东西，你可以非常快速地解答这方面的测验问题。否则，我们会看到人们拿出计算器，很快就迷失方向，不知所措。好的，让我们看看。</p>
        <p>So that’s going to be an axis where we go from 0 to 1. So let’s just find a couple of useful values. And by
            the way, it pays to pay attention to these curves, because if you pay attention to this stuff, you can work
            the quiz questions on this very rapidly. Otherwise, we see people getting out their calculators and quickly
            becoming both lost and screwed. OK so let’s see.</p>
        <p>假设正数的数量等于负数的数量。所以我们得到了一个完全混合的集合。它在两个方向上都没有偏差。所以在这种情况下，如果 P 除以 T 等于 1/2，那么它等于 -1/2，乘以 1/2
            的对数。我想，因为它们都一样，我们可以乘以 2。这个值是多少？计算结果是多少？</p>
        <p>Let’s suppose that the number of positives is equal to the number of negatives. So we’ve got a completely
            mixed up set. It has no bias in either direction. So in that case, if P over T is equal to 1/2, then this is
            equal to minus 1/2, times the logarithm of 1/2. And I guess, since they’re both the same, we can multiply by
            two. And what’s that value? what does that calculate out to?</p>
        <p>学生：减号 帕特里克·温斯顿：减号 好吧，如果加上减号，你只需要把参数倒过来，这样它就是 log(2)。那么 log(2) 是什么？以 2 为底的对数为 2.1！所以整个事情就是这样。 学生：1。
            帕特里克·温斯顿：1。所以她以她温和的方式说，好吧，让我们看看。2 乘以 1/2。这抵消了。</p>
        <p>STUDENT: Minus PATRICK WINSTON: Minus Well, with a minus sign, you just turn the argument upside down, so
            it’s log(2). So what’s log(2)? Logarithm of base 2 of 2.1! So this whole thing is. STUDENT: 1. PATRICK
            WINSTON: 1. So in her soft way, says, well, let’s see. 2 times 1/2. That cancels out.</p>
        <p>减法，就是翻转参数，所以它是以 2 为底数的对数，也就是 1。所以整个事情，你算出代数，它给出 1。这很酷。所以就在它们相等的中间，我们得到的值是 1。接下来我们要做的就是计算如果 P 除以 T 等于 1
            会发生什么。也就是说，一切都是正数。有什么猜测吗？</p>
        <p>The minus, that flips the arguments so it’s log to the base 2 of 2, and that’s 1. So this whole thing, You
            work out the algebra, it gives you 1. So that’s cool. So right here in the middle where they’re equal, we
            get a value of 1. Next thing we need to do is let’s calculate what happens if P over T is equal to 1. That
            is to say, everything is a positive. Any guesses?</p>
        <p>也许 10,20，减 15？我们来算一下。所以如果 P 除以 T 等于 1，那就是减 1 乘以 1 的底数为 2 的对数。那是什么？学生：帕特里克·温斯顿：0？哦，是的。因为 2 的 0 倍数是 1。所以这部分是
            0。现在，另一部分呢？如果一切都是 P，那么就没有什么是 N。所以我们得到了 0。我们可以结束了。嗯，还不完全是。</p>
        <p>Maybe 10,20, minus 15? Let’s work it out. So if P over T equal 1, that would be minus 1 times the log to the
            base 2 of 1. What’s that? STUDENT: PATRICK WINSTON: A 0? Oh, yeah. Because 2 raise to the 0 is one. So this
            part is 0. Now, what about this other part? If everything’s a P, then nothing’s an N. So we’ve got 0. And we
            can quit already. Well, not quite.</p>
        <p>我们应该算出来。以 2 为底数的 0 的对数。那是什么？ 学生：帕特里克·温斯顿：谁？减无穷大？哦哦。0 乘以减无穷大是 我上高中时没搞明白。最后，1801 有所不同。最后。答案是什么。我们感兴趣的是 N 除以 T
            趋于 0 时的极限，对吗？当你遇到这样的情况时，你会怎么做？</p>
        <p>We ought to work it out. Log 2 to the base 2 of 0. What’s that? STUDENT: PATRICK WINSTON: Who? Minus
            infinity? Uh oh. 0 times minus infinity is What I didn’t get that when I was in high school. Finally, 1801
            makes a difference. Finally. What’s the answer. We’re interested in the limit as N over T goes to 0, right?
            And when you have a deal like this, what do you do?</p>
        <p>你用的是那个著名的规则，我们一看到它就都会发错音，对吧？我们使用古老的埃尔霍必达规则。好吧，这是洛必达规则。洛必达规则。你必须区分。我想我们把这个家伙区分为一个比率或某种东西，看看当它变为 0
            时会发生什么。当我们使用洛必达规则时，我们得到的结果是，哦，谢天谢地，这仍然是零。</p>
        <p>You use that famous rule, that we all mispronounce when we see it written, right? We use the good old El
            Hospital’s rule. OK, it’s L’Hopital. L’Hopital’s Rule. You have to differentiate the. I guess we
            differentiate this guy as a ratio or something, and see what happens when it goes to 0. And what we get when
            we use L’Hopital’s Rule is that, oh thank God, this is still zero.</p>
        <p>现在我们知道了上面有一个点，下面有一个点。现在我们在曲线上有三个点，我们可以画出来了。它就像那样。不，它不是那样的。它显然是高斯分布，对吧？因为自然界中的一切都是高斯分布。你能把那台笔记本电脑收起来吗？自然界中的一切都是高斯分布，所以它看起来像这样。对吧？
        </p>
        <p>So now we know that we have a point up there and a point down there. So now we’ve got three points on the
            curve, and we can draw it. It goes like that. No, it doesn’t go like that. It’s obviously a Gaussian, right?
            Because everything in a nature is a Gaussian. Can you put that laptop away, please? Everything in nature is
            a Gaussian, so it looks like this. That right?</p>
        <p>不，实际上，自然界中并非所有事物都是高斯的。特别是，这个也不是高斯的。它看起来更像他们过去称之为拱形屋的金属物体之一。它看起来就是这样。砰，就像这样。所以这就是感兴趣的曲线。现在，上帝是否说过使用这种测量无序性的方法是最好的方法？不，Got
            在这里没有表明任何选择。</p>
        <p>No, actually, not everything in nature is a Gaussian. And in particular, this one isn’t a Gaussian either. It
            looks more like one of those metal things they used to call quonset huts. That’s what it looks like. Boom,
            like so. So that is the curve of interest. Now, did God say that using this way of measuring disorder was
            the best way? No, Got has not indicated any choice here.</p>
        <p>我们之所以使用这种方法，是因为它是一种方便的机制，似乎很有道理，但与使​​用信息论的原因相反，它并不是某种优雅数学的结果。它只是借用了某种似乎行之有效的方法。任何这些曲线都可以起到同样的作用，因为我们用它所做的就是测量一个集合的无序程度。
        </p>
        <p>We use this because it’s a convenient mechanism, it seems to make sense, but in contrast to the reason it’s
            used information theory, it’s not the result of some elegant mathematics. It’s just a borrowing of something
            that seems to work pretty well. Any of those curves would work just about the same, because all we’re doing
            with it is measuring how disordered a set is.</p>
        <p>因此，需要注意的一点是，在这种情况下，我们面临两个选择。P 和 N，正数和负数。我们得到一条曲线，其最大值为 1。请注意，它会很快上升到 1。事实上，如果你在这里处于 2/3 的位置，那么你在这里的位置大约是 0.9。
        </p>
        <p>So one thing to note here is that in this situation, where we’re dealing with two choices. P and N, positives
            and negatives. we get a curve that maxes out at one. And notice that it kind of gets up there pretty fast.
            In fact, if you’re down here at 2/3, are you’re up here, this is about 0.9.</p>
        <h2 id="measuring-quality">测量质量</h2>
        <h2>Measuring Quality</h2>
        <p>因此，它会给出中间相当一部分区域的较大数字。</p>
        <p>So it gives you a large number for quite a bit of that area in the middle.</p>
        <p>因此，不幸的是，这仍然没有告诉我们所有我们需要知道的信息。这告诉我们如何测量这些集合中的一种疾病。但我们想知道如何衡量测试的整体质量。所以我们需要某种机制，假设这个测试产生了三个不同的集合，并且我们现在可以测量每个集合中的疾病，那么我们如何衡量测试的整体质量？
        </p>
        <p>So that, unfortunately, still doesn’t tell us everything we need to know. That tells us how to measure a
            disorder in one of these sets. But we want to know how to measure the quality of the test overall. So we
            need some mechanism that says, OK, given that this test produces three different sets, and we now have a
            measure of the disorder in each of these sets, how do we measure the overall quality of the test?</p>
        <p>好吧，你可以把无序性加起来。我们把它写下来，因为听起来不错。所以你可以说测试的质量等于所产生的集合的总和。我们要做的就是把每个集合的无序性加起来。我快到家了，只是这意味着我们要给几乎没有任何东西的分支赋予相同的权重。
        </p>
        <p>Well, you could just add up the disorder. Let’s write that down, because that sounds good. So you can say
            that the quality of a test is equal to some sum over the sets produced. And what we’re going to do is we’re
            going to add up the disorder of each of those sets. I’m almost home, except that this means we’re going to
            give equal weight to a branch that has almost nothing down it.</p>
        <p>我们会给这个分支赋予与几乎所有分支相同的权重。所以这似乎没有意义。所以最后一个点是，我们会根据最终进入该分支的样本比例来加权这个总和。所以，像往常一样，写下来比说出来更容易。</p>
        <p>We’re going to give the same weight to that as a branch that has almost everything going down it. So that
            doesn’t seem that make sense. So one final flourish is we’re going to weight this sum according to the
            fraction of the samples that end up down that branch. So it’s, as usual, easier to write it down than to say
            it.</p>
        <p>因此，我们要将其乘以集合中的样本数，再除以测试处理的样本数。因此，如果一半的样本进入一个分支，并且该分支具有某种无序性，那么我们要将该无序性乘以
            1/2。好的。现在让我们看看它如何解决我们的样本问题。好吧，这是我们的样本数据。我们不需要任何花哨的东西。</p>
        <p>So we’re going to multiply that times the number of samples in the set, divided by the number of samples
            handled by test. So if half the samples go down a branch, and if that branch has a certain disorder, then
            we’re going to multiply that disorder times 1/2. All right. So now let’s see how it works with our sample
            problem. Well, here is our sample data. And we didn’t need anything fancy for it.</p>
        <p>但是我们假设它是一个很大的数据集。好吧，让我们看看。我们会怎么做？好吧，往下看，那个方向有 4 个样本。这是样本总数的一半。所以无论我们在那里找到什么，我们都要乘以 1/2。这个我们要乘以 3/8。这个我们要乘以
            1/8。现在，我们在这些东西的底部到底找到了什么？</p>
        <p>But let’s pretend it was a large data set. Well, let’s see. What would we do? Well, go down this way, there
            are 4 samples down that direction. That’s half of the total number of samples. So whatever we find down
            there, we’re going to multiply by 1/2. This one we’re going to multiply by 3/8. And this one we’re going to
            multiply by 1/8. Now, what do we actually find at the bottom of these things?</p>
        <p>嗯，这是个齐次集。一切都一样。所以我们看那条曲线，说齐次集的无序性是多少？它是零。让我们看看，它们都一样。我猜这意味着它那边是
            0。所以这组三个样本的无序性是零。这组一个样本的无序性，都一样，是零。这个集合的无序性。嗯，让我们看看。</p>
        <p>Well, here’s a homogeneous set. Everything’s the same. So we go to that curve and say, what is the disorder
            of a homogeneous set? It’s zero. Let’s see, they’re all the same. I guess that means it’s 0 over there. So
            the disorder of this set of three samples is zero. The disorder of this set of one sample, all the same, is
            zero. The disorder of this set. well, let’s see.</p>
        <p>那里的样本有一半是正数，一半是负数，所以我们看曲线，说，如果某个东西的正数和负数混合相等，它的无序性是多少？那就是 1。所以这个东西的无序性就是 1。</p>
        <p>Half of the samples there are plus, and half are minus, so we go over to our curve, and we say, what’s the
            disorder of something with equal mixture of pluses and minuses? And that’s one. So the disorder of this guy
            is one.</p>
        <p>所以现在我们得到了 1/2 乘以 1，3/8 乘以 0，1/8 乘以 0。所以这个特定测试的质量（由它产生的集合的无序性决定）是 1/5.0.5。我们来做这个。所以我们有 3/8 以这种方式下降，5/8
            以这种方式下降。3/8 乘以一组均匀事物的无序性。这就是无序性 0。所以这边的这个，让我们看看。这是 2/5 和 3/5 相乘。</p>
        <p>So now we’ve got 1/2 times 1, and 3/8 times 0,1/8 times 0. So the quality of this particular test, as
            determined by the disorder of the sets it produces, is 1/5.0.5. Let’s do this one. So we have 3/8 coming
            down this way, 5/8 coming down this way. 3/8 is multiplied by the disorder of a set of uniform things.
            That’s disorder 0. So this guy over here, let’s see. That’s 2/5 and 3/5 multiplied.</p>
        <p>你知道，这是这样的交易之一，如果你看曲线，你会发现它非常接近中间值。这条曲线一直上升到 0.9 左右。</p>
        <p>You know, this is one of those deals where if you look at the curve, you’re pretty close to the middle. And
            that curve goes all the way up to about 0.9 there.</p>
        <p>所以你可以看看这个，用眼睛观察，然后说，不管它是什么，总的来说，这将是乘以 5/8。比如 0.9 乘以 5/8。所以，为了讨论的方便，我们假设这个数字大约是
            0.6，我认为这个数字的准确度在百分之一以内。只是猜测。好吧，现在我们开始行动了。</p>
        <p>So you can kind of just look at this, and eyeball it, and say, well, whatever it is, the overall, this is
            going to be something multiplied times 5/8. Something like 0.9 times 5/8. So let’s just say, for the sake of
            discussion, that’s going to be about 0.6, which is within a hundredth, I think, of being right. Just kind of
            guessing. OK, well now we’re on a roll.</p>
        <p>这里，我们有 3/8 下到这个分支，3/8 下到这个分支，1/4 下到这个分支。这是 0。这是其中两个约为 0.9 的交易之一。所以看起来将是 3/8 加 3/8 等于 3/4。乘以约 0.9。所以结果是约
            0.7。所以最后再来一次。3/8、3/8 和 1/4。哦，这很有趣。</p>
        <p>Here, we have 3/8 coming down this branch, 3/8 coming down this branch, 1/4 coming down this branch. This is
            0. And this is one of those deals where these two are about 0.9. So it looks like it’s going to be 3/8 plus
            3/8 is 3/4. Times about 0.9. So that’s going to turn out to be about 0.7. So one last go here. 3/8,3/8, and
            1/4. Oh, that’s interesting.</p>
        <p>因为这两个就是我们得到的 0.7 的贡献值。这个是 0.4 倍。这是均等的，所以无序度为 1。所以这会比我们这里得到的数字大 0.25。所以最终结果大约是
            0.95。所以感谢上帝，我们的答案与我们通过简单的课堂无序度测量得到的答案相同。</p>
        <p>Because these two are what we got contributed up to that 0.7. This one is 0.4 times. this is evenly divided,
            so that’s going to have disorder of 1. So that’s going to be 0.25 bigger than the number we got over here.
            So that’s going to end up being about 0.95. So thanks god our answer is the same as we got with our simple
            classroom measurement of disorder.</p>
        <p>除了这是在测量事物的无序程度之外，我们想要的是小数字，而不是大数字。所以再一次，基于这个分析，你一定会选择阴影投射，因为 0.5 小于 0.6，小于 0.7，小于
            0.95。所以那个口音测试真的很糟糕。不要使用它。仅仅因为某人口音很重并不意味着他们是吸血鬼。</p>
        <p>Except this is measuring how disordered stuff is, we want the small number, not the big number. So once
            again, based on this analysis, you’ll be sure to pick the shadow cast, because 0.5 is less than 0.6, which
            is less than 0.7, which is less than 0.95. So that accent test is really horrible. Don’t use it. Just
            because somebody has a heavy accent doesn’t mean they’re a vampire.</p>
        <p>事实上，正如我之前提到的，大多数吸血鬼都在努力练习他们的口音。好吧，现在我们知道我们仍然会选择阴影测试作为我们的第一步。这很好。现在，让我们看看我们是否可以用我们的第二个选择重复这个练习，我们必须用这个选择来区分那些人。这会更容易，因为要处理的东西更少。哦，哇，看。
        </p>
        <p>In fact, most vampires have worked very hard on their accent, as I mentioned before. All right, so now we
            know that we’re still going to pick the shadow test as our first go. So that’s good. Now, let’s see if we
            can repeat the exercise with our second selection, the one we have to have to pick those guys apart. And
            this is going to be easier, because there are fewer things to work with. Ooh, wow, look.</p>
        <p>那是 0。那是 0。那是 1/2。那是 1/2。所以这个家伙的无序度是 0.0。所以这是 1/4,1/4,1/2,0.0。1/2 乘以 1。哦，那是 0.5。这很简单。这个呢？哦，他说 1。让我们看看。那是 1。那是
            1。那是 1/2。那是 1/2。是的，它是 1。所以可以肯定的是，答案也和之前我们做简单的直觉练习时一样。所以我不知道。</p>
        <p>That’s 0. That’s 0. That’s 1/2. That’s 1/2. So the disorder of this guy is 0.0. So this is 1/4,1/4,1/2,0.0.
            1/2 times 1. Ooh, that’s 0.5. That was easy. How about this one? Oh, he says 1. Let’s see. That’s 1. That’s
            1. That’s 1/2. That’s 1/2. Yeah, it is one. So sure enough, the answer also comes out to be the same as
            before, when we did our just simple intuition exercise. So I don’t know.</p>
        <p>克里斯托弗，这都是关于使用信息理论吗？</p>
        <p>Christopher, is this all about using information theory?</p>
        <h2 id="intuition">直觉</h2>
        <h2>Intuition</h2>
        <p>学生：不。帕特里克·温斯顿：不，不，不。你看，这与数学无关。这与直觉有关。直觉就是你想建立一棵尽可能简单的树。如果你查看数据，然后说，嗯，哪个测试最能将事物分开，那么你就可以建立一棵尽可能简单的树。</p>
        <p>STUDENT: No.&nbsp;PATRICK WINSTON: No, no, no. See, it’s not about the math. It’s about the intuition. And
            the intuition is that you want to build a tree that’s as simple as possible. And you can build a tree that’s
            as simple as possible if you look at the data, and say, well, which test does the best job of splitting
            things up?</p>
        <p>哪个测试最能在其下构建尽可能同质的子集？所以所有这些信息理论，所有这些熵的东西，都只是一种方便的机制，可以做一些直觉上合理的事情。好吗？这与信息理论无关。这与一种合理的直觉有关。哦，顺便说一句。这种东西在实践中被用过吗？成千上万次。
        </p>
        <p>Which test does the best job of building subsets underneath it that are as homogeneous as possible? So all
            this information theory, all this entropy stuff, is just a convenient mechanism for doing something that is
            intuitionally sound. OK? It’s not about information theory. It’s about a sound intuition. Oh, by the way.
            Does this kind of stuff ever get used in practice? 10s of thousands of times.</p>
        <p>这是一个屡屡被使用的制胜机制，即使数据是数字。如果是数字数据，它会如何工作？好吧，让我们想一想。假设我们有一个机会。我们是急救医疗技术员或类似的，我们在医务室工作。现在他们怎么称呼它？别的什么。但无论如何，你在那种领域工作，你有机会测量人们的体温。
        </p>
        <p>This is a winning mechanism that’s used over and over again, even when the data is numeric. How would it work
            if it’s numeric data? Well, let’s think about that for a little bit. So let’s suppose that we have an
            opportunity. We’re an EMT or something, we work in the infirmary. What do they call it these days? Something
            else. But anyhow, you work in that kind of area, and you have the opportunity to take people’s temperature.
        </p>
        <p>随着时间的推移，您积累了一些有关人们体温的数据。也许您发现这里有一个吸血鬼，体温大约为 102 度。这里有一个正常人，体温大约为 98.6
            度。但是他们分散在各地。有些人进来时会发烧。所以问题是，有没有办法使用数字数据。你可以用实数表示的东西。有没有办法用这种机制来使用它？答案是肯定的。</p>
        <p>And so over time, you’ve accumulated some data on the temperature of people. And maybe you’ve found that
            there’s a vampire here at about 102. There’s a normal person here, about 98.6. But then they’re scattered
            around. Some people have fevers when they come in. So the question is, is there a way of using numerical
            data. things that you can put real numbers. is there a way of using that with this mechanism? And the answer
            is yes.</p>
        <p>你只要说，温度是高于还是低于某个阈值？这给了你一个测试，一个二元测试，就像任何其他测试一样。对吧？但我该把阈值放在哪里？我想我可以把它放在平均值。但这可能不是将样本分成同质组的最佳位置。克里斯托弗？</p>
        <p>You just say, is the temperature greater than or less than some threshold? And that gives you a test, a
            binary test, just like any of these other tests. Right? But where would I put the threshold? I suppose I
            could just put it at the average value. But that might not be the place that does the best job of splitting
            the samples into homogeneous groups. Christopher?</p>
        <p>学生：所以你在不同的地方用不同的阈值运行这个数值分析。帕特里克·温斯顿：所以你尝试不同的地方，他说。他是对的。因为这是一台计算机，这是我们的奴隶。我们不关心它为找出正确的阈值做了多少工作。</p>
        <p>STUDENT: So you run this numerical analysis on different places with different thresholds. PATRICK WINSTON:
            So you try different places, he says. And he’s right. Because this is a computer, this is our slave. We
            don’t care how much it works to figure out the right threshold.</p>
        <p>因此，我们的做法是，也许阈值介于这两个人的中间，或者介于这两个人的中间，或者这两个人的中间，或者这两个人的中间，或者这两个人的中间。所以我们可以尝试比样本少一个的阈值。我们不在乎是否有 10,000
            个样本，因为这是一台计算机，我们不在乎它是否整晚都在工作。这就是你找到数字测试阈值的方法。</p>
        <p>So what we do is we say, well, maybe the threshold’s halfway between those two guys, or halfway between those
            two guys, or those two guys, or those two guys, or those two guys. So we can try one less threshold than we
            have samples. And we don’t care if there are 10,000 samples, because this is a computer, and we don’t care
            if it works all night. So that’s how you find the threshold for a numeric test.</p>
        <p>顺便说一句，我之前向你保证过，你绝不会重复使用同一个测试。这次是这样吗？是的，你绝不会重复使用同一个测试。但你可能会在下一次对同一个测量使用不同的阈值。因此，当你开始获得数值数据时，你可能会发现自己使用同一个测试，轴相同，但值不同。好吧。
        </p>
        <p>By the way, I assured you earlier on you would never use the same test twice. Is that true for this? Yes, you
            would still never use the same test twice. But what you might do is you might use a different threshold on
            the same measurement the next time around. So when you start having numerical data, you may find yourself
            using the same test with the same axis but with a different value. All right.</p>
        <p>现在我们有了这个，我们就可以回过头来比较一下当我们将它与我们上次讨论的带有电气盖的东西放在一起时这种方法会是什么样子。</p>
        <p>So now that we have this, then we can go back and compare how this method would look when we put it up
            against the sort of stuff we were talking about last time, with the electrical covers.</p>
        <h2 id="decision-boundaries">决策边界</h2>
        <h2>Decision Boundaries</h2>
        <p>所以对于电气盖，我们遇到了这样的情况。我不知道。我们有像这样的地方的样本，并且我们对空间进行了划分，看起来与此非常相似。</p>
        <p>So with the electrical covers, we had a situation like this. I don’t know. We had samples that were places
            like this, and we had a division of the space that look pretty much like that.</p>
        <p>虽然不是完全在正确的位置，但非常接近。因此，这些是我们使用最近邻来划分数据的情况下的决策边界。如果这是四种不同的东西，并且我们使用这种机制，那么决策边界会是什么样子？也许有很多样本都聚集在这样的地方。决策边界会是什么样子？
        </p>
        <p>Not quite exactly in the right spots, but pretty close. So these are the decision boundaries for the
            situation where we are using nearest neighbors to divide up the data. What would the decision boundaries
            look like if these were four different kinds of things, and we were using this kind of mechanism? And maybe
            there’s a lot of samples all clustered around places like that. What would the decision boundaries look
            like?</p>
        <p>它们会和这个一样吗？天哪，我希望不会。为什么？因为我们要做的是在每个轴上使用一个阈值。因此，决策边界将与一个轴或另一个轴平行。因此，我们可以决定，例如。哦，该死。我想我会再画一次，因为如果我把它画在另一个上面，它会混淆。所以它看起来像这样。
        </p>
        <p>Would they be the same as this? god, I hope not. Why? Because what we’re going to do is we’re going to use a
            threshold on each axis. So therefore, the decision boundaries are going to be parallel to one axis or the
            other. So we might decide, for example. Oh, shoot. I think I’ll draw it again, because it’ll get confused if
            I draw it over the other one. So it looks like this.</p>
        <p>这就是最近邻算法的工作原理。但是识别树方法会沿着一个轴或另一个轴选择一个阈值。假设是这个轴。它只有一个选择。所以它会在那里画一条线。现在，它接下来要做什么？嗯，它仍然需要区分这两种不同的东西。我们假设我们有四种不同的东西。所以它会说，哦！
        </p>
        <p>And that’s how nearest neighbors does it. But a identification tree approach will pick a threshold along one
            axis or the other. Let’s say it’s this axis. It’s only got one choice there. So it’s going to put a line
            there. And now, what’s the next thing it does? Well, it still has these two different kinds of things to
            separate. We’re going to assume we’ve got four different kinds of things. So it’s going to say, oh!</p>
        <p>我已经得出了负面结论，所以我需要对剩余的数据设置一个阈值。现在只剩下这两个了。所以我唯一的选择就是在那里设置一个阈值。现在我保证，绝对保证。在测验中，有人。大概是那些不去上课的人。会把那条线划到对面。这是完全错误的。因为我们已经把这个数据集分成了两半。
        </p>
        <p>I’ve Come down the negative side, so I need a threshold on the remaining data. And these are the only two
            things that are now remaining. So my only choice is to put a threshold in there. Now I guarantee this,
            absolutely guaranteed. on the quiz, somebody. presumably somebody who doesn’t go to lectures. will draw that
            line all the way across. And that’s desperately wrong. Because we’ve already divided this data set in half.
        </p>
        <p>现在，我们在这里做什么的选择仅取决于我们看到的剩余样本，这两个。所以阈值将像这样进入。这就是你回去时发生的事情。这被使用了数万次。一直被使用。它的优点是什么？第一，你不会使用所有的测试。</p>
        <p>Now the choice of what we do over here is governed only by the remaining samples that we see, these two. And
            so the threshold is going to go in there like that. So that’s what happens when you go back. This is used
            10s of thousands of times. Always used. What are the virtues of it? Number one, you don’t use all the tests.
        </p>
        <p>你只使用那些看起来对你有用的测试。这意味着你的工作做得更好，因为你的测量技术更简单。而且成本更低，因为你不必花费大量金钱进行所有的测试。所以这是真正的赢家。但你知道吗？有些人，不是科学家，但我指的是医生之类的人。他们不喜欢看这些树。
        </p>
        <p>You use only the test that seem to be doing some useful work for you. So that means that you do a better job,
            because your measurement technique is simpler. And it costs less, because you’re not going to the expense of
            doing all of the testing. So it’s a real winner. But you know what? Some classes of people. not scientists,
            but I mean people like doctors and stuff. They don’t like to look at these tress.</p>
        <p>它们有点规则导向。所以他们用一棵树来确定你患有哪种甲状腺疾病，它里面可能有 20
            多个测试，测试各种激素，比如甲状腺素等等。他们说，啊，我们无法处理这个问题。所以我们必须与他们合作。所以我们要做的就是将树转换成一组规则。</p>
        <p>They’re kind of rule oriented. So they look a tree like this for determining what kind of thyroid disease you
            have, and it would have maybe 20 or so tests in it of various kinds of hormones, like thyroxine and this and
            that. And they say, ah, we can’t deal with that. So we have to work with them. So what we do is we convert
            the tree into a set of rules.</p>
        <p>我们如何将这棵树转换成一套规则？哎呀，错了。走开，走开。这就是我想要的。是的，很好。我们如何将这棵树转换成一套规则？这很简单。我们该怎么做？学生：你基本上只要查看每个分支。帕特里克·温斯顿：你基本上只要查看每个分支直到一片叶子。所以你说，例如，这是一条规则。
        </p>
        <p>How do we convert the tree into a set of rules? Oops, wrong one. Go away, go away. Here’s what I want. Yeah,
            good. How would we convert this tree into a set of rules? It’s straightforward. what do we do? STUDENT:
            You’d basically just look down each branch. PATRICK WINSTON: You’d basically just go down each branch to a
            leaf. So you say, for example, here’s one rule.</p>
        <p>如果阴影等于问号，大蒜等于哦，那么要选择否。不吃大蒜。不。我想我会说是。是的。这改变了答案。那么如果它吃大蒜，它就不是吸血鬼，对吧？这是四条可能规则之一，因为有四个叶节点。现在，几乎完成了。我们完成了，除了一件事。我们实际上可以采用这四条规则，并开始思考如何简化它们。
        </p>
        <p>If shadow equals question mark, and garlic equals oh, want to choose No.&nbsp;Doesn’t eat garlic. No.&nbsp;I
            think I’ll say Yes. Yes. That changes the answer. Then if it eats garlic, it’s not a vampire, right? That’s
            one of four possible rules, because there are four leaf nodes. Now, almost done. We are done, except for one
            thing. We can actually take these four rules, and start thinking about how to simplify them.</p>
        <p>你可以问这样的问题，如果我有一条规则可以同时测试影子和大蒜，我是否真的需要这两个前提条件？答案是，在很多情况下，不需要。特别是在这种情况下，不需要。因为如果我们查看我们的数据集，我们会发现，如果我们谈论的是影子问号。哦，我想我有更好的选择。哦，不。
        </p>
        <p>You can ask questions like, if I have a rule that tests both the shadow and the garlic, do I actually need
            both of those antecedents? And the answer is, in many cases, no. And in particular, in this case, no.
            Because if we look at our data set, what we discover is that in the event that we’re talking about a shadow
            question mark. oh, I guess I had a better choice the other way. Oh, no.</p>
        <p>如果你看看大蒜，所有的大蒜。是的，是的，是的。答案是否定的，与阴影条件无关。所以我们可以看看规则，在某些情况下，我们会发现我们的树比它需要的要复杂一些。我们实际上可以去掉一些条款。</p>
        <p>If you look at the garlic, all the garlics. Yes, Yes, and Yes. it turns out that the answer is no,
            independent of what the shadow condition is. So we can look at the rules, and in some cases, we’ll discover
            that our tree is a little bit more complicated than it needs to be. We can actually get rid of some of the
            clauses.</p>
        <p>所以最后，我们可以开发一种非常简单的机制，该机制基于良好的传统规则行为，就像您在主题开头看到的那样，它可以完成这项工作。现在，无需任何版税，您可以自由地将其放入您的 PDA
            中，并在未来的日子里用它来保护自己，尤其是万圣节即将来临。</p>
        <p>So in the end, we can develop a very simple mechanism based on good old fashioned rule based behavior, like
            you saw almost in the beginning of the subject, that does the job. And now, without any royalty, you’re all
            free to put this in your PDA and use it to protect yourself in the days to com, especially since Halloween’s
            just around the corner.</p>
        <h1 id="a-neural-nets">12a：神经网络</h1>
        <h1>12a: Neural Nets</h1>
        <p><img
                src="data:image/jpeg;base64,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">8
            年前 (2016 年 4 月 21 日) — 50:43 <a
                href="https://youtube.com/watch?v=uXt8qF2Zzfo">https://youtube.com/watch?v=uXt8qF2Zzfo</a></p>
        <p> 8 years ago (Apr 21, 2016) — 50:43 <a
                href="https://youtube.com/watch?v=uXt8qF2Zzfo">https://youtube.com/watch?v=uXt8qF2Zzfo</a></p>
        <h2 id="unknown-182">未知</h2>
        <h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，地址为 fsae@mit.edu。PATRICK WINSTON：那是在 2010 年，是的，没错。那是在 2010 年。我们正在进行年度讨论，讨论要从 6034
            中删除哪些内容，以便为其他内容腾出空间。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or to view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at fsae@mit.edu. PATRICK WINSTON: It was in
            2010, yes, that’s right. It was in 2010. We were having our annual discussion about what we would dump fro
            6034 in order to make room for some other stuff.</p>
        <p>我们几乎消灭了神经网络。这看起来可能很奇怪，因为我们的脑袋里塞满了神经元。如果你打开头骨，把它们全部拔出来，你就不再思考了。因此，神经网络似乎是一个基本且无懈可击的话题。但我们中的许多人认为，当时的神经模型并不能真实地反映出我们脑袋里到底发生了什么。
        </p>
        <p>And we almost killed off neural nets. That might seem strange because our heads are stuffed with neurons. If
            you open up your skull and pluck them all out, you don’t think anymore. So it would seem that neural nets
            would be a fundamental and unassailable topic. But many of us felt that the neural models of the day weren’t
            much in the way of faithful models of what actually goes on inside our heads.</p>
        <p>除此之外，从来没有人制造过一个值得一试的神经网络。所以我们几乎要放弃它了。但后来我们说，好吧，如果每个人都参加了人工智能课程，却没有学习任何有关神经网络的知识，那么他们就会觉得自己被骗了，然后他们就会自己发明神经网络。他们会浪费很多时间。所以我们保留了这个主题。
        </p>
        <p>And besides that, nobody had ever made a neural net that was worth a darn for doing anything. So we almost
            killed it off. But then we said, well, everybody would feel cheated if they take a course in artificial
            intelligence, don’t learn anything about neural nets, and then they’ll go off and invent them themselves.
            And they’ll waste all sorts of time. So we kept the subject in.</p>
        <h2 id="unknown-183">未知</h2>
        <h2>Unknown</h2>
        <p>两年后，多伦多大学的 Jeff Hinton 用他开发的用于识别和分类图片的神经网络震惊了世界。他发表了一篇论文，我现在将向你们展示其中的几个例子。顺便说一下，Jeff 的神经网络有 6000 万个参数。它的目的是确定
            1000 个类别中的哪一个最能描述一张图片。就是这样。</p>
        <p>Then two years later, Jeff Hinton from the University of Toronto stunned the world with some neural network
            he had done on recognizing and classifying pictures. And he published a paper from which I am now going to
            show you a couple of examples. Jeff’s neural net, by the way, had 60 million parameters in it. And its
            purpose was to determine which of 1,000 categories best characterized a picture. So there it is.</p>
        <p>这里有一个多伦多神经网络能够识别或犯错的样本。我将稍微放大一下。我想我将特别关注标有集装箱船的示例。</p>
        <p>There’s a sample of things that the Toronto neural net was able to recognize or make mistakes on. I’m going
            to blow that up a little bit. I think I’m going to look particularly at the example labeled container ship.
        </p>
        <p>所以你在这里看到的是，程序根据可能性、概率或确定性返回了它对排名的最佳估计，即前五名，它认为某个特定类别是图片的特征。所以你可以看到，这张照片非常有信心它是一艘集装箱船。它也对它可能是一艘救生艇的想法感到相当感动。
        </p>
        <p>So what you see here is that the program returned its best estimate of what it was ranked, first five,
            according to the likelihood, probability, or the certainty that it felt that a particular class was
            characteristic of the picture. And so you can see this one is extremely confident that it’s a container
            ship. It also was fairly moved by the idea that it might be a lifeboat.</p>
        <h2 id="unknown-184">未知</h2>
        <h2>Unknown</h2>
        <p>现在，我不确定你的看法，但我认为这看起来不太像救生艇。但它看起来像一艘集装箱船。所以如果我只看最佳选择，它看起来相当不错。这是他们做得相当好的其他事情，得到正确答案是第一个选择。是这个第一个选择。所以在左边，你看到它决定这张图片是一张螨虫的图片。
        </p>
        <p>Now, I’m not sure about you, but I don’t think this looks much like a lifeboat. But it does look like a
            container ship. So if I look at only the best choice, it looks pretty good. Here are the other things they
            did pretty well, got the right answer is the first choice. is this first choice. So over on the left, you
            see that it’s decided that the picture is a picture of a mite.</p>
        <p>螨虫根本不在图片中心附近，但它不知怎么地找到了中心。又是集装箱船。有一辆摩托车，上面坐着几个人。但它正确地将图片归类为摩托车。然后在右边，是一只豹子。其他一切都是某种猫。所以它似乎做得很好。事实上，它确实做得很好。
        </p>
        <p>The mite is not anywhere near the center of the picture, but somehow it managed to find it. the container
            ship again. There is a motor scooter, a couple of people sitting on it. But it correctly characterized the
            picture as a motor scooter. And then on the right, a Leopard. And everything else is a cat of some sort. So
            it seems to be doing pretty well. In fact, it does do pretty well.</p>
        <p>但任何从事这种工作的人都有义务向你展示一些效果不佳或不太正确的东西。所以这些图片也出现在 Hinton
            的论文中。所以第一个被描述为烤架。但正确的答案应该是可转换的。哦，不，是的，是的，正确的答案是可转换的。在第二种情况下，特征是蘑菇。</p>
        <p>But anyone who does this kind of work has an obligation to show you some of the stuff that doesn’t work so
            well on or doesn’t get quite right. And so these pictures also occurred in Hinton’s paper. So the first one
            is characterized as a grill. But the right answer was supposed to be convertible. Oh, no, yes, yeah, right
            answer was convertible. In the second case, the characterization is of a mushroom.</p>
        <h2 id="unknown-185">未知</h2>
        <h2>Unknown</h2>
        <p>而所谓的正确答案是
            agaric。发音正确吗？原来那是一种蘑菇。所以没问题。在下一个案例中，它说它是樱桃。但它应该是斑点狗。现在，我认为斑点狗是那张特定图片的完全合法的答案。所以很难因此而指责它。而在最后一个案例中，正确答案不在前五名中。
        </p>
        <p>And the alleged right answer is agaric. Is that pronounced right? It turns out that’s a kind of mushroom. so
            no problem there. In the next case, it said it was a cherry. But it was supposed to be a dalmatian. Now, I
            think a dalmatian is a perfectly legitimate answer for that particular picture. so hard to fault it for
            that. And the last case, the correct answer was not in any of the top five.</p>
        <p>我不知道你是否见过马达加斯加猫的帽子。但这是一张图片。将它与程序的首选松鼠猴进行比较很有趣。这是两个并排的图片。因此，在某种程度上，它认为马达加斯加猫是松鼠猴的图片并不奇怪。这真是令人印象深刻。它击败了竞争对手。它的表现好得多，第二名甚至都差得远。
        </p>
        <p>I’m not sure if you’ve ever seen a Madagascar cap. But that’s a picture of one. And it’s interesting to
            compare that with the first choice of the program, the squirrel monkey. This is the two side by side. So in
            a way, it’s not surprising that it thought that the Madagascar cat was a picture of a squirrel monkey. so
            pretty impressive. It blew away the competition. It did so much better the second place wasn’t even close.
        </p>
        <p>这是第一次证明神经网络确实可以发挥作用。从那时起，三年来，人们在神经网络技术上投入了大量精力，有人说这就是答案。</p>
        <p>And for the first time, it demonstrated that a neural net could actually do something. And since that time,
            in the three years since that time, there’s been an enormous amount of effort put into neural net
            technology, which some say is the answer.</p>
        <h2 id="unknown-186">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们今天和明天要做的就是看看这些东西，问问自己为什么它有效，什么时候可能无效，需要做什么，已经做了什么，所有这些问题都会浮现出来。所以，我想要做的第一件事就是想想我们受到什么启发。我们受到头脑中那些东西的启发。
        </p>
        <p>So what we’re going to do today and tomorrow is have a look at this stuff and ask ourselves why it works,
            when it might not work, what needs to be done, what has been done, and all those kinds of questions will
            emerge. So I guess the first thing to do is think about what it is that we are being inspired by. We’re
            being inspired by those things that are inside our head.</p>
        <p>全部都是 10 的 11 次方。因此，如果我们从 10 的 11 次方中取出一个并查看它，您就会从 700 左右知道神经元的大致形状。顺便说一下，我将在本次讲座中教您如何回答有关神经生物学的问题，并且有 80%
            的可能性您会给出与神经生物学家相同的答案。让我们开始吧。这是一个神经元。</p>
        <p>All 10 to the 11th of them. And so if we take one of those 10 to the 11th and look at it, you know from 700
            something or other approximately what a neuron looks like. And by the way, I’m going to teach you in this
            lecture how to answer questions about neurobiology with an 80% probability that you will give the same
            answer as a neurobiologist. So let’s go. So here’s a neuron.</p>
        <p>它有一个细胞体。还有一个细胞核。然后这里有一个长条形的东西，它可能会分裂一点，但不会分裂太多。我们称之为轴突。然后在这里，我们有一个分支更多的结构，看起来可能有点像这样。可能像那样。这个东西分支很多。这部分称为树突树。
        </p>
        <p>It’s got a cell body. And there is a nucleus. And then out here is a long thingamajigger which divides maybe
            a little bit, but not much. And we call that the axon. So then over here, we’ve got this much more branching
            type of structure that looks maybe a little bit like so. Maybe like that. and this stuff branches a whole
            lot. And that part is called the dendritic tree.</p>
        <h2 id="unknown-187">未知</h2>
        <h2>Unknown</h2>
        <p>现在，我们可以注意到一些事情，这些神经元通过轴突与树突相连。所以在这里，它们会形成所谓的突触前增厚。而在这里，则是其他神经元的树突。同样，在这里，其他神经元的轴突会进入这里，撞击占据图片大部分的树突。</p>
        <p>Now, there are a couple of things we can note about this is that these guys are connected axon to dendrite.
            So over here, they’ll be a so called pre synaptic thickening. And over here will be some other neuron’s
            dendrite. And likewise, over here some other neuron’s axon is coming in here and hitting the dendrite of our
            the one that occupies most of our picture.</p>
        <p>因此，如果轴突树或树突树的这一侧有足够的刺激，那么一个尖峰就会沿着轴突向下。它就像一条传输线。然后，在那之后，神经元会安静一段时间，因为它正在恢复强度。这被称为不应期。现在，如果我们更详细地看一下这种联系，这里的这个小片段看起来有点像这样。
        </p>
        <p>So if there is enough stimulation from this side in the axonal tree, or the dendritic tree, then a spike will
            go down that axon. It acts like a transmission line. And then after that happens, the neuron will go quiet
            for a while as it’s recovering its strength. That’s called the refractory period. Now, if we look at that
            connection in a little more detail, this little piece right here sort of looks like this.</p>
        <p>这是轴突。它里面有一大堆小囊泡。然后这里有一个树突。当轴突受到刺激时，它会将所有这些囊泡倾倒到这个内部突触空间中。长期以来，人们并不知道这些东西是否真的分开了。我认为是 Raamon 和 Cahal
            证明了一个神经元实际上不是下一个神经元的一部分。它们实际上是被这些突触间隙分开的。</p>
        <p>Here’s the axon coming in. It’s got a whole bunch of little vesicles in it. And then there’s a dendrite over
            here. And when the axon is stimulated, it dumps all these vesicles into this inner synaptic space. For a
            long time, it wasn’t known whether those things were actually separated. I think it was Raamon and Cahal who
            demonstrated that one neuron is actually not part of the next one. They’re actually separated by these
            synaptic gaps.</p>
        <h2 id="unknown-188">未知</h2>
        <h2>Unknown</h2>
        <p>就是这样。我们如何建模，诸如此类的事情？好吧，通常的做法是这样的。这是神经网络文献中的做法。首先，我们有一些二进制输入，因为这些东西要么触发，要么不触发。所以这是一种全有或全无的情况。所以在这里，我们有某种输入值。我们称之为
            x1。它要么是 0，要么是 1。所以它就在这里。</p>
        <p>So there it is. How can we model, that sort of thing? Well, here’s what’s usually done. Here’s what is done
            in the neural net literature. First of all, we’ve got some kind of binary input, because these things either
            fire or they don’t fire. So it’s an all or none kind of situation. So over here, we have some kind of input
            value. We’ll call it x1. And is either a 0 or 1. So it comes in here.</p>
        <p>然后将其乘以某种权重。我们将其称为
            w1。因此，这部分是对突触连接的建模。它可能更强或更弱。如果更强，则权重会增加。如果不那么强，则权重会降低。因此，这反映了突触对整个轴突是否决定受到刺激的影响。然后我们在这里得到了其他输入。</p>
        <p>And then it gets multiplied times some kind of weight. We’ll call it w1. So this part here is modeling this
            synaptic connection. It may be more or less strong. And if it’s more strong, this weight goes up. And if
            it’s less strong, this weight goes down. So that reflects the influence of the synapse on whether or not the
            whole axon decides it’s stimulated. Then we got other inputs down here.</p>
        <p>X sub n，也是 0 或 1。它还乘以权重。我们将其称为 w sub
            n。现在，我们必须以某种方式表示这些输入被收集在一起的方式。它们如何产生集体力量。我们将非常非常简单地对此进行建模，只需说，好的，我们将像这样在整个夏天运行它。</p>
        <p>X sub n, also 0 or 1. It’s also multiplied by a weight. We’ll call that w sub n.&nbsp;And now, we have to
            somehow represent the way in which these inputs are collected together. how they have collective force. And
            we’re going to model that very, very simply just by saying, OK, we’ll run it through a summer like so.</p>
        <h2 id="unknown-189">未知</h2>
        <h2>Unknown</h2>
        <p>但随后我们必须决定所有这些输入的综合影响是否足以使神经元激发。所以我们将通过像这样的阈值框来做到这一点。这是根据输入和输出之间的关系得出的框。您可以看到，在输入超过某个阈值 t 之前，什么都不会发生。</p>
        <p>But then we have to decide if the collective influence of all those inputs is sufficient to make the neuron
            fire. So we’re going to do that by running this guy through a threshold box like so. Here is what the box
            looks like in terms of the relationship between input and the output. And what you can see here is that
            nothing happens until the input exceeds some threshold t.</p>
        <p>如果发生这种情况，则输出 z 为 1。否则，输出为 0。所以二进制，二进制输出。我们通过这些乘数来模拟突触权重。我们通过加法来模拟所有输入到神经元的累积效应。我们通过这个阈值框来判断它是全 1 还是全
            1，然后看看乘积之和是否超过阈值。</p>
        <p>If that happens, then the output z is a 1. Otherwise, it’s a 0. So binary, binary out. we model the synaptic
            weights by these multipliers. We model the cumulative effect of all that input to the neuron by a summer. We
            decide if it’s going to be an all or none 1 by running it through this threshold box and seeing if the sum
            of the products add up to more than the threshold.</p>
        <p>如果是，我们得到的是 1。那么，我们最终实际上在建模什么呢？好吧，有了这个模型，我们有数字 1，全有或全无。数字 2，累积影响。数字
            3，哦，我想是突触权重。但这并不是真实神经元中可以建模的全部。我们可能想要处理不应期。在我们构建神经网络的这些生物模型中，我们可能想要建模轴突分叉。</p>
        <p>If so, we get a 1. So what, in the end, are we in fact modeling? Well, with this model, we have number 1, all
            or none. number 2, cumulative influence. number 3, oh, I, suppose synaptic weight. But that’s not all that
            there might be to model in a real neuron. We might want to deal with the refractory period. In these
            biological models that we build neural nets out of, we might want to model axonal bifurcation.</p>
        <h2 id="unknown-190">未知</h2>
        <h2>Unknown</h2>
        <p>我们确实在神经元的轴突中发现了一些分裂。结果表明，脉冲要么沿着一个分支，要么沿着另一个分支。它沿着哪个分支下降取决于分裂附近的电活动。所以这些东西实际上可能是一个奇妙的巧合探测器。但我们没有对此进行建模。我们不知道它是如何工作的。所以轴突分叉可能被建模。我们也可以看看时间模式。
        </p>
        <p>We do get some division in the axon of the neuron. And it turns out that pulse will either go down one branch
            or the other. And which branch it goes down depends on electrical activity in the vicinity of the division.
            So these things might actually be a fantastic coincidence detectors. But we’re not modeling that. We don’t
            know how it works. So axonal bifurcation might be modeled. We might also have a look at time patterns.</p>
        <p>瞧，我们不知道的是，我们不知道这些脉冲到达树突的时间是否与神经元将要识别的内容有关。所以这里有很多未知数。现在，我将向你们展示如何回答一个关于神经生物学的问题，你们有 80%
            的概率会答对。只要说我们不知道。神经生物学家有 80% 的概率会说这个。</p>
        <p>See, what we don’t know is we don’t know if the timing of the arrival of these pulses in the dendritic tree
            has anything to do with what that neuron is going to recognize. so a lot of unknowns here. And now, I’m
            going to show you how to answer a question about neurobiology with 80% probability you’ll get it right. Just
            say, we don’t know. And that will be with 80% probability what the neurobiologist would say.</p>
        <p>所以这是一个受我们大脑活动启发的模型。但我们建模的内容是否是这些神经元让我们能够做到的本质，这一点还远不清楚。不过，这就是我们要开始的地方。这就是我们要去的地方。所以我们得到了神经元功能的模型。那么这些神经元的集合又会做什么呢？
        </p>
        <p>So this is a model inspired by what goes on in our heads. But it’s far from clear if what we’re modeling is
            the essence of why those guys make possible what we can do. Nevertheless, that’s where we’re going to start.
            That’s where we’re going to go. So we’ve got this model of what a neuron does. So what about what does a
            collection of these neurons do?</p>
        <h2 id="unknown-191">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，我们可以把你的头骨想象成一个装满神经元的大盒子。也许更好的理解方式是，你的头里充满了神经元。而这些神经元又充满了权重和阈值。因此，这个盒子里有各种输入 x1 到
            xm。这些输入会进入这群神经元的内部。然后从这里输出一堆输出 c1 到 zn。</p>
        <p>Well, we can think of your skull as a big box full of neurons. Maybe a better way to think of this is that
            your head is full of neurons. And they in turn are full of weights and thresholds like so. So into this box
            come a variety of inputs x1 through xm. And these find their way to the inside of this gaggle of neurons.
            And out here come a bunch of outputs c1 through zn.</p>
        <p>可能有很多这样的。还有很多这样的输入。这些输入通过阈值权重的影响以某种方式产生一组输出。所以我们可以把它写得更花哨一点，只要说 z 是一个向量，它是输入向量的函数，也是权重向量和阈值向量的函数。</p>
        <p>And there a whole bunch of these maybe like so. And there are a lot of inputs like so. And somehow these
            inputs through the influence of the weights of the thresholds come out as a set of outputs. So we can write
            that down a little fancier by just saying that z is a vector, which is a function of, certainly the input
            vector, but also the weight vector and the threshold vector.</p>
        <p>这就是神经网络的全部。当我们训练神经网络时，我们能做的就是调整权重和阈值，这样我们就能得到我们想要的结果。所以神经网络是一个函数逼近器。好好想想吧。它是一个函数逼近器。</p>
        <p>So that’s all a neural net is. And when we train a neural net, all we’re going to be able to do is adjust
            those weights and thresholds so that what we get out is what we want. So a neural net is a function
            approximator. It’s good to think about that. It’s a function approximator.</p>
        <h2 id="unknown-192">未知</h2>
        <h2>Unknown</h2>
        <p>因此，也许我们有一些样本数据，这些数据会给出一个输出向量，它是输入的另一个函数，我们不用考虑权重和阈值。这就是我们想要得到的。因此，通过将期望值与实际值进行比较，就可以确定我们的表现如何。</p>
        <p>So maybe we’ve got some sample data that gives us an output vector that’s desired as another function of the
            input, forgetting about what the weights and the thresholds are. That’s what we want to get out. And so how
            well we’re doing can be figured out by comparing the desired value with the actual value.</p>
        <p>因此，我们可能会认为，我们可以通过构建一些性能函数来了解我们的表现，这些性能函数由期望向量和输入向量决定。抱歉，期望向量和某些特定输入或某些输入集的实际输出向量。问题是这个函数应该是什么？</p>
        <p>So we might think then that we can get a handle on how well we’re doing by constructing some performance
            function, which is determined by the desired vector and the input vector. sorry, the desired vector and the
            actual output vector for some particular input or for some set of inputs. And the question is what should
            that function be?</p>
        <p>考虑到我们想要的结果和实际得到的结果，我们应该如何衡量绩效？好吧，一个简单的方法就是测量差异的大小。这是有道理的。但当然，这会给我们一个性能函数，它是这些向量之间距离的函数，如下所示。但最终这在数学上是不方便的。
        </p>
        <p>How should we measure performance given that we have what we want out here and what we actually got out here?
            Well, one simple thing to do is just to measure the magnitude of the difference. That makes sense. But of
            course, that would give us a performance function that is a function of the distance between those vectors
            would look like this. But this turns out to be mathematically inconvenient in the end.</p>
        <h2 id="unknown-193">未知</h2>
        <h2>Unknown</h2>
        <p>那么你觉得我们要怎么把它调高一点呢？观众：把它标准化？帕特里克·温斯顿：那是什么？观众：把它标准化？帕特里克·温斯顿：嗯，我不知道。我们把它平方一下怎么样？这样我们就可以把这个小尖点变成更像那个的东西。所以当然，当差值为
            0 时是最好的。</p>
        <p>So how do you think we’re going to turn it up a little bit? AUDIENCE: Normalize it? PATRICK WINSTON: What’s
            that? AUDIENCE: Normalize it? PATRICK WINSTON: Well, I don’t know. How about just we square it? And that way
            we’re going to go from this little sharp point down there to something that looks more like that. So it’s
            best when the difference is 0, of course.</p>
        <p>远离 0 时，情况会变得更糟。但我们在这里试图做的是，我们试图得到一个最小值。我希望你能原谅我。我只是不喜欢我们在这里的方向，因为我喜欢把改进看作是上坡而不是下坡。所以我要再修饰一下。在那里放一个减号。</p>
        <p>And it gets worse as you move away from 0. But what we’re trying to do here is we’re trying to get to a
            minimum value. And I hope you’ll forgive me. I just don’t like the direction we’re going here, because I
            like to think in terms of improvement as going uphill instead of down hill. So I’m going to dress this up
            one more step. put a minus sign out there.</p>
        <p>然后我们的性能函数看起来是这样的。它总是负数。它的最佳值可能是零。所以我们要使用它，因为我就是我。这没关系，对吧？不过，你还是想最小化或最大化某个性能函数。好的，那么我们该怎么做呢？我想我们可以做的是处理这个问题。好吧，我们已经知道该怎么做了。
        </p>
        <p>And then our performance function looks like this. It’s always negative. And the best value it can possibly
            be is zero. So that’s what we’re going to use just because I am who I am. And it doesn’t matter, right?
            Still, you’re trying to either minimize or maximize some performance function. OK, so what do we got to do?
            I guess what we could do is we could treat this thing. well, we already know what to do.</p>
        <h2 id="unknown-194">未知</h2>
        <h2>Unknown</h2>
        <p>我甚至不确定我们为什么要把这堂课专门用于讨论这个问题，因为很明显，我们要做的是尝试调整权重和阈值，以最大限度地提高性能。所以我们可以在这里用一个只有两个权重的简单神经网络制作一个小轮廓图。也许它看起来像这样。轮廓图。
        </p>
        <p>I’m not even sure why we’re devoting our lecture to this, because it’s clear that what we’re trying to do is
            we’re trying to take our weights and our thresholds and adjust them so as to maximize performance. So we can
            make a little contour map here with a simple neural net with just two weights in it. And maybe it looks like
            this. contour map.</p>
        <p>在任何给定时间，我们都有一个特定的 w1 和特定的 w2。我们正在尝试找到更好的 w1 和 w2。所以现在我们在这里。这是等高线图。它是
            6034。那么我们该怎么做？观众：爬山。帕特里克·温斯顿：爬山很简单，对吧？所以我们会朝每个方向迈出一步。如果我们朝那个方向迈出一步，那就不太好。实际上，情况会很糟糕。</p>
        <p>And at any given time we’ve got a particular w1 and particular w2. And we’re trying to find a better w1 and
            w2. So here we are right now. And there’s the contour map. And it’s a 6034. So what do we do? AUDIENCE:
            Climb. PATRICK WINSTON: Simple matter of hill climbing, right? So we’ll take a step in every direction. If
            we take a step in that direction, not so hot. That actually goes pretty bad.</p>
        <p>这两个真的很丑。啊，但是那个。那个让我们有点上坡了。所以我们结束了，除了我刚才提到的 Hinton 的神经网络中有 6000 万个参数。所以我们不会用 6000
            万个参数爬坡，因为你必须处理的权重数量会呈指数级增长。你可以采取的步骤数量。所以这种方法在计算上是难以处理的。</p>
        <p>These two are really ugly. Ah, but that one. that one takes us up the hill a little bit. So we’re done,
            except that I just mentioned that Hinton’s neural net had 60 million parameters in it. So we’re not going to
            hill climb with 60 million parameters because it explodes exponentially in the number of weights you’ve got
            to deal with. the number of steps you can take. So this approach is computationally intractable.</p>
        <h2 id="unknown-195">未知</h2>
        <h2>Unknown</h2>
        <p>幸运的是，你们都已经完成了 1801 或同等级别的考试。所以你们有了更好的想法。我们要做的是求一些偏导数，而不是在每个方向上都迈出一步。我们将看看它们对我们如何在太空中行走有什么建议。</p>
        <p>Fortunately, you’ve all taken 1801 or the equivalent thereof. So you have a better idea. Instead of just
            taking a step in every direction, what we’re going to do is we’re going to take some partial derivatives.
            And we’re going to see what they suggest to us in terms of how we’re going to get around in space.</p>
        <p>因此，我们可能对 w1 取该性能函数的偏导数。我们也可能对 w2 取该函数的偏导数。这些将告诉我们，通过朝这些方向稍微移动，我们能获得多大的改进，对吗？假设我们只是沿着轴线移动，变化有多大。</p>
        <p>So we might have a partial of that performance function up there with respect to w1. And we might also take a
            partial derivative of that guy with respect to w2. And these will tell us how much improvement we’re getting
            by making a little movement in those directions, right? How much a change is given that we’re just going
            right along the axis.</p>
        <p>所以也许我们应该做的是，如果这个比那个大很多，那就意味着我们主要想朝这个方向移动，或者用 1801 术语来说，我们要做的就是跟随梯度。因此 w 向量的变化将等于这个偏导数乘以 I 加上这个偏导数乘以 j。</p>
        <p>So maybe what we ought to do is if this guy is much bigger than this guy, it would suggest we mostly want to
            move in this direction, or to put it in 1801 terms, what we’re going to do is we’re going to follow the
            gradient. And so the change in the w vector is going to equal to this partial derivative times I plus this
            partial derivative times j.</p>
        <h2 id="unknown-196">未知</h2>
        <h2>Unknown</h2>
        <p>因此，按照这个公式，我们最终会在这个特定情况下朝那个方向移动，直到山坡最陡峭的部分。而我们要移动多少是一个问题。因此，我们只需设置一个速率常数
            R，它决定我们的步幅有多大。现在您认为我们完成了。好吧，我们这边太糟糕了。我们还没有完成。</p>
        <p>So what we’re going to end up doing in this particular case by following that formula is moving off in that
            direction right up to the steepest part of the hill. And how much we move is a question. So let’s just have
            a rate constant R that decides how big our step is going to be. And now you think we were done. Well, too
            bad for our side. We’re not done.</p>
        <p>我们不能使用梯度上升是有原因的，或者在我画出梯度的情况下，如果我们以另一种方式采用性能函数，则使用下降。为什么我们不能使用它？听众：局部最大值。帕特里克·温斯顿：注释是局部最大值。这当然是正确的。但这不是我们的第一个障碍。为什么梯度上升不起作用？听众：所以你使用的是阶跃函数。帕特里克·温斯顿：啊，我们的函数有问题。没错。
        </p>
        <p>There’s a reason why we can’t use. create ascent, or in the case that I’ve drawn our gradient, descent if we
            take the performance function the other way. Why can’t we use it? AUDIENCE: Local maxima. PATRICK WINSTON:.
            The remark is local maxima. And that is certainly true. But it’s not our first obstacle. Why doesn’t
            gradient ascent work? AUDIENCE: So you’re using a step function. PATRICK WINSTON: Ah, there’s something
            wrong with our function. That’s right.</p>
        <p>它是非线性的，而是不连续的。因此梯度上升需要连续的空间和连续的表面。我们这边太糟糕了。它不是。那该怎么办？好吧，25 年来没人知道该怎么办。人们花了 25 年时间训练神经网络，直到 1974 年哈佛大学的 Paul
            Werbos 才给我们答案。现在我想告诉你答案是什么。</p>
        <p>It’s non linear, but rather, it’s discontinuous. So gradient ascent requires a continuous space, continuous
            surface. So too bad our side. It isn’t. So what to do? Well, nobody knew what to do for 25 years. People
            were screwing around with training neural nets for 25 years before Paul Werbos sadly at Harvard in 1974 gave
            us the answer. And now I want to tell you what the answer is.</p>
        <h2 id="unknown-197">未知</h2>
        <h2>Unknown</h2>
        <p>答案的第一部分是这些阈值很烦人。它们只是需要处理的额外负担。我们真正喜欢的是，c 不是 xw 和 t 的函数，而是我们希望 c prime 是 x 和权重的 f prime
            函数。但我们必须以某种方式考虑阈值。所以这是你这样做的方法。你要做的是，让我们为这个神经元添加另一个输入。</p>
        <p>The first part of the answer is those thresholds are annoying. They’re just extra baggage to deal with. What
            we really like instead of c being a function of xw and t was we’d like c prime to be a function f prime of x
            and the weights. But we’ve got to account for the threshold somehow. So here’s how you do that. What you do
            is you say let us add another input to this neuron.</p>
        <p>它将具有权重 w0。它将连接到始终为 -1 的输入。你明白了吗？现在我们要做的就是让 w0 等于 t。这会对阈值的移动产生什么影响？</p>
        <p>And it’s going to have a weight w0. And it’s going to be connected to an input that’s always minus 1. You
            with me so far? Now what we’re going to do is we’re going to say, let w0 equal t. What does that do to the
            movement of the threshold?</p>
        <p>它的作用是取该阈值并将其移回 0。因此，这里的小技巧是取这个粉色阈值并重新执行，以便新的阈值框看起来像这样。想想看。如果这是 t，这是负 1，那么这是负 t。所以如果一切都结束了，这个东西应该会触发。如果总和超过
            0。所以这是有道理的。</p>
        <p>What it does is it takes that threshold and moves it back to 0. So this little trick here takes this pink
            threshold and redoes it so that the new threshold box looks like this. Think about it. If this is t, and
            this is minus 1, then this is minus t. And so this thing ought to fire if everything’s over. if the sum is
            over 0. So it makes sense.</p>
        <h2 id="unknown-198">未知</h2>
        <h2>Unknown</h2>
        <p>这样就消除了阈值问题。现在我们可以只考虑权重了。但是，我们仍然有阶跃函数。这并不好。所以我们要做的是平滑它。所以这是第二个技巧。我们将使用我们亲切地称为 S 形函数的东西来代替阶跃函数，因为它有点像 S 形。</p>
        <p>And it gets rid of the threshold thing for us. So now we can just think about weights. But still, we’ve got
            that step function there. And that’s not good. So what we’re going to do is we’re going to smooth that guy
            out. So this is trick number two. Instead of a step function, we’re going to have this thing we lovingly
            call a sigmoid function, because it’s kind of from an s type shape.</p>
        <p>我们要使用的函数是这个。1，嗯，最好让它有点不同。1/1 加上 e 减去输入值。我们将输入称为 alpha。这有意义吗？如果 alpha 为 0，那么它就是 1/1 加上 1 加上一半。如果 alpha
            非常大，那么即使减去 alpha 也非常小。它变成 1。</p>
        <p>And the function we’re going to use is this one. one, well, better make it a little bit different. 1 over 1
            plus e to the minus whatever the input is. Let’s call the input alpha. Does that makes sense? Is alpha is 0,
            then it’s 1 over 1 plus 1 plus one half. If alpha is extremely big, then even the minus alpha is extremely
            small. And it becomes one.</p>
        <p>在这里，它逐渐增加到 1 的渐近值。另一方面，如果 alpha 为极负值，那么负 alpha 就是极正值。它逐渐趋近于
            0。所以我们得到了该函数的正确视图。这是一个非常方便的函数。上帝是否说过神经元应该如此。该阈值应该如此工作？不，上帝没有这么说。谁这么说的？数学这么说的。</p>
        <p>It goes up to an asymptotic value of one here. On the other hand, if alpha is extremely negative, than the
            minus alpha is extremely positive. And it goes to 0 asymptotically. So we got the right look to that
            function. It’s a very convenient function. Did God say that neurons ought to be. that threshold ought to
            work like that? No, God didn’t say so. Who said so? The math says so.</p>
        <h2 id="unknown-199">未知</h2>
        <h2>Unknown</h2>
        <p>它具有正确的形状、外观和数学。事实证明，它具有正确的数学，稍后您会看到。让我们看看。我们在哪里？我们决定我们想要做的是取这些偏导数。我们知道有这些阈值很尴尬。所以我们摆脱了它们。我们注意到不可能有阶跃函数。所以我们摆脱了它。
        </p>
        <p>It has the right shape and look and the math. And it turns out to have the right math, as you’ll see in a
            moment. So let’s see. Where are we? We decided that what we’d like to do is take these partial derivatives.
            We know that it was awkward to have those thresholds. So we got rid of them. And we noted that it was
            impossible to have the step function. So we got rid of it.</p>
        <p>现在，我们可以实际取这些偏导数，看看它是否能给我们提供一种训练神经网络的方法，使实际输出符合我们的期望。因此，为了解决这个问题，我们必须使用世界上最简单的神经网络。现在，如果我们有一个神经元，它就不是一个网络。但如果我们有两个词神经元，我们就有了一个网络。
        </p>
        <p>Now, we’re a situation where we can actually take those partial derivatives, and see if it gives us a way of
            training the neural net so as to bring the actual output into alignment with what we desire. So to deal with
            that, we’re going to have to work with the world’s simplest neural net. Now, if we’ve got one neuron, it’s
            not a net. But if we’ve got two word neurons, we’ve got a net.</p>
        <p>事实证明，这是世界上最简单的神经元。所以我们要研究它。不是 6000 万个参数，实际上只有几个，只有两个参数。让我们把它画出来。我们有输入 x。它进入乘法器。它乘以 w1。它进入一个 S
            形框，像这样。顺便说一下，我们将其称为 p1，即乘积 1。这里是 y。Y 乘以另一个权重。</p>
        <p>And it turns out that’s the world’s simplest neuron. So we’re going to look at it. not 60 million parameters,
            but just a few, actually, just two parameters. So let’s draw it out. We’ve got input x. That goes into a
            multiplier. And it gets multiplied times w1. And that goes into a sigmoid box like so. We’ll call this p1,
            by the way, product number one. Out here comes y. Y gets multiplied times another weight.</p>
        <h2 id="unknown-200">未知</h2>
        <h2>Unknown</h2>
        <p>我们将其称为 w2。颈部产生另一种产品，我们将其称为 p2。然后将其放入 S 形框中。然后输出结果为 z。z 是我们用来确定表现如何的数字。我们的性能函数 p 将是 1/2 减
            1/2，因为我喜欢事情朝着一个方向发展，乘以期望输出和实际输出平方之间的差值。</p>
        <p>We’ll call that w2. The neck produces another product which we’ll call p2. And that goes into a sigmoid box.
            And then that comes out as z. And z is the number that we use to determine how well we’re doing. And our
            performance function p is going to be one half minus one half, because I like things are going in a
            direction, times the difference between the desired output and the actual output squared.</p>
        <p>现在让我们决定这些偏导数是什么。让我在这里做。那么我们要计算什么呢？性能函数 p 关于 w2 的偏导数。好的。好吧，让我们看看。我们试图弄清楚当我们摆动那个时，这个摆动了多少。但你知道它会经过这个变量
            p2。所以也许我们可以做的是弄清楚这个摆动了多少。</p>
        <p>So now let’s decide what those partial derivatives are going to be. Let me do it over here. So what are we
            trying to compute? Partial of the performance function p with respect to w2. OK. Well, let’s see. We’re
            trying to figure out how much this wiggles when we wiggle that. But you know it goes through this variable
            p2. And so maybe what we could do is figure out how much this wiggles.</p>
        <p>当我们摆动 p2 时，z 摆动多少，然后当我们摆动 w2 时，p2
            摆动多少。我只是把它们相乘。我忘了。那叫什么？N180。什么的。观众：链式法则帕特里克·温斯顿：链式法则。所以我们要做的是使用链式法则重写那个偏导数。它所做的就是说有一个中间变量。</p>
        <p>How much z wiggles when we wiggle p2 and then how much p2 wiggles when we wiggle w2. I just multiplied those
            together. I forget. What’s that called? N180. something or other. AUDIENCE:. The chain rule PATRICK
            WINSTON:. The chain rule. So what we’re going to do is we’re going to rewrite that partial derivative using
            chain rule. And all it’s doing is saying that there’s an intermediate variable.</p>
        <h2 id="unknown-201">未知</h2>
        <h2>Unknown</h2>
        <p>我们可以通过乘以其他部分的摆动量来计算该端相对于该端的摆动量。让我把它写下来。这在数学上更有意义。因此，这将能够将 p 相对于 z 的偏函数乘以 z 相对于 p2 的偏函数。请继续关注。</p>
        <p>And we can compute how much that end wiggles with respect how much that end wiggles by multiplying how much
            the other guys wiggle. Let me write it down. It makes more sense in mathematics. So that’s going to be able
            to the partial of p with respect to z times the partial of z with respect to p2. Keep me on track here.</p>
        <p>z 关于 w2 的偏导数。现在，我要做一些让自己讨厌的事情。我要从黑板上擦掉一些东西。我不喜欢这样做。但你知道我要做什么，对吧？我要根据链式法则说这是正确的。但你看，我也可以把这个家伙拿出来，用链式法则把它弄乱。
        </p>
        <p>Partial of z with respect to w2. Now, I’m going to do something for which I will hate myself. I’m going to
            erase something on the board. I don’t like to do that. But you know what I’m going to do, don’t you? I’m
            going to say this is true by the chain rule. But look, I can take this guy here and screw around with it
            with the chain rule too.</p>
        <p>事实上，我要做的是将其替换为 z 相对于 p2 的偏导数和 p2 相对于 w2 的偏导数。所以我最终没有将其抹去。但你可以看到我接下来要做什么。现在，我要对另一个偏导数做同样的事情。</p>
        <p>And in fact, what I’m going to do is I’m going to replace that with partial of z with respect to p2 and
            partial of p2 with respect to w2. So I didn’t erase it after all. But you can see what I’m going to do next.
            Now, I’m going to do same thing with the other partial derivative.</p>
        <h2 id="unknown-202">未知</h2>
        <h2>Unknown</h2>
        <p>但这次，我不会写下来再写，而是想一次性将其全部展开。因此，p 相对于 w1 的偏导等于 p 相对于 z 的偏导，z 相对于 p2 的偏导，p2 相对于什么的偏导？Y？</p>
        <p>But this time, instead of writing down and writing over, I’m just going to expand it all out in one go, I
            think. So partial of p with respect to w1 is equal to the partial of p with respect to z, the partial of z
            with respect to p2, the partial of p2 with respect to what? Y?</p>
        <p>y 相对于 p1 的偏导数。p1 相对于 w1
            的偏导数。因此，这就像沿着变量串拉拉链一样，使用链式法则逐个展开，直到到达末尾。因此，有一些表达式可以提供这些偏导数。但现在，请原谅我，这样写出来很方便。这符合我头脑中的直觉。但我要把它们反过来。</p>
        <p>Partial of y with respect to p1. partial of p1 with respect to w1. So that’s going like a zipper down that
            string of variables expanding each by using the chain rule until we got to the end. So there are some
            expressions that provide those partial derivatives. But now, if you’ll forgive me, it was convenient to
            write them out that way. That matched the intuition in my head. But I’m just going to turn them around.</p>
        <p>这只是一个乘积。我只是要把它们倒过来。所以部分 p2、部分 w2、乘以部分 z、部分 p2、乘以 p 相对于 z 的部分。同样的事情。现在，这个。让我继续往下看，因为如果这里发生突变，那将是致命的。部分 p1。部分
            w1、部分 y、部分 p1、部分 p2、部分 y、部分 z。</p>
        <p>It’s just a product. I’m just going to turn them around. So partial p2, partial w2, times partial of z,
            partial p2, times the partial of p with respect to z. same thing. And now, this one. Keep me on track,
            because if there’s a mutation here, it will be fatal. Partial of p1. partial of w1, partial of y, partial
            p1, partial of p2, partial of y, partial of z.</p>
        <h2 id="unknown-203">未知</h2>
        <h2>Unknown</h2>
        <p>这是 p2 的偏函数，是相对于 z 的性能函数的偏函数。现在，我们要做的就是找出这些偏函数是什么。我们已经解决了这个简单的神经网络。所以这很容易。我的电路板空间在哪里？让我们看看，p2
            相对于什么的偏函数。这就是乘积。z 的偏函数。相对于 z 的性能函数。</p>
        <p>There’s a partial of p2, partial of a performance function with respect to z. Now, all we have to do is
            figure out what those partials are. And we have solved this simple neural net. So it’s going to be easy.
            Where is my board space? Let’s see, partial of p2 with respect to. what? That’s the product. The partial of
            z. the performance function with respect to z.</p>
        <p>哦，现在我明白为什么我要这样写下来了。让我们看看。它将是 d 减去 e。我们可以在脑海中完成这个。那么 p2 相对于 w2 的偏函数呢？嗯，p2 等于 y 乘以 w2，所以这很容易。这只是 y。现在，我们要做的就是找出
            z 相对于 p2 的偏函数。哦，糟糕，它要经过这个阈值框。</p>
        <p>Oh, now I can see why I wrote it down this way. Let’s see. It’s going to be d minus e. We can do that one in
            our head. What about the partial of p2 with respect to w2. Well, p2 is equal to y times w2, so that’s easy.
            That’s just y. Now, all we have to do is figure out the partial of z with respect to p2. Oh, crap, it’s
            going through this threshold box.</p>
        <p>所以我不知道那个偏导数到底是什么。所以我们必须弄清楚，对吧？因为关联它们的函数就是这个。所以我们必须找出它相对于 alpha
            的偏导数。好吧，我们必须这么做。没有办法绕过它。所以我们必须摧毁一些东西。好的，我们要摧毁我们的神经元。</p>
        <p>So I don’t know exactly what that partial derivative is. So we’ll have to figure that out, right? Because the
            function relating them is this guy here. And so we have to figure out the partial of that with respect to
            alpha. All right, so we got to do it. There’s no way around it. So we have to destroy something. OK, we’re
            going to destroy our neuron.</p>
        <h2 id="unknown-204">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们要处理的函数是 beta，等于 1/1 加上 e 减去 alpha。我们想要的是 beta 对 alpha 的导数。它等于 d 除以 d
            alpha。你知道，我永远都记不住那些商公式。所以我要用稍微不同的方式重写它。</p>
        <p>So the function we’re dealing with is, we’ll call it beta, equal to 1 over 1 plus e to the minus alpha. And
            what we want is the derivative with respect to alpha of beta. And that’s equal to d by d alpha of. you know,
            I can never remember those quotient formulas. So I am going to rewrite it a little different way.</p>
        <p>我打算把它写成 1 减 e 减 alpha 减 1，因为我记不住商的微分公式。好的，让我们来微分一下。所以这等于 1 减 e 减 alpha 减
            2。我们得到这个减号来自它的那部分。然后我们要对这个表达式的内部进行微分。</p>
        <p>I am going to write it as 1 minus e to the minus alpha to the minus 1, because I can’t remember the formula
            for differentiating a quotient. OK, so let’s differentiate it. So that’s equal to 1 minus e to the minus
            alpha to the minus 2. And we got that minus comes out of that part of it. Then we got to differentiate the
            inside of that expression.</p>
        <p>当我们对表达式内部进行微分时，我们得到 e 减去 alpha。 观众：温斯顿博士。 帕特里克·温斯顿：是吗？ 观众：应该是 1 加。 帕特里克·温斯顿：哦，抱歉，谢谢。这是你刚刚避免的致命错误之一。所以这是 1
            加。这里也是 1 加。 好的，所以我们对它进行了微分。我们把它变成了减 2。我们把减号带到外面。然后我们对内部进行微分。</p>
        <p>And when we differentiate the inside of that expression, we get e to the minus alpha. AUDIENCE:
            Dr.&nbsp;Winston. PATRICK WINSTON: Yeah? AUDIENCE: That should be 1 plus. PATRICK WINSTON: Oh, sorry, thank
            you. That was one of those fatal mistakes you just prevented. So that’s 1 plus. That’s 1 plus here too. OK,
            so we’ve differentiated that. We’ve turned that into a minus 2. We brought the minus sign outside. Then
            we’re differentiating the inside.</p>
        <h2 id="unknown-205">未知</h2>
        <h2>Unknown</h2>
        <p>导数和指数都是指数。然后我们要对它求导数。这样我们就可以去掉我们引入的减号。这就是导数。我不知道这有多大帮助，但我要在这里表演一个小把戏，把这个表达式改写成这样。</p>
        <p>The derivative and the exponential is an exponential. Then we got to differentiate that guy. And that just
            helps us get rid of the minus sign we introduced. So that’s the derivative. I’m not sure how much that helps
            except that I’m going to perform a parlor trick here and rewrite that expression thusly.</p>
        <p>我们想说的是，这将是 e 减 alpha 除以 1 加上 e 减 alpha 乘以 1 除以 1 再加上 e 减 alpha。这样可以吗？这里有很多人点头。所以我觉得我没问题。但现在，我要表演另一个魔术。我要加
            1，这意味着我还必须减 1。好吗？这很合理，不是吗？</p>
        <p>We want to say that’s going to be e to the minus alpha over 1 plus e to the minus alpha times 1 over 1 plus e
            to the minus alpha. That OK? I’ve got a lot of nodding heads here. So I think I’m on safe ground. But now,
            I’m going to perform another parlor trick. I am going to add 1, which means I also have to subtract 1. All
            right? That’s legitimate isn’t it?</p>
        <p>所以现在，我可以将其改写为 1 加 e 减 alpha 除以 1 加 e 减 alpha 减 1 除以 1 加 e 减 alpha 乘以 1 除以 1 加 e 减
            alpha。任何高中生都可以做到这一点。我想我是安全的。哦，等等，这是 beta。这是 beta。观众：那是错误的一面。帕特里克·温斯顿：哦，对不起，错了。</p>
        <p>So now, I can rewrite this as 1 plus e to the minus alpha over 1 plus e to the minus alpha minus 1 over 1
            plus e to the minus alpha times 1 over 1 plus e to the minus alpha. Any high school kid could do that. I
            think I’m on safe ground. Oh, wait, this is beta. This is beta. AUDIENCE: That’s the wrong side. PATRICK
            WINSTON: Oh, sorry, wrong side.</p>
        <h2 id="unknown-206">未知</h2>
        <h2>Unknown</h2>
        <p>最好将这个 beta 和这个设为 1。任何高中生都可以做到。好的，那么我们得到的结果是，这等于 1 减去 beta 乘以
            beta。这就是导数。这很奇怪，因为输出相对于输入的导数完全由输出给出。这很奇怪。这并不重要。但这很奇怪。我们从中得到的是那里的偏导数。</p>
        <p>Better make this beta and this 1. Any high school kid could do it. OK, so what we’ve got then is that this is
            equal to 1 minus beta times beta. That’s the derivative. And that’s weird because the derivative of the
            output with respect to the input is given exclusively in terms of the output. It’s strange. It doesn’t
            really matter. But it’s a curiosity. And what we get out of this is that partial derivative there.</p>
        <p>这等于，输出是 p2。不，输出是 z。所以它是 z 乘以 1 减去 e。所以每当我们看到这些 S 形函数之一对其输入的导数时，我们只需将输出乘以 1 减去
            alpha，就得到了它。这就是它在数学上方便的原因。它在数学上方便，因为当我们进行这种微分时，我们会得到一个非常简单的输出表达式。</p>
        <p>That’s equal to well, the output is p2. No, the output is z. So it’s z time 1 minus e. So whenever we see the
            derivative of one of these sigmoids with respect to its input, we can just write the output times one minus
            alpha, and we’ve got it. So that’s why it’s mathematically convenient. It’s mathematically convenient
            because when we do this differentiation, we get a very simple expression in terms of the output.</p>
        <p>我们得到了一个非常简单的表达式。这就是我们真正需要的。那么你想看一个演示吗？这是世界上最小的神经网络的演示。神经网络在哪里？我们来看看。这就是我们的神经网络。我们要做的就是训练它什么都不做。我们要做的就是训练它使输出与输入相同。
        </p>
        <p>We get a very simple expression. That’s all we really need. So would you like to see a demonstration? It’s a
            demonstration of the world’s smallest neural net in action. Where is neural nets? Here we go. So there’s our
            neural net. And what we’re going to do is we’re going to train it to do absolutely nothing. What we’re going
            to do is train it to make the output the same as the input.</p>
        <h2 id="unknown-207">未知</h2>
        <h2>Unknown</h2>
        <p>这并不是我所说的智能飞跃。但让我们看看会发生什么。哇！什么都没发生。好吧，它终于到了最大误差（不是性能，而是最大误差）低于我之前确定的阈值的地步。所以如果你看看这里的输入，并将其与最右边的期望输出进行比较，你会看到它产生的输出与期望输出相比非常接近。
        </p>
        <p>Not what I’d call a fantastic leap of intelligence. But let’s see what happens. Wow! Nothing’s happening.
            Well, it finally got to the point where the maximum error, not the performance, but the maximum error went
            below a threshold that I had previously determined. So if you look at the input here and compare that with
            the desired output on the far right, you see it produces an output, which compared with the desired output,
            is pretty close.</p>
        <p>因此我们可以像这样用另一种方式进行测试。我们可以看到，在这种情况下，期望的输出也非常接近实际输出。并且需要 694 次迭代才能完成。让我们再试一次。到
            823。当然，这都是从随机权重开始的结果。顺便说一句，如果你从所有权重都相同开始，会发生什么？</p>
        <p>So we can test the other way like so. And we can see that the desired output is pretty close to the actual
            output in that case too. And it took 694 iterations to get that done. Let’s try it again. To 823. of course,
            this is all a consequence of just starting off with random weights. By the way, if you started with all the
            weights being the same, what would happen?</p>
        <p>什么也没有，因为它总是保持不变。所以你必须在开始时进行一些随机化。所以这花了很长时间。也许问题是我们的速率常数太小了。所以让我们稍微提高一下速率计数，看看会发生什么。这相当快。让我们看看这是否是随机机会的结果。运行。不，那里相当快。57
            次迭代。第三次尝试。</p>
        <p>Nothing because it would always stay the same. So you’ve got to put some randomization in the beginning. So
            it took a long time. Maybe the problem is our rate constant is too small. So let’s crank up the rate counts
            a little bit and see what happens. That was pretty fast. Let’s see if it was a consequence of random chance.
            Run. No, it’s pretty fast there. 57 iterations. third try.</p>
        <h2 id="unknown-208">未知</h2>
        <h2>Unknown</h2>
        <ol class="incremental" start="67" type="1">
            <li>所以看起来我的初始速率常数太小了。所以如果 0.5 不如 5.0，我们为什么不把它调高到 50 看看会发生什么。哦，在这种情况下是 124。我们再试一次。啊，在这种情况下是
                117。所以它实际上变得更糟了。而且不仅变得更糟了。</li>
            <li>So it looks like at my initial rate constant was too small. So if 0.5 was not as good as 5.0, why don’t
                we crank it up to 50 and see what happens. Oh, in this case, 124. let’s try it again. Ah, in this case
                117. so it’s actually gotten worse. And not only has it gotten worse.</li>
        </ol>
        <p>你会发现，在它朝着解决方案前进的过程中，出现了一些不稳定性。所以看起来，如果你的速率常数太小，它就会永远消失。如果你的速率常数太大，它可能会跳得太远，就像我的图表中在板子下面的某个地方一样，你可以一路穿过山丘到达另一边。
        </p>
        <p>You’ll see there’s a little a bit of instability showing up as it courses along its way toward a solution. So
            what it looks like is that if you’ve got a rate constant that’s too small, it takes forever. If you’ve get a
            rate constant that’s too big, it can of jump too far, as in my diagram which is somewhere underneath the
            board, you can go all the way across the hill and get to the other side.</p>
        <p>所以你必须小心速率常数。所以你真正想要做的是，当你向最佳表现迈进时，你的速率常数会随着发生的事情而变化。所以，如果你的表现在跳跃时下降，你就知道你的速率常数太大了。如果你的表现在跳跃时上升，也许你想增加它。</p>
        <p>So you have to be careful about the rate constant. So what you really want to do is you want your rate
            constant to vary with what is happening as you progress toward an optimal performance. So if your
            performance is going down when you make the jump, you know you’ve got a rate constant that’s too big. If
            your performance is going up when you make a jump, maybe you want to increase.</p>
        <h2 id="unknown-209">未知</h2>
        <h2>Unknown</h2>
        <p>稍微增加一点，直到看起来不那么好了。那么就这些了吗？嗯，不完全是，因为这是世界上最简单的神经网络。也许我们应该看看世界上第二简单的神经网络。现在，我们把它称为。好吧，我们把它称为
            x。我们要做的就是我们将有第二个输入。我不知道。也许这有点奇怪。</p>
        <p>Bump it up a little bit until it doesn’t look so good. So is that all there is to it? Well, not quite,
            because this is the world’s simplest neural net. And maybe we ought to look at the world’s second simplest
            neural net. Now, let’s call this. well, let’s call this x. What we’re going to do is we’re going to have a
            second input. And I don’t know. Maybe this is screwy.</p>
        <p>我将在这里使用颜色编码来区分两个输入和它们所经历的内容。也许我会将其称为 z2、z1 以及 x1 和
            x2。现在，如果我这样做。如果我有两个输入和两个输出，那么我的性能函数中将有两个数字。两个期望值和两个实际值。我将有两个输入。但它们是相同的东西。</p>
        <p>I’m just going to use color coding here to differentiate between the two inputs and the stuff they go
            through. Maybe I’ll call this z2 and this z1 and this x1 and x2. Now, if I do that. if I’ve got two inputs
            and two outputs, then my performance function is going to have two numbers in it. the two desired values and
            the two actual values. And I’m going to have two inputs. But it’s the same stuff.</p>
        <p>我只是重复了我用白色所做的操作，只是我把它变成了橙色。哦，但是如果我这样做会发生什么？比如说，在那里放一些交叉连接。所以这两个流会相互作用。然后可能会有一些。这个 y
            可以进入这里的另一个乘数，然后进入这里的夏季。同样，这个 y 可以上升到这里，然后进入一个乘数，就像这样。</p>
        <p>I just repeat what I did in white, only I make it orange. Oh, but what happens if. what happens if I do this?
            Say put little cross connections in there. So these two streams are going to interact. And then there might
            be some. this y can go into another multiplier here and go into a summer here. And likewise, this y can go
            up here and into a multiplier like so.</p>
        <h2 id="unknown-210">未知</h2>
        <h2>Unknown</h2>
        <p>到处都是权重，就像这样。这个家伙在这里上升。现在会发生什么？现在，我们面临一场灾难，因为这个网络中有各种各样的路径。你可以想象，如果这不仅仅是两个神经元深度，而是三个神经元深度，我会找到看起来像这样的表达式。但你可以走这条路，然后向下穿过，然后到这里。
        </p>
        <p>And there are weights all over the place like so. This guy goes up in here. And now what happens? Now, we’ve
            got a disaster on our hands, because there are all kinds of paths through this network. And you can imagine
            that if this was not just two neurons deep, but three neurons deep, what I would find is expressions that
            look like that. But you could go this way, and then down through, and out here.</p>
        <p>或者你可以走这条路，然后再从这里返回。这样看起来，通过该网络的路径数量呈指数增长。所以我们又回到了指数爆炸。而且这行不通。是的，这行不通，除非我们需要让数学稍微发挥作用。我们需要看看这幅图。</p>
        <p>Or you could go this way and then back up through here. So it looks like there is an exponentially growing
            number of paths through that network. And so we’re back to an exponential blowup. And it won’t work. Yeah,
            it won’t work except that we need to let the math sing to us a little bit. And we need to look at the
            picture.</p>
        <p>我把这个家伙转过来的原因实际上是因为从让数学为我们唱歌的角度来看，这块和这块是一样的。</p>
        <p>And the reason I turned this guy around was actually because from a point of view of letting the math sing to
            us, this piece here is the same as this piece here.</p>
        <h2 id="unknown-211">未知</h2>
        <h2>Unknown</h2>
        <p>因此，计算关于 w1 的偏导数时，我们所需要做的部分工作在计算关于 w2 的偏导数时已经完成了。不仅如此，如果我们在两个层面上计算关于这些绿色 w 的偏导数，我们会发现这种重复会一再发生。</p>
        <p>So part of what we needed to do to calculate the partial derivative with respect to w1 has already been done
            when we calculated the partial derivative with respect to w2. And not only that, if we calculated the
            partial wit respect to these green w’s at both levels, what we would discover is that sort of repetition
            occurs over and over again.</p>
        <p>现在，我将尝试让大家直观地了解这里发生的事情，而不是仅仅写下数学公式并对其表示敬意。这是一种从直观角度思考问题的方法。无论这些 p 背后的性能函数发生什么变化，那边的东西只能通过经过 p 才能影响
            p，并且只能通过这一列 p 才能影响性能。</p>
        <p>And now, I’m going to try to give you an intuitive idea of what’s going on here rather than just write down
            the math and salute it. And here’s a way to think about it from an intuitive point of view. Whatever happens
            to this performance function that’s back of these p’s here, the stuff over there can influence p only by
            going through, and influence performance only going through this column of p’s.</p>
        <p>而且这些元素的数量是固定的。所以这取决于网络的宽度，而不是深度。所以这些东西对 p 的影响最终会通过这些元素。最终我们会发现，我们需要在一列中计算的很多东西已经在右边的列中计算出来了。</p>
        <p>And there’s a fixed number of those. So it depends on the width, not the depth of the network. So the
            influence of that stuff back there on p is going to end up going through these guys. And it’s going to end
            up being so that we’re going to discover that a lot of what we need to compute in one column has already
            been computed in the column on the right.</p>
        <h2 id="unknown-212">未知</h2>
        <h2>Unknown</h2>
        <p>所以它不会呈指数级增长，因为影响。让我再说一遍。当我们回到网络的这一部分时，我们关心的只是 p 的变化对性能的影响，因为这些东西除了通过这一列 p 之外无法影响性能。所以它不会呈指数级增长。我们将能够重复使用大量计算。
        </p>
        <p>So it isn’t going to explode exponentially, because the influence. let me say it one more time. The
            influences of changes of changes in p on the performance is all we care about when we come back to this part
            of the network, because this stuff cannot influence the performance except by going through this column of
            p’s. So it’s not going to blow up exponentially. We’re going to be able to reuse a lot of the computation.
        </p>
        <p>所以这就是重用原则。我们以前见过重用原则吗？不完全是。但你还记得关于扩展列表的那件事吗？我们知道我们见过。我们知道我们以前见过一些东西。所以我们可以停止计算。就是这样。我们将能够重用计算。我们已经这样做了，以防止指数爆炸。顺便说一句，对于那些了解快速傅立叶变换的人来说。
        </p>
        <p>So it’s the reuse principle. Have we ever seen the reuse principle at work before. Not exactly. But you
            remember that little business about the extended list? We know that we’ve seen. we know we’ve seen something
            before. So we can stop computing. It’s like that. We’re going to be able to reuse the computation. We’ve
            already done it to prevent an exponential blowup. By the way, for those of you who know about fast Fourier
            transform.</p>
        <p>类似的想法。重复使用部分结果。那么最后，我们能说什么呢？最后，我们可以说的是，深度是线性的。也就是说，如果我们将层数增加到所谓的深度，那么我们将以线性方式增加所需的计算量，因为任何列中所需的计算都是固定的。</p>
        <p>Same kind of idea. reuse of partial results. So in the end, what can we say about this stuff? In the end,
            what we can say is that it’s linear in depth. That is to say if we increase the number of layers to so
            called depth, then we’re going to increase the amount of computation necessary in a linear way, because the
            computation we need in any column is going to be fixed.</p>
        <h2 id="unknown-213">未知</h2>
        <h2>Unknown</h2>
        <p>那么宽度方面呢？嗯，就宽度而言，这里的任何神经元都可以连接到下一行中的任何神经元。所以我们要做的工作量将与连接数成正比。因此，就宽度而言，它将是 w 平方。但事实是，最终这些东西很容易计算出来。</p>
        <p>What about how it goes with respect to the width? Well, with respect to the width, any neuron here can be
            connected to any neuron in the next row. So the amount of work we’re going to have to do will be
            proportional to the number of connections. So with respect to width, it’s going to be w squared. But the
            fact is that in the end, this stuff is readily computed.</p>
        <p>令人惊讶的是，这个观点被忽视了 25
            年。那么它到底是什么呢？归根结底，这是一个非常简单的想法。所有伟大的想法都很简单。为什么没有更多这样的想法呢？因为这种简单性通常涉及找到几个技巧并进行一些观察。所以通常，我们人类几乎从不超越一个技巧或一个观察。
        </p>
        <p>And this, phenomenally enough, was overlooked for 25 years. So what is it in the end? In the end, it’s an
            extremely simple idea. All great ideas are simple. How come there aren’t more of them? Well, because
            frequently, that simplicity involves finding a couple of tricks and making a couple of observations. So
            usually, we humans are hardly ever go beyond one trick or one observation.</p>
        <p>但是如果你把几个串联起来，有时会出现一些事后看来非常简单的奇迹。这就是为什么我们在工作中运用了重用原则。以及我们的重用计算。在这种情况下，奇迹是两个技巧加上一个观察的结果。总体思路是，所有伟大的想法都很简单，在四分之一世纪里很容易被忽视。
        </p>
        <p>But if you cascade a few together, sometimes something miraculous falls out that looks in retrospect
            extremely simple. So that’s why we got the reuse principle at work. and our reuse computation. In this case,
            the miracle was a consequence of two tricks plus an observation. And the overall idea is all great ideas are
            simple and easy to overlook for a quarter century.</p>
        <h1 id="b-deep-neural-nets">12b：深度神经网络</h1>
        <h1>12b: Deep Neural Nets</h1>
        <p><img
                src="data:image/jpeg;base64,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">8
            年前 (2016 年 4 月 21 日) — 49:06 <a
                href="https://youtube.com/watch?v=VrMHA3yX_QI">https://youtube.com/watch?v=VrMHA3yX_QI</a></p>
        <p> 8 years ago (Apr 21, 2016) — 49:06 <a
                href="https://youtube.com/watch?v=VrMHA3yX_QI">https://youtube.com/watch?v=VrMHA3yX_QI</a></p>
        <h2 id="unknown-214">未知</h2>
        <h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。PATRICK H.</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or to view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PATRICK H.</p>
        <p>温斯顿：好吧，我们今天要做的是攀登一座相当大的山峰，因为我们要从具有两个参数的神经网络转到讨论人们最终要处理 6000 万个参数的神经网络。所以这将是一个相当大的飞跃。在此过程中，我想强调一下我们之前的讨论中的几点。
        </p>
        <p>WINSTON: Well, what we’re going to do today is climb a pretty big mountain because we’re going to go from a
            neural net with two parameters to discussing the kind of neural nets in which people end up dealing with 60
            million parameters. So it’s going to be a pretty big jump. Along the way are a couple things I wanted to
            underscore from our previous discussion.</p>
        <p>上次，我尝试对在小型神经网络中实际计算权重如何变化时使用的公式类型进行一些直觉理解。我试图强调的主要一点是，当你拥有像这样的神经网络时，所有内容都会在每一列中被划分。</p>
        <p>Last time, I tried to develop some intuition for the kinds of formulas that you use to actually do the
            calculations in a small neural net about how the weights are going to change. And the main thing I tried to
            emphasize is that when you have a neural net like this one, everything is sort of divided in each column.
        </p>
        <h2 id="unknown-215">未知</h2>
        <h2>Unknown</h2>
        <p>如果不经过有限数量的输出变量 y1，您就无法让基于此输出的性能影响这里的一些权重变化。顺便说一句，没有 y2 和 y4。没有 y2 和 y3。处理这个问题真的是一场符号噩梦，我昨天花了很多时间试图稍微清理一下。</p>
        <p>You can’t have the performance based on this output affect some weight change back here without going through
            this finite number of output variables, the y1s. And by the way, there’s no y2 and y4. there’s no y2 and y3.
            Dealing with this is really a notational nightmare, and I spent a lot of time yesterday trying to clean it
            up a little bit.</p>
        <p>但基本上，我想说的与我使用的符号无关，而是与这样一个事实有关：即使通过这个网络的路径数量可以呈指数增长，但影响这一点的方法数量有限。因此，下面的这些方程式是从试图找出输出性能如何取决于这里的一些权重而得出的。</p>
        <p>But basically, what I’m trying to say has nothing to do with the notation I have used but rather with the
            fact that there’s a limited number of ways in which that can influence this, even though the number of paths
            through this network can be growing exponential. So those equations underneath are equations that derive
            from trying to figure out how the output performance depends on some of these weights back here.</p>
        <p>我计算的是，我计算了性能对 w1 朝那个方向的依赖性，我还计算了性能对 w1 朝那个方向的依赖性。所以这是我得到的方程之一。另一个方程处理 w3，它涉及朝这个方向和朝那个方向。</p>
        <p>And what I’ve calculated is I’ve calculated the dependence of the performance on w1 going that way, and I’ve
            also calculated the dependence of performance on w1 going that way. So that’s one of the equations I’ve got
            down there. And another one deals with w3, and it involves going both this way and this way.</p>
        <h2 id="unknown-216">未知</h2>
        <h2>Unknown</h2>
        <p>我在两种情况下，在所有四种情况下所做的就是对这些权重求性能的偏导数，然后使用链式法则将其展开。当我这样做时，这就是我得到的东西。这只是一堆偏导数。</p>
        <p>And all I’ve done in both cases, in all four cases, is just take the partial derivative of performance with
            respect to those weights and use the chain rule to expand it. And when I do that, this is the stuff I get.
            And that’s just a whole bunch of partial derivatives.</p>
        <p>但如果你仔细观察并理解一下，你会发现计算中存在大量冗余。例如，这里的这个元素，相对于 w1 的部分性能，当然取决于两条路径。但看看这里的第一个元素，这些元素就在这里。</p>
        <p>But if you look at it and let it sing a little bit to you, what you see is that there’s a lot of redundancy
            in the computation. So for example, this guy here, partial of performance with respect to w1, depends on
            both paths, of course. But look at the first elements here, these guys right here.</p>
        <p>看看表达式中用于计算性能相对于 w3 的偏导数的第一个元素，这些元素。它们是相同的。不仅如此，如果您查看这些表达式并查看此处的特定部分，您会发现这是计算下游权重之一所需的表达式，即下游权重之一的变化。</p>
        <p>And look at the first elements in the expression for calculating the partial derivative of performance with
            respect to w3, these guys. They’re the same. And not only that, if you look inside these expressions and
            look at this particular piece here, you see that is an expression that was needed in order to calculate one
            of the downstream weights, the changes in one of the downstream weights.</p>
        <h2 id="unknown-217">未知</h2>
        <h2>Unknown</h2>
        <p>但它恰好和你在这里看到的一样。同样，这部分和你在这里看到的一样。所以每次你从输出端向输入端移动得越来越远时，你都在重复使用大量已经完成的计算。所以我试图找到一种方法来表达这一点，我想到的是，已经做过的事已经做过了，不能再做了。不，不。
        </p>
        <p>But it happens to be the same thing as you see over here. And likewise, this piece is the same thing you see
            over here. So each time you move further and further back from the outputs toward the inputs, you’re reusing
            a lot of computation that you’ve already done. So I’m trying to find a way to sloganize this, and what I’ve
            come up with is what’s done is done and cannot be. no, no.</p>
        <p>这不太对，对吧？计算过的内容已经计算过了，不需要重新计算。对吧？这就是这里发生的事情。这就是为什么这个计算在神经网络的深度上是线性的，而不是指数的。关于这些神经网络，我还想指出另一件事。</p>
        <p>That’s not quite right, is it? It’s what’s computed is computed and need not be recomputed. OK? So that’s
            what’s going on here. And that’s why this is a calculation that’s linear in the depths of the neural net,
            not exponential. There’s another thing I wanted to point out in connection with these neural nets.</p>
        <p>这与我们观察单个神经元时发生的事情有关，请注意，我们得到的是一堆权重，您可以像这样将其乘以一堆输入。然后，它们在求和框中全部相加，然后进入某种非线性函数，在我们的例子中是 S 型函数。</p>
        <p>And that has to do with what happens when we look at a single neuron and note that what we’ve got is we’ve
            got a bunch of weights that you multiply times a bunch of inputs like so. And then those are all summed up
            in a summing box before they enter some kind of non linearity, in our case a sigmoid function.</p>
        <h2 id="unknown-218">未知</h2>
        <h2>Unknown</h2>
        <p>但如果我让你写下我们得到的值的表达式，它是什么？嗯，它只是 w 乘以 x 的总和。那是什么？那就是点积。还记得几节课前我说过，我们中的一些人认为点积是我们头脑中进行的基本计算吗？这就是我们这么认为的原因。</p>
        <p>But if I ask you to write down the expression for the value we’ve got there, what is it? Well, it’s just the
            sum of the w’s times the x’s. What’s that? That’s the dot product. Remember a few lectures ago I said that
            some of us believe that the dot product is a fundamental calculation that takes place in our heads? So this
            is why we think so.</p>
        <p>如果神经网络正在做类似的事情，那么一些权重和一些输入值之间就会有一个点积。现在，这是一种有趣的点积，因为在我们一直使用的模型中，这些输入变量要么全部存在，要么没有，要么是 0 或 1。但这没关系。</p>
        <p>If neural nets are doing anything like this, then there’s a dot product between some weights and some input
            values. Now, it’s a funny kind of dot product because in the models that we’ve been using, these input
            variables are all or none, or 0 or 1. But that’s OK.</p>
        <p>我有充分的证据证明，我们大脑中的神经元产生的值并非完全是全部或没有，而是与它们具有某种比例关系。因此，您可以从中得到真正的点积类型的运算。</p>
        <p>I have it on good authority that there are neurons in our head for which the values that are produced are not
            exactly all or none but rather have a kind of proportionality to them. So you get a real dot product type of
            operation out of that.</p>
        <h2 id="unknown-219">未知</h2>
        <h2>Unknown</h2>
        <p>在我们进入今天的讨论中心之前，我想强调一下这两个话题，也就是所谓的深度网络。现在，让我们看看深度网络是做什么的？从上次开始，您知道深度网络可以做这种事情，看看这里的一些产品很有趣。顺便问一下，2012 年的表现如何？
        </p>
        <p>So that’s by way of a couple of asides that I wanted to underscore before we get into the center of today’s
            discussion, which will be to talk about the so called deep nets. Now, let’s see, what’s a deep net do? Well,
            from last time, you know that a deep net does that sort of thing, and it’s interesting to look at some of
            the offerings here. By the way, how good was this performance in 2012?</p>
        <p>事实证明，系统在前五个选项中给出正确答案的概率约为 15%。而系统给出的正确答案与最佳答案完全相符的概率约为 37%。错误，如果你将其算作错误，则为 15% 的错误。我在说什么？如果你的答案在前五名中，那么你答对了。
        </p>
        <p>Well, it turned out that the fraction of the time that the system had the right answer in its top five
            choices was about 15%. And the fraction of the time that it got exactly the right answer as its top pick was
            about 37%. error, 15% error if you count it as an error if it’s. what am I saying? You got it right if you
            got it in the top five.</p>
        <p>计算错误率约为 15%。如果你说只有当它是你的首选时你才能正确计算，那么错误率约为
            37%。这相当不错，尤其是因为有些事情对我们来说都是非常模糊的。什么样的系统能做到这一点？嗯，它不是看起来完全一样的系统，尽管这是它的本质。系统实际上看起来就是这样的。</p>
        <p>An error rate on that calculation, about 15%. If you say you only get it right if it was your top choice,
            then the error rate was about 37%. So pretty good, especially since some of these things are highly
            ambiguous even to us. And what kind of a system did that? Well, it wasn’t one that looked exactly like that,
            although that is the essence of it. The system actually looked like that.</p>
        <h2 id="unknown-220">未知</h2>
        <h2>Unknown</h2>
        <p>这里面有很多东西。我要讲的并不是这个系统，而是这个系统的组成材料，因为这个系统并没有什么特别之处。它只是一种特殊的组件组合，当有人做这种神经网络的东西时，它往往会重新出现。让我这样解释一下。</p>
        <p>There’s quite a lot of stuff in there. And what I’m going to talk about is not exactly this system, but I’m
            going to talk about the stuff of which such systems are made because there’s nothing particularly special
            about this. It just happens to be a particular assembly of components that tend to reappear when anyone does
            this sort of neural net stuff. So let me explain that this way.</p>
        <p>我首先要谈的是概念。好吧，我不喜欢这个术语。它被称为卷积。我不喜欢这个术语，因为在研究所的第二好课程“信号与系统”中，你会学习脉冲响应和卷积积分等内容。这暗示了这一点，但这不是一回事，因为在处理这些信号时，没有记忆。
        </p>
        <p>First thing I need to talk about is the concept of. well, I don’t like the term. It’s called convolution. I
            don’t like the term because in the second best course at the Institute, Signals and Systems, you learn about
            impulse responses and convolution integrals and stuff like that. And this hints at that, but it’s not the
            same thing because there’s no memory involved in what’s going on as these signals are processed.</p>
        <p>但他们还是称其为卷积神经网络。所以你得到了某种图像。即使拥有强大的计算能力和 GPU 以及所有类似的东西，我们谈论的也不是 400 万像素的图像。我们谈论的是边长可能是 256 的图像。正如我所说，我们谈论的不是
            1,000 x 1,000 或 4,000 x 4,000 或类似尺寸的图像。</p>
        <p>But they call it convolutional neural nets anyway. So here you are. You got some kind of image. And even with
            lots of computing power and GPUs and all that sort of stuff, we’re not talking about images with 4 million
            pixels. We’re talking about images that might be 256 on a side. As I say, we’re not talking about images
            that are 1,000 by 1,000 or 4,000 by 4,000 or anything like that.</p>
        <h2 id="unknown-221">未知</h2>
        <h2>Unknown</h2>
        <p>它们往往被压缩成 256 x 256 的图像。现在我们要做的就是用一个只看 10 x 10
            正方形的神经元来处理它，就像这样，它会产生一个输出。接下来，我们再次处理它，稍微移动了这个神经元，就像这样。然后我们接下来要做的就是再次移动它，这样我们就可以在那里得到输出。</p>
        <p>They tend to be kind of compressed into a 256 by 256 image. And now what we do is we run over this with a
            neuron that is looking only at a 10 by 10 square like so, and that produces an output. And next, we went
            over that again having shifted this neuron a little bit like so. And then the next thing we do is we shift
            it again, so we get that output right there.</p>
        <p>因此，每个神经元的部署都会产生一个输出，并且该输出与图像中的特定位置相关联。这个过程被称为卷积。现在，这个家伙，或者这个卷积运算，会在这里产生一堆点。接下来我们要对这些点进行的操作是查看局部邻域并查看最大值是多少。
        </p>
        <p>So each of those deployments of a neuron produces an output, and that output is associated with a particular
            place in the image. This is the process that is called convolution as a term of art. Now, this guy, or this
            convolution operation, results in a bunch of points over here. And the next thing that we do with those
            points is we look in local neighborhoods and see what the maximum value is.</p>
        <p>然后我们取最大值，并使用该最大值构建图像的另一个映射。然后我们像这样滑动它，得到另一个值。然后我们用不同的颜色再滑动一次，现在我们得到了另一个值。这个过程称为池化。因为我们取最大值，所以这种特殊的池化称为最大池化。
        </p>
        <p>And then we take that maximum value and construct yet another mapping of the image over here using that
            maximum value. Then we slide that over like so, and we produce another value. And then we slide that over
            one more time with a different color, and now we’ve got yet another value. So this process is called
            pooling. And because we’re taking the maximum, this particular kind of pooling is called max pooling.</p>
        <h2 id="unknown-222">未知</h2>
        <h2>Unknown</h2>
        <p>现在让我们看看接下来会发生什么。这是获取一个特定的神经元并将其运行到图像上。我们称之为核，再次借用《信号与系统》中的一些术语。但现在我们要说的是，我们可以使用一大堆核。因此，我用一个核生成的东西现在可以重复多次。事实上，典型的次数是
            100 次。</p>
        <p>So now let’s see what’s next. This is taking a particular neuron and running it across the image. We call
            that a kernel, again sucking some terminology out of Signals and Systems. But now what we’re going to do is
            we’re going to say we could use a whole bunch of kernels. So the thing that I produce with one kernel can
            now be repeated many times like so. In fact, a typical number is 100 times.</p>
        <p>现在我们得到了一个 256 x 256 的图像。我们用 10 x 10 的核对其进行了处理。我们取了彼此接近的最大值，然后重复了 100 次。现在我们可以将其取出，并将所有这些结果输入某种神经网络。</p>
        <p>So now what we’ve got is we’ve got a 256 by 256 image. We’ve gone over it with a 10 by 10 kernel. We have
            taken the maximum values that are in the vicinity of each other, and then we repeated that 100 times. So now
            we can take that, and we can feed all those results into some kind of neural net.</p>
        <p>然后，我们可以通过最后几层上的全连接作业，然后在最终输出中，我们会得到某种迹象，表明所看到的东西是螨虫的可能性有多大。这就是这些东西的大致工作原理。那么到目前为止我们讨论了什么？我们讨论了池化，讨论了卷积。现在我们可以讨论一些好东西了。
        </p>
        <p>And then we can, through perhaps a fully connected job on the final layers of this, and then in the ultimate
            output we get some sort of indication of how likely it is that the thing that’s being seen is, say, a mite.
            So that’s roughly how these things work. So what have we talked about so far? We’ve talked about pooling,
            and we’ve talked about convolution. And now we can talk about some of the good stuff.</p>
        <h2 id="unknown-223">未知</h2>
        <h2>Unknown</h2>
        <p>但在此之前，我想先说说我们现在能做的事情，你可以将其与过去做的工作进行比较。在大规模计算出现之前，人们已经完成了这项工作，这是一种更容易看到的神经网络活动。在过去，你可能只有足够的计算能力来处理一小块图像元素网格，也就是所谓的像素。
        </p>
        <p>But before I get into that, this is what we can do now, and you can compare this with what was done in the
            old days. It was done in the old days before massive amounts of computing became available is a kind of
            neural net activity that’s a little easier to see. You might, in the old days, only have enough computing
            power to deal with a small grid of picture elements, or so called pixels.</p>
        <p>然后，这些中的每一个都可能是作为某种神经元的输入的值。因此，你可能有一列神经元在查看图像中的这些像素。然后可能会有少数几列神经元从中衍生出来。</p>
        <p>And then each of these might be a value that is fed as an input into some kind of neuron. And so you might
            have a column of neurons that are looking at these pixels in your image. And then there might be a small
            number of columns that follow from that.</p>
        <p>最后，这个神经元正在寻找数字 1 的东西，也就是说，图像中看起来像数字 1 的东西。所以当你有大量的计算时，你可以做这些事情，相对于你过去看到的东西。那么有什么不同呢？</p>
        <p>And finally, something that says this neuron is looking for things that are a number 1, that is to say,
            something that looks like a number 1 in the image. So this stuff up here is what you can do when you have a
            massive amount of computation relative to the kind of thing you used to see in the old days. So what’s
            different?</p>
        <h2 id="unknown-224">未知</h2>
        <h2>Unknown</h2>
        <p>不同之处在于，我们拥有的参数远不止几百个。我们拥有的类别不是 10 个，而是 1,000 个。我们拥有的样本不是几百个，而是每个类别可能有 1,000 个样本。这样一来，样本数量就达到一百万个。我们拥有 6,000
            万个参数可供使用。令人惊讶的是，最终结果是我们得到了一个让所有人惊讶的函数逼近器。</p>
        <p>Well, what’s different is instead of a few hundred parameters, we’ve got a lot more. Instead of 10 digits, we
            have 1,000 classes. Instead of a few hundred samples, we have maybe 1,000 examples of each class. So that
            makes a million samples. And we got 60 million parameters to play with. And the surprising thing is that the
            net result is we’ve got a function approximator that astonishes everybody.</p>
        <p>没有人确切知道它为什么有效，除非你把大量的计算投入到这种安排中，就有可能获得意想不到的性能。所以这就是底线。但现在，除了这些之外，还有一些我认为特别有趣的想法，我想谈谈这些想法。</p>
        <p>And no one quite knows why it works, except that when you throw an immense amount of computation into this
            kind of arrangement, it’s possible to get a performance that no one expected would be possible. So that’s
            sort of the bottom line. But now there are a couple of ideas beyond that I think are especially interesting,
            and I want to talk about those.</p>
        <p>第一个特别有趣的想法是自动编码的想法，下面是自动编码的想法的工作原理。我的板子空间快用完了，所以我想我就在这里做。你有一些输入值。它们进入一层神经元，即输入层。然后有一个所谓的隐藏层，它要小得多。所以在这个例子中，也许这里有
            10 个神经元，这里只有几个神经元。</p>
        <p>First idea that’s especially interesting is the idea of autocoding, and here’s how the idea of autocoding
            works. I’m going to run out of board space, so I think I’ll do it right here. You have some input values.
            They go into a layer of neurons, the input layer. Then there is a so called hidden layer that’s much
            smaller. So maybe in the example, there will be 10 neurons here and just a couple here.</p>
        <h2 id="unknown-225">未知</h2>
        <h2>Unknown</h2>
        <p>然后这些扩展到输出层，如下所示。现在我们可以取输出层 z1 到 zn，并将其与期望值 d1 到 dn
            进行比较。到目前为止，您明白了吗？现在，诀窍是说，好吧，期望值是什么？让我们将期望值设为输入值。所以我们要做的是训练这个网络，使输出与输入相同。</p>
        <p>And then these expand to an output layer like so. Now we can take the output layer, z1 through zn, and
            compare it with the desired values, d1 through dn. You following me so far? Now, the trick is to say, well,
            what are the desired values? Let’s let the desired values be the input values. So what we’re going to do is
            we’re going to train this net up so that the output’s the same as the input.</p>
        <p>这有什么好处呢？好吧，我们要把它强行通过这个网络。所以，如果这个网络要成功地把​​这里的所有可能性都考虑进去，并把它们塞进这个较小的内层，也就是所谓的隐藏层，这样它就可以重现输入和输出，它就必须对它在输入中看到的东西进行某种概括。
        </p>
        <p>What’s the good of that? Well, we’re going to force it down through this piece of network. So if this network
            is going to succeed in taking all the possibilities here and cramming them into this smaller inner layer,
            the so called hidden layer, such that it can reproduce the input the output, it must be doing some kind of
            generalization of the kinds of things it sees on its input.</p>
        <p>这是一个非常聪明的想法，它以各种形式出现在大量关于深度神经网络的论文中。但现在我想讨论一个例子，以便向你们展示一个演示。好吗？我们没有
            GPU，也没有三天的时间来做这件事。所以我要编一个非常简单的例子，它让人想起这里发生的事情，但几乎不涉及任何计算。</p>
        <p>And that’s a very clever idea, and it’s seen in various forms in a large fraction of the papers that appear
            on deep neural nets. But now I want to talk about an example so I can show you a demonstration. OK? So we
            don’t have GPUs, and we don’t have three days to do this. So I’m going to make up a very simple example
            that’s reminiscent of what goes on here but involves hardly any computation.</p>
        <h2 id="unknown-226">未知</h2>
        <h2>Unknown</h2>
        <p>我设想我们试图根据动物投下的阴影来判断它们的高度。所以我们要识别三种动物，猎豹、斑马和长颈鹿，它们都会像我一样在黑板上投下阴影。这里不涉及吸血鬼。我们要做的就是将阴影作为神经网络的输入。好吗？</p>
        <p>What I’m going to imagine is we’re trying to recognize animals from how tall they are from the shadows that
            they cast. So we’re going to recognize three animals, a cheetah, a zebra, and a giraffe, and they will each
            cast a shadow on the blackboard like me. No vampire involved here. And what we’re going to do is we’re going
            to use the shadow as an input to a neural net. All right?</p>
        <p>让我们看看它是如何工作的。这就是我们的网络。如果我点击其中一个测试样本，那就是猎豹在墙上投下的阴影的高度。每个阴影层对应 10 个输入神经元。它们通过三个内层神经元，然后从中扩散并成为外层值。</p>
        <p>So let’s see how that would work. So there is our network. And if I just clicked into one of these test
            samples, that’s the height of the shadow that a cheetah casts on a wall. And there are 10 input neurons
            corresponding to each level of the shadow. They’re rammed through three inner layer neurons, and from that
            it spreads out and becomes the outer layer values.</p>
        <p>我们将外层值与期望值进行比较，但期望值与输入值相同。所以这一列是输入值列。最右边是期望值列。我们还没有训练这个神经网络。我们得到的只是其中的随机值。所以如果我们运行测试样本，我们就会得到这个结果。</p>
        <p>And we’re going to compare those outer layer values to the desired values, but the desired values are the
            same as the input values. So this column is a column of input values. On the far right, we have our column
            of desired values. And we haven’t trained this neural net yet. All we’ve got is random values in there. So
            if we run the test samples through, we get that and that.</p>
        <h2 id="unknown-227">未知</h2>
        <h2>Unknown</h2>
        <p>是的，猎豹很矮，斑马中等身高，长颈鹿很高。但是对于所有这些动物，对于所有这些阴影高度，我们的输出几乎都只有
            0.5，好吧，到目前为止还没有训练。让我们运行这个东西。我们只是使用简单的，就像我们世界上最简单的神经网络一样。看看会发生什么很有趣。你看到所有这些值都在变化吗？</p>
        <p>Yeah, cheetahs are short, zebras are medium height, and giraffes are tall. But our output is just pretty much
            0.5 for all of them, for all of those shadow heights, all right, no training so far. So let’s run this
            thing. We’re just using simple just like on our world’s simplest neural net. And it’s interesting to see
            what happens. You see all those values changing?</p>
        <p>现在，我需要指出的是，当你看到绿色连接时，这意味着它是一个正权重，绿色的密度表示它的正值。红色是负权重，红色的强度表示它的红色程度。所以在这里你可以看到，我们仍然从随机输入中获得各种红色和绿色值。</p>
        <p>Now, I need to mention that when you see a green connection, that means it’s a positive weight, and the
            density of the green indicates how positive it is. And the red ones are negative weights, and the intensity
            of the red indicates how red it is. So here you can see that we still have from our random inputs a variety
            of red and green values.</p>
        <p>我们并没有进行太多训练，所以一切看起来都是随机的。让我们来运行这个程序。在对这些示例进行 1,000
            次迭代并尝试使输出与输入相同之后，我们达到了错误率下降的程度。事实上，它下降了很多，重新审视测试用例很有趣。所以这里有一个测试用例，其中有一只猎豹。</p>
        <p>We haven’t really done much training, so everything correctly looks pretty much random. So let’s run this
            thing. And after only 1,000 iterations going through these examples and trying to make the output the same
            as the input, we reached a point where the error rate has dropped. In fact, it’s dropped so much it’s
            interesting to relook at the test cases. So here’s a test case where we have a cheetah.</p>
        <h2 id="unknown-228">未知</h2>
        <h2>Unknown</h2>
        <p>现在，输出值实际上非常接近所有输出神经元的期望值。所以如果我们再看一个，再一次，右边两列有对应关系。如果我们看最后一个，是的，右边两列有对应关系。现在，你回过头来说，好吧，这是怎么回事？事实证明，你不是在训练这个东西来对动物进行分类。
        </p>
        <p>And now the output value is, in fact, very close to the desired value in all the output neurons. So if we
            look at another one, once again, there’s a correspondence in the right two columns. And if we look at the
            final one, yeah, there’s a correspondence in the right two columns. Now, you back up from this and say,
            well, what’s going on here? It turns out that you’re not training this thing to classify animals.</p>
        <p>你正在训练它去理解它在环境中看到的事物的本质，因为它看到的只是阴影的高度。它对你要从中得到的分类一无所知。它看到的只是它在输入值上看到的数据类型具有某种一致性。对吗？</p>
        <p>You’re training it to understand the nature of the things that it sees in the environment because all it sees
            is the height of a shadow. It doesn’t know anything about the classifications you’re going to try to get out
            of that. All it sees is that there’s a kind of consistency in the kind of data that it sees on the input
            values. Right?</p>
        <p>现在，你可能会说，好吧，这很酷，因为隐藏层肯定在发生某种事情，因为所有东西都被迫通过那个狭窄的管道，所以它肯定在进行某种概括。所以如果我们点击每个神经元，我们应该看到它专门化到特定的高度，因为这就是输入中呈现的东西。
        </p>
        <p>Now, you might say, OK, oh, that’s cool, because what must be happening is that hidden layer, because
            everything is forced through that narrow pipe, must be doing some kind of generalization. So it ought to be
            the case that if we click on each of those neurons, we ought to see it specialize to a particular height,
            because that’s the sort of stuff that’s presented on the input.</p>
        <h2 id="unknown-229">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，让我们看看隐藏层神经元上的最大刺激实际上是什么。所以当我点击这些家伙时，我们将看到最大程度刺激该神经元的输入值。顺便说一句，我不知道结果会怎样，因为初始化都是随机的。好吧，这很好。这个看起来概括了短的概念。
        </p>
        <p>Well, let’s go see what, in fact, is the maximum stimulation to be seen on the neurons in that hidden layer.
            So when I click on these guys, what we’re going to see is the input values that maximally stimulate that
            neuron. And by the way, I have no idea how this is going to turn out because the initialization’s all
            random. Well, that’s good. That one looks like it’s generalized the notion of short.</p>
        <p>呃，这看起来不像中等。事实上，最大刺激不涉及来自较低神经元的任何刺激。看这个。这看起来不像高。所以我们得到了一个看起来很矮的，还有两个看起来完全随机的。</p>
        <p>Ugh, that doesn’t look like medium. And in fact, the maximum stimulation doesn’t involve any stimulation from
            that lower neuron. Here, look at this one. That doesn’t look like tall. So we got one that looks like short
            and two that just look completely random.</p>
        <p>因此，事实上，也许我们最好放弃认为隐藏层中发生的事情是泛化的想法，而认为那里发生的事情可能是泛化的编码。它看起来不像我们能看到的编码，但确实存在一种泛化。让我从头开始。我们在刺激值中看不到泛化。相反，我们有某种编码的泛化。
        </p>
        <p>So in fact, maybe we better back off the idea that what’s going on in that hidden layer is generalization and
            say that what is going on in there is maybe the encoding of a generalization. It doesn’t look like an
            encoding we can see, but there is a generalization that’s. let me start that over. We don’t see the
            generalization in the stimulating values. What we have instead is we have some kind of encoded
            generalization.</p>
        <h2 id="unknown-230">未知</h2>
        <h2>Unknown</h2>
        <p>因为我们对这些东西进行了编码，所以这些神经网络非常难以理解。我们不明白它们在做什么。我们不明白它们为什么能识别猎豹。我们不明白为什么它们在某些情况下能识别校车，但在其他情况下却不能，因为我们并不真正了解这些神经元对什么作出反应。嗯，这并不完全正确。
        </p>
        <p>And because we got this stuff encoded, it’s what makes these neural nets so extraordinarily difficult to
            understand. We don’t understand what they’re doing. We don’t understand why they can recognize a cheetah. We
            don’t understand why it can recognize a school bus in some cases, but not in others, because we don’t really
            understand what these neurons are responding to. Well, that’s not quite true.</p>
        <p>最近有很多工作试图理清这个问题，但这个世界上仍有很多未解之谜。无论如何，这就是自动编码的想法。它有各种形式。有时人们会谈论玻尔兹曼机和类似的东西。但基本上都是同一种想法。所以你可以一层一层地做。</p>
        <p>There’s been a lot of work recently on trying to sort that out, but it’s still a lot of mystery in this
            world. In any event, that’s the autocoding idea. It comes in various guises. Sometimes people talk about
            Boltzmann machines and things of that sort. But it’s basically all the same sort of idea. And so what you
            can do is layer by layer.</p>
        <p>训练完输入层后，就可以使用该层来训练下一层，然后可以训练下一层。只有在最后，你才会对自己说，好吧，现在我已经积累了很多关于环境以及在环境中可以看到什么的知识。也许是时候开始使用某些特定类别的样本并对这些类别进行训练了。
        </p>
        <p>Once you’ve trained the input layer, then you can use that layer to train the next layer, and then that can
            train the next layer after that. And it’s only at the very, very end that you say to yourself, well, now
            I’ve accumulated a lot of knowledge about the environment and what can be seen in the environment. Maybe
            it’s time to get around to using some samples of particular classes and train on classes.</p>
        <h2 id="unknown-231">未知</h2>
        <h2>Unknown</h2>
        <p>这就是关于自动编码的故事。现在，接下来要讨论的是最后一层。让我们看看最后一层可能是什么样子。让我们看看，它可能看起来像这样。这里有一个负 1。不。让我们看看，这里有一个负 1。那里有一个负
            1。这里有一个乘数。那里有一个阈值。现在，同样，这里还有一些其他输入值。</p>
        <p>So that’s the story on autocoding. Now, the next thing to talk about is that final layer. So let’s see what
            the final layer might look like. Let’s see, it might look like this. There’s a There’s a minus 1 up here.
            No.&nbsp;Let’s see, there’s a minus 1 up. There’s a minus 1 up there. There’s a multiplier here. And there’s
            a threshold value there. Now, likewise, there’s some other input values here.</p>
        <p>我把这个称为 x，然后乘以某个权重。然后它也进入。然后，它又进入一个 S 形函数，如下所示。最后，你会得到一个输出，我们将其称为 z。</p>
        <p>Let me call this one x, and it gets multiplied by some weight. And then that goes into the as well. And that,
            in turn, goes into a sigmoid that looks like so. And finally, you get an output, which we’ll z.</p>
        <p>因此，很明显，如果您使用我们上次使用的公式，将 z 的值写出为取决于这些输入，那么您会看到 z 等于 1/1 加上 e 的负 w 乘以 x 减 T。我猜是加上 T。对吧？所以这是一个 S
            型函数，它取决于该权重的值和该阈值的值。</p>
        <p>So it’s clear that if you just write out the value of z as it depends on those inputs using the formula that
            we worked with last time, then what you see is that z is equal to 1 over 1 plus e to the minus w times x
            minus T. plus T, I guess. Right? So that’s a sigmoid function that depends on the value of that weight and
            on the value of that threshold.</p>
        <h2 id="unknown-232">未知</h2>
        <h2>Unknown</h2>
        <p>让我们看看这些值可能会如何改变事物。这里我们有一个普通的 S 形函数。如果我们用阈值移动它会发生什么？如果我们改变该阈值，那么它将移动 S 形函数下降的位置。因此，T
            的变化可能会导致这个东西向那个方向移动。如果我们改变 w 的值，这可能会改变这个东西的陡峭程度。</p>
        <p>So let’s look at how those values might change things. So here we have an ordinary sigmoid. And what happens
            if we shift it with a threshold value? If we change that threshold value, then it’s going to shift the place
            where that sigmoid comes down. So a change in T could cause this thing to shift over that way. And if we
            change the value of w, that could change how steep this guy is.</p>
        <p>因此，我们可能会认为，由于性能取决于 w 和 T，因此应该进行调整，以使分类正确。但什么是正确的呢？这取决于我们看到的样本。例如，假设这是我们的 S 型函数。</p>
        <p>So we might think that the performance, since it depends on w and T, should be adjusted in such a way as to
            make the classification do the right thing. But what’s the right thing? Well, that depends on the samples
            that we’ve seen. Suppose, for example, that this is our sigmoid function.</p>
        <p>我们看到某个类别的一些示例，某个类别的一些正示例，它们的值位于该点、该点和该点。我们有一些值对应于该类别不是与该神经元相关的事物之一的情况。在这种情况下，我们看到的是位于此处附近的示例。</p>
        <p>And we see some examples of a class, some positive examples of a class, that have values that lie at that
            point and that point and that point. And we have some values that correspond to situations where the class
            is not one of the things that are associated with this neuron. And in that case, what we see is examples
            that are over in this vicinity here.</p>
        <h2 id="unknown-233">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们在这个世界上见到这个特定人的概率与 S 型曲线上的值相关。因此，你可以将其视为该正例的概率，这是该正例的概率，这是该正例的概率。这个负例的概率是多少？嗯，它是 1 减去该曲线上的值。这个是 1
            减去该曲线上的值。</p>
        <p>So the probability that we would see this particular guy in this world is associated with the value on the
            sigmoid curve. So you could think of this as the probability of that positive example, and this is the
            probability of that positive example, and this is the probability of that positive example. What’s the
            probability of this negative example? Well, it’s 1 minus the value on that curve. And this one’s 1 minus the
            value on that curve.</p>
        <p>所以我们可以进行计算。我们要确定的是，为了最大限度地提高在一系列实验中看到这些数据、这些特定内容的概率，为了最大限度地提高这个概率，我们必须调整 T 和
            w，以使这条曲线达到最佳效果。这没什么神秘的。它只是更多的偏导数和诸如此类的东西。</p>
        <p>So we could go through the calculations. And what we would determine is that to maximize the probability of
            seeing this data, this particular stuff in a set of experiments, to maximize that probability, we would have
            to adjust T and w so as to get this curve doing the optimal thing. And there’s nothing mysterious about it.
            It’s just more partial derivatives and that sort of thing.</p>
        <p>但最重要的是，看到这些数据的概率取决于曲线的形状，而曲线的形状取决于这些参数。如果我们想最大限度地提高看到这些数据的概率，那么我们必须相应地调整这些参数。让我们来看一个演示。好的。这是一条普通的 S
            形曲线。以下是几个正面例子。以下是反面例子。</p>
        <p>But the bottom line is that the probability of seeing this data is dependent on the shape of this curve, and
            the shape of this curve is dependent on those parameters. And if we wanted to maximize the probability that
            we’ve seen this data, then we have to adjust those parameters accordingly. Let’s have a look at a
            demonstration. OK. So there’s an ordinary sigmoid curve. Here are a couple of positive examples. Here’s a
            negative example.</p>
        <h2 id="unknown-234">未知</h2>
        <h2>Unknown</h2>
        <p>让我们在这里放入一些更积极的例子。现在让我们运行经典的梯度上升算法。结果如下。您已经看到了概率，当我们调整曲线的形状时，看到该类的那些例子的概率上升，而看到非例子的概率下降。那么如果我们放入更多例子会怎么样？</p>
        <p>Let’s put in some more positive examples over here. And now let’s run the good, old gradient ascent algorithm
            on that. And this is what happens. You’ve seen how the probability, as we adjust the shape of the curve, the
            probability of seeing those examples of the class goes up, and the probability of seeing the non example
            goes down. So what if we put some more examples in?</p>
        <p>如果我们把一个反面例子放在那里，不会发生太多变化。如果我们把一个正面例子放在那里，会发生什么？然后我们将开始看到曲线形状的一些显著变化。所以这可能是一个噪声点。但我们可以把更多的反面例子放在那里，看看它如何调整曲线。好的。这就是我们正在做的。
        </p>
        <p>If we put a negative example there, not much is going to happen. What would happen if we put a positive
            example right there? Then we’re going to start seeing some dramatic shifts in the shape of the curve. So
            that’s probably a noise point. But we can put some more negative examples in there and see how that adjusts
            the curve. All right. So that’s what we’re doing.</p>
        <p>我们将这个输出值视为与看到某个类别的概率相关的事物。我们正在调整该输出层的参数，以便最大化我们手头上的样本数据的概率。对吗？现在，还有一件事。因为我们在这里得到的是反向传播的基本思想，它有一层又一层的附加层。让我恭维一下，称它们为顶层思想。
        </p>
        <p>We’re viewing this output value as something that’s related to the probability of seeing a class. And we’re
            adjusting the parameters on that output layer so as to maximize the probability of the sample data that
            we’ve got at hand. Right? Now, there’s one more thing. Because see what we’ve got here is we’ve got the
            basic idea of back propagation, which has layers and layers of additional. let me be flattering and call
            them ideas layered on top.</p>
        <h2 id="unknown-235">未知</h2>
        <h2>Unknown</h2>
        <p>这是下一个想法，它位于顶层。所以我们这里有一个输出值。毕竟它是一个函数，它有一个值。如果我们有 1,000 个类，我们将有 1,000
            个输出神经元，每个神经元都会产生某种值。我们可以将该值视为一个概率。但我还不想写出概率。</p>
        <p>So here’s the next idea that’s layered on top. So we’ve got an output value here. And it’s a function after
            all, and it’s got a value. And if we have 1,000 classes, we’re going to have 1,000 output neurons, and each
            is going to be producing some kind of value. And we can think of that value as a probability. But I didn’t
            want to write a probability yet.</p>
        <p>我只想说，我们得到的这个输出神经元是第 1 类的函数。然后会有另一个输出神经元，它是第 2 类的函数。这些值可能会更高。如果我们实际上研究第 1 类，这个值会更高。如果我们研究第 m 类，这个值实际上会更高。</p>
        <p>I just want to say that what we’ve got for this output neuron is a function of class 1. And then there will
            be another output neuron, which is a function of class 2. And these values will be presumably higher. this
            will be higher if we are, in fact, looking at class 1. And this one down here will be, in fact, higher if
            we’re looking at class m.</p>
        <p>因此，我们希望做的是，我们不只是选择其中一个输出，然后说，好吧，你得到了最高的值，所以你赢了。我们想要做的是，我们希望将某种概率与每个类别相关联。因为毕竟，我们想要做的事情是找到最有可能的五个。</p>
        <p>So what we would like to do is we’d like to not just pick one of these outputs and say, well, you’ve got the
            highest value, so you win. What we want to do instead is we want to associate some kind of probability with
            each of the classes. Because, after all, we want to do things like find the most probable five.</p>
        <h2 id="unknown-236">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们所做的就是说，好吧，所以第 1 类的实际概率等于该 S 型函数的输出除以所有函数的总和。因此，这会获取整个输出向量并将每个输出值转换为一个概率。因此，当我们使用该 S 型函数时，我们将其视为一个概率。</p>
        <p>So what we do is we say, all right, so the actual probability of class 1 is equal to the output of that
            sigmoid function divided by the sum over all functions. So that takes all of that entire output vector and
            converts each output value into a probability. So when we used that sigmoid function, we did it with the
            view toward thinking about that as a probability.</p>
        <p>事实上，我们在提出这个论点时假设它是一个概率。但最终，每个类别都有一个输出。因此，我们最终得到的并不是一个确切的概率，除非我们除以一个标准化因子。顺便说一下，这被称为。不在我的清单上，但很快就会有。</p>
        <p>And in fact, we assumed it was a probability when we made this argument. But in the end, there’s an output
            for each of those classes. And so what we get is, in the end, not exactly a probability until we divide by a
            normalizing factor. So this, by the way, is called. not on my list of things, but it soon will be.</p>
        <p>由于我们讨论的不是取最大值并用它来对图片进行分类，所以我们要做的是使用所谓的
            softmax。所以我们将给出一系列分类，并将每个分类与一个概率相关联。这就是你在所有这些样本中看到的。你看到了，是的，这是，但也可能是这样、那样，或者第三、第四和第五个东西。</p>
        <p>Since we’re not talking about taking the maximum and using that to classify the picture, what we’re going to
            do is we’re going to use what’s called softmax. So we’re going to give a range of classifications, and we’re
            going to associate a probability with each. And that’s what you saw in all of those samples. You saw, yes,
            this is but maybe it’s also this, that, or a third, or fourth, and fifth thing.</p>
        <h2 id="unknown-237">未知</h2>
        <h2>Unknown</h2>
        <p>所以这是对所涉及的内容的一个很好的总结。但现在我们还有一步，因为我们现在可以做的是，我们可以采用这个输出层的想法，这个 softmax
            的想法，我们可以将它们与自动编码的想法结合起来。所以我们只训练了一层。现在我们要将它从输出层分离出来，但保留将输入连接到隐藏层的权重。</p>
        <p>So that is a pretty good summary of the kinds of things that are involved. But now we’ve got one more step,
            because what we can do now is we can take this output layer idea, this softmax idea, and we can put them
            together with the autocoding idea. So we’ve trained just a layer up. And now we’re going to detach it from
            the output layer but retain those weights that connect the input to the hidden layer.</p>
        <p>当我们这样做时，我们将看到类似这样的结果。现在我们有一个经过训练的第一层，但有一个未经训练的输出层。我们将冻结输入层并使用 S 型曲线训练输出层，看看这样做会发生什么。哦，顺便说一下，让我们运行一下测试样本。</p>
        <p>And when we do that, what we’re going to see is something that looks like this. And now we’ve got a trained
            first layer but an untrained output layer. We’re going to freeze the input layer and train the output layer
            using the sigmoid curve and see what happens when we do that. Oh, by the way, let’s run our test samples
            through.</p>
        <p>您可以看到它什么都没做，尽管我们已经训练了中间层，但每个类别的输出都是一半。所以我们必须训练外层。让我们看看需要多长时间。哇，这相当快。现在输出和期望输出之间存在非常好的匹配。这就是自动编码思想和 softmax
            思想的结合。</p>
        <p>You can see it’s not doing anything, and the output is half for each of the categories even though we’ve got
            a trained middle layer. So we have to train the outer layer. Let’s see how long it takes. Whoa, that was
            pretty fast. Now there’s an extraordinarily good match between the outputs and the desired outputs. So
            that’s the combination of the autocoding idea and the softmax idea.</p>
        <h2 id="unknown-238">未知</h2>
        <h2>Unknown</h2>
        <p>还有一个值得一提的想法，那就是 dropout
            的想法。任何神经网络的弊端在于它会陷入某种局部最大值。因此，我们发现，如果在每次迭代中为每个神经元抛一枚硬币，这些神经元的训练效果会更好。如果硬币最后反面朝上，你就认为它已经死了，对输出没有影响。这叫做
            dropout 这些神经元。</p>
        <p>Just one more idea that’s worthy of mention, and that’s the idea of dropout. The plague of any neural net is
            that it gets stuck in some kind of local maximum. So it was discovered that these things train better if, on
            every iteration, you flip a coin for each neuron. And if the coin ends up tails, you assume it’s just died
            and has no influence on the output. It’s called dropping out those neurons.</p>
        <p>在下一次迭代中，您将删除另一个集合。这似乎可以防止该事物进入冻结的局部最大值状态。这就是深度网络。顺便说一句，它们应该被称为宽网络，因为它们往往非常宽，但深度很少超过 10 列。现在，让我们看看从这里开始该怎么做？
        </p>
        <p>And in our next iteration, you drop out a different set. So what this seems to do is it seems to prevent this
            thing from going into a frozen local maximum state. So that’s deep nets. They should be called, by the way,
            wide nets because they tend to be enormously wide but rarely more than 10 columns deep. Now, let’s see,
            where to go from here?</p>
        <p>也许我们应该谈谈当前技术水平的惊人好奇心。那就是，所有复杂的输出层都是概率，并使用自动编码或玻尔兹曼机进行训练，相对于简单、老式的反向传播，它似乎没有多大帮助。因此，使用卷积网络的反向传播似乎和其他方法一样好。</p>
        <p>Maybe what we should do is talk about the awesome curiosity in the current state of the art. And that is that
            all of sophistication with output layers that are probabilities and training using autocoding or Boltzmann
            machines, it doesn’t seem to help much relative to plain, old back propagation. So back propagation with a
            convolutional net seems to do just about as good as anything.</p>
        <h2 id="unknown-239">未知</h2>
        <h2>Unknown</h2>
        <p>既然我们讨论的是普通的深度网络，我想在这里讨论一下深度网络的情况。好吧，这是一个课堂深度网络。我们将在其中放入五层，它的工作仍然是做同样的事情。它会根据动物投射的阴影高度将其分类为猎豹、斑马或长颈鹿。和以前一样，如果它是绿色，则表示为阳性。
        </p>
        <p>And while we’re on the subject of an ordinary deep net, I’d like to examine a situation here where we have a
            deep net. well, it’s a classroom deep net. And we’ll will put five layers in there, and its job is still to
            do the same thing. It’s to classify an animal as a cheetah, a zebra, or a giraffe based on the height of the
            shadow it casts. And as before, if it’s green, that means positive.</p>
        <p>如果是红色，则表示为负数。目前，我们还没有进行训练。因此，如果我们运行测试样本，无论动物是什么，输出始终是 1/2。好吗？所以我们要做的就是使用普通的反向传播，与黑板下面的样本相同。只是现在我们有更多的参数。</p>
        <p>If it’s red, that means negative. And right at the moment, we have no training. So if we run our test samples
            through, the output is always a 1/2 no matter what the animal is. All right? So what we’re going to do is
            just going to use ordinary back prop on this, same thing as in that sample that’s underneath the blackboard.
            Only now we’ve got a lot more parameters.</p>
        <p>我们有五列，每列有 9 或 10
            个神经元。让我们运行一下。现在，看看右边的东西。它们全都变成了红色。起初我以为这是我的程序中的一个错误。但这完全说得通。如果你不知道实际的动物是什么，而且有很多可能性，你最好对每个人都说不。</p>
        <p>We’ve got five columns, and each one of them has 9 or 10 neurons in it. So let’s let this one run. Now, look
            at that stuff on the right. It’s all turned red. At first I thought this was a bug in my program. But that
            makes absolute sense. If you don’t know what the actual animal is going to be and there are a whole bunch of
            possibilities, you better just say no for everybody.</p>
        <h2 id="unknown-240">未知</h2>
        <h2>Unknown</h2>
        <p>就像生物学家说“我们不知道”一样。这是最有可能的答案。但是，最终，经过大约 160,000
            次迭代后，它似乎已经找到了答案。让我们运行测试样本。现在它表现很好。让我们再做一次，看看这是否是侥幸。右侧全是红色，最后，你开始看到那里的最后几层发生了一些变化。</p>
        <p>It’s like when a biologist says, we don’t know. It’s the most probable answer. Well, but eventually, after
            about 160,000 iterations, it seems to have got it. Let’s run the test samples through. Now it’s doing great.
            Let’s do it again just to see if this is a fluke. And all red on the right side, and finally, you start
            seeing some changes go in the final layers there.</p>
        <p>如果你看看底部的错误率，你会发现它有点像是跌落悬崖。所以很长一段时间内什么都没有发生，然后它就跌落悬崖了。现在，如果这个神经网络不那么宽会发生什么？好问题。但在回答这个问题之前，我要做的是，我要对 dropout
            的主题做一个有趣的变奏。</p>
        <p>And if you look at the error rate down at the bottom, you’ll see that it kind of falls off a cliff. So
            nothing happens for a real long time, and then it falls off a cliff. Now, what would happen if this neural
            net were not quite so wide? Good question. But before we get to that question, what I’m going to do is I’m
            going to do a funny kind of variation on the theme of dropout.</p>
        <p>我要做的是，我要删除每一列中的一个神经元，然后看看我是否可以重新训练网络以做正确的事情。所以我要将它们重新分配给其他用途。所以现在网络中少了一个神经元。如果我们重新运行它，我们会看到它训练得非常快。</p>
        <p>What I’m going to do is I’m going to kill off one neuron in each column, and then see if I can retrain the
            network to do the right thing. So I’m going to reassign those to some other purpose. So now there’s one
            fewer neuron in the network. If we rerun that, we see that it trains itself up very fast.</p>
        <h2 id="unknown-241">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们似乎仍然足够接近一个解决方案，我们可以不使用每列中的一个神经元。让我们再试一次。现在它上升了一点，但很快就落入了一个解决方案。再试一次。很快就落入了一个解决方案。哦，天哪，我要做多少呢？每次我敲掉一些东西并重新训练时，它都会很快找到解决方案。
        </p>
        <p>So we seem to be still close enough to a solution we can do without one of the neurons in each column. Let’s
            do it again. Now it goes up a little bit, but it quickly falls down to a solution. Try again. Quickly falls
            down to a solution. Oh, my god, how much of this am I going to do? Each time I knock something out and
            retrain, it finds its solution very fast.</p>
        <p>哇哦，我把每列神经元的数量降到了两个，它仍然有解决方案。这很有趣，你不觉得吗？但让我们重复这个实验，但这次我们要做得稍微不同。我们将采用五层，在进行任何训练之前，我将删除每列中除两个神经元之外的所有神经元。</p>
        <p>Whoa, I got it all the way down to two neurons in each column, and it still has a solution. It’s interesting,
            don’t you think? But let’s repeat the experiment, but this time we’re going to do it a little differently.
            We’re going to take our five layers, and before we do any training I’m going to knock out all but two
            neurons in each column.</p>
        <p>现在，我知道每列有两个神经元，我就得到了一个解决方案。我刚刚展示了它。我刚刚展示了一个。但让我们这样运行它。这看起来越来越糟。发生的事情是，这个傻瓜已经陷入了局部最大值。所以现在您可以看到为什么神经网络学习方面取得了突破。这是因为当您扩大网络时，您会将局部最大值变成鞍点。
        </p>
        <p>Now, I know that with two neurons in each column, I’ve got a solution. I just showed it. I just showed one.
            But let’s run it this way. It looks like increasingly bad news. What’s happened is that this sucker’s got
            itself into a local maximum. So now you can see why there’s been a breakthrough in this neural net learning
            stuff. And it’s because when you widen the net, you turn local maxima into saddle points.</p>
        <h2 id="unknown-242">未知</h2>
        <h2>Unknown</h2>
        <p>所以现在它有办法爬过这个广阔的空间，而不会像这个建议的那样被困在局部最大值上。好的。所以我认为这些是通过这些演示来观察的一些有趣的东西。但现在我想回到我的幻灯片集，向你们展示一些例子，这些例子将解决这些问题：这些东西是否像我们看到的那样。
        </p>
        <p>So now it’s got a way of crawling its way through this vast space without getting stuck on a local maximum,
            as suggested by this. All right. So those are some, I think, interesting things to look at by way of these
            demonstrations. But now I’d like to go back to my slide set and show you some examples that will address the
            question of whether these things are seeing like we see.</p>
        <p>所以你可以在网上尝试这些例子。有各种各样的网站允许你输入自己的图片。还有一种家庭手工业，在期刊上发表论文来欺骗神经网络。所以在这种情况下，只有极少量的像素被改变了。</p>
        <p>So you can try these examples online. There are a variety of websites that allow you to put in your own
            picture. And there’s a cottage industry of producing papers in journals that fool neural nets. So in this
            case, a very small number of pixels have been changed.</p>
        <p>你看不出区别，但这足以让这个特定的神经网络从高度确信它看到的是一辆校车转变为认为它不是校车。这些都是它认为是校车的东西。</p>
        <p>You don’t see the difference, but it’s enough to take this particular neural net from a high confidence that
            it’s looking at a school bus to thinking that it’s not a school bus. Those are some things that it thinks
            are a school bus.</p>
        <h2 id="unknown-243">未知</h2>
        <h2>Unknown</h2>
        <p>因此，似乎引发校车结果的原因是，它看到了足够多的本地证据，表明这不是其他 999
            个班级之一，并且从这些本地调查中获得了足够多的积极证据，可以得出结论，这是一辆校车。那么你看到了这些吗？我没有。在这里你可以说，好吧，看看那辆棒球车。</p>
        <p>So it appears to be the case that what is triggering this school bus result is that it’s seeing enough local
            evidence that this is not one of the other 999 classes and enough positive evidence from these local looks
            to conclude that it’s a school bus. So do you see any of those things? I don’t. And here you can say, OK,
            well, look at that baseball one.</p>
        <p>是的，看起来里面有一点棒球的纹理。所以也许它正在做的是查看纹理。这些是谷歌最近发表的一篇非常著名的论文中的一些例子，该论文使用基本相同的想法为图片添加标题。顺便说一句，这就是引发人们对人工智能巨大担忧的原因。</p>
        <p>Yeah, that looks like it’s got a little bit of baseball texture in it. So maybe what it’s doing is looking at
            texture. These are some examples from a recent and very famous paper by Google using essentially the same
            ideas to put captions on pictures. So this, by the way, is what has stimulated all this enormous concern
            about artificial intelligence.</p>
        <p>因为一个天真的观众看到这张图片会说，哦，天哪，这个东西知道玩耍、年轻、移动或飞盘是什么感觉。当然，它对这些都一无所知。它只知道如何给这张图片贴标签。值得称赞的是，撰写这篇论文的人展示了一些不太好的例子。所以，是的，它是一只猫，但它没有撒谎。
        </p>
        <p>Because a naive viewer looks at that picture and says, oh, my god, this thing knows what it’s like to play,
            or be young, or move, or what a Frisbee is. And of course, it knows none of that. It just knows how to label
            this picture. And to the credit of the people who wrote this paper, they show examples that don’t do so
            well. So yeah, it’s a cat, but it’s not lying.</p>
        <h2 id="unknown-244">未知</h2>
        <h2>Unknown</h2>
        <p>哦，这是个小女孩，但她没有吹泡泡。这个呢？所以我们一直在实验室里做这方面的工作。下面这组图片的制作方式是这样的。你拍一张照片，然后把它分成一堆切片，每片代表一个特定的频带。</p>
        <p>Oh, it’s a little girl, but she’s not blowing bubbles. What about this one? So we’ve been doing our own work
            in my laboratory on some of this. And the way the following set of pictures was produced was this. You take
            an image, and you separate it into a bunch of slices, each representing a particular frequency band.</p>
        <p>然后你进入其中一个频带，从图片中剔除一个矩形，然后重新组装这个东西。如果你没有剔除那一块，当你重新组装它时，它看起来会和你开始时一模一样。所以我们要做的是剔除尽可能多的东西，同时仍然保留神经网络的印象，即它是它最初认为的东西。
        </p>
        <p>And then you go into one of those frequency bands and you knock out a rectangle from the picture, and then
            you reassemble the thing. And if you hadn’t knocked that piece out, when you reassemble it, it would look
            exactly like it did when you started. So what we’re doing is we knock out as much as we can and still retain
            the neural net’s impression that it’s the thing that it started out thinking it was.</p>
        <p>那么你觉得这是什么？神经网络将它识别为火车车厢，因为这是它开始使用的图像。这个怎么样？这很简单，对吧？这是一把吉他。我们无法对它进行太多的破坏，同时仍然保留它的吉他特质。这个怎么样？观众：一盏灯？帕特里克·H·温斯顿：那是什么？观众：灯。帕特里克·H·温斯顿：什么？观众：灯。帕特里克·H·温斯顿：一盏灯。
        </p>
        <p>So what do you think this is? It’s identified by a neural net as a railroad car because this is the image
            that it started with. How about this one? That’s easy, right? That’s a guitar. We weren’t able to mutilate
            that one very much and still retain the guitar ness of it. How about this one? AUDIENCE: A lamp? PATRICK H.
            WINSTON: What’s that? AUDIENCE: Lamp. PATRICK H. WINSTON: What? AUDIENCE: Lamp. PATRICK H. WINSTON: A lamp.
        </p>
        <h2 id="unknown-245">未知</h2>
        <h2>Unknown</h2>
        <p>还有其他想法吗？观众：观众：帕特里克·H·温斯顿：肯，你觉得那是什么？观众：马桶。帕特里克·H·温斯顿：看，他是这方面的专家。它被鉴定为杠铃。那是什么？观众：帕特里克·H·温斯顿：什么？观众：大提琴。帕特里克·H·温斯顿：大提琴。你没看到那个小女孩或教练。这个怎么样？观众：帕特里克·H·温斯顿：什么？观众：帕特里克·H·温斯顿：不。观众：帕特里克·H·温斯顿：这是一只蚱蜢。
        </p>
        <p>Any other ideas? AUDIENCE: AUDIENCE: PATRICK H. WINSTON: Ken, what do you think it is? AUDIENCE: A toilet.
            PATRICK H. WINSTON: See, he’s an expert on this subject. It was identified as a barbell. What’s that?
            AUDIENCE: PATRICK H. WINSTON: A what? AUDIENCE: Cello. PATRICK H. WINSTON: Cello. You didn’t see the little
            girl or the instructor. How about this one? AUDIENCE: PATRICK H. WINSTON: What? AUDIENCE: PATRICK H.
            WINSTON: No.&nbsp;AUDIENCE: PATRICK H. WINSTON: It’s a grasshopper.</p>
        <p>这是什么？ 观众：一只狼。 帕特里克·H·温斯顿：哇，你说得真棒。其实这不是双头狼。而是两只靠得很近的狼。 观众：帕特里克·H·温斯顿：那是一只鸟，对吧？ 观众：帕特里克·H·温斯顿：你真棒。这是一只兔子。那怎么样？
            帕特里克·H·温斯顿：俄罗斯猎狼犬。 观众：帕特里克·H·温斯顿：如果你去过威尼斯，你就会认出这个。 观众：帕特里克·H·温斯顿：所以最重要的是，这些东西是工程奇迹，能做伟大的事情，但它们看到的和我们不一样。</p>
        <p>What’s this? AUDIENCE: A wolf. PATRICK H. WINSTON: Wow, you’re good. It’s actually not a two headed wolf.
            It’s two wolves that are close together. AUDIENCE: PATRICK H. WINSTON: That’s a bird, right? AUDIENCE:
            PATRICK H. WINSTON: Good for you. It’s a rabbit. How about that? PATRICK H. WINSTON: Russian wolfhound.
            AUDIENCE: PATRICK H. WINSTON: If you’ve been to Venice, you recognize this. AUDIENCE: PATRICK H. WINSTON: So
            bottom line is that these things are an engineering marvel and do great things, but they don’t see like we
            see.</p>
        <h1 id="learning-genetic-algorithms">13.学习：遗传算法</h1>
        <h1>13. Learning: Genetic Algorithms</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 47:16 <a
                href="https://youtube.com/watch?v=kHyNqSnzP8Y">https://youtube.com/watch?v=kHyNqSnzP8Y</a></p>
        <p> 11 years ago (Jan 11, 2014) — 47:16 <a
                href="https://youtube.com/watch?v=kHyNqSnzP8Y">https://youtube.com/watch?v=kHyNqSnzP8Y</a></p>
        <h2 id="unknown-246">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：我有个非常坏的消息。今年的万圣节是在星期天。但是我们 6.034 班不愿意遭受厄运的打击。所以我们决定今天是万圣节，就 6.034
            班而言。肯尼，你能帮我一下吗？如果你能把它拿到那边。学生：帕特里克·温斯顿：嗯。就给他们吧。你想拿多少就拿多少。</p>
        <p>PATRICK WINSTON: I have extremely bad news. Halloween falls this year on a Sunday. But we in 6.034 refuse to
            suffer the slings and arrows of outrageous fortune. So we’ve decided that Halloween is today, as far 6.034
            is concerned. Kenny, could you give me a hand, please? If you could take that and put it over there.
            STUDENT: PATRICK WINSTON: Mm hm. Just give it to them. You can take as much of this as you like.</p>
        <p>剩下的将分给后面那群蜂拥而至的新生。这是合法药物的宝库。巧克力确实能让人产生一种轻微的兴奋感，我建议在测验和讲课前吃巧克力。我有一个朋友，他是诺贝尔生物学奖获得者之一，他总是在讲课前吃巧克力。这让他有点兴奋。否则，他会很沮丧。所以我推荐吃巧克力。
        </p>
        <p>The rest will be given to that herd of stampeding freshman that comes in after. It’s a cornucopia of legal
            drugs. Chocolate does produce a kind of mild high, and I recommend it before quizzes and giving lectures. I
            have a friend of mine, one of the Nobel laureates in biology, always eats chocolate before he lectures.
            Gives him a little edge. Otherwise, he’ll be flat. So I recommend it.</p>
        <p>我想，要消化神经网络的东西需要一点时间。这比数学上的内容要丰富一些。所以今天，我们将讨论另一种模仿生物学的努力。这很简单。这只是概念上的。你不会在下一次测验中看到它。但你会在期末考试中看到它。这是五类测验问题之一，我会问你问题，看看你是否在这里并且醒着。
        </p>
        <p>It will take, I suppose, a little while to digest that neural net stuff. A little richer than usual in
            mathematics. So today, we’re going to talk about another effort at mimicking biology. This is easy stuff.
            It’s just conceptual. And you won’t see this on the next quiz. But you will see it on the final. It’s one of
            those quiz five type problems, where I ask you questions to see if you were here and awake.</p>
        <h2 id="unknown-247">未知</h2>
        <h2>Unknown</h2>
        <p>因此，一个典型的问题可能是，温斯顿教授是神创论者，或诸如此类。答案并不难。无论如何，如果很难真正理解智能，偶尔人们会希望通过模仿生物学或模仿进化，绕过所有问题。而这些努力之一就是模仿进化。</p>
        <p>So a typical question might be, Professor Winston is a creationist, or something like that. Not too hard to
            answer. In any event, if it’s been hard to develop a real understanding of intelligence, occasionally the
            hope is that by mimicking biology or mimicking evolution, you can circumnavigate all the problems. And one
            of those kinds of efforts is ever to imitate evolution.</p>
        <p>今天我们要讨论的是所谓的遗传算法，它是模仿幼稚进化的幼稚尝试。现在，我意识到大多数麻省理工学院的学生对有性生殖有基本的了解。但我在与学生交谈时发现，很多时候他们对一些细节有点模糊。所以让我们先来思考一下它的工作原理。让我们看看，我们需要粉色和蓝色。
        </p>
        <p>So we’re going to talk today about so called genetic algorithms, which are naive attempts to mimic naive
            evolution. Now, I realize that most MIT students have a basic grasp of sexual reproduction. But I’ve found
            in talking with students that many times, they’re a little fuzzy on some of the details. So let’s start off
            by reflecting a little bit about how that works. So let’s see, we need pink and blue.</p>
        <p>这是我们的细胞，这是细胞核，这是爸爸妈妈的染色体。我们假设只有一对染色体。通常，在普通细胞分裂中，你会得到两个细胞。两个细胞都有一个细胞核，而产生它们的过程涉及染色体的复制。然后一对染色体最终会进入每个子细胞，这就是全部。这就是有丝分裂。
        </p>
        <p>And here’s our cell, and here is its nucleus, and here are mommy and daddy’s chromosomes. We’ll just pretend
            there’s one pair. Now ordinarily, in ordinary cell division, you get two cells. Both have a nucleus, and the
            process of producing them involves the duplication of those chromosomes. And then one pair ends up in each
            of the child cells, and that’s all there is to that. That’s mitosis.</p>
        <h2 id="unknown-248">未知</h2>
        <h2>Unknown</h2>
        <p>但是，当我们谈论生殖时，情况就变得更加复杂，因为这些染色体会全部扭曲，断裂，然后重新结合。因此，当我们谈论从这些生殖细胞中分离出来的细胞时，不再适合谈论粉色和蓝色的细胞，因为粉色和蓝色的细胞都混在一起了。所以这里有两条染色体，这里也有两条。
        </p>
        <p>But then, when we talk about reproduction, it’s more complicated because those chromosomes get all twisted
            up, and they break, and they recombine. So when we talk about the cells that split off from one of these
            germ cells, it’s no longer appropriate to talk about the pink one and the blue one, because the pink one and
            the blue one are all mixed up. So you get two chromosomes here and two here.</p>
        <p>但大自然的奇迹总是让我惊叹不已，这两个细胞又分裂成四个细胞。底部的四个细胞分别获得了通过缠绕和重组产生的一条染色体。然后，一个特殊的时刻到来了。现在我们可以认为这是蓝色的，这是粉色的。</p>
        <p>But through some miracle of nature, which has always amazed me, these two cells split, in turn, into four
            cells altogether. And each of those four cells at the bottom gets one of the chromosomes that was produced
            by the twisting up of the rope and recombination. Then, along comes a special occasion. And now we can think
            of this as being a blue one and this as being a pink one.</p>
        <p>它们结合在一起，你就得到了一个新的人，就像这样。请注意，你母亲和父亲的染色体永远不会重组。重组的是你的祖父母的染色体。就是这样。关于这一点，要注意的主要事情是。嗯，有几件事。如果你碰巧是女性，这个过程的这一部分。这部分。发生在你出生之前。
        </p>
        <p>And they come together, and you get a new person, like so. Note that your mother and father’s chromosomes are
            never, never recombined. It’s your grandparents chromosomes that recombine. So that’s what it’s like. And
            the main thing to note about this is. well, a couple things. If you happen to be female, this part of the
            process. this part over here. took place before you were born.</p>
        <h2 id="unknown-249">未知</h2>
        <h2>Unknown</h2>
        <p>如果你碰巧是男性，那么现在我们说话的时候，这种情况正在发生，这可能解释了一些事情。但无论如何，这种情况现在正在发生。但无论何时发生，都有很多机会掷骰子。所以上帝在你出生前就掷了所有的骰子，如果你碰巧是女性，在这个过程中。
        </p>
        <p>If you happen to be male, it’s going on right now as we speak, which probably explains something. But in any
            event, it’s going on right now. But whenever it goes on, there are lots of opportunities for throwing dice.
            So God threw all the dice before you were born, if you happen to be a female, in this part of the process
            over here.</p>
        <p>然后，当然，在决定哪些特定细胞可以融合形成新个体时，这里就出现了更多的骰子。所以我想打破这种“选择多多”的想法。当我们谈论遗传算法和自然时，其中有很多选择。这意味着有很多选择可以干预，可以胡闹，可以让事情按照你想要的方式发展。
        </p>
        <p>Then, of course, more dice got thrown here when the decision was made about which particular cells got to
            fuse to form a new individual. So I want to beat on this idea of lots of choice. When we talk about genetic
            algorithms, and we talk about nature, there are lots of choices in there. And that means there are lots of
            choices to intervene, to screw around, to make things work out the way you want.</p>
        <p>但无论如何，我们做到了。这是基本思想，一切都从染色体开始。所以我们可以考虑用您在研讨会 1 中学到的 ACTG 知识来实现​​一些模仿这一点的东西。但我们是计算机科学家。我们不喜欢 4 进制。我们喜欢 2
            进制。所以我只建议在我们将要构建的这个系统中我们的染色体是二进制的。所以这可能是一条染色体。</p>
        <p>But in any event, there we are. That’s the basic idea, and it all starts with chromosomes. So we could think
            of implementing something that imitates that with the ACTG stuff that you learned all about in Seminar 1.
            But we’re computer scientists. We don’t like Base 4. We like Base 2. So I’m going to just suggest that our
            chromosomes are binary in this system that we’re going to build. So that might be a chromosome.</p>
        <h2 id="unknown-250">未知</h2>
        <h2>Unknown</h2>
        <p>而且它不一定是二进制的。它可以是秋天的象征，我关心。但它只是一些决定最终系统如何运作的东西。所以一切都从这些染色体中的一些开始，这些模拟染色体中的一些，模拟的、简化的和朴素的。还有一群染色体。染色体群。它可能会发生一些突变。
        </p>
        <p>And it doesn’t have to be binary. It can be symbolic for fall I care. But it’s just some string of things
            that determine how the ultimate system behaves. So it all starts out, then, with some of these chromosomes,
            some of these simulated chromosomes, simulated, simplified, and naive. And there’s a population of
            chromosomes. The population of chromosomes. it might be subject to a little bit of mutation.</p>
        <p>也就是说，零变成一，或者一变成零。这种情况经常发生。当事物扭曲并重新组合时，就会发生突变。有复制错误之类的。宇宙射线一直撞击它。有各种各样的原因可能导致一个变化点。这会产生突变效应。所以在这里，我们有一个从这里开始的种群。
        </p>
        <p>That is to say, a zero becomes a one, or a one becomes a zero. That happens a lot. That mutation stuff
            happens over here when things get twisted up and recombined. There are copying errors and stuff. Cosmic rays
            hit it all the time. All sorts of reasons why there might be a single point of change. That produces the
            mutation effect. So here, we have a population that starts off over here.</p>
        <p>其中有些东西会发生突变。你会注意到，这里已经有很多选择。例如，你允许每条染色体发生多少次突变？有多少条染色体会不发生任何突变就溜走？这些都是你可以做出的选择。一旦你做出了这些选择，我们就会看到交叉现象。让我们将其中一个染色体确定为粉色染色体，将其中一个染色体确定为蓝色染色体。
        </p>
        <p>And some of those things are subject to mutation. And you’ll note, here’s a whole bunch of choice already.
            How many of these mutations do you allow per chromosome, for example? How many of the chromosomes just slip
            through without any mutation? Those are choices you can make. Once you’ve made those choices, then we have
            the crossover phenomenon. Let’s identify one of these guys as the pink one and one of these guys as the blue
            one.</p>
        <h2 id="unknown-251">未知</h2>
        <h2>Unknown</h2>
        <p>现在粉色的和蓝色的都顺利通过了。粉色和蓝色的杂交产生了一条新的染色体，就像自然界中一样。所以我们取其中一个染色体的前半部分和另一个染色体的后半部分，然后将它们融合在一起。有些染色体可能会漏掉，就像这样。</p>
        <p>And so now we have the pink one cruised along as well as the blue one. The pink one and the blue one cross
            and produce a new chromosome, just like in nature. So we take the front part of one, back part of the other,
            and we fuse them together. And some may slip by without any of that, like so.</p>
        <p>嗯，这些东西本来是要成对组合的，就像这样，但它们可能没有任何交叉。所以你有另一组选择。每次重组允许多少次交叉？你有另一组选择。所以现在我们通过突变和交叉得到了一群经过修改的染色体。所以接下来要做的就是让基因型转变为表型。也就是说，染色体决定了个体。
        </p>
        <p>Well, these things are meant to be combined in pairs, like so, but they may not have any crossover in them.
            So you have another set of choices. How many crossovers do you allow per recombination? You get another set
            of choices. So now we’ve got a population of modified chromosomes through mutation and crossover. So the
            next thing to do is we have the genotype to phenotype transition. That is to say the chromosome determines
            the individual.</p>
        <p>它可能是一个人。它可能是一头牛。它可能是一段计算机程序。我不在乎是什么。但下面的代码必须被解释成某种东西。所以它是基因型，它必须被解释成表现型，也就是下面的东西编码的东西。所以在这里，我们有一群个体。</p>
        <p>It may be a person. It may be a cow. It may be a computer program. I don’t care what. But that code down
            there has to be interpreted to be a something. So it is the genotype, and it has to be interpreted to be
            something which is the phenotype, the thing that the stuff down there is encoding for. So here, we have a
            bunch of individuals.</p>
        <h2 id="unknown-252">未知</h2>
        <h2>Unknown</h2>
        <p>现在，由于每个个体的染色体组成不同，因此适应度也不同。所以这些适应度可能……好吧，谁知道它们会如何评分。但我们是计算机科学家。我们不妨使用数字。所以也许这个人的适应度是 88，这个人的适应度是
            77，等等。现在我们得到了适应度。顺便说一句，请注意其中涉及的所有选择。</p>
        <p>Now, each of those individuals, because they have varying chromosomal composition, will have a different
            fitness. So these fitnesses might be. well, who knows how they might be scored. But we’re computer
            scientists. We might as well use numbers. So maybe this guy’s fitness is 88, and this guy’s fitness is 77,
            and so on. So now that we’ve got fitness. by the way, notice all the choices involved there.</p>
        <p>选择如何解释基因型，选择如何让表现型产生适应性。现在我们可以选择如何让适应性产生概率，比如 0.8 和
            0.1，或者类似的。存活到下一代的概率。所以现在，一旦我们得到了这些概率，我们实际上就有了选择。那些表现型会产生基因型，一组新的染色体，这就完成了我们回到那里的循环。</p>
        <p>Choice of how you interpret the genotype, choice about how the phenotype produces the fitness. And now we
            have a choice about how the fitness produces a probability, like 0.8 and 0.1, or something like that.
            probability of survival into the next generation. So now, once we’ve got those probabilities, we actually
            have selection. And those phenotypes out there produce genotypes, a new set of chromosomes, and that
            completes our loop that goes back in there.</p>
        <p>这就是新一代。听起来很简单。所以如果你想要让这个工作正常进行，当然，你有无数种选择，我会反复强调。你的选择之一是，例如，在适应度的情况下，如何计算下一代存活的概率？所以我们必须以某种方式从这样的数字转向这样的概率。
        </p>
        <p>And so that’s the new generation. Sounds simple. So if you’re going to make this work, of course, you have a
            million choices, as I’m going to emphasize over and over again. And one of your choices is, for example, how
            do you compute the probability of survival to the next generation given the fitness? So we have to go,
            somehow, from numbers like these to probabilities like those.</p>
        <h2 id="unknown-253">未知</h2>
        <h2>Unknown</h2>
        <p>所以我要谈谈几种方法。它们都不是魔法。它们都不是上帝指定和规定的正确方法。但它们在这种处理方面具有越来越好的属性。所以你可以做的最简单的事情。计算的第一个想法。</p>
        <p>So I’m going to talk about several ways of doing it. None of them are magic. None of them was specified and
            stipulated by God as the right way. But they have increasingly good properties with respect to this kind of
            processing. So the simplest thing you can do. idea number one for computing the.</p>
        <p>瞧，你要做的就是把这一大堆个体都收集起来，然后决定谁能存活到下一代。所以在每一步，银行里的所有东西都有可能成为你挑选出来并存入下一代的那个。所以在任何一步，这些人的概率总和都是 1，因为这就是高概率的工作原理。</p>
        <p>See, what you do is you get this whole bag of individuals, and you have to decide who’s going to survive to
            the next generation. So at each step, everything in the bank has a probability of being the one you pick out
            and put in the next generation. So at any step, the sum of the probabilities for each of those guys is 1,
            because that’s how high probability works.</p>
        <p>完整集合的概率加起来就是 1。所以你可以说抽到个体 I 的概率等于或可能与该个体的适应度成正比。我还没有完成表达式，所以它还不是概率，因为它的某些部分加起来不等于 1。我如何确保它加起来等于 1？</p>
        <p>The probability of a complete set, added all up, is probability of 1. So one thing you can do is you can say
            that the probability that you’re going to draw individual I is equal to, or maybe is proportional to, the
            fitness of that individual. I haven’t completed the expression, so it’s not a probability yet, because some
            piece of it won’t add up to 1. How can I ensure that it will add up to 1?</p>
        <h2 id="unknown-254">未知</h2>
        <h2>Unknown</h2>
        <p>这很简单。对。我所要做的就是除以适应度之和除以
            i。因此，有一个从适应度中产生的概率测量。是的。学生：你需要确保适应度不是负数。帕特里克·温斯顿：必须确保适应度是什么？学生：不是负数。帕特里克·温斯顿：他说我必须确保适应度不是负数。是的，如果是负数，那就太尴尬了。
        </p>
        <p>That’s easy. Right. All I have to do is divide by the sum of the fitnesses over i. So there’s a probability
            measure that’s produced from the fitnesses. Yeah. STUDENT: You need to make sure that the fitnesses aren’t
            negative. PATRICK WINSTON: Have to make sure the fitnesses are what? STUDENT: Aren’t negative. PATRICK
            WINSTON: He says I have to make sure the fitnesses aren’t negative. Yeah, it would be embarrassing if they
            were.</p>
        <p>因此，我们将任何值都设为 0。您可以有很多选择来计算适应度。也许您会得到负数，在这种情况下，您必须多考虑一下。那么现在，举个例子怎么样？好吧，我要给您看一个例子。我为什么不给您看这个例子呢？</p>
        <p>So we’ll just anything like that as 0. You’ve got a lot of choice how you can calculate the fitness. And
            maybe you will produce negative numbers, in which case you have to think a little bit more about it. So now,
            what about an example? Well, I’m going to show you an example. Why don’t I show you the example.</p>
        <p>我们要做的是使用遗传算法在空间中寻找最优值。这就是空间。现在，你会注意到空间中有一堆等高线，一堆山丘。让我向你展示一下这个空间是如何产生的。</p>
        <p>What we’re going to do is we’re going to have a genetic algorithm that looks for an optimal value in a space.
            And there’s the space. Now, you’ll notice it’s a bunch of contour lines, a bunch of hills in that space. Let
            me show you how that space was produced.</p>
        <h2 id="unknown-255">未知</h2>
        <h2>Unknown</h2>
        <p>适应度是 x 和 y 的函数，它等于某个常数的正弦乘以 x 的平方，乘以某个常数的正弦 y 的平方，e 加上 x 加上 y 除以某个常数。所以 sigma 和 omega
            就在那里，这样就可以形成一幅很好的演示图。所以有一个空间。</p>
        <p>The fitness is a function of x and y, and it’s equal to the sine of some constant times x, quantity squared,
            times the sine of some constant y, quantity squared, e to the plus x plus y divided by some constant. So
            sigma and omega there are just in there so that it kind of makes a nice picture for demonstration. So
            there’s a space.</p>
        <p>显然，你想在这个空间中的位置是右上角。这是最优值。但我们有一个什么都不知道的遗传算法。它只知道如何变异和交叉。所以它将从 1 个种群开始。它是左下角的一个小红点。这就是它将如何进化。</p>
        <p>And clearly, where you want to be in this space is in the upper right hand corner. That’s the optimal value.
            But we have a genetic algorithm that doesn’t know anything. All it knows how to do is mutate and cross over.
            So it’s going to start off with a population of 1. It’s a little red dot down in the lower left. So here’s
            how it’s going to evolve.</p>
        <p>染色体将由两个数字组成，一个 x 数字和一个 y 数字，比如说 0.3 和 0.7。这里还有另一个，可能是 0.6 和 0.2。因此，变异运算符将取其中一个值并对其进行一些更改。因此，它可能会说，好吧，我们取
            3，然后将其改为 0.2。交叉操作将交换成对的 x 和 y 值。</p>
        <p>There’s going to be s chromosome consisting of two numbers, an x number and a y number, like, say, 0.3 and
            0.7. Here’s another one, which might be 0.6 and 0.2. So the mutation operator is going to take one of those
            values and change it a little bit. So it might say, well, we’ll take 3, and we’ll make it 0.2. And the
            crossover operation is going to exchange the x and y values of pairs.</p>
        <h2 id="unknown-256">未知</h2>
        <h2>Unknown</h2>
        <p>所以如果我们在这里有一个交叉，那么我们将从中得到什么呢？好吧，我们将得到这两个的组合。它看起来会像这样。因为我们要做的是取 x 值 1 并将其与另一个的 y 值相结合。</p>
        <p>So if we have a crossover here, then what we’re going to get out from this one. well, we’re going to get out
            a combination of these two. And it’s going to look like this. Because what we’re going to do is we’re going
            to take the x value of 1 and combine it with the y value of the other one.</p>
        <p>所以这将是 0.2。我的变异值和 0.2。这将是 0.6 和 0.7。这就是我的小遗传算法的工作原理。编写完代码后，我们现在可以看到它的流程。让我们运行 10 代。因此，种群迅速扩展到某个固定限制。我忘了它是多少。30
            左右。我们可以运行 100 代。</p>
        <p>So this is going to be 0.2. my mutated value and 0.2. and this is going to be 0.6 and 0.7. So that’s how my
            little genetic algorithm heck is going to work. So having coded this up, we can now see how it flows. Let’s
            run it 10 generations. So the population is rapidly expanded to some fixed limit. I forgot what it is. 30 or
            so. And we can run that 100 generations.</p>
        <p>所以这似乎有点卡住了，对吧？那么问题是什么？问题是局部最大值。这基本上是一种爬山机制。请注意，到目前为止我还没有包括任何交叉。所以如果我有交叉，那么如果我有一个好的 x 值和一个好的 y
            值，我可以将它们交叉并让它们都处于相同的情况。但尽管如此，这个东西似乎效果不太好。</p>
        <p>And so this seems to be getting stuck, kind of, right? So what’s the problem? The problem is local maxima.
            This is fundamentally a hill climbing mechanism. Note that I have not included any crossover so far. So if I
            do have crossover, then if I’ve got a good x value and a good y value, I can cross them over and get them
            both in the same situation. But nevertheless, this thing doesn’t seem to be working very well.</p>
        <h2 id="unknown-257">未知</h2>
        <h2>Unknown</h2>
        <p>学生：教授，我有个问题。帕特里克·温斯顿：是的。学生：那幅图只是该函数的轮廓线。帕特里克·温斯顿：该函数的轮廓线。所以你看到右上方有很多轮廓线的原因是它变得更高，因为有一个指数项，当你向右上方移动时，它会增加。所以我不知道，它看起来。让我们加入一些交叉并重复这个体验。
        </p>
        <p>STUDENT: Professor, I have a question. PATRICK WINSTON: Yeah. STUDENT: That picture is just the contour lines
            of that function. PATRICK WINSTON:. The contour lines of that function. So the reason you see a lot of
            contour lines in the upper right is because it gets much higher because there’s that exponential term that
            increases as you go up to the right. So I don’t know, it looks. let’s put some crossover in and repeat the
            experience.</p>
        <p>我们将运行 100
            代。我不知道。它似乎没有任何进展。有时，它会直接达到全局最大值。有时需要很长时间。它里面有一个随机数生成器，所以我无法控制它。所以它会到达那里。我无法判断交叉是否有用。哦，好吧，这是一个。让我们让它更复杂一点。假设那是空间。
        </p>
        <p>We’ll run 100 generations. I don’t know. It just doesn’t seem to be going anywhere. Sometimes, it’ll go right
            to the global maximum. Sometimes it takes a long time. It’s got a random number generator in there, so I
            have no control over it. So it’s going to get there. I couldn’t tell whether the crossover was doing any
            good or not. Oh, well, here’s one. Let’s make this a little bit more complicated. Suppose that’s the space.
        </p>
        <p>现在它真的有麻烦了，因为它永远无法越过那道护城河。你知道，你会认为它会爬到 x 最大值或 y
            最大值，但它不会表现得很好。即使有交叉，它也不会表现得很好，因为它正在爬那些当地的山丘。有人知道我们可以做一件简单的事情来让它更好地工作吗？</p>
        <p>Now it’s going to be in real trouble, because it’ll never get across that moat. You know, you would think
            that it would climb up to the x maximum or to the y maximum, but it’s not going to do very well. Even with
            crossover, it’s just not going to do very well, because it’s climbing up those local hills. Anybody got an
            idea about one simple thing we could do to make it work better?</p>
        <h2 id="unknown-258">未知</h2>
        <h2>Unknown</h2>
        <p>是的，你可以增加步长，对吧？让我看看这是否有帮助。你知道，即使这样似乎也没什么帮助。所以我们必须得出结论。我们是否得出结论说这是一个坏主意？好吧，我们不必立即得出结论说这是一个坏主意，因为我们可能只会看一眼并问五次为什么。
        </p>
        <p>Yeah, you could increase step size, right? Let me you see if that will help. You know, even that doesn’t seem
            to help. So we have to conclude. do we conclude that this is a bad idea? Well, we don’t have to conclude
            it’s a bad idea yet, because we may just look at it and ask why five times.</p>
        <p>我们可能会问，好吧，也许我们可以在其中找到一个更好的机制来将适应度转化为生存概率。无论如何，使用这个公式有点奇怪，因为假设温度是你的适应度特征之一。温度越高越好。那么你与你身边的人的生存概率之比将取决于你测量的温度是摄氏度还是华氏度，对吗？
        </p>
        <p>And we might ask, well, maybe we can get a better mechanism in there to translate fitness into probability of
            survival. Using this formula is kind of strange, anyway, because suppose temperature is one of your fitness
            characteristics. The hotter, the better. Then the ratio of the probability that you’ll survive versus the
            person next to you, that ratio will depend on whether you’re measuring the temperature in Celsius or
            Fahrenheit, right?</p>
        <p>因为您改变了原点，从而改变了比率，进而改变了成功概率。因此，将这些事情直接转化为概率似乎有点奇怪。因此，一个更好的想法。第二个想法是说，好吧，也许我们并不关心实际的适应度是多少。我们真正关心的是所有候选人的排名顺序。
        </p>
        <p>Because you’ve shifted the origin that shifts the ratio that shifts the probability of success. So it seems
            kind of strange to just take these things right straight into probabilities. So a better idea. idea number
            two. is to say, well, shoot, maybe we don’t care about what the actual fitnesses are. All we really care
            about is the rank order of all the candidates.</p>
        <h2 id="unknown-259">未知</h2>
        <h2>Unknown</h2>
        <p>因此，适应度最高的候选者进入下一代的概率最大。适应度第二高的候选者进入下一代的概率第二高，依此类推。但我们不会使用实际的适应度本身来做出决定。相反，我们要用第二种机制。这是秩空间方法。是这样的。</p>
        <p>So the candidate with the most fitness will have the most probability of getting into the next generation.
            The candidate with the second most fitness will have the second highest probability, and so on. But we’re
            not going to use the actual fitnesses themselves to make the determination. Instead, what we’re going to do
            with this mechanism number two. this is the rank space method. is this.</p>
        <p>我们会说，排名最高的个体进入下一代的概率是某个常数 P sub c，当然，你可以选择它。你还有另一个选择。那么，如果这个人没有被选中，排名第二的个体进入下一代的概率将是这个人没有进入的概率。这是 1 减去 P sub
            c 乘以相同的概率常数。</p>
        <p>We’re going to say that the probability of the highest ranking individual of getting into the next generation
            is some constant P sub c, which, of course, you can select. You have another choice. Then, if that guy
            doesn’t get selected, the probability of the second highest ranking individual getting in the next
            generation is going to be the probability that guy didn’t get in there. That’s 1 minus P sub c times the
            same probability constant.</p>
        <p>所以你可以看到这是怎么回事。P3 等于 1 减去 P sub c 平方项 P sub c。P sub n 减 1 等于 1 减去概率常数 n 减去 n 的 2 倍 P sub
            c。然后只剩下一个人。然后如果你把这些人都选了一遍，还没有选出任何人，那么你就必须选择最后一个人。</p>
        <p>And so you can see how this is going. P3 will be equal to 1 minus P sub c squared terms P sub c.&nbsp;P sub n
            minus 1 will be equal to 1 minus that probability constant to the n minus. n minus 2 times P sub c.&nbsp;And
            then there’s only one individual left. And then if you got through all these guys and haven’t got anybody
            selected, then you’ve got to select the last guy.</p>
        <h2 id="unknown-260">未知</h2>
        <h2>Unknown</h2>
        <p>所以你选中最后一个人的概率是 1 减去 P，c 减去 n，然后减 1。所以这是你错过了前 n 减 1
            个选择中所有人的概率。是的，确实如此。你看，这是最后一个人被选中的概率。这不是最后一个人被选中的概率，因为其他人还没有被选中。</p>
        <p>And so the probability you’re going to select the last guy is going to be 1 minus P sub c to the n minus 1.
            So it’s a probability you’ve missed all those guys in the first n minus 1 choices. Yeah, it is, honest to
            God. See, this is the probability that this last guys is going to get selected. It’s not the probability
            that it’s the last guy getting selected, given that the others haven’t been select.</p>
        <p>相信我，这是正确的。你认为应该是 1 吗？学生：什么？帕特里克·温斯顿：你认为应该是 1
            吗？学生：不，我在想，我想知道你为什么要重新掷骰子。帕特里克·温斯顿：你正在重新掷骰子。每次你都有概率，除了最后一次，当然，你必须接受它。什么都没有了。没有其他选择。</p>
        <p>Trust me, it’s right. Are you thinking it ought to be 1? STUDENT: What? PATRICK WINSTON: Were you thinking it
            ought to be 1? STUDENT: No, I was thinking that I was wondering why you were re rolling the dice, so to
            speak. PATRICK WINSTON: You are re rolling the dice. You’ve got a probability each time, except for the last
            time, when, of course, you have to take it. There’s nothing left. There’s no other choice.</p>
        <p>学生：我有个问题。帕特里克·温斯顿：是的，学生：所以当你从这一点跳出来时，是有道理的。帕特里克·温斯顿：这是前两个选择的概率。学生：是的，但第二个选择的概率是 1 减去 P sub c 乘以 P sub
            c，不是。帕特里克·温斯顿：这样想想。这是你没有选择前两个的概率。所以你没有选择第一个的概率是 1 减去 P sub c。&nbsp;</p>
        <p>STUDENT: I have a question. PATRICK WINSTON: Yeah, STUDENT: So when you jump from that makes sense. PATRICK
            WINSTON: It’s the probability the first two choices. STUDENT: Yeah, but the second choice had probability
            one minus P sub c times P sub c, not. PATRICK WINSTON: Think about it this way. It’s the probability you
            didn’t choose the first two. So the probability you didn’t choose the first one is one minus P sub c.&nbsp;
        </p>
        <h2 id="unknown-261">未知</h2>
        <h2>Unknown</h2>
        <p>您没有选择下一个的概率也是如此，因为您选择下一个的概率是 P sub c，它是它的平方。所以这可能更有效。让我们试一试。让我们回到我们最初的空间选择，然后我们设置并切换到等级适应度方法。我们将用完 100
            代。哇！发生了什么？那太快了。也许我使用了较大的步长。</p>
        <p>The probability you didn’t choose the next one, as well, because you’re choosing that next one with
            probability P sub c, it’s the square of it. So that might work better. Let’s give it a shot. Let’s go back
            to our original space choice, and we set and switch to the rank fitness method. And we’ll run out 100
            generations. Whoa! What happened there? That was pretty fast. Maybe I used a big step size.</p>
        <p>是的，这样更合理一些。哎呀。发生了什么？它真的卡在了局部最大值上。显然，我选择了一个常数 P sub c，这样它就直接把它推到了最近的山上。另一方面，如果我稍微改变一下步长，也许我可以让它散开。我确实做到了。</p>
        <p>Yeah, that’s a little bit more reasonable. Oops. what happened? It’s really getting stuck on a local maximum.
            So evidently, I’ve choosed a constant P sub c such that it just drove it right up the nearest hill. On the
            other hand, if I change the step size a little bit, maybe I can get it to spread out. I sure did.</p>
        <p>现在它已经成功进化到那个最大值，现在我可以再次限制步长。现在它不再表现出多样性。它只是锁定了那个全局最大值。所以这与进化有时所做的没什么不同。有时，物种会崩溃到 5 亿或 6 亿年都不会改变的状态，例如鲨鱼。</p>
        <p>And now that it’s managed to evolve over there to find the maximum value, now I can clamp down on the step
            size again. And now it shows no more diversity. It’s just locked on to that global maximum. So this is not
            unlike what evolution sometimes does. Sometimes, species collapse into a state where they don’t change for
            500 million or 600 million years, like sharks, for example.</p>
        <h2 id="unknown-262">未知</h2>
        <h2>Unknown</h2>
        <p>有时，只有当它们的生活方式具有多样性，它们才能适应栖息地的变化，才能生存下来。现在，当你增加步长时，因为你被困在局部最大值上，这就像加热金属一样。你让一切都震动得更厉害，迈出更大的步伐。</p>
        <p>Sometimes, they only survive if they’ve got a lot of diversity built into their way of life so that they can
            adjust to habitat changes. Now, when you increase the step size, because you’re stuck on a local maximum,
            it’s like heating up a metal. You make everything kind of vibrate more, make bigger steps.</p>
        <p>因此，这种过程称为模拟退火，即您可能从较大的步长开始，然后逐渐减小步长，因为它就像让金属冷却下来一样。因此，您从较大的温度开始。较大的步长。覆盖整个空间。然后您慢慢减小步长，这样您实际上就可以爬升到可用的局部最大值。所以这似乎效果很好。
        </p>
        <p>So this kind of process, where you may start with a big step size and then gradually reduce the step size, is
            called simulated annealing, because it’s like letting a metal cool down. So you start off with a big
            temperature. big step size. that covers the space. And then you slowly reduce the step size, so you actually
            crawl up to the local maxima that are available. So that seemed to work pretty well.</p>
        <p>让我们看看我们能否让它解决护城河这个更难的问题。所以它表现得不太好。最好增加步长。不，它仍然有点卡住了。即使它有能力跨越，它还是卡在右下角，它无法到达垂直分支，到达交叉点，在右上角产生一个值。</p>
        <p>Let’s see if we can get it to work on the harder problem of the moat. So it’s not doing very well. Better
            increase the step size. No, it’s still kind of stuck. Even though it’s got the capacity to cross over, it’s
            so stuck on that lower right hand corner, it can’t get up that vertical branch to get to a point where a
            crossover will produce a value up there in the upper right hand corner.</p>
        <h2 id="unknown-263">未知</h2>
        <h2>Unknown</h2>
        <p>所以我们还没找到答案。那么问题是什么呢？问题在于适应机制只会将事物推向局部最大值。这真是太不幸了。该怎么办？好吧，你可以这样做。你可以说，好吧，如果问题是我们失去了种群的多样性，那么我们可以衡量多样性。</p>
        <p>So we’re still not home yet. So what’s the trouble? The trouble is that the fitness mechanism is just driving
            things up to the local maximum. It’s just terribly unfortunate. What to do? Well, here’s something you could
            do. You can say, well, if the problem is we’ve lost the diversity in our population, then we can measure the
            diversity.</p>
        <p>我们不仅可以衡量所选个体的适应度，还可以衡量它们与已选出的下一个种群的个体之间的差异。换句话说，如果我们在进行选择时不仅考虑它们的适应度，还考虑它们与已选个体之间的差异，那么我们既可以得到多样化的种群，也可以得到适合的种群。所以这就是第三个机制。
        </p>
        <p>Not only the fitness of the set of individuals we’re selecting from, but we can measure how different they
            are on the individuals we’ve already selected for the next population. In other words, we can get a diverse
            population as well as a fit population if, when we make our selection, we consider not only their fitness
            but how different they are from the individuals that have already been selected. So that’s going to be
            mechanism number three.</p>
        <p>现在我们有了一个空间，我们可以沿着一个轴测量适应度。普通适应度。这是秩空间适应度，所以将是 P sub c。总会有一些个体具有最高的适应度。而在这里。实际上，这可能不是 P sub c。</p>
        <p>So now we have a space, and we can measure fitness along one axis. ordinary fitness. and this is rank space
            fitness, so that’s going to be P sub c.&nbsp;There will always be some individual with the highest fitness.
            And over here. that might not be P sub c, actually.</p>
        <h2 id="unknown-264">未知</h2>
        <h2>Unknown</h2>
        <p>但是总有一些个体具有最大的适应度，并且在选择下一个种群的任何给定步骤中，总有一些个体与迄今为止为下一代选择的所有个体具有最大的差异性。那么你想为下一代选择什么样的个体呢？嗯，具有最高适应度等级和最高多样性等级的个体。
        </p>
        <p>But there’ll be some individual with a maximal fitness, and at any given step in the selection of the next
            population, there’ll be some individual that’s maximally diverse from all of the individuals that have been
            selected for the next generation so far. So what kind of individual would you like to pick for the next
            generation? Well, the one with the highest fitness rank and the one with the highest diversity rank.</p>
        <p>所以你真正想要的是，你希望有人能满足你的要求。如果你不能找到合适的人，如果没有人能同时满足最大适应度和最大多样性，那么也许你可以画出 iso
            优度线，就像这样，这就是你与理想状态的距离。让我们总结一下。你必须为下一个种群挑选一些个体。</p>
        <p>So what you’d really like is you’d like to have somebody right there. And if you can’t have somebody right
            there, if there’s nobody right there with a maximum fitness, a maximum diversity at the same time, then
            maybe you can draw in iso goodness lines, like so, which are just how far you are from that ideal. So let’s
            summarize. You’ve got to pick some individuals for the next population.</p>
        <p>当我们选择第一个个体时，我们只需要考虑该个体的适应性，因为下一代中没有其他人了。在选出第一个个体后，我们就可以查看我们的候选集，我们可以说哪个候选集与我们已经选择的候选集相比差异更大。这将获得最高的多样性排名，依此类推。
        </p>
        <p>When we pick the first individual, all we’ve got to go on is how fit the individual is, because there’s
            nobody else in that next generation. After the first individual is selected, then we can look at our set of
            candidates, and we can say which candidate would be more different from the set of things we’ve already
            selected than all the others. That would get the highest diversity rank and so on down the candidate list.
        </p>
        <h2 id="unknown-265">未知</h2>
        <h2>Unknown</h2>
        <p>让我们看看它是如何工作的。所以我们将使用适应度等级和多样性等级的组合。到目前为止，我们只使用简单的一个。我们将使用较小的步长，并让它运行 100
            代，看看会发生什么。Bingo。它爬到了那里，因为它试图让自己保持分散。它使用多样性测量来做到这一点。</p>
        <p>So let’s see how that might work. So we’re going to use a combination of fitness rank and diversity rank. And
            we’ll just use the simple one so far. We’ll use a small step size, and we’ll let this run 100 generations to
            see what happens. Bingo. It crawls right up there, because it’s trying to keep itself spread out. It uses
            that diversity measurement to do that.</p>
        <p>与此同时，它正在寻求高适应度，所以这就是它爬到右上角的原因。但最终，多样性因素会让事物分散开来。假设你是一条鲨鱼或什么东西。你不再关心多样性，我们可以把它关掉。那东西还在运行吗？回到适应度等级。宾果。所以你就是这样。你被困了
            6 亿年。</p>
        <p>And at the same time, it’s seeking high fitness, so that’s why it’s crawling up to the upper right hand
            corner. But in the end, that diversity piece of it is keeping the things spread out. So suppose you’re a
            shark or something. You don’t care about diversity anymore, And we could just turn that off. Is that thing
            still running? Go back to fitness rank. bingo. So there you are. you’re stuck for 600 million years.</p>
        <p>让我们看看这是否能解决护城河问题。瞧，我们的步长仍然很小。我们就让它运行。因此，P 子集的多样性使其保持分散，很快，宾果，它就在那里。它跨越了那条大护城河，因为它具有结合了 x 的最佳部分和 y
            的最佳部分的交叉机制。所以这似乎效果很好。</p>
        <p>So let’s see if this will handle the moat problem. See, our step size is still small. We’ll just let this
            run. So the diversity of P sub is keeping it spread out, pretty soon, bingo, it’s right in there. It’s
            across that big moat, because it’s got the crossover mechanism that combines the best of the x’s and the
            best of the y’s. So that seems to work pretty well.</p>
        <h2 id="unknown-266">未知</h2>
        <h2>Unknown</h2>
        <p>好的，所以你看，这些是你在思考时可以考虑的一些事情。哦，当然，我们是鲨鱼，我们会忘记多样性。我们将选择方法从适应度和多样性等级改为多样性。它会下降到最高的山丘。是的，什么？学生：步长如何转化为突变？帕特里克·温斯顿：哦，只是。问题是，步长如何转化为突变？
        </p>
        <p>OK, so see, these are some of the things that you can think about when you’re thinking. oh, and of course,
            we’re a shark, we’re going to forget about diversity. We’ll change the selection method from fitness and
            diversity rank to just diversity. It collapses down on to the highest hill. Yeah, what? STUDENT: How does
            step size translate into mutations? PATRICK WINSTON: Oh, just the. question is, how does step size translate
            into mutation?</p>
        <p>根据某种分布，我可能不会允许自己迈出 1/10 的步数，而是允许自己迈出 3/10
            的步数。那么，对于这一切，该说些什么呢？这很诱人，因为这是自然，对吧？问题是，这是天真的自然。就进化理论而言，这是可怕的。这是天真的。所以我们想使用真正的进化理论，但我们没有真正的进化理论。进化仍然是一个谜。</p>
        <p>Instead of allowing myself to take steps as big as 1/10, I might allow myself to take steps as big as 3/10,
            according to some distribution. So what to say about all this? It’s very seductive, because it’s nature,
            right? The trouble is, it’s naive nature. And as evolutionary theories go, this is horrible. This is naive.
            So we’d like to use real evolutionary theory, except we don’t have real evolutionary theory. Evolution is
            still a mystery.</p>
        <p>有些事情很明显。你可以培育出快马。就像这样。问题是，我们对物种形成的过程以及许多进化过程如何进行没有任何真正的了解，因为所有这些染色体都以非常复杂的方式与其表型结果相关联，没有人完全理解。因此，在基因型到表型的转变中存在着许多神奇之处，没有人真正理解。
        </p>
        <p>Some things are pretty obvious. You can breed fast race horses. That works just like so. The trouble is, we
            don’t have any real good idea about how speciation takes place and how a lot of evolution works, because all
            these chromosomes are connected to their phenotype consequences in very complicated ways that nobody fully
            understands. So there’s a great deal of magic in that genotype to phenotype transition that nobody really
            understands very well.</p>
        <h2 id="unknown-267">未知</h2>
        <h2>Unknown</h2>
        <p>因此，当人们编写这些所谓的遗传算法风格的程序时，他们就像在拍摄高中生物学，并花费大量时间基于这种幼稚的想法构建程序。但这种幼稚的想法有很多可以干预的地方，因为看看在从一代传到下一代的过程中你可以摆弄的所有东西。顺便问一下，突变有什么作用？
        </p>
        <p>So when people write these programs that are in the style of so called genetic algorithm, they’re taking a
            photograph of high school biology, and they’re spending a long time building programs based on that naive
            idea. But that naive idea has lots of places for intervention, because look at all the things you can screw
            around with in that process of going from one generation to the next. By the way, what does mutation do?</p>
        <p>这基本上就是爬山，对吧？它会产生一些分散，你使用适应度来爬山。所以你有很多选择来决定如何处理这个问题。然后你有很多选择来决定你正在做的交叉。交叉有什么作用？它可能将多个个体的强大特征组合成一个个体。所以你有各种各样的选择。然后是你的基因型到表型的翻译。
        </p>
        <p>It’s basically hill climbing, right? It’s producing a little spread out, and you’re using the fitness thing
            to climb the hill. So you get a lot of choices about how you handle that. Then you get a lot of choices
            about much crossover you’re doing. What does crossover do? It kind of combines strong features of multiple
            individuals into one individual, maybe. So you’ve got all kinds of choices there. And then your genotype to
            phenotype translation.</p>
        <p>例如，你如何将那些 0 和 1
            之类的东西解释为“如果那么”规则，解释为设计师手中的东西？然后你得到了所有其余的东西，所有这些都留给了设计师。所以最后，你真的不得不问。当你看到一个令人印象深刻的演示时，你必须说，功劳在哪里？</p>
        <p>How do you interpret something like those zeroes and ones as an if then rule, for example, as something
            that’s in the hands of the designer? Then you’ve got all the rest of those things, all which are left up to
            the designer. So in the end, you really have to ask. when you see an impressive demonstration, you have to
            say, where does the credit lie?</p>
        <h2 id="unknown-268">未知</h2>
        <h2>Unknown</h2>
        <p>我故意说这个双关语，因为通常那些声称功劳的人都在谎报功劳的来源。但尽管如此，让我给你举几个例子，说明这个方法在哪些方面有实际、真正的实际应用。所以当你寻找实际应用时，你可能会说，好吧，在什么样的问题中，好的前片和好的后片结合起来会产生一个整体的好东西？
        </p>
        <p>And I mean that pun intentionally, because usually the people who are claiming the credit are lying about
            where it’s coming from. But nevertheless, let me give you a couple of examples of where this has found
            actual, bona fide practical application. So when you look for practical application, you might say, well, in
            what kind of problem does a good front piece combine with a good back piece to produce a good thing overall?
        </p>
        <p>答案是，当你制定计划时。所以你可能在计划中遇到问题，需要你采取一系列步骤。你可能有两个计划，每个计划都是一系列步骤。你可以将它们结合起来产生新的东西，它是前半部分和后半部分。所以这是第一个实际应用。</p>
        <p>And the answer is, when you’re making a plan. So you might have a problem in planning that requires you to
            take a series of steps. And you might have two plans, each of which is a series of steps. And you might
            combine these to produce something new that’s the front half of one and the back half of another. So that’s
            practical application number one.</p>
        <p>这就要求你将自己的染色体解读为计划中各个步骤的指标。另一个例子来自几年前一个学生为我做的一个项目。他当时是个大一新生。他来找我说，我想做一个项目。我说，你修过 6.034
            吗？他说没有。我说，走开。他说，我不想走开。我想做一个项目。</p>
        <p>And that requires you to interpret your chromosome as an indicator of the steps in the plan. Another example
            is drawn from a project a student did for me some years ago. He was a freshman. He came to me and said, I
            want to do a project. And I said, have you taken 6.034? And he said no. And I said, go away. And he said, I
            don’t want to go away. I want to do a project.</p>
        <h2 id="unknown-269">未知</h2>
        <h2>Unknown</h2>
        <p>于是我说，你读过我的书吗？他说没有。我说，那好，走吧。他说，我不想走。我想做一个 UROP
            项目。所以我说，我没有任何项目。他说，没关系。我有自己的项目。他是金融方面的，所以他想知道他是否可以建立一个基于规则的专家系统，可以预测赛马的获胜者。</p>
        <p>So I said, have you read my book? He said no. I said, well, go away, then. And he said, I don’t want to go
            away. I want to do a UROP project. So I said, I don’t have any projects. He said, that’s OK. I’ve got my
            own. He’s a finance type guy, so he was interested in whether he could build a rule based expert system that
            could predict the winners at horse races.</p>
        <p>因此，他的基于规则的专家系统由如下规则组成。如果 x 和 y，则得出某个结论。如果 l 和 m，则得出某种结论。根据这些规则，他会制定规则，例如如果 x 是素数。这是 x 先行词的略微变异版本。如果
            m，则得出某个结论。因此，这是变异和交叉。他能够制定一个似乎与报纸上的预测者一样有效的系统。</p>
        <p>So his rule based expert system consisted of rules like this. If x and y, then some conclusion. If l and m,
            then some kind of conclusion. And from these, he would produce rules like if x prime. that’s a slightly
            mutated version of the x antecedent. and m, then some conclusion. So it’s mutation and crossover. And he was
            able to produce a system that seemed to work about as well as the handicappers in the newspaper.</p>
        <p>因此，他开始以较慢的速度亏损。他们说，他现在在股票市场上有所作为。不过，他并没有过多谈论这件事。但这是一个有趣的应用。他想出了这样的规则：如果骑师在起跑线上的体重总和较低，那就很好。</p>
        <p>So he started losing money at a less fast rate. He is now doing something in the stock market, they say.
            Doesn’t talk so much about it, though. But an interesting application. He came up with rules like, if the
            sum of the jockey’s weight on the post position is low, that’s good.</p>
        <h2 id="unknown-270">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，这最终是有道理的，因为骑师的体重总是在 100 到 110 磅之间，而起跑位置总是在 1 到 10
            之间，所以它们是相称的值。事实上，较低的值是好的。还不错。但两者都不是。我的意思是，这是真实的东西。我的公司使用这类东西来做一些规划工作。</p>
        <p>Well, that makes sense in the end, because the jockey’s weight is always between 100 and 110 pounds, and the
            post position is always between 1 and 10 or something, so they were commensurate values. And a low one is
            good, in fact. Not bad. But neither of those. I mean, this is real stuff. My company uses this sort of stuff
            to do some planning work.</p>
        <p>但这些都不如我将要向你们展示的涉及生物进化的演示令人印象深刻。这些生物由块状物体组成，就像这样。它们像这样组合，等等。那么你如何从染色体中得出这样的特征呢？好吧，染色体中的一些位被解释为物体的数量。其他的则被解释为物体的大小。
        </p>
        <p>But neither of those is as impressive as the demonstration I’m about to show you that involves the evolution
            of creatures. And these creatures consist of block like objects, like so. And they combine like this, and so
            on. And so how can you make a feature like that from a chromosome? Well, some of the bits in the chromosome
            are interpreted as the number of objects. Others are interpreted as the sizes of the objects.</p>
        <p>其他一些则被解释为物体如何表达的结构。还有一些则被解释为固定生物操作的控制算法。所以你知道这大概是怎么回事吗？你想看一部关于这个动作的电影吗？是的。好的。我总是喜欢看电影。学生：你如何衡量该图中的多样性？帕特里克·温斯顿：问题是，我如何衡量图表的多样性？
        </p>
        <p>Others are interpreted as the structure of how the objects are articulated. And still others are interpreted
            as fixing the control algorithm by which the creature operates. So you see how that roughly goes? Would you
            like to see a film of that in action? Yes. OK. always likes to see the films. STUDENT: How would you measure
            diversity in that graph? PATRICK WINSTON:. The question is, how do I measure the diversity of the graph?</p>
        <h2 id="unknown-271">未知</h2>
        <h2>Unknown</h2>
        <p>我采用的方法与测量适应度的方法相同。也就是说，我计算了距离。实际度量距离。所有下一代候选者与所有已被选中的候选者之间的距离。我将其相加。然后，根据该总和，我可以根据它们与下一代中已有个体的差异程度对它们进行排序。
        </p>
        <p>I did it the same way I measured the fitness. That is to say, I calculated the distance. the actual metric
            distance. of all the candidates for the next generation from all of the candidates that had already been
            selected. I summed that up. And from that sum, I could rank them according to how different they were from
            the individuals that were already in the next generation.</p>
        <p>这就像给出一个排名，然后根据这个排名，我用那种计算方法来确定适应度，即生存概率，然后我只是把这两种概率结合起来。学生：所以你总是保留……每次你开始做某事时，那些。</p>
        <p>It’s like giving a rank, and then from the rank, I use that kind of calculation to determine a fitness, ie, a
            probability of survival, and then I just combine the two kinds of probabilities. STUDENT: So you always
            kept. every time that you started something, those.</p>
        <p>你保留了所有你曾经做过的
            帕特里克·温斯顿：我每一步都使用已经被选择的个体，所以每一步都有点不同，因为它处理的是一组已经被选择用于下一代的新的个体。好吗？让我们看看这是怎么回事。这显示了某些游泳生物的进化。它们的进化取决于它们的游泳能力和游得有多快。
        </p>
        <p>And you kept everything that you’ve ever PATRICK WINSTON: I’m always using the individuals that have already
            been selected at every step, so every step is a little different because it’s working with a new set of
            individuals that have already been selected for the next generation. OK? So let’s see how this works. So
            this is showing the evolution of some swimming creatures. And they’re evolved according to how well they can
            swim, how fast they can go.</p>
        <h2 id="unknown-272">未知</h2>
        <h2>Unknown</h2>
        <p>有些生物拥有相当奇特的机制，有些则相当自然。那看起来像是一个漂浮在那里的精子细胞。一旦这些生物进化了，那么当然，它们就可以一起进化。所以你已经看到一些生物进化成会游泳了。这些生物进化成可以在陆地上移动了。这很有趣。
        </p>
        <p>Some of them have quite exotic mechanisms, and some of them quite natural. That looked like a sperm cell
            floating away there. Once you have these things evolving, then of course, you can get groups of them to
            evolve together. So you saw already some that were evolving to swim. These are evolving to move around on
            the land. It’s interesting.</p>
        <p>这项任务由卡尔·西姆斯完成，当时他任职于当时蓬勃发展的 Thinking Machines
            公司，这是麻省理工学院刚刚成立的子公司。因此，他使用一台在当时非常强大的超并行计算机（拥有数千个处理器）来完成这项任务。这证明了大量计算可以做什么。</p>
        <p>This was done by Karl Sims, who at the time was at a then thriving company, Thinking Machines, a fresh
            spinoff from MIT. So he was using a vastly parallel computer, super powerful for its day, thousands of
            processors, to do this. And it was a demonstration of what you could do with lots of computing.</p>
        <p>不过，在实验的早期阶段，它的物理概念还不够完善，所以有些生物进化出了通过击打自己的胸部来移动的能力，而且不知道动量守恒。我觉得这真是太棒了。所以它们就在这里，在外面做些更进一步的事情。所以你看着这些，你会说，哇，这里面一定有什么。这很有趣。这些很复杂。
        </p>
        <p>In the early stages of the experimentation, though, its notion of physics wasn’t quite complete, so some of
            the creatures evolved to move by hitting themselves in the chest and not knowing about the conservation of
            momentum. I thought that was just great. So here they are, out doing some further. So you look at these, and
            you say, wow, there must be something to this. This is interesting. These are complicated.</p>
        <h2 id="unknown-273">未知</h2>
        <h2>Unknown</h2>
        <p>我认为这是其中一种，最初被训练游泳，然后进行陆地运动。所以最终，卡尔开始思考如何让这些东西进化，以便它们能够竞争食物。顺便说一句，我认为这是陆地运动最快的。所以这就是训练它们。进化它们跳跃。这是进化它们跟随一个小红点。
        </p>
        <p>I think this is one of the ones that was trained, initially, to swim and then to do land locomotion. So
            eventually, Karl got around to thinking about how to make these things evolve so that they would compete for
            food. That’s the fastest, I think, by the way, of the land locomotors. So that was training them to.
            evolving them to jump. This is evolving them to follow a little red dot.</p>
        <p>正如您所见，其中一些人偶然发现了一些非常奇特的方法。似乎在四处乱窜，但不知何故设法做到了。有点像看着人们参加测验。在测验中取得进步。但现在我们开始讨论食物竞赛。所以他们中的一些人去争夺食物，而另一些人去将对手从食物中排除，实际上并不太在意他们是否能得到食物。这就是孩子们会做的事情。
        </p>
        <p>Some of them have stumbled upon quite exotic methods, as you can see. Seem to be flailing around, but somehow
            manage to. sort of like watching people take a quiz. Making progress on it. But now we’re on to the food
            competition. So some of them go for the food, and some of them go to excluding their opponent from the food,
            not actually caring too much about whether they get it. That’s sort of what children do.</p>
        <p>有这样一种曲棍球运动员。现在，这里有两个曲棍球运动员。看这个。他们有点。一个成功了。这让我想起了曲棍球、橄榄球之类的东西。有时，他们只是有点困惑，直接追赶对手，忘记了食物。放弃。我认为他们是这场淘汰赛的总冠军。我不太明白。好吧，所以你看到这个，你会说，哇，这很酷。
        </p>
        <p>There’s a kind of hockey player. now, here’s two hockey players. Watch this. They’re kind of. one succeeds.
            it reminds me a little bit of hockey, rugby, something like that. Sometimes, they just get kind of confused,
            go right after their opponent, forgetting about the food. Gives up. I think these are the overall winners in
            this elimination contest. I can’t quite get there. OK, so you look at that, and you say, wow, that’s cool.
        </p>
        <h2 id="unknown-274">未知</h2>
        <h2>Unknown</h2>
        <p>遗传算法一定是未来的发展方向。我还记得第一次看这部电影的情景。那是在克雷斯基。我和托马·波吉奥一起走出礼堂，我们互相看了看，同时说了同样的话。我们并没有说遗传算法才是未来的发展方向。我们说的是，哇，这个空间里有丰富的解决方案。
        </p>
        <p>Genetic algorithms must be the way to go. I remember the first time I saw this film. It was over in Kresge. I
            was walking out of the auditorium with Toma Poggio And we looked at each other, and we said the same thing
            simultaneously. We didn’t say that genetic algorithms were the way to go. What we said was, wow, that space
            is rich in solutions.</p>
        <p>我们惊讶的不是简单的遗传算法能产生解决方案，而是这个空间里有如此丰富的解决方案，几乎任何在这个空间里寻找的机制都能找到它们。但还有另一种思考方式，那就是你可以说，哇，看看卡尔·西姆斯有多聪明，因为卡尔·西姆斯是那个掌控所有杠杆、所有那些选择的人。
        </p>
        <p>What we were amazed by was not that simple minded genetic algorithms produced solutions but that the space
            was so rich with solutions that almost any mechanism that was looking around in that space would find them.
            But there’s yet another way of thinking about it, and that is you could say, wow, look at how smart Karl
            Sims is, because Karl Sims is the one who had his hands on all the levers, all those choices.</p>
        <p>我一直强调，所有这些选择都让他能够欺骗这个东西，从某种意义上说，在一个保证有丰富解决方案的空间里偶然发现解决方案。所以你必须问。首先，多样性是好的。我们注意到，当我们将多样性纳入遗传算法计算时，我们在寻找解决方案方面要好得多。
        </p>
        <p>And I kept emphasizing, all those choices that enabled him to trick this thing, in some sense, into stumbling
            across the solutions in a space that was guaranteed to be rich with solutions. So you have to ask. so first
            of all, diversity is good. We noticed when we put diversity into the genetic algorithm calculations, we were
            much better at finding solutions.</p>
        <h2 id="unknown-275">未知</h2>
        <h2>Unknown</h2>
        <p>但是，我真心希望你们记住的下一个金星想法是，你们必须问一问功劳在哪里。功劳是程序员的聪明才智还是算法本身的价值？在这种情况下，尽管令人印象深刻，但功劳在于空间的丰富性和程序员的智慧，而不一定在于遗传算法的理念。</p>
        <p>But the next gold star idea that I’d really like to have you go away with is the idea that you have to ask
            where the credit lies. Does it lie with the ingenuity of the programmer or with the value of the algorithm
            itself? In this case, impressive as it is, the credit lies in the richness of the space and in the
            intelligence of the programmer, not necessarily in the idea of genetic algorithms.</p>
        <h1 id="learning-sparse-spaces-phonology">14.学习：稀疏空间、音系学</h1>
        <h1>14. Learning: Sparse Spaces, Phonology</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 47:49 <a
                href="https://youtube.com/watch?v=L73hY1pBcQI">https://youtube.com/watch?v=L73hY1pBcQI</a></p>
        <p> 11 years ago (Jan 11, 2014) — 47:49 <a
                href="https://youtube.com/watch?v=L73hY1pBcQI">https://youtube.com/watch?v=L73hY1pBcQI</a></p>
        <h2 id="unknown-276">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：今天，我们将围绕课堂上要讨论的主题，讨论一些学习奇迹。我们首先讨论了一些基本方法。我们讨论了最近邻。我们讨论了识别树。这些都是已经存在很长时间的基本方法。仍然很有用。</p>
        <p>PATRICK WINSTON: So today we’re gonna talk about a few miracles of learning in the context of the theme that
            we’re developing here in the class. We started off with a discussion of some basic methods. We talked about
            nearest neighbors. And we talked about identification trees. And those are kind of basic things that have
            been around for a long time. Still useful.</p>
        <p>当你面临学习问题而不确定要尝试什么方法时，这仍然是正确的做法。然后我们继续讨论一些简单的生物模仿。我们讨论了神经网络。我们讨论了遗传算法。你看看这些东西，思考并反思我们讨论的内容。你必须问自己，这些想法是毫无价值的吗？也许是愚蠢的想法？
        </p>
        <p>Still the right things to do when you’re faced with a learning problem and you’re not sure what method to
            try. Then we went on to talk about some naive biological mimicry. We talked about neural nets. And we talked
            about genetic algorithms. And you look at those things and you think and reflect back on what we talked
            about. And you have to say to yourself, are these nugatory ideas? Perhaps pistareens?</p>
        <p>或者说，这些想法只是多余的，值得成为焦点？有人知道这些词是什么意思吗？pistareen？pistareen 是西班牙的硬币。它很小，价值不高。神经网络、遗传算法等想法，我把它们归类为
            pistareen，因为让它们做某事就像让狗用后腿走路一样。你可以让它发生，但它们从来都做不好。</p>
        <p>Or are they supererogatory ideas that deserve to be center stage? Does anybody know what those words mean? A
            pistareen? Well, a pistareen is a Spanish coin. It was so small. It was of little worth. These ideas like
            neural nets, genetic algorithms, I classify them as pistareens because getting them to do something is
            rather like getting a dog to walk on its hind legs. You can make it happen, but they never do it very well.
        </p>
        <h2 id="unknown-277">未知</h2>
        <h2>Unknown</h2>
        <p>你必须认为，要实现这一点需要很多技巧和训练。所以个人对这些想法并不看重。但我们还是会教你这些，因为当然，我们只是在部分时间发表社论，而另一部分时间我们喜欢报道该领域的情况。今天，我们将开始讨论一些机制、想法或需要了解的完全不同的事物。
        </p>
        <p>And you have to think it took a lot of trickery and training to make it happen. So not too personally high on
            those ideas. But we teach them to you anyway because, of course, we only editorialize part of time and part
            of time we like to cover what’s in the field. Today we’re starting a couple of discussions of mechanisms or
            ideas or things to know about that are quite different.</p>
        <p>因为现在我们要关注问题而不是机制。稍后我们将讨论深层理论，FIOS，就其本身而言。但本周我想谈谈所设计的机制。我想谈谈所做的研究。我不要说机制。我要说的是所做的研究，旨在尝试解释我们人类擅长的一些事情。</p>
        <p>Because now we’re going to focus on the problem rather than on the mechanism. And then a later on we’re going
            to talk about deep theory, FIOS, for its own sake. But this week I want to talk about mechanisms that were
            devised. I want to talk about research that was done. Let me not say mechanisms. Let me say research that
            was done to attempt an account of some of the things that we humans do well.</p>
        <p>有时我们甚至不知道我们在说这些。现在克里希纳告诉我，他的母语是泰卢固语。泰卢固语。我曾经有另一个学生，他的母语是泰卢固语。我对他说，那一定是那些鲜为人知的印度语言之一。他说，是的。有 5600 万人说泰卢固语。有
            5200 万人说法语。帕特里克·温斯顿：他将成为我们的实验对象。克里希纳，如果我把单词复数化的话。</p>
        <p>Sometimes without even knowing that we do it. Now Krishna here tells me his first language was Telugu.
            Telugu. I once had another student whose first language was Telugu. I said to him, that must be one of those
            obscure Indian languages. And he said, yes. It’s spoken by 56 million people. French is spoken by 52.
            PATRICK WINSTON: He’s going to be our experimental subject. Krishna, if I pluralize words.</p>
        <h2 id="unknown-278">未知</h2>
        <h2>Unknown</h2>
        <p>你知道单词复数的含义。例如，如果我说 horse，那么如果我问你复数，你会说 horses。那么如果我说 dog，它的复数是什么？ 学生：那么是 dogs。或者在我的语言中是这样的？
            帕特里克·温斯顿：不，不，不。在英语中。 学生：哦，是 dogs。 帕特里克·温斯顿：那么 cat 呢？ 学生：是 Cats。 帕特里克·温斯顿：他答对了。这难道不是个奇迹吗？你什么时候开始说英语的？</p>
        <p>You know what it means to pluralize a word. So if I say for example, horse, then if I ask you for the plural
            you’ll say horses. So if I say dog, what’s the plural? STUDENT: Then dogs. Or in my language? PATRICK
            WINSTON: No, no, no. In English. STUDENT: Oh, dogs. PATRICK WINSTON: Well, what about cat? STUDENT: Cats.
            PATRICK WINSTON: And he got it right. Isn’t that a miracle? When did you start speaking English?</p>
        <p>学生：二年级。帕特里克·温斯顿：二年级。但他还是答对了。但他从未意识到他实际上是在用不同的方式复数这些单词。但他确实在用不同的方式复数这些单词。所以当你将 dog 复数化时，后面的音是什么？是 az
            音。Zzzzzz。Dogzzz。如果你把手指伸到这里，你可能会感觉到你的声带在振动。如果你把一张纸放在嘴前，你会看到它在振动。</p>
        <p>STUDENT: Second grade. PATRICK WINSTON: Second grade. But he still got it right. But he never learned that
            he’s actually pluralizing those words differently. But he is. So when you pluralize dog, what’s the sound
            that comes after? It’s a z sound. Zzzzzz. Dogzzz. If you stick your fingers up here you can probably feel
            your vocal cords vibrating. If you stick a piece of paper in front of your mouth you’ll see it vibrate.</p>
        <p>但是当你说 cats 时，复数音是 sss，就像那样。没有发声。声带没有振动。老 Krishna
            和你们其他非英语母语人士一样，毫不费力地、不知不觉地学会了这条规则。你学会了。但你总是说对。怎么可能呢？好吧，到时候你就会知道那可能是怎么回事了。</p>
        <p>But when you say cats, the pluralizing sound is sss, like that. No vocalizing. No vibration of the vocal
            cords. And old Krishna here learned that rule, as did all of you other non native speakers of English,
            effortlessly and without noticing it. You learned it. Buy you always get it right. How can that possibly be?
            Well, be the end of hour you’ll know how that might be.</p>
        <h2 id="unknown-279">未知</h2>
        <h2>Unknown</h2>
        <p>您将体验一个案例研究，了解如何从工程角度来处理此类问题。您可以说，如果上帝是一名工程师会怎样？或者，如果我是上帝，我是一名工程师会怎样？想想事情可能会如何发生。所以我们想了解机器如何学习这样的规则。音系规则。</p>
        <p>And you’ll experience a case study in how questions of that sort can be approached with a sort of engineering
            point of view. You can say, what if God were an engineer? Or alternatively, what if I were God and I am an
            engineer? Think about how it might happen that way. So we want to understand how it might be that the
            machine could learn rules like that. Phonological rules.</p>
        <p>不只是那一条，还包括你在音系学课程中学到的所有音系规则。口语中处理音节和亚音节声音的那部分。语言的音素。所以当 Yip 和 Sussman
            着手解决这个工程问题时，他们都是敬业的工程师，他们做的第一件事就是学习科学。所以他们去坐在 Morris Halle 的脚下，后者将负责开发。</p>
        <p>Not just that one, but all the phonological rules you’d acquire in a course on phonology. That part of
            speaking that deals with those syllabic and sub syllabic sounds. The phones of the language. So when Yip and
            Sussman undertook to solve this engineering problem, both being dedicated engineers, the first thing they
            did was learn the science. So they went to sit at the foot of Morris Halle, who would develop.</p>
        <p>在很大程度上负责了所谓独特特征的发展理论。这一切的运作方式如下。首先，一个人想说点什么。然后从这个人的嘴里发出某种声压波。如果我说，你好，乔治。你也说你好，乔治。每个人都会明白我们说的是同一件事。但那个声波波形看起来一点也不一样。对我们所有人来说，它都是非常不同的。
        </p>
        <p>Was largely responsible for the development theories of so called distinctive features. And here’s how all
            that works. You start off with a person who wants to say something. And out that person’s mouth comes some
            sort of acoustic pressure wave. And if I say, hello, George. And you say hello, George. Everybody will
            understand that we said the same thing. But that acoustic waveform won’t look anything alike. It’ll be very
            different for all of us.</p>
        <h2 id="unknown-280">未知</h2>
        <h2>Unknown</h2>
        <p>所以，单词能够被理解真是个奇迹。无论如何，它进入耳朵。然后被处理。然后输出一系列独特的特征。向量。独特特征是一个二进制变量，就像电话声音一样。也就是说，当你说这句话时，你的声带会振动吗？如果是，那么就是正浊音。如果没有，那就是负浊音。
        </p>
        <p>So it’s a miracle that words can be understood. In any case, it goes into an ear. And it’s processed. And out
            comes a sequence of distinctive feature. Vectors. A distinctive feature is a binary variable like is the
            phone voices or not. That is to say, are your vocal cords vibrating when you say it? If so, then that’s plus
            voiced. If not, it’s minus voiced.</p>
        <p>因此，根据原始区别特征理论，并且与自原始理论以来得出的大多数理论一致，有大约 14 个区别特征可以确定您所说的音素。因此，如果您说啊，那就是这些二进制特征的一种组合。如果您说啊，那就是这些二进制特征的另一种组合。一共有
            14 个。</p>
        <p>So according to the original distinctive feature theory and consistent with most of the theories that have
            been derived since the original one, there are on the order of 14 of these distinctive features that
            determine which phone you’re saying. So if you say ah, that’s one combination of these binary features. If
            you say tuh, that’s another combination of these binary features. 14 of them.</p>
        <p>那么，从理论上讲，这意味着一种语言中可以有多少种声音？塞巴斯蒂安：2 的 14 次方。帕特里克·温斯顿：塞巴斯蒂安，2 的 14 次方是多少？嗯，应该是 16,000 个左右，你不觉得吗？2 的 10 次方是
            1,000。2 的 4 次方是 16。所以可能的组合有 16,000 种。但地球上没有一种语言的音素超过 100 个。这很奇怪，不是吗？</p>
        <p>So how many sounds does that mean, in principle, there could be in a language? SEBASTIAN: 2 to the 14th.
            PATRICK WINSTON: And what’s 2 the 14th, Sebastian? Well, it ought to be about 16,000, don’t you think? 2 to
            the 10th is 1,000.2 the fourth is 16. So there are about 16,000 possible combination. But no language on
            Earth has more than 100 phones. That’s strange, isn’t it?</p>
        <h2 id="unknown-281">未知</h2>
        <h2>Unknown</h2>
        <p>因为其中一些选择可能在物理层面上被排除在外。但大多数选择并非如此。因此，我们的语言中音素的数量可能比实际数量多得多。英语中音素的数量约为
            40。因此，可以将独特特征序列视为在经过一系列漫长的操作后产生意义。但最终，这些操作会在这里产生反馈，因为许多独特特征实际上是幻觉。</p>
        <p>Because some of those choices are probably excluded on physical ground. But most of them are not. So we could
            have a lot more phones in our language than we actually do. English is about 40. So the sequence of
            distinctive features could be viewed as then producing meaning after, perhaps, a long series of operations.
            But in the end, those operations feedback in here because many of the distinctive features are actually
            hallucinated.</p>
        <p>我们以为我们听到了它们，但它们并不在那里。或者它们甚至不在声波中。它们在那里是为了方便语音学家根据它们制定规则。令人惊讶的是，这种反馈的数量如此之多，甚至来自其他模态的注入也是如此。你们中的许多人可能听说过麦格克效应。这就是麦格克效应的作用。看着我说
            ga、ga、ga、ga、ga、ga。好的。我说的是 g a。</p>
        <p>We think we heard them, but they’re not there. Or they’re not even in the acoustic waveform. They’re there
            for the convenience of the phonologist who make rules out of them. It’s remarkable how much of this feedback
            there is, and even injection from other modalities. Many of you may have heard about the McGurk Effect.
            Here’s who the McGurk Effect works. Look at me while I say ga, ga, ga, ga, ga, ga. OK. I said, g a.</p>
        <p>现在，ba、ba、ba、ba 怎么样？好的。我说 ba 就像羊一样。但是，如果我把我说 ba 时发出的声音拿出来，并在你拍摄我说 ga 的视频时播放它，你认为你听到了什么？你听不到 ba。有些人报告说他们听到 ada
            听起来像 da。当我看着它时，我无法理解它。</p>
        <p>Now how about ba, ba, ba, ba. OK. I said ba like a sheep. But if I take the sound I make when I say ba and
            play it while you’re taking video of me saying ga, what do you think you hear? You don’t hear ba. Some
            people report that they hear a d a sound like da. When I look at it, I can’t make any sense out of it.</p>
        <h2 id="unknown-282">未知</h2>
        <h2>Unknown</h2>
        <p>看起来演讲和视频之间似乎存在脱节。但听起来不像是 ba。但如果我闭上眼睛说 ba、ba，那么很明显是 b
            a。所以你看到的内容对你听到的内容有很大的影响。这也很有趣。虽然这只是一个次要问题。同样有趣的是，如果你看不到演讲者，就很难正确发音。</p>
        <p>It looks like there’s a disconnection between the speech and the video. But it does not sound like ba. But if
            I shut my eyes and say ba, ba, it’s absolutely clear that it’s b a. So what you see has a large influence on
            what you hear. It’s also interesting. although a side issue. it’s also interesting to note that it’s very
            difficult pronounced things correctly if you don’t see the speaker.</p>
        <p>很多人在学习外语时都想知道为什么他们不能像母语人士一样说话。答案是，他们没有注意说话者的嘴巴。有一次我和一个德国朋友聊天，我说，你知道，我就是不能正确地发出该死的变音。他说，哦，你们美国人的问题是，你们没有意识到美国的牛是哞哞叫，而德国的牛是哞哞叫。
        </p>
        <p>So many people wonder when they learn foreign languages why they can’t speak like a native. And the answer
            is, they’re not watching the mouth of the speaker. I was talking to a German friend once and said, you know,
            I just can’t say the damned umlaut right. And he said, oh, the trouble with you Americans is you don’t
            realize that American cows say moo but German cows say muu.</p>
        <p>帕特里克·温斯顿：当然，我立刻就明白了，因为我看到变音符号的发音需要突出嘴唇，而英语中没有任何声音需要突出嘴唇。啊，但回到我们从语音学家那里了解到的关于所有这些事情。如果你和莫里斯·哈勒谈谈，他会告诉你的。我喜欢把它想象成一个木偶。这里有五块肉。
        </p>
        <p>PATRICK WINSTON: And, of course, I got instantly because I could see that the umlaut sounds are produced with
            protruding lips, which we don’t have any sounds an English that require that. Ah, but back to what we know
            from the phonologists about all this stuff. If you talk to Morris Halle, he will tell you that over here. I
            like to think of it as a marionette. There are five pieces of meat down here.</p>
        <h2 id="unknown-283">未知</h2>
        <h2>Unknown</h2>
        <p>而您要说出的这些独特特征的组合就像是木偶对这五块肉的控制。所以，如果您想发出一个 a 音，木偶控制器就会进入产生该组合的位置。让我们看看。典型单词的独特特征序列是什么样的？好吧，这是一个单词。A ep
            l。Apples。</p>
        <p>And the combination of distinctive features that you’re trying to utter are like the control of a marionette
            on those five pieces of meat. So if you want to say an a sound, the marionette control goes into a position
            that produces that combination. So let’s see. What does that distinctive feature sequence look like for
            typical word? Well, here’s a word. A e p l. Apples.</p>
        <p>我们可以讨论一下这种特殊的音素组合具有哪些独特的功能。他们喜欢讨论的功能之一就是音节。音节。这大致意味着，那个声音能形成音节的核心吗？答案是可以，但不能。所以是加号、减号、减号、减号。再往下一点，你就会遇到浊音功能。
        </p>
        <p>And we can talk about what distinctive features are arrayed in that particular combination of phones. So one
            of the features that they like to talk about is syllabic. Syllabic. That roughly means, can that sound form
            the sort of core of a syllable? And the answer is a can, buy these can’t. So it’s plus, minus, minus, minus.
            Down here a little ways you’ll run into the voiced feature.</p>
        <p>至于浊音特征，我们可以自己做实验。啊。听起来像是声音。啪。不。那不是浊音。哦。是的。Zzz。我们已经说过那是浊音。所以这就是你在发出苹果声时看到的组合，表示浊音特征。然后另一个是持续音。这大致说明你的发声器官是否畅通？没有阻塞吗？所以啊加啪是收缩的。哦，畅通。Zzz，畅通。
        </p>
        <p>And for the voiced feature, well, we can do the experiment ourselves. Ahh. Sounds like it’s voices to me. Pa.
            No.&nbsp;That’s not voiced. Oo. Yep. Zzz. We already said that was voiced. So that’s the combination you see
            when you utter apples for the voiced feature. Then another one is the continuent one. That roughly says is
            your vocal apparatus open? Is there no obstruction? And so ahh plus pa is constricted. Oo, open. Zzz, open.
        </p>
        <h2 id="unknown-284">未知</h2>
        <h2>Unknown</h2>
        <p>所以，这个音恰好与那个特定单词中的浊音相符。哦，一共有 14 个。但是让我再写一个。尖锐音。它说的是，你用舌头形成一小股气流吗？所以你不会在 aa、pa、oo 上发这个音。但你会在 z 上发这个音。所以这是一个加分项。
        </p>
        <p>So that one happens to run right along with voiced in that particular word. Oh, and there are 14 altogether.
            But let me just write down one more. The strident one. That says, do you use your tongue to form a little
            jet of air? So you don’t on aa, pa, oo. Buy you do on z. So that gets a plus.</p>
        <p>所以，我们可以通过吸管瞥见苹果这个词，它是一组按顺序排列的独特特征。所以它是一个特征矩阵。沿着列向下看，我们有独特的特征。沿着列向下看，我们有时间。</p>
        <p>So that’s a glimpse through a soda straw of what it would like to represent the word apples as a set of
            distinctive features all arranged in a sequence. So it’s a matrix of features. Going down in the columns we
            have our distinctive features. And going across we have time.</p>
        <p>因此，为了理解语音规则是如何学习的，Sussman 和 Yip
            做的第一件事就是设计一台机器，它可以解释单词、声音和你看到的东西，从而产生语言的声音。所以他们设想了以下这种机器。这台机器上有一个神秘的装置，可以观察世界，看看那里有什么。</p>
        <p>So as the first thing Sussman and Yip did in their effort to understand how phonological rules could be
            learned is to design a machine that would interpret words and sounds and things that you see so as to
            produce the sounds of the language. So they imagined the following kind of machine. The machine has some
            kind of mystery apparatus over here that looks out into the world and sees what’s there.</p>
        <h2 id="unknown-285">未知</h2>
        <h2>Unknown</h2>
        <p>所以我环顾四周，看到了两个苹果。那么这台机器可能会在某个时候决定那里有两个苹果。然后，从这些计算机工程师的角度来看，他们会想到一组寄存器，这些寄存器保存名词、动词和复数等概念的值。我们还没有对机器做任何事情。我们没有提供任何输入。
        </p>
        <p>So I’m looking out in the world and I see two apples. So what this machine might do then is, at some point,
            decide that there are two apples out there. Then, thinking in terms of these guys as computer engineers,
            they think in terms of a set of registers that hold values for concepts like noun and verb and plural. And
            we’ve not done anything with the machine yet. We’ve provided no input.</p>
        <p>所以这些寄存器都是空的。然后，在这里，我们有一组单词。它们是各种各样的单词。Apple
            就是其中之一。上面的这些单词知道如何将概念呈现为一系列手机，即一系列不同的功能。然后，在这里，最重要的是，它们有一组约束。所以我们将讨论一个特定的约束，即复数约束。复数约束一号。</p>
        <p>So those registers are all empty. Then, up in here, we have a set of words. And they’re all kinds of words.
            Apple is one of them. And those words up there know about how the concept is rendered as a sequence of a
            phones, that is to say a sequence of distinct features. Then, over here, most importantly, they have a set
            of constraints. So we’ll talk about a particular constrain, the plural constraint. Plural constraint number
            one.</p>
        <p>它会四处走动，连接到机器的其他部分。最后，有一个要发出的音素缓冲区。它们会以这种方式流出到说话者的嘴里，并被转换成声波形式。这些就是机器的元素。那么这些元素是如何连接在一起的呢？</p>
        <p>And it’s going to reach around and connect itself to some other parts of the machine. Finally, there’s a
            buffer of phones to be uttered. And they’re going to flow out this way to the speaker’s mouth and get
            translated into a acoustic wave form. So those are the elements of the machine. Now how are the elements
            connected together?</p>
        <h2 id="unknown-286">未知</h2>
        <h2>Unknown</h2>
        <p>嗯，当然，单词连接到用于生成最左边声音的缓冲区。复数寄存器与你在世界上看到的东西有关。你在世界上看到的东西不仅与复数寄存器有关，还与单词库中的所有对象有关。</p>
        <p>Well, the words are connected, of course, into the buffer that is used to generate the sound over here on the
            far left. The plural register is connected to what you see in the world. What you see in the world is
            connected not only to plural register, but to all of the objects in the word repertoire.</p>
        <p>此处的复数约束值得特别注意，因为它将渴望在事件中自我启动，但世界上观察到的事物是复数。它们有很多。因此它将连接到复数端口。下面将有一个声音端口连接到缓冲区中的文件元素。</p>
        <p>This plural constraint here deserves extra attention because it’s going to be desirous of actuating itself in
            the event but the thing observed in the world is plural. There are lots of them. So it’s going to be
            connected then to the plural port. There’s going to be a z sound port down here connecting to that file
            element in the buffer.</p>
        <p>最后，这里将是一个带加号的浊音端口，它将连接到序列中的第二个音素。这就是机器的排列方式。当然，这只是众多约束之一。但这个约束有一个非常特殊的属性。信息可以通过多种方式流过它。所以我们认为大多数程序都有输入和输出。
        </p>
        <p>And finally, over here is going to be a plussed voiced port, which is going to be connected to the second
            phoneme in the sequence. That’s how the machine is going to be arranged. An of course, this is just one of
            many constraints. But it’s a constraint that has a very peculiar property. Information can flow through it
            in multiple ways. So we think of most programs as having an input and an output.</p>
        <h2 id="unknown-287">未知</h2>
        <h2>Unknown</h2>
        <p>但我尽量小心地在这里画圆圈而不是箭头。因为这些是端口，信息可以沿着它们向任何方向流动。我现在想做的是向你们展示如果我突然给这台机器一对苹果，它会如何反应。因此，假设视觉装置进入并产生两个苹果的概念。一旦发生这种情况。
        </p>
        <p>But I try to be careful to draw circles here instead of arrows. Because these are ports and information can
            flow in any direction along them. What I want to do now is to show you how this machine would react if I
            suddenly present it with a pair of apples like so. So the assumption is that the vision apparatus comes in
            and produces the notion, the concept, of two apples. So once that has happened.</p>
        <p>这是操作一。然后信息从意义寄存器流到这里，到达苹果这个词。所以这是第二阶段的一部分。第二阶段的另一部分是信息沿着这条线流动，并将其标记为加复数。所以操作一是视觉系统的活动。活动二是信息从视觉系统流到词汇表和复数寄存器。到目前为止一切顺利。
        </p>
        <p>That’s operation number one. then information flows from that meaning register up here to the apple word. So
            that’s part of stage number two. Another part of stage number two is information flows along this wire and
            marks that as plus plural. So operation number one is the activity of the vision system. Activity number two
            is the flow of information from that vision system into the word lexicon and into this plural register. So
            far so good.</p>
        <p>这是活动 3。这个单词也连接到寄存器。信息沿着这些线路流动，以表明它是一个名词而不是动词。这是部件 3 的一部分。同时，部件 3，信息沿着这条线路流动，并将 apl 写入缓冲区的元素中。</p>
        <p>Here’s activity number three. This word is also connected to the registers. And information flows along those
            wires so as to indicate that it’s a noun but not a verb. That’s part of part number three. At the same time,
            part number three, information flows down this wire and writes a p l into those are elements of the buffer.
        </p>
        <h2 id="unknown-288">未知</h2>
        <h2>Unknown</h2>
        <p>现在，这里的这个约束，这个框，说，好吧，我现在可以在该缓冲区中看到一些以前不存在的东西。所以它说，我是否在我的端口上看到了足够多的东西，以至于可以兴奋地表达其他端口上的值？好吧，让我们看看。它有什么？它有这个缓冲区中的元素。同样，在步骤三中，这里流动的是复数。所以它知道这个词是复数。
        </p>
        <p>Now this constraint up here, this box, says, well, I can now see some stuff in that buffer that wasn’t there
            before. So it says, do I see enough stuff on my ports to get excited about expressing values on other ports?
            Well, let’s see. What has it got? It’s got the elements in this buffer. Also up here in step three flow the
            plural thing. So it know that the word is plural.</p>
        <p>所以它会说，这是有声的吗？P 是 pa。那不是有声的。这是 az 声吗？不，那不是 z
            声。所以它只能在三个端口中的一个上看到它喜欢的东西。所以它会说，我不打算做任何事情。我没有参加这场特定的战斗。到目前为止一切顺利。接下来会发生什么？接下来发生的事情是一段时间过去了。</p>
        <p>So it says, is this voiced? P is pa. That’s not voiced. Is this a z sound. No, that’s not as z sound. So it
            sees what it likes on only one of its three ports. So it says, I’m not going to do anything. I’m not in this
            particular combat. So far so good. What happens next? What happens next is that some time passes.</p>
        <p>缓冲区的元素向左流动，朝向说话者的嘴部。所以我们得到了 a、p、l。和之前一样，但移位了。现在会发生什么？现在发生的事情是 l
            现在处于倒数第二的位置。所以信息在这里流动。项目四。哦，我猜那是项目五。项目四是单词的左流。</p>
        <p>And the elements of the buffer flow to the left toward the speaker’s mouth. So we get an a, p, l. Same as we
            had before, but shifted over. Now what happens? Now what happens is that the l is now in the penultimate
            position. So information flows up here. Item number four. oh, I guess that’s item number five. Item number
            four is the leftward flow of the word.</p>
        <h2 id="unknown-289">未知</h2>
        <h2>Unknown</h2>
        <p>因此，在阶段 5 中，p 由该约束见证。p 是。抱歉，l 由该约束见证。我们将其移过 1。L 是 lll。L 是浊音。因此，我们这里有一些这样的流程。这是阶段
            5。现在我们有浊音和复数。这里什么都没有。因此，这个缓冲区非常希望写入一些东西。</p>
        <p>So in phase number five, the p is witnessed by this constraint. p is. sorry, l is witnessed by this
            constraint. We moved it over one. L is lll. L is voiced. So we have some flow up here like that. That’s
            number five. Now we have voiced and we have plural. And we have nothing here. So there’s a great desire of
            this buffer to have something written into it.</p>
        <p>所以现在那里有一个向下流动的
            z，即项目编号六。这就是机器表达视野中有苹果的想法的工作原理。嗯。真正的苹果。不是塑料仿制品。这就是机器的工作原理。但所有这些连接都是可逆的。所以如果我听到苹果，我就让机器反向运行，我的视觉器官就可以想象那里有苹果。
        </p>
        <p>So now there’s a flow down in there, of z, as item number six. So that’s how the machine would work in
            expressing the idea that there are apples in the field of view. Mmm. Real apples. Not plastic imitations. So
            that’s how the machine works. But all those connections are reversible. So if I hear apples then I get the
            machine running backwards and my visual apparatus can imagine that there are apples out there.</p>
        <p>这就是它的工作原理。这只是他们一旦学会音系规则就能看到机器使用的背景知识。所有音系规则都表达在这些约束中。但由于这些约束使得信息可以向任何方向流动，因此它们应该被称为传播器。在每个人都学习 6.001
            的美好旧日时光里，他们学习了传播器作为构建复杂系统的一种架构。</p>
        <p>That’s how it works. That’s just by way of background the machine that they could see it for using the
            phonological rules once they’re learned. All the phonological rules are expressed in these constraints. But
            since these constraints are such that information can flow in any direction, they deserve to be called
            propagators. And in the good old days when everyone took 6.001, they learned about propagators as a kind of
            architecture for building complex systems.</p>
        <h2 id="unknown-290">未知</h2>
        <h2>Unknown</h2>
        <p>但无论如何，有 Sussman Yip 机器。现在有一个大问题。你如何学习这样的规则规则？好吧，我们需要的是一些正面例子和一些负面例子。对于简单的课堂例子，我选择了向 Krishna
            提出的相同挑战。我们将有猫和狗。所以我们将研究与这些词相关的显着特征。音节。浊音。连续音。和尖锐音。</p>
        <p>But in any event, there’s the Sussman Yip machine. And now comes the big question. How do you learn rule
            rules like that? Well, what we need is we need some positive examples and some negative examples. And for
            the simple classroom example I’ve chosen the same challenge that I presented to Krishna. We’re gonna have
            cats and dogs. So we’re gonna look at the distinctive features that are associated with those words.
            Syllabic. Voiced. Continuent. And strident.</p>
        <p>这些单词的每个声音都与 14 个特征相关，只有 4 个特征。请您关上笔记本电脑好吗？仅作为示例，列出这些单词中排列的独特特征。所以这里我们有 kat
            z。语音拼写。如果我们搞清楚了，让我们看看。什么是音节？那不是。那是。那是。那不是。浊音？Ka。不。啊。是的。T。不。Z。是的。那不可能。猫。</p>
        <p>Just four of the 14 features that are associated with each of the sounds on those words. Could you close the
            laptop, please? Just for the distinctive features that are arrayed in those words by way of illustration. So
            here we have k a t z. Phonetically spelled. And if we work that out, let’s see. What is syllabic? That’s
            not. That is. That is. That’s not. Voiced? Ka. Nope. Ah. Yep. T. Nope. Z. Yes. That can’t be right. Cats.
        </p>
        <p>我拼错了。因为猫。嘶嘶声。这是嘶嘶声，但没有浊音。所以这不是 z 音。这是 s 音。所以这不是加浊音。这是减浊音。继续。让我们看看。我说 k
            时嘴巴张开吗？不。啊？是的。T？不。S？是的。而且尖锐。减号、减号、减号、加号。只有发 s 音时我的舌头才会形成那种喷射。现在我们可以看看狗了。</p>
        <p>I misspelled it. Because cats. Sss. His a hissing sound but there’s no voicing. So that’s not as z sound.
            That’s an s sound. So that’s not plus voiced. It’s minused voiced. Continuent. Let’s see. Is my mouth open
            when I say k? No.&nbsp;Ah? Yes. T? No.&nbsp;S? Yes. And strident. Minus, minus, minus, plus. It’s only with
            the s sound that I have that kind of jet forming with my tongue. Now we can look at dogs.</p>
        <h2 id="unknown-291">未知</h2>
        <h2>Unknown</h2>
        <p>现在我们有了 z 音作为复数。我们知道这一点，因为当我们说 dogzz 时。是的。它读出来是
            a。我们只看最后两列，因为它们是我们唯一关心的。所以那是加号。那是减号。Gu，gu，gu，gu。那是加号。那是加号。它们都是浊音。对吗？Dogu？Gu。Gu。g 音是浊音吗？</p>
        <p>And now we have the z sound as the pluralization. We know that because when we say it, dogzz. Yep. There it
            comes out as a. we’re only gonna look at the last two columns because they’re the only ones that are going
            to matter to us. So that’s plus. And that’s minus. Gu, gu, gu, gu. That’s plussed. And that’s plussed.
            They’re both voiced. Is that right? Dogu? Gu. Gu. Is g sound voiced?</p>
        <p>是的，我不这么认为。G 音是浊音？你看。哦。哦，它是浊音，但它不是连音。就是这样。是的。Cat，dogu
            zz。是的。它是浊音。而且它必须是浊音，我的例子才能成立。那就是减号、减号、减号、加号。所以我们感兴趣的是，为什么一个词是 s 音，而另一个词是 az 音？</p>
        <p>Yeah, I didn’t think so. G sound is voiced? Look. oh. Oh, it is voiced buy it’s not a continuent. Just like
            that. Yeah. Cat, dogu zz. Yeah. It is voiced. And it has to be for my example to work out. And that’s minus,
            minus, minus, plus. So what we’re interested in is, how come one word gets an s sound and how come the other
            words gets a z sound?</p>
        <p>嗯，这个空间相当稀疏。我们已经确定有 14,000 个可能的音素，而该语言中只有 40
            个。所以这是我们可以考虑的一件事。我们可以想到的另一件事是，嗯，也许这是一个逻辑问题。就像你在设计计算机时会遇到的那种问题。因此，Sussman 和 Yip 花了三个月的时间以这种方式思考这个问题。</p>
        <p>Well, it’s a pretty sparse space out there. We’ve already decided that there are 14,000 possible phonemes and
            there are only 40 in the language. So that’s one thing we can consider. The other thing that we can think is
            that, well, maybe this is a logical problem. Like the kind of problem you’d face if you were designing a
            computer. And so Sussman and Yip got stuck for three months thinking about the problem that way.</p>
        <h2 id="unknown-292">未知</h2>
        <h2>Unknown</h2>
        <p>根本无法取得任何进展。当你搜索时，这种情况经常发生。你认为你已经找到了解决方法。试着让它奏效。你熬夜。又熬夜。还是没用。最后，你放弃了，尝试别的方法。于是他们开始说，好吧，让我们看看。我们关心的只是两个结尾音之前的内容。
        </p>
        <p>Couldn’t make any progress whatsoever. And that happens a lot when you’re doing a search. You think you’ve
            got a way of approaching it. Try to make it work. You stay up all night. Stay up all night again. Still
            can’t make it work. Eventually, you abandon ship and try something else. So then they began to say, well,
            let’s see. All we care about is the stuff before the two ending sounds.</p>
        <p>我们关心矩阵的这一部分。我们关心矩阵的这一部分。我们可以问，这些东西在哪些方面不同？它们在各个方面都不同。这就是为什么它们是不同的词。我们可以用稍微不同的方式来问这个问题。我们可以说，我们可以不关心什么？</p>
        <p>We care about that part of the matrix. And we care about that part of the matrix. And we can ask, in what
            ways are those things different? And they’re different all over the place. That’s why they’re different
            words. We can ask the question a little bit differently. And we can say, what can we not care about?</p>
        <p>并且仍然保留对单词差异的足够理解，以便为它们加上适当的复数结尾。他们为此担心了很长时间。找不到解决方案。搜索空间太大了。然后他们说，也许我们应该考虑在这里概括这个家伙，这样我们就不会关心它了。所以现在我们不关心那个家伙了。
        </p>
        <p>And still retain enough of an understanding of how the words are different so as to put the proper plural
            ending on them. And they worried about that for a long time. Couldn’t find a solution. The search space was
            too big. And then they said, maybe what we ought to do is we ought to think about generalizing this guy here
            so that we don’t care about it. So now we don’t care about that guy.</p>
        <h2 id="unknown-293">未知</h2>
        <h2>Unknown</h2>
        <p>然后他继续说道，好吧，让我们看看什么时候我们不得不停止概括。因为我们把一切都搞砸了，我们再也无法将 z 音词与 s
            音词区分开来。所以最终归结为以下算法。他们做的第一件事是收集正面和负面的例子。有一个正面的例子和一个负面的例子。这还不足以正确完成。</p>
        <p>And then he went down through here saying, well, let’s see when we have to stop generalizing. Because we’ve
            screwed everything up and we can no longer keep the z sound words separated from the s sound words. So that
            eventually distilled itself down to the following algorithm. First thing they did was to collect positive
            and negative examples. And there’s a positive example and a negative example. That’s not enough to do it
            right.</p>
        <p>但这足以说明这个想法。所以他们接下来做的事情是学习任何东西时非常常见的。那就是选择一个正面的例子作为开始。从正面的例子开始学习任何东西其实都不是一个坏主意。所以他们选了一个正面的例子，并称之为种子。所以在我们的具体例子中，猫就是我们的种子。
        </p>
        <p>But that’s enough to illustrate the idea. So the next thing they did was something that’s extremely common in
            learning anything. And that is to pick a positive example to start from. It’s actually not a bad idea in
            learning anything to start with a positive example. So they picked a positive example and they called that a
            seed. So in our particular case, cats is going to be our seed.</p>
        <p>我们要问的问题是，像 cat 这样的复数词有哪些？所以我们有一个正例和一个反例。我们选了一个种子。现在，下一步是概括。我所说的概括是指你在音素矩阵中选择一些你不关心的地方。所以你可以选择一个正例。你不关心它。</p>
        <p>And the question we’re going to ask is, what are the words that get pluralized like cat? So we’ve got a
            positive and negative example. We’ve picked a seed. And now, the next step is to generalize. And what I mean
            by generalize is you pick some places in the phoneme matrix that you just don’t care about. So you may pick
            a positive example. And you don’t care about it.</p>
        <h2 id="unknown-294">未知</h2>
        <h2>Unknown</h2>
        <p>因此，您可以将其更改为星号，或者，正如我将要向您展示的程序中所示，将其更改为球。或者，您可以选择一个负数，然后将其转换为球。Bo。因此，cats，这个种子，变成了一个模式。为了以这种方式将单词复数化，您必须匹配这里的所有内容。
        </p>
        <p>So you change it to an asterisk or, as demonstrated in the program I’m about show you, a ball. Or you pick
            one that’s negative and you turn it to a ball. Bo. So cats, this seed, becomes a pattern. And in order to
            pluralize the word this way, you have to match all the stuff in here.</p>
        <p>但现在我们要做的就是逐渐将其中一些元素变成不关心符号，直到我们不再关心太多东西，以至于我们认为我们也用 s
            音将其复数化。所以我们继续概括，直到我们覆盖，也就是说我们承认或匹配一个反面例子。这就是它的工作原理。所以我们疯狂地概括。</p>
        <p>But now what we’re going to do is we’re going to gradually turn some of those elements into don’t care
            symbols until we get to a point where we’ve not cared about so much stuff that we think that we pluralize
            that one with an s sound too. So we keep generalizing until we cover, that is to say we admit or match, a
            negative example. So that’s how it works. So we generalize like crazy.</p>
        <p>一旦我们覆盖了一个反面例子，我们就停止。否则，我们只需回到这里并进一步概括。现在我们必须选择一种搜索技术来决定何时真正概括其中的哪一个。我们可以随机选择一个。他们试过了。没有成功。所以他们决定，对复数影响最大的是相邻的音素。
        </p>
        <p>And as soon as we cover a negative example, we quit. Otherwise, we just go back up here and generalize some
            more. And now we’ve got to pick a search technique to decide which of these guys to actually generalize
            when. We could pick one at random. And they tried that. It didn’t work. So what they decided is that the
            thing that influences the pluralization most is the adjacent phoneme.</p>
        <h2 id="unknown-295">未知</h2>
        <h2>Unknown</h2>
        <p>如果这不是解决问题的方法，那么下一个方法就是。换句话说，你越接近，就越有可能决定结果。所以这边的这些因素最不可能产生影响。而这些因素是首先被概括的。如果我们这样做，会发生什么？</p>
        <p>And if that isn’t the thing that solves the problem, it’ll be the one next to that. So in other words, the
            closer you are, the more likely you are to determine the outcome. So these guys over here are least likely
            to matter. And those are the ones that are generalized first. So if we do that, what happens?</p>
        <p>看起来我们将会在这里看到非浊音 t 和浊音 g 之间的巨大差异。但这只是猜测，因为我只向你们展示了 14 个不同特征中的一小部分。所以我想你们想看演示。是的。这就是我们的 14 个特征。</p>
        <p>Looks like we’re going to come in here and see that there’s a big difference between the non voiced t and the
            voiced g. But that’s only a guess because I’ve only shown you a fraction of the 14 distinctive features that
            are involved. So I suppose you like to see a demonstration. Yeah. So there’s our 14 features.</p>
        <p>这就是我们的种子，它醒目地显示在显示屏上，带有加号和减号，表示所有三部手机的独特特征值。那个有趣的左括号不是错误。这只是在 cat 中渲染 ah
            音的一种惯例。因此，仅从该矩阵很难判断将正面示例与负面示例区分开来的决定性特征是什么。</p>
        <p>And that’s our seed there, sitting prominently in the display with pluses and minuses indicating the values
            of the distinctive features for all three of the phones involved. That funny left bracket isn’t a mistake.
            That’s just one convention for rendering the ah sound in cat. So it’s pretty hard to tell from just that
            matrix what’s going to be the determining feature that separates the positive examples from the negative
            examples.</p>
        <h2 id="unknown-296">未知</h2>
        <h2>Unknown</h2>
        <p>您会注意到，下面有两个例子。一个是 cat ，另一个是 duck 。ducks 发 s 音吗？Ducks ？是的。所以 dogs 和 ducks 都以 s 音来复数。然后是 beach ，也就是 beach 。Dog
            。我们知道那是 z 。Gun 。Gunz 。所以那不属于这个组。所以我们可以进行这个实验。现在我们开始。我们疯狂地进行概括。概括、概括、从左到右进行概括。</p>
        <p>You notice that there are actually two examples down here. There’s cat and duck. Is ducks got an s sound?
            Ducks? Yep. So dogs and ducks. They both get pluralized with an s sound. And then we have beach doesn’t.
            That’s beaches. Dog. We know that’s a z. Gun. Gunz. So that’s not in the group. So we can run this
            experiment. Now here we go. We’re generalizing like crazy. Generalizing, generalizing, generalizing from
            left to right.</p>
        <p>因此，前两列中的单词无关紧要。现在我们来看看 t。哇。就是这样。所以看起来你用 as
            音来复数。sss。当且仅当，你没有发浊音，而且在倒数第二个音中你没有发出刺耳的声音。在你试图复数化的单词的最后一个音素中。所以这是系统学到的一条音系规则。你猜怎么着？</p>
        <p>So nothing in the first two columns matters. Now we get to the t. Wow. There it is. So it looks like you
            pluralize with a s sound. The sss. If, and only if, you’re not voiced and you’re not strident in the second
            to the last. in the last phone of the word that you’re trying to pluralize. So that’s one phonological rule
            that the system has learned. And guess what?</p>
        <p>这与语音学教科书中的规则相同。现在我们可以尝试另一个实验。这次我们尝试处理狗和枪。我们的负数是之前的正数加上海滩，海滩仍然作为负数存在。让我们看看这个实验如何进行。除了最后一列，最后一个电话，其他都无关紧要。</p>
        <p>It’s the same rule that’s found in phonological textbooks. So now we can try another experiment. So this time
            we’re trying to deal with dog and gun. And our negatives are what was previously positive plus beach, which
            is still in there as a negative example. So let’s see how that one works. Nothing matters except for the
            last column, the last phone.</p>
        <h2 id="unknown-297">未知</h2>
        <h2>Unknown</h2>
        <p>现在我们发现，如果最后一个音是浊音，那么复数形式就会变成 z 音，即浊音限定词。最后，我们来处理 beach。这是 beach 的有趣拼音。所以现在，如果单词的最后一个音是 strident，如果它有 jetty
            音。beach。Beach。那么它就会变成 ea 音。让我们回到第一个实验。</p>
        <p>And now we find out that if the last sound is voiced, then the pluralization gets the z sound, a voiced
            determinator. And finally, just to deal with beaches. That’s beach in it’s funny phonetic spelling. So now,
            if the final sound in the word is strident, if its got this jetty sound. beach. Beach. Then it gets the ea
            sound. So let’s go back to experiment number one.</p>
        <p>因为我想指出这个工作方式的一个小问题。您会注意到，它在左下角讨论覆盖和排除。排除，嗯，有三个负面例子，所以最好全部排除。您不想覆盖任何负面。但它说覆盖，两个和两个。那是因为它实际上正在做。现在我们有了快速说出来的词汇。
        </p>
        <p>Because I want to point out one small thing about the way this works. You’ll notice that it talks about
            coverage and excluded down here in the lower left hand corner. Excluded, well, there are three negative
            examples, so they better all be excluded. You don’t want to cover any of the negatives. But it says
            coverage, two and two. That’s because it actually is doing. and now we have the vocabulary to say it
            quickly.</p>
        <p>它正在对这个空间进行束搜索。因此，它不仅仅是进行深度优先搜索。它进行束搜索是为了减少忽略解决方案的可能性。所以它说，哦，覆盖范围。两个束搜索元素都覆盖了两个正面例子。事实上，它们已经收敛到同一个解决方案。这就是
            Sussman 和 Yip 的工作原理。</p>
        <p>It’s doing a beam search through this space. So it’s not just doing a depth first search. It’s doing a beam
            search so as to reduce the possibility of overlooking a solution. So it says, oh, the coverage. Both of the
            beam search elements cover both of the positive examples. And they, in fact, have converged to the same
            solution. So that’s how the Sussman and Yip thing worked.</p>
        <h2 id="unknown-298">未知</h2>
        <h2>Unknown</h2>
        <p>那么接下来要问的问题当然是，为什么它能起作用？答案正如 Sussman 和 Yip 所阐述的那样。或者说 Sussman 的观点更多。或者说 Yip 的观点更多，Sussman 的观点略少。Yip
            认为它之所以起作用，是因为它是一个稀疏空间。</p>
        <p>And then the next question to ask is, of course, why did it work? And so the answer, as articulated by
            Sussman and Yip. or rather more by Sussman. Or rather more by Yip and a little bit less by Sussman. Yip
            thinks that it worked because it’s a sparse space.</p>
        <p>当你有一个高维稀疏空间时，很容易将超平面放入空间中，将一组示例与另一组示例分开。让我们考虑以下情况。假设我们有一个一维情况。我们有两个白色示例和两个紫色示例。好吧，我们无法将它们分开，这对我们来说太糟糕了。现在假设这实际上是一个二维空间的投影，看起来像这样。
        </p>
        <p>And when you have a high dimensional sparse space, it’s easy to put a hyperplane into the space to separate
            one set of examples for another set of examples. So let’s consider the following situation. Suppose we have
            a one dimensional situation. And we have two white examples and we have two purple examples. Well, too bad
            for us you can’t separate them. Now suppose that this is actually the projection of a two dimensional space
            that looks like this.</p>
        <p>下面是白色的例子。上面是紫色的例子。现在很容易看出，你可以用一条线将它们分开。现在我们再进一步，假设这实际上是三维空间的投影。它看起来像这样。这将是第一维度。回到那里，这将是第二维度。这将是这里的第三维度。</p>
        <p>Here are the white examples down here. And here are the purple examples up here. Now it’s easy to see that
            you can separate them with just a line that goes across like that. Now let’s take this one more step and
            suppose that this is actually a projection of a three dimensional space. It looks like this. This will be
            dimension one. This’ll be two going back there. And this will be three up here.</p>
        <h2 id="unknown-299">未知</h2>
        <h2>Unknown</h2>
        <p>假设正例就在这条线上。假设是这样。好吧，我们要画一个小立方体，像这样。那些是上面的紫色例子。有多少种方法可以沿着这些轴划分空间？嗯，现在它们甚至不只是两种。它们是三种。所以将紫色与白色分开的一种方法是绘制一个超平面。
        </p>
        <p>And suppose that the positive examples are right here on this line. Let’s say this is. well, we’re gonna draw
            a little old cube like so. Those are purple examples that are up there. How many ways are there of
            partitioning the space along those axes? Well, now they’re not even just two. They’re three. So one way to
            separate the purple from the white is to draw a hyperplane.</p>
        <p>或者在这种情况下，它是三维的，所以是一个平面。通过这里的三号轴。你也可以在那个轴上放一个平面。或者你可以同时做这两件事。所以在一种情况下，你的分界线将是。让我们看看。在第一个轴上，那将是
            1/2。然后是无所谓。无所谓。另一个解决方案是无所谓。</p>
        <p>Or in this case it’s a three dimension, so a plane. through here on the number three axis. You could also put
            a plane in on that axis. Or you could do both. So in one case your dividing line would be. let’s see. On the
            first axis that would be 1/2. And then the don’t care. Don’t care. Another solution that would be don’t
            care.</p>
        <p>然后我们在 2 号轴上用 1/2 处的平面进行划分，并且不关心。或者我们可以用 1/2,1/2 进行划分，并且不关心。因此，空间的维度越高，有时就越容易放入一个平面来分隔数据。这就是为什么 Sussman 和 Yip
            认为我们使用的音素空间如此之少。因为它使事物变得可学习。这是一种可能性。</p>
        <p>And then we divide on the number 2 axis with a plane at 1/2 and don’t care. Or we could do it with 1/2,1/2,
            and don’t care. So the higher the dimension of the space, the easier it is sometimes to put in a plane that
            separates the data. That’s why Sussman and Yip think that we use so little of possible phoneme space.
            Because it makes the thing learnable. That’s one possibility.</p>
        <h2 id="unknown-300">未知</h2>
        <h2>Unknown</h2>
        <p>因此，稀疏空间的一个解释是可学习性。还有另一种有趣的可能性，那就是如果你有一个稀疏空间，即具有 14 个维度的高维空间，并且如果你的语言的 40
            个点均匀分布在该空间中。现在让我换一种说法。如果它们随机放置在该空间中，那么根据中心极限定理，它们之间的距离大致相等。</p>
        <p>So one explanation for sparse space is learnability. There’s another interesting possibility, and that is
            that if you have a sparse space, high dimensional space with 14 dimensions, and if the 40 points of your
            language are spread evenly throughout that space. now let me say it the other way. If they are placed at
            random in that space, then according to the central limit theorem, then they’ll be about equally distant
            from each other.</p>
        <p>因此，它可以确保在说话时音素很容易分离。但如果你去问语言学家这是否属实，他们也不知道。因为他们没有从计算的角度来看待它。好吧，我们可以从计算的角度来看待它。所以我这样做了。在 Sussman 和 Yip
            发表他们的论文之后。这是结果。</p>
        <p>So it ensures that the phonemes are easily separated when you speak. But if you go to ask a linguist if
            that’s true, they don’t know. Because they’re not looking at it from a computational point of view. Well, we
            can look at it from a computational point of view. So I did that. After Sussman and Yip published their
            paper. And here’s the result.</p>
        <p>这个图表显示了所有音素，它们之间只存在一个区别性特征。所以如果你看这个角落，你会看到常数 w 和 x
            之间只存在一个区别性特征。所以它们在空间中彼此距离并不远。另一方面，相对于元音，它们很容易分开。元音位于图表的这一部分。</p>
        <p>This is a diagram that shows all of the phonemes that are separated by exactly one distinctive feature. So if
            you look over in this corner here, you’ll see that the constants. w and x. are separated by exactly one
            distinctive feature. So they’re not exactly distant from each other in the space. On the other hand, they
            are pretty easy to separate relative to the vowels. Which are here in this part of the diagram.</p>
        <h2 id="unknown-301">未知</h2>
        <h2>Unknown</h2>
        <p>它们全都缠在一起，而且元音都彼此靠近。所以你猜怎么着？元音比常数更难分离。这并不奇怪，因为有很多对不同的元音。而且只有一个独特的特征。好吧。现在你回过头来说，好吧，天哪。这一切都很有趣。但它教会了我们如何做科学之类的事情？它教会了我们什么。
        </p>
        <p>Which are all tangled up and the vowels are all close to each other. So guess what? Vowels are much harder to
            separate than constants. Not surprisingly, because there are many pairs of them that are different. And only
            one distinctive feature. All right. So now you back up and you say, well, gosh. That’s all been sort of
            interesting. But what does it teach us about how to do science and stuff? And what it teaches us is.</p>
        <p>这是一个例子。哎哟。这个例子可以用来阐明我在之前的讲座中谈到的 David Marr 的一些想法，与视觉的联系。但这是 Marr 的问答。我的拼写不太好，所以我不会尝试重新拼写它。但这是 Marr 的问答。所以
            Marr 说的是，当你处理 AI 问题时，首先要做的是指定问题。哎哟，这听起来很正常。</p>
        <p>This is an example. Ow. This is an example which we can use to illuminate some of thoughts of David Marr, who
            I spoke of in a previous lecture, connection with vision. But here’s Marr’s catechism. I can’t spell very
            well so I won’t try to respell it. But this is Marr’s catechism. So what Marr said is, when you’re dealing
            with an AI problem, first thing to do is to specify the problem. Gee, that sounds awfully normal.</p>
        <p>接下来是设计一种适合问题的表达方式。第三件事，词汇量各不相同，但它有点像确定一种方法。有时被认为是一种方法。然后第四，选择一种机制或设计一种算法。最后，第五，实验。当然，它永远不会像这样线性进行。你从问题开始，然后你在这里经历很多循环。有时甚至改变问题。
        </p>
        <p>The next thing is to devise a representation suited to the problem. The third thing to do, vocabulary varies,
            but it’s something like determine an approach. Sometimes thought of as a method. And then four, pick a
            mechanism or devise an algorithm. And, finally, five, experiment. And of course, it never goes linearly like
            that. You start with the problem and then you go through a lot of loops up here. Sometimes even changing the
            problem.</p>
        <h2 id="unknown-302">未知</h2>
        <h2>Unknown</h2>
        <p>但这只是科学方法，对吧？你从问题开始，最后进行实验。但这并不是人工智能领域人士在其存在的大部分时间里倾向于做的事情。他们倾向于爱上特定的机制。然后他们试图将这些机制应用于每个问题。所以你可能会说，好吧，天哪，神经网络太酷了。
        </p>
        <p>But that’s just the scientific method, right? You start with the problem and you end up with the experiment.
            But that’s not what people in AI, over the bulk of its existence, have tended to do. What they tended to do
            is to fall in love with particular mechanisms. And then they attempt to apply those mechanisms to every
            problem. So you might say, well, gee, neural nets are so cool.</p>
        <p>我认为，人类的所有智能都可以用合适的神经网络来解释。但这不是正确的做法。因为这是机制嫉妒。你会爱上机制。你会尝试将它应用到不合适的地方。这就是从问题入手，为问题带来正确的表示，天哪，独特的特征。</p>
        <p>I think all of human intelligence can be explained with a suitable neural net. That’s not the right way to do
            it. Because that’s mechanism envy. You fall in love with mechanism. You try to apply it where it isn’t the
            right thing. This is example starting with the problem and bringing to the problem the right
            representations, gosh, distinctive features.</p>
        <p>一旦我们得到了正确的表示，约束就会出现，这使我们能够设计一种方法、编写算法并进行实验。正如他们所做的那样。所以 Sussman Yip 的这个事情就是以与 Marr 的教义相一致的方式进行 AI
            工作的一个例子。我强烈推荐。他们本可以来这里说，好吧，我们是神经网络理念的忠实拥护者。</p>
        <p>Once we’ve got the right representation, then the constraints emerge, which enable us to devise an approach,
            write an algorithm, and do an experiment. As they did. So this Sussman Yip thing is an example of doing AI
            stuff in a way that’s congruent with the Marr’s catechism. Which I highly recommend. They could have come in
            here and said, well, we’re devotees of the idea of neural nets.</p>
        <h2 id="unknown-303">未知</h2>
        <h2>Unknown</h2>
        <p>让我们看看我们是否能制造出一台能够使用神经网络正确复数化单词的机器。那将是一个失败者。因为它没有将问题与机制相匹配。它试图将机制强行塞入某种普罗克拉斯提斯之床，但实际上它并不能很好地工作。因此，这当然留下了一个问题，那就是，什么是好的表示？这是马尔教义问答的另一半。
        </p>
        <p>Let’s see if we can make a machine that will properly pluralize words using a neural net. That’s a loser.
            Because it doesn’t match the problem to the mechanism. It tries to force fit the mechanism into some
            Procrustean bed where it doesn’t actually work very well. So what this leaves open, of course, is the
            question of, well, what is a good representation? And here’s the other half Marr’s catechism.</p>
        <p>第一个特点是它使正确的事情变得明确。因此在这个特定情况下，它使独特的特征变得明确。Marr
            的另一个著名之处是立体视觉。因此在那个特定的世界中，当你跨越边缘时，图像中的不连续性就会变得明确。一旦你得到了一个使正确的事情变得明确的表示，你可以说，它是否也暴露了约束？</p>
        <p>Characteristic number one is that it makes the right things explicit. So in this particular case, it makes
            distinctive features explicit. Another thing that Marr was noted for was stereo vision. So in that
            particular world, discontinuities in the image, when you go across an edge with the things that were made
            explicit. Once you’ve got to a representation that makes the right things explicit, you can say, does it
            also expose constraint?</p>
        <p>如果你有一个能揭示约束的表示，那么你就可以开始行动了。因为你需要约束才能进行处理，从而得到解决方案。所以没有正确的表示。如果它没有揭示约束，你就无法用它制作出非常好的模型。最后，还有一种局部性标准。</p>
        <p>And if you have a representation that exposes constraint, then you’re off and running. Because it’s
            constraint that you need in order to do the processing that leads to a solution. So don’t have the right
            representation. If it doesn’t expose constraints, you’re not going to be able to make a very good model out
            of it. And finally, there’s a kind of localness criteria.</p>
        <h2 id="unknown-304">未知</h2>
        <h2>Unknown</h2>
        <p>如果你有一个可以通过查看吸管描述来找到正确答案的表示，那么它可能比散开的表示更好。程序也是如此，对吧？如果你可以通过查看吸管来了解它们的工作原理，那么如果你必须查看下一页和下一个文件，那么你就能更好地理解某些事情。
        </p>
        <p>If you have a representation in which you can see the right answer by looking at descriptions through soda
            straw, that’s probably a better representation than one that’s all spread out. It’s true with programs,
            right? If you can see how they work by looking through a soda straw, you’re in much better situation to
            understand something if you have to look here and there and on the next page and in the next file.</p>
        <p>所以所有这些基本上都是常识。但这种常识会让你作为一名工程师和科学家变得更聪明。尤其是作为一名科学家，因为如果你带着机制嫉妒的态度去解决问题，你很容易以一种幼稚的方式研究机制，永远找不到令人满意的解决方案。</p>
        <p>So all this is basically common sense. But this is kind of common sense that makes you smarter as an engineer
            and scientist. Especially as a scientist because if you go into a problem with mechanism envy, you’re apt to
            study mechanisms in a naive way and never reach a solution that will be satisfactory.</p>
        <h1 id="learning-near-misses-felicity-conditions">15. 学习：险些失误，幸福条件</h1>
        <h1>15. Learning: Near Misses, Felicity Conditions</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 46:54 <a
                href="https://youtube.com/watch?v=sh3EPjhhd40">https://youtube.com/watch?v=sh3EPjhhd40</a></p>
        <p> 11 years ago (Jan 11, 2014) — 46:54 <a
                href="https://youtube.com/watch?v=sh3EPjhhd40">https://youtube.com/watch?v=sh3EPjhhd40</a></p>
        <h2 id="unknown-305">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿教授：你们当中有些人，比如索尼娅、克里希纳、肖莎娜，我不知道。有些人每次都能来。有些人偶尔才来。</p>
        <p>PROFESSOR PATRICK WINSTON: You know, some of you who for instance. I don’t know, Sonya, Krishna, Shoshana.
            some of you I can count on being here every time. Some of you show up once in a while.</p>
        <p>你们当中偶尔出现的人如果选择今天，那你们就很幸运了，因为今天我们要讲的事情可能会对你的一生产生重大影响。因为我要告诉你如何让自己变得更聪明。不是开玩笑。</p>
        <p>The ones of you who show up once in a while happen to be very lucky if you picked today, because what we’re
            going to do today is I’m going to tell you stuff that might make a big difference in your whole life.
            Because I’m going to tell you how you can make yourself smarter. No kidding.</p>
        <p>我还会告诉你如何整理你的想法，这样你就会成为被选中的人，而不是其他懒汉。这就是我们今天要做的事情。这是本学期最重要的一堂课。睡眠课只是第二重要的。这是最重要的。</p>
        <p>And I’m also going to tell you how you can package your ideas so you’ll be the one that’s picked instead of
            some other slug. So that’s what we’re going to do today. It’s the most important lecture of the semester.
            The sleep lecture is only the second most important. This is the most important.</p>
        <h2 id="unknown-306">未知</h2>
        <h2>Unknown</h2>
        <p>现在，让我们开始讨论如何以一种类似于我们上次讨论的方式进行学习。因为上次我们从少量示例中学到了一些非常明确的东西。这次更进一步，展示了如何以类似人类的方式一次性从单个示例中进行学习。</p>
        <p>Now the vehicle that’s going to get us there is a discussion about how it’s possible to learn in a way that
            is a little reminiscent of what we talked about last time. Because last time we learned something very
            definite from a small number of examples. This takes it one step further and shows how it’s possible to
            learn in a human like way from a single example in one shot.</p>
        <p>所以它与你之前见过的一切都截然不同。一切都需要从成千上万的试验和无数的例子中学习，并且从每个例子中只学习一点点。这将从每个例子中学到一些明确的东西。这是课堂上的例子。这是什么？这是一座拱门。我知道建筑师们抱怨说这不是建筑领域的拱门。这是一个柱子和过梁结构。
        </p>
        <p>So it’s extremely different, very different from everything you’ve seen before. Everything that involves
            learning from thousands of trials and gazillions of examples and only learning a little tiny bit, if
            anything, from each of them. This is going to learn something definite from every example. So here’s the
            classroom example. What’s this? It’s an arch. I know the architects are complaining that it’s not an arch in
            architecture land. It’s a post and lintel construction.</p>
        <p>但对于我们今天来说，它将是一道拱门。现在，如果你来自火星，不知道什么是拱门，我可能会向你介绍这一点，你会对一些可能成为因素的事情有一个大致的了解，但你不知道什么才是真正重要的。所以我会说，那不是拱门。你会从中学到一些非常明确的东西。
        </p>
        <p>But for us today it’s going to be an arch. Now if you were from Mars and didn’t know what an arch was, I
            might present this to you and you’d get a general idea of some things that might be factors, but you’d have
            no idea what’s really important. So then I would say, that’s not an arch. And you would learn something very
            definite from that.</p>
        <h2 id="unknown-307">未知</h2>
        <h2>Unknown</h2>
        <p>然后我会把它们拼在一起，再放回去，我会说，那也不是拱门。你会从中学到一些非常明确的东西。然后我可以把上面的那个漆成蓝色，你会学到一些与此截然不同的东西。问题是，这是怎么发生的？具体是如何发生的？这对人类学习意味着什么？你如何让自己变得更聪明？
        </p>
        <p>And then I would shove these together and put this back on, and I would say, that’s not an arch either. And
            you’d learn something very definite from that. And then I could paint the top one blue, and you’d learn
            something very different from that. And how can that happen is the question? How can that happen in detail,
            and what might it mean for human learning and how you can make yourself smarter?</p>
        <p>这就是我们要去的地方。好吗？那么我们如何才能编写一个像火星人一样聪明的程序来学习这类东西呢？好吧，如果你正在编写这个程序，你肯定会做的第一件事就是尽快摆脱图片，进入符号世界，在那里你会更清楚地知道哪些是重要的部分。
        </p>
        <p>And that’s where we’re going to go. All right? So how can we make a program that’s a smart as a martian about
            learning things like that? Well, if you were writing that program, surely the first thing you would do is
            you’d try to get off the picture as quickly as possible and into symbol land where things are clearer about
            what the important parts are.</p>
        <p>因此，您将看到一个可能看起来像这样的初始示例。我们将其称为示例。它不仅仅是一个例子。它是初始模型。这是起点。现在我们要将它与实际上不是拱门但看起来很像拱门的东西结合起来，至少在我们即将讨论的描述层面上是如此。</p>
        <p>So you’d be presented with an initial example that might look like this. We’ll call that an example. And it’s
            more than just an example. It’s the initial model. That’s the starting point. And now we’re going to couple
            that with something that’s not actually an arch but looks a whole lot like one, at least on the descriptive
            level to which we’re about to go.</p>
        <h2 id="unknown-308">未知</h2>
        <h2>Unknown</h2>
        <p>所以这不是拱门，但它的描述与拱门没有太大区别。事实上，如果我们要用一种网络来画出它，我们会得到一个像这样的描述，这些关系将是支持关系。它会像这样画出来。唯一的区别是。唯一的区别是我们在初始模型中拥有的那些支持关系。
        </p>
        <p>So here’s something that’s not an arch, but its description doesn’t differ from that of an arch very much. In
            fact, if we were to draw this out in a kind of network, we would have a description that looks like this,
            and these relations would be support relations. And this would be drawn out like so. And the only difference
            would be. the only difference would be that those support relations that we had in the initial model.</p>
        <p>示例。已经消失在这种配置中。但由于它与模型没有太大区别，我们将其称为“差点成功”。现在，您看，我们已经抽象出了所有对我们来说不重要的细节。上次我们谈到了良好的表示具有某些品质。品质如使正确的事情明确。好吧，这使结构明确，并抑制了有关表面瑕疵的信息。
        </p>
        <p>The example. have disappeared down out here in this configuration. But since it’s not very different from the
            model, we’re going to call this a near miss. And now, you see, we’ve abstracted away from all the details
            that don’t matter to us. Last time we talked about a good representation having certain qualities. qualities
            like making the right things explicit. Well, this makes the structure explicit, and it suppresses
            information about blemishes on the surface.</p>
        <p>我们不太关心物体有多高。我们认为它们由什么制成并不重要。所以这是一个满足上次第一个标准的表示。它使正确的事情明确化。通过使正确的事情明确化，它揭示了一些关于拱门需要什么的约束。我们看到，如果缺少这些支撑关系，它就不是拱门。
        </p>
        <p>We don’t care much about how tall the objects are. We don’t think it matters what they’re made of. So this is
            a representation that satisfies the first of the criteria from last time. It makes the right things
            explicit. And by making the right things explicit, it’s exposing some constraint here with respect to what
            it takes to be an arch. And we see that if those support relations are missing, it’s not an arch.</p>
        <h2 id="unknown-309">未知</h2>
        <h2>Unknown</h2>
        <p>所以我们应该能从中学到一些东西。我们要做的就是把这两件事放在一起。我们要描述两者之间的差异。我们会得出这样的结论：由于只有一个差异。一种差异，两种表现形式，即消失的支持关系，我们会得出结论，这些支持关系很重要。我们会把它们变成红色，因为它们非常重要。
        </p>
        <p>So we ought to be able to learn something from that. What we’re going to do is we’re going to put these two
            things together. We’re going to describe the difference between the two. And we’re going to reach the
            conclusion that since there’s only one difference. one kind of difference with two manifestations to
            disappearing support relations, we’re going to conclude that those support relations are important. And
            we’re going to turn them red because they’re so important.</p>
        <p>我们将把名称从“支持”改为“必须支持”。这就是我们的新模型。这是一个不断发展的模型，现在它包含重要信息。因此，如果您要将某个东西与此模型进行匹配，则必须存在这些支持关系。如果不存在。如果不存在，则它不是拱门。好吗？因此，我们从一个示例中学到了一些明确的东西。
        </p>
        <p>And we’re going to change the name from “support” to “must support.” So this is our new model. This is an
            evolving model that now is decorated with information about what’s important. So if you’re going to match
            something against this model, it must be the case that those support relations are there. If it’s not there.
            if they’re not there, it’s not an arch. All right? So we’ve learned something definite from a single
            example.</p>
        <p>这不是 10,000
            次试验。这是老师向学生展示一些东西，学生一步一步立即了解拱门中的重要内容。让我们再做一次。这太有趣了。让我们做这个。与之前相同，只是现在当我们描述这个东西时，有一些额外的关系。这些关系，那些是接触关系。所以现在当我们比较它时。那是拱门吗？不是。&nbsp;
        </p>
        <p>This is not 10,000 trials. This is a teacher presenting something to the student and the student learning
            something immediately in one step about what’s important in an arch. So let’s do it again. That was so much
            fun. Let’s do this one. Same as before except that now when we describe this thing, there are some
            additional relations. these relations, and those are touch relations. So now when we compare that. is that
            an arch? No.&nbsp;</p>
        <h2 id="unknown-310">未知</h2>
        <h2>Unknown</h2>
        <p>这是一次险些成功的案例。当我们将这次险些成功的案例与我们不断发展的模型进行比较时，我们立即发现，这里再次存在一个差异，即两种表现形式，即触碰关系。因此，我们可以立即得出结论，这些触碰关系干扰了我们认为这可能是拱门的信念。那么我们该怎么做呢？我们将它们重新组合在一起，构建一个新模型。它与旧模型非常相似。
        </p>
        <p>It’s a near miss. When we compare that near miss with our evolving model, we see immediately that once again
            there’s exactly one difference, two manifestations, the touch relations. So we can immediately conclude that
            these touch relations are interfering with our belief that this could be an arch. So what do we do with
            that? We put those together again and we build ourselves a new model. It’s much like the old model.</p>
        <p>这里仍然有命令。我们必须有支撑关系。但现在在这里。我们在那里画出不接触的标志。这些是不能接触的关系。所以现在如果这两个侧面支撑相互接触，你就无法与该模型匹配。所以在短短的两个步骤中，我们了解了两个重要的事情，即为了将此东西解释为拱门，必须具备哪些条件。
        </p>
        <p>It still has the imperatives up here. We have to have the support relations. But now down here. and we draw
            not signs through there. these are must not touch relations. So now you can’t match against that model if
            those two side supports are touching each other. So in just two steps, we’ve learned two important things
            about what has to be in place in order for this thing to be construed to be an arch.</p>
        <p>所以我们的火星机器人正在取得巨大进步。但我们的火星机器人还没有完成，因为我们可能还想让它知道一些关于拱门性质的事情。例如，我们可能会向它展示这个。嗯，这看起来就像我们最初的例子。这是一个和我们最初的例子一样的例子。但这次顶部被漆成了红色。我仍然认为这是一个拱门。
        </p>
        <p>So our martian is making great progress. But our martian isn’t through, because there’s some more things we
            might want it to know about the nature of arches. For example, we might present it with this one. Well, that
            looks just like our initial example. It’s an example just like our initial example. But this time the top
            has been painted red. And I’m still saying that’s an arch.</p>
        <h2 id="unknown-311">未知</h2>
        <h2>Unknown</h2>
        <p>因此，再说一次，只有一个区别，那就是在这个对象的描述中，我们有一个额外的信息，即顶部的颜色是红色。我们一直没有说出来，进化模型中顶部的颜色是白色。所以现在我们知道顶部不一定是白色的。它可以是红色或白色。</p>
        <p>So once again, there’s only one difference and that difference is that in the description of this object, we
            have the additional information that the color of the top is red. And we’ve been carrying along without
            saying so, that the color of the top in the evolving model is white. So now we know that the top doesn’t
            have to be white. It can be either red or white.</p>
        <p>所以我们将两者结合起来，得到一个新的模型。这个新模型这次又有三个部分。它将具有关系，我们现在一直使用的命令形式，必须支持和不能接触，但现在我们要将颜色关系本身变成命令。我们会说顶部必须是红色或白色。</p>
        <p>So we’ll put those two together and we’ll get a new model. And that new model this time once again will have
            three parts. It will have the relations, an imperative form that we’ve been carrying along now, the must
            support and the must not touch, but now we’re going to turn that color relation itself into an imperative.
            And we’re going to say that the top has to be either red or white.</p>
        <p>现在，我们又一次在一步中明确了关于拱形的知识。还有两步。假设现在我们用这个例子来展示它。这是一个例子。这次这里还要加一点油漆。这次我们要把顶部涂成蓝色，就像这样。所以描述会像这样。现在我们必须以某种方式把它与我们不断发展的模型结合起来，形成一个新的模型。
        </p>
        <p>So now, once again, in one step we’ve learned something definite about archness. Two more steps. Suppose now
            we present it with this example. It’s an example. And this time there’s going to be a little paint added
            here as well. This time we’re going to have the top painted blue like so. So the description will be like
            so. And now we have to somehow put that together with our evolving model to make a new model.</p>
        <h2 id="unknown-312">未知</h2>
        <h2>Unknown</h2>
        <p>这里有一些选择。我们的选择在某种程度上取决于我们所处世界的性质。假设我们在旗帜世界工作。只有三种颜色。红色、白色和蓝色。现在我们已经看到了它们。如果我们已经看到了它们，那么我们要做的就是，现在的进化模型再次进行了调整。哦。但这些仍然是必要的。让我继续说下去。
        </p>
        <p>And there’s some choices here. And our choice depends somewhat on the nature of the world that we’re working
            in. So suppose we’re working in flag world. There are only three colors. red, white, and blue. Now we’ve
            seen them all. If we’ve seen them all, then what we’re going to do is we’re going to say that the evolving
            model now is adjusted yet again like so. Oh. but those are imperatives still. Let me carry that along.</p>
        <p>此时，这个颜色关系可以指向任何事物。所以我们可以不画它，但这样我们就会忘记我们已经知道任何东西都可以存在的事实。所以我们要保留这个关系，但让它指向“任何东西都可以”标记。好吧，我们取得了很大的进展，我说只剩一件事要做了。
        </p>
        <p>At this time, this guy. the color relation. goes out here to anything at all. So we could have just not drawn
            it at all, but then we would have lost track of the fact that we’ve actually learned that anything can be
            there. So we’re going to retain the relation but have it point to the “anything goes” marker. Well, we’re
            making great progress and I said there’s just one more thing to go.</p>
        <p>所以让我把它压缩到这个区域。这次我要补充的是，我要说的是，这个例子就像你以前见过的一切一样，只是顶部现在是那种儿童积木。所以你实际上可以选择这是否是拱门。但如果我说，是的，它仍然是拱门，那么我们会在它的描述中添加一些内容。
        </p>
        <p>So let me compress that into this area here. What I’m going to add this time is I’m going to say that the
            example is like everything you’ve seen before except that the top is now one of those kinds of child’s
            bricks. So you have a choice actually about whether this is an arch or not. But if I say, yeah, it’s still
            an arch, then we’d add a little something to its description.</p>
        <h2 id="unknown-313">未知</h2>
        <h2>Unknown</h2>
        <p>所以这个描述看起来是这样的。与我们之前在支持方面看到的一样，但现在我们有一个关系，说这个顶部是一个楔子。在这里。我们一直带着但没有写下来的东西。这个顶部是一个块。我想用当今的语言来说，是一块砖。</p>
        <p>So this description would look like this. Same things that we’ve seen before in terms of support, but now
            we’d have a relation that says that this top is a wedge. And over here. something we’ve been carrying along
            but not writing down. this top is a block. A brick, I guess in the language of the day.</p>
        <p>所以如果我们说它可以是楔子或上面的砖块，我们该怎么做呢？同样，这取决于表示的性质，但如果我们说我们有一个表示，它有一个部分的层次结构。所以砖块和楔子都是儿童积木和儿童盒子或玩具。然后我们可以考虑在这里画出一点层次结构，然后说，好吧，让我们看看。
        </p>
        <p>So if we say that it can be either a wedge or a brick on top, what do we do with that? Once again, it depends
            on the nature of representation, but if we say that we have a representation, that has a hierarchy of parts.
            So bricks and wedges are both children’s blocks and children’s box or toys. Then we can think of drawing in
            a little bit of that hierarchy right here and saying well, let’s see.</p>
        <p>紧接着上面是砖块或楔子。再往上一点是积木。再往上一点是玩具。再往上一点最终会得到任何物理对象。那么，面对这种情况，它会做什么呢？您可以选择。</p>
        <p>Immediately above that we’ve got the brick or wedge. And a little bit above that we’ve got block. And a
            little bit above that we’ve got toy. And a little bit above that we eventually get to any physical object.
            So what does it do in response to that kind of situation? You have the choice.</p>
        <h2 id="unknown-314">未知</h2>
        <h2>Unknown</h2>
        <p>但我所说的程序实际上所做的是在这里做出保守的概括，只是为了说它是那些家伙之一。所以它再次学到了一些明确的东西。让我看看。让我数一下步骤。一、二、三、四、五。我刚刚学到了四件事。</p>
        <p>But what the program I’m speaking of actually did was to make a conservative generalization up here just to
            say that it’s one of those guys. So once again it’s learned something definite. Let me see. Let me count the
            steps. One, two, three, four, five. And I just learned four things.</p>
        <p>因此，对于颜色的泛化，需要两个步骤才能达到“不关心”的程度。因此请注意它与你在神经网络中看到的任何东西的对比。或者你将在我们将要讨论的一些其他学习技术中看到的任何东西，这些技术涉及使用数千个样本来学习它是什么。学习任何想要学习的东西。
        </p>
        <p>So the generalization of a color, it took two steps to get all the way up to “don’t care.” So note how it
            contrasts with anything you’ve seen in a neural net. Or anything you will see downstream in some of the
            other learning techniques that we’ll be talking about that involve using thousands of samples to learn what
            it is. to learn whatever it is that is intended to be learned.</p>
        <p>让我再举一个例子来说明这些启发式方法是如何发挥作用的。有两组图画。我们有上图和下图。你们这些在大量并行工作中的聪明人，你们的任务是给我一个关于上图火车的描述，以区别和区分它们和下图火车。你懂了吗？没人懂吗？好吧，让我试试你。
        </p>
        <p>Let me show you another example of how these heuristics can be put to work. So there are two sets of
            drawings. We have the upper set and the lower set. And your task, you smart humans working in vast
            parallelism, your task is to give me a description of the top trains that distinguishes and separates them
            from the trains on the bottom. You got it? Nobody’s got it? Well, let me try one on you.</p>
        <h2 id="unknown-315">未知</h2>
        <h2>Unknown</h2>
        <p>最上面的火车都是短车厢，顶部封闭。那么计算机怎么可能弄明白呢？事实证明，它用我在这里展示的与拱门相关的相同设备弄明白了这一点，只是部署方式略有不同。在这个特殊情况下，老师会一次一个地展示例子，因为老师希望学生能学到东西。
        </p>
        <p>The top trains all have a short car with a closed top. So how is it possible that a computer could have
            figured that out? It turns out that it figured it out with much the same apparatus that I’ve shown you here
            in connection with the arches, just deployed in a somewhat different manner. In this particular case, the
            examples are presented one at a time by a teacher who’s eager for the student to learn.</p>
        <p>在这种情况下，示例会同时呈现，机器需要找出区分两组的描述。它的工作原理如下。你要做的是从其中一个开始。但你有很多。你有一些例子。</p>
        <p>In this case, the examples are presented all at once and the machine is expected to figure out a description
            that separates the two groups. And here’s how it works. What you do is you start with one of them. But you
            have a lot of them. You have some examples.</p>
        <p>我们将上面的示例称为“正示例”，将下面的示例称为“负示例”。所以你要做的第一件事就是选择一个正示例来处理。有人能猜出我们要叫它什么吗？是的，你们知道。我们将其称为种子。这与我们上次所做的非常相似，但现在的水平大不相同。
        </p>
        <p>We’ll call the examples on top the “plus examples” and the examples on the bottom the “negative examples.” So
            the first thing that you do is you pick one of the positive examples to work with. Anybody got any good
            guesses about what we’re going to call that? Yeah, you do. We’re going to call that the seed. It’s just
            highly reminiscent of what we did last time when we were doing but now at a much different level.</p>
        <h2 id="unknown-316">未知</h2>
        <h2>Unknown</h2>
        <p>我们会从中挑选一个作为种子，然后采用这些启发式方法，寻找一个可以放宽这种描述的种子，以便它涵盖更多的正面内容。你看，如果你有一个种子，它恰好描述了某个特定事物，并且你坚持认为一切都是那样，那么除了它自己之外，没有任何东西可以匹配。
        </p>
        <p>We’re going to pick one of those guys to be the seed, and then we’re going to take these heuristics and we’re
            going to search for one that loosens this description so that it covers more of the positives. You see, if
            you have a seed that is exactly a description of a particular thing and you insist that everything be just
            like that, then nothing will match except itself.</p>
        <p>但是你可以使用这些启发式方法来扩大描述的覆盖范围，使其更宽松，从而涵盖更多的正面内容。因此，在第一步中，你可能会涵盖例如那组对象。</p>
        <p>But you can use these heuristics to expand the coverage of the description, to loosen it so that it covers
            more of the positives. So in your first step you might cover, for example, that group of objects.</p>
        <p>对你来说太糟糕了，在那个特定情况下，你在描述中也包含了一个负面例子，但也许在下一步中，你会消除所有这些负面例子，并将注意力集中在所有正面例子上。那么如何构建一个可以做这种事情的程序呢？好吧，想想选择。</p>
        <p>Too bad for your side, you’ve also in that particular case included a negative example in your description,
            but perhaps in this next step beyond that you’ll get to the point where you’ve eliminated all of those
            negative examples and zeroed in on all the positive examples. So how might a program be constructed that
            would do that sort of thing? Well, think about the choices.</p>
        <h2 id="unknown-317">未知</h2>
        <h2>Unknown</h2>
        <p>你的第一个选择是选择一个正面的例子作为种子。一旦你选择了一个特定的例子作为种子，那么你就可以应用启发式方法，所有你拥有的方法，来创建一个可以更好地覆盖数据的新描述。它可能比你上一步中的正面更多，负面更少。</p>
        <p>The first choice that you have it is to pick a positive example to be the seed. And once you’ve picked a
            particular example to be the seed, then you can apply heuristics, all of them that you have, to make a new
            description that may cover the data better. It may have more of the positives and fewer of the negatives
            than in your previous step.</p>
        <p>但是，如果你有很多启发式方法，而且因为那组火车中有很多描述，所以有很多启发式方法，你可以用这些启发式方法做很多事情，因为你可以将它们应用到任何地方。所以这棵树非常大。那么你该怎么做才能控制它呢？现在你已经下意识地知道了这些问题的答案，对吧？
        </p>
        <p>But this, if you have a lot of heuristics, and these are a lot of heuristics because there’s a lot of
            description in that set of trains, there are lots of possible things that you could do with those heuristics
            because you could apply them anywhere. So this tree is extremely large. So what do you do to keep it under
            control? Well, now you have answers to questions like that by knee jerk, right?</p>
        <p>分支因子太大了。你想让一些解决方案继续下去。你有一些方法来衡量你做得有多好，所以你可以使用光束搜索。这部分最初是由我的一个朋友在伊利诺伊大学工作时完成的，可惜现在已经去世了。当然，他对玩具火车不感兴趣，他只是对大豆疾病感兴趣。
        </p>
        <p>The branching factor is too big. You want to keep a few solutions going. You have some way of measuring how
            well you’re doing so you can use a beam search. This piece here was originally worked out by a friend of
            mine, now, alas, deceased, when he was at the University of Illinois. And of course, he wasn’t interested in
            toy trains, he was just interested in soybean diseases.</p>
        <h2 id="unknown-318">未知</h2>
        <h2>Unknown</h2>
        <p>因此，这个程序被用来描述大豆疾病。事实证明，它比植物病理学书籍更好。我们现在有两种方法来部署相同的启发式方法。但我的词汇量需要丰富，因为我正在谈论“那些”启发式方法。很久以前，它为我做的一件好事就是给它们每一个都起了个名字。
        </p>
        <p>And so this exact program was used to build descriptions of soybean diseases. It turned out to be better than
            the plant pathology books. We now have two ways of deploying the same heuristics. But my vocabulary is in
            need of enrichment, because I’m talking about “those” heuristics. And one of the nice things that did for me
            a long time ago is give each of them a name.</p>
        <p>所以这些名字是由这里发生了什么？你正在从原始模型转变为理解。有些东西是必不可少的。所以他称之为“需要链接”启发式。在下一步中，我们禁止某些东西出现在那里。所以称这种启发式为“禁止链接”启发式。在下一步中，我们说它可以是红色或白色。所以我们有一组颜色，我们正在扩展它。
        </p>
        <p>So here are the names that were developed by What’s happening here? You’re going from an original model to an
            understanding. some things are essential. So he called this the “require link” heuristic. And here in the
            next step, we’re forbidding some things from being there. So called that heuristic the “forbid link”
            heuristic. And in the next step, we’re saying it can be either red or white. So we have a set of colors and
            we’re extending it.</p>
        <p>在这个启发式方法中，从红色或白色到任何颜色都可以，这实际上是完全忘记了颜色，所以我们将其称为“放下链接”，尽管出于跟踪的原因，我们实际上并没有删除它。我们只是让它指向“任何颜色”标记。最后，在这最后一步中，我们对这个类别树所做的就是将其向上攀升一步。
        </p>
        <p>And over here in this heuristic, going from red or white to anything goes, that’s essentially forgetting
            about color altogether, so we’re going to call that “drop link” even though for reasons of keeping track, we
            don’t actually get rid of it. We just have it pointing to the “anything” marker. And finally, in this last
            step, what we’re doing with this tree of categories is we’re climbing up it one step.</p>
        <h2 id="unknown-319">未知</h2>
        <h2>Unknown</h2>
        <p>所以他称之为“爬树”启发式方法。所以现在我们有了一套可以在学习过程中做的事情的词汇表，有了这些词汇表，我们就可以掌控它，对吧？因为这些都是名字。我们现在可以说，好吧，你这里需要的是“放下链接”启发式方法。而你那边需要的是“扩展集”启发式方法。所以现在我想再回过头来说，好吧，让我们看看。
        </p>
        <p>So he called that the “climb tree” heuristic. So now we have a vocabulary of things we can do in the learning
            process, and having that vocabulary gives us power over it, right? Because those are names. We can now say,
            well, what you need here is the “drop link” heuristic. And what you need over there is the “extend set”
            heuristic. So now I want to back up yet another time and say, well, let’s see.</p>
        <p>当我们研究音系学时，我所做的只是概括。我们只是在概括吗？不。我们既在概括，也在专门化。因此，当我说第一步中开发的链接必不可少时，这是一个专门化步骤。当我说它们不可能是接触关系时，这是一个专门化步骤。</p>
        <p>When we were working with that phonology stuff, all I did was generalize. Are we just generalizing here?
            No.&nbsp;We’re both generalizing and specializing. So when I say that the links over here that are developed
            in our first step are essential, this is a specialization step. And when I say they can’t be. they cannot be
            touch relations, that’s a specialization step.</p>
        <p>因为当我们说不能有接触关系时，我们能够匹配的东西越来越少。但是在这里，当我在这里说，好吧，它不一定是白色的。它也可以是红色的。这是一个概括。现在我们可以匹配更多的东西。当我完全放弃链接时，这是一个概括。当我爬上树时，这是一个概括。
        </p>
        <p>Because we’re able to match fewer and fewer things when we say you can’t have touch relations. But over here,
            when I go here and say, well, it doesn’t have to be white. It can also be red. That’s a generalization. Now
            we can match more things. And when I drop the link altogether, that’s a generalization. And when I climb the
            tree, that’s a generalization.</p>
        <h2 id="unknown-320">未知</h2>
        <h2>Unknown</h2>
        <p>这就是为什么当我绘制这个概念图来描述当程序通过树搜索找到火车问题的解决方案时会发生什么时，它们都是专业化步骤，可以绘制出可以匹配的事物的数量，而泛化步骤则使其更加广泛。所以，让我们看看。我们也有近乎失误的概念。我们有例子的概念。其中一些是例子，一些是近乎失误。
        </p>
        <p>And that’s why when I do this notional picture of what happens when program does a tree search to find a
            solution to the train problem, they’re both specialization steps which draw in the number of things that can
            be matched, and generalization steps that make it broader. So, let’s see. We’ve also got the notion of near
            miss. And we’ve got the notion of example. some of these things are examples, some are near misses.</p>
        <p>我们有概括和专门化。两者是相辅相成的，还是彼此之间关系混乱？你能概括和专门化差点失误吗？你怎么看？你认为。你不这么认为，你怎么看？学生：专门化。帕特里克·温斯顿教授：导致专门化。让我们看看这是不是对的。所以我们这里有专门化，这是一个差点失误。我们这里有专门化，这是一个差点失误。
        </p>
        <p>We’ve got generalization specialization. Does one go with one or the other, or are they all mixed up in their
            relationship to each other? Can you generalize and specialize with near misses? What do you think? You
            think. you don’t think so, What do you think? STUDENT: specialization. PROFESSOR PATRICK WINSTON: lead to
            specialization. Let’s see if that’s right. So we’ve got specialization here, and that’s a near miss. We’ve
            got specialization here, and that’s a near miss.</p>
        <p>我们在这里得到了概括，这就是一个例子。我们在这里得到了概括，这就是一个例子。我们在这里得到了概括，这就是一个例子。所以已经搞定了。例子总是概括性的，而差点出错总是专门化的。所以我们有设备可以让我们扩大我们能匹配的范围并缩小我们能匹配的范围。那么这有什么关系呢？
        </p>
        <p>We’ve got generalization here, and that’s an example. And we’ve got generalization here, and that’s an
            example. And we’ve got generalization here, and that’s an example. So has got that one nailed. The examples
            always generalize, and the near misses always specialize. So we’ve got apparatuses in place that allow us to
            both expand what we could match and shrink what we could match. So what has this got to do anything?</p>
        <h2 id="unknown-321">未知</h2>
        <h2>Unknown</h2>
        <p>那么，顺便问一下，哪种方法更好呢？这个方法。这个方法需要老师来组织一切。这个方法可以以批处理模式处理。这个方法需要人类来做，因为我们没有太多的内存。那个方法则是计算机擅长的事情，因为它有大量的内存。那么，哪种方法更好呢？
        </p>
        <p>Well, which one of these methods is better, by the way? This one. this one requires a teacher to organize
            everything up. This one can handle it in batch mode. This one is the sort of thing you would need to do with
            a human because we don’t have much memory. That one is the sort of thing that a computer’s good at because
            it has lots of memory. So which one’s better?</p>
        <p>嗯，这取决于你想做什么。如果你想要建立一个分析股票市场的机器，你可能想这样做。或者大豆疾病，或者各种实际问题中的任何一个。如果你想要模拟人类，那么也许这种方法值得进一步研究。你如何解决所有这些问题？</p>
        <p>Well, it depends on what you’re trying to do. If you’re trying to build a machine that analyzes the stock
            market, you might want to go that way. Or soybean diseases, or any one of a variety of practical problems.
            If you’re trying to model people, then maybe this is a way that deserves additional merit. How do you get
            all that sorted out?</p>
        <p>好吧，解决这一切的一个方法是谈论有时被称为“幸福条件”的东西。所以当我谈论幸福条件时，我指的是老师和学生以及他们之间的契约。这是老师。那是我。这是学生。那是你。</p>
        <p>Well, one way to get it all sorted out is to talk in terms of what are sometimes called “felicity
            conditions.” So when I talk about felicity conditions, I’m talking about a teacher and a student and
            covenants that hold between them. So here’s the teacher. That’s me. And here’s the student. That’s you.</p>
        <h2 id="unknown-322">未知</h2>
        <h2>Unknown</h2>
        <p>互动的目的是将初始知识状态转化为新知识状态，这样学生就会更聪明，能够利用新知识做以前做不到的事情。所以学生在这里就是学习者。他有利用所学知识的东西。老师在这里有风格。</p>
        <p>And the objective of interaction is to transform an initial state of knowledge into a new state of knowledge
            so that the student is smarter and able to make use of that new knowledge to do things that couldn’t be done
            before by the student. So the student over here has a learner. And he has something that uses what is
            learned. And the teacher over here has a style.</p>
        <p>因此，如果要进行任何学习，一方必须了解另一方。例如，如果老师了解学生的初始状态，这将很有帮助。这里有一种思考方式。你可以把你所知道的东西看作形成了一种网络。所以最初，你什么都不知道。但随着你的学习，你开始积累知识。
        </p>
        <p>So if any learning is to take place, one side has to know something about the other side. For example, it’s
            helpful if the teacher understands the initial state of the student. And here’s one way of thinking about
            that. You can think of what you know as forming a kind of network. So initially, you don’t know anything.
            But as you learn, you start developing quanta of knowledge.</p>
        <p>这些知识量子都通过先决条件关系联系在一起，这些先决条件关系可能表明你如何从一个量子到达另一个量子。所以，也许你有泛化链接，也许你有专门化链接，也许你有组合链接，但你可以把你所知道的东西看作是形成这种网络。现在，你在任何特定时间的知识状态都可以被视为该空间中的一种波前。
        </p>
        <p>And these quanta of knowledge are all linked together by prerequisite relationships that might indicate how
            you get from one quantum to another. So maybe you have generalization links, maybe you have specialization
            links, maybe you have combination links, but you can think of what you know as forming this kind of network.
            Now your state of knowledge at any particular time can then be viewed as a kind of wavefront in that space.
        </p>
        <h2 id="unknown-323">未知</h2>
        <h2>Unknown</h2>
        <p>那么，如果我，老师，知道你的波前在哪里，我是否可以更好地教你东西？当然，出于这个原因。假设你犯了一个错误，m1，这取决于q1。远远落后于你的波前。如果我知道你犯了这种错误，我该怎么办？哦，我只能说，哦，你忘了在这种语句后需要一个分号。
        </p>
        <p>So if I, the teacher, know where your wavefront is, can I do a better job of teaching you stuff? Sure, for
            this reason. Suppose you make a mistake, m1, that depends on q1. Way, way behind your wavefront. What do I
            do if I know that you made a mistake of that kind? Oh, I just say, oh, you forgot you need a semicolon after
            that kind of statement.</p>
        <p>我只是提醒你一些你肯定知道但你忽略了的事情。对吧？另一方面，假设你犯了一个错误，而这个错误取决于你之前所学的知识。这种错误，m2。那我该对你说什么呢？你觉得怎么样，帕特里克？如果你犯了这种错误，你认为我会说什么？学生：帕特里克·温斯顿教授：不。&nbsp;
        </p>
        <p>I just remind you of something that you certainly know, you just overlooked. Right? On the other hand,
            suppose you make a mistake that depends on a piece of knowledge way out here. That kind of mistake, m2. What
            do I say to you then? What do you think, Patrick? What do you think I would say if you made that kind of
            mistake? STUDENT: PROFESSOR PATRICK WINSTON: No.&nbsp;</p>
        <p>我不会这么说。学生：你会说我们还不知道。帕特里克·温斯顿教授：我会这么说。我建议说。哦，别担心。我们会做到的。我们还没有准备好。所以在这种情况下，我会提醒某人他们已经知道的事情。在这种情况下，我会告诉他们以后会了解。那么我该如何处理第三个错误？
        </p>
        <p>That’s not what I would say STUDENT: You’d tell us that we don’t know that yet. PROFESSOR PATRICK WINSTON: I
            would say something like that. What suggested I would say. Oh, don’t worry about that. We’ll get to it.
            We’re not ready for it yet. So in this case, I remind somebody of something they already know. In this case,
            I tell them they’ll learn about it later. So what do I do with mistake number three?</p>
        <h2 id="unknown-324">未知</h2>
        <h2>Unknown</h2>
        <p>这就是学习的时刻。这就是我可以将波前推出的地方。因为一切都已准备就绪，可以学习下一个半径的东西了。所以如果我知道学生在那个波前上犯了一个错误，那就是我说的，这是教学时刻。这就是我解释某事的时候。所以这就是为什么老师要有一个好的模型来了解学生的初始知识状态是很重要的。
        </p>
        <p>That’s the learning moment. That’s where I can push the wavefront out. Because everything’s in place to learn
            the stuff at the next radius. So if I know that the student has made a mistake on that wavefront, that’s
            when I say, this is the teaching moment. This is when I explain something. So that’s why it’s important for
            the teacher to have a good model of where the student is in the initial state of knowledge.</p>
        <p>老师需要了解的下一件重要的事情是学生的学习方式。因为如果学生是计算机，他们可以批量处理内容。这是一方面。如果学生是三年级学生，存储内容的能力有限，那么你的教学方式就会有所不同。你可以用这种方式教三年级学生，也可以用这种方式教埋在黑板下面的计算机。
        </p>
        <p>Next thing that’s important for the teacher to know is the way that the student learns. Because if the
            student is a computer, they can handle the stuff in batch. That’s one thing. If the student is a third
            grader who has a limited capacity to store stuff, then that makes a difference in how you teach it. You
            might teach it that way to the third grader, and that way, buried underneath this board, to a computer.</p>
        <p>因此，你需要了解学习者的方式​​。学习者的计算能力。还需要了解下面用户框的计算能力，因为有时你可能会学到一些你实际上无法使用的东西。所以现在，你们大多数人都尝试阅读上面的句子，对吧？它看起来很古怪，对吧？它似乎难以理解，也许？这是一个花园小径句子。
        </p>
        <p>So you need to understand the way that the learner. the computational capacity of the learner. And there’s
            also a need to understand the computational capacity of the user box down there, because sometimes you can
            be taught stuff that you can’t actually use. So by now, most of you have attempted to read that sentence up
            there, right? And it seems screwy, right? It seems unintelligible, perhaps? It’s a garden path sentence.</p>
        <h2 id="unknown-325">未知</h2>
        <h2>Unknown</h2>
        <p>它可以写出完美的英语，但通常情况下，你读起来却不是这样，因为你的语言处理器的缓冲空间有限。这是什么意思？你希望这是“to”。问题。但这实际上是一个命令。事情是这样的。有人必须给学生打分。好吧，我们可以让他们的父母来做这件事。那么，让父母给学生打分。所以这是一个命令。
        </p>
        <p>It makes perfectly good English, but the way you generally read it, it doesn’t, because you have a limited
            buffer in your language processor. What does this mean? You’re expecting this to be “to.” Question. But it’s
            actually a command. Here’s the deal. Somebody’s got to give the students their grades. Well, we can have
            their parents do it. Have the grades given to their students by their parents, then. So it’s a command.</p>
        <p>你会走上歧路，因为你的语言处理器的缓冲空间有限。所以，有了括号，你就能理解它。你可以了解它。你可以看到它是好的英语，但你通常无法处理这种句子，除非你回头重新开始。那反过来呢？我们是否必须在这里达成协议，让学生了解一些关于老师的事情？
        </p>
        <p>And you garden path on it, because you have limited buffer space in your language processor. So with
            parentheses you can understand it. You can learn about it. You can see that it’s good English, but you can’t
            generally process that kind of sentence without going back and starting over. And what about going the other
            way? Are there covenants that we have to have here that involve the student understanding some things about
            the teacher?</p>
        <p>嗯，首先是信任。学生必须假设老师教给学生的是正确信息，而不是对学生撒谎。你们之所以来到这里，是因为你们可能认为我不是想通过告诉你们谎言来欺骗你们。这里还有这种事情。了解老师的风格。</p>
        <p>Well, first thing there is trust. The student has to presume that the teacher is teaching the student correct
            information, not lying to student. Ratified that you’re all here because presumably you all think that I’m
            not trying to screw you by telling you stuff that’s a lie. There’s also this sort of thing down here.
            Understanding of the teacher’s style.</p>
        <h2 id="unknown-326">未知</h2>
        <h2>Unknown</h2>
        <p>所以你可能会说，好吧，x
            教授，他所做的就是在课堂上给我们读幻灯片，那为什么要去呢？你的想法并不完全错误。这是对一种风格的理解。或者你可以说，好吧，老温斯顿，他试图在每节课上告诉我们一些明确的事情，并传达一系列强有力的想法。所以也许值得你在早上
            10 点起床。</p>
        <p>So you might say, well, professor x, all he does is read slides to us in class, so why go? You wouldn’t be
            entirely misadvised. That’s an understanding of one kind of style. Or you can say, well, old Winston, he
            tries to tell us something definite and convey a family of powerful ideas in every class. So maybe it’s
            worth dragging yourself out of bed at 10 o’clock in the morning.</p>
        <p>这些都是风格问题，学生会利用这些因素来确定自己的风格与老师的风格如何匹配。这有助于我们理解或思考风格的差异，以便我们能够理解我们是否应该以这种方式学习，这种方式是否是深层的方式，就像你教计算机的方式，教计算机大豆疾病的方式。
        </p>
        <p>Those are style issues, and those are things that the student uses to determine how to match the student’s
            style against that of the instructor. So that helps us to interpret or think about differences in style so
            that we can appreciate whether we ought to be learning that way, where that way is the way that’s underneath
            down here, the way you would teach a computer, the way taught a computer about soybean diseases.</p>
        <p>我们可以这样做，或者我们也可以这样做，让老师精心组织和塑造学习顺序，以方便处理能力有限的学生。现在你们是人类，对吧？所以想想机器在这里要做什么。机器。为了学习每个步骤中确定的东西，机器必须建立一个描述。所以它必须向自己描述这些例子。
        </p>
        <p>We can do it that way, or we can do it this way with a teacher who deliberately organizes and shapes the
            learning sequence for the benefit of a student who has a limited processing capability. Now you’re humans,
            right? So think about what the machine has to do here. The machine. in order to learn anything definite in
            each of those steps, the machine has to build a description. So it has to describe the examples to itself.
        </p>
        <h2 id="unknown-327">未知</h2>
        <h2>Unknown</h2>
        <p>这是毫无疑问的，对吧？因为它所做的就是观察差异。所以它不能观察差异，除非它有事物的描述。所以如果你像机器一样，那么除非你建立描述，否则你就学不到任何东西。除非你自言自语。如果你自言自语，你就是在建立一种描述，使你能够进行学习。你对我说，我是麻省理工学院的学生。
        </p>
        <p>That’s unquestioned, right? Because what it’s doing is looking at the differences. So it can’t look at the
            differences unless it’s got descriptions of things. So if you’re like the machine, then you can’t learn
            anything unless you build descriptions. Unless you talk to yourself. And if you talk to yourself, you’re
            building the kind of descriptions that make it possible for you to do the learning. And you say to me, I’m
            an MIT student.</p>
        <p>我想看数字。所以让我给你看数字。当我要展示数字时。我要展示的数字会告诉你自言自语的好处。所以这是实验。这个实验是我的一个朋友 Michelene Chi 做的。似乎总是用 Mickey Chi
            这个名字。他就是这样。所以事情是这样的。</p>
        <p>I want to see the numbers. So let me show you the numbers. And when I’m going to show numbers. the numbers
            that I’m going to show you show you the virtues of talking to yourself. So here’s the experiment. The
            experiment was done by a friend of mine, Michelene Chi. Always seems to go by the name Mickey Chi. There he
            is. So here’s the deal.</p>
        <p>她教的学生需要学习基础物理。801 类的东西。她选了八门科目，然后给他们做了一系列例子，然后给他们考试。所以有八门科目，所以他们被分成两组。下半部分和上半部分。成绩优于平均水平的人和成绩低于平均水平的人。</p>
        <p>The students that she worked with were expected to learn about elementary physics. 801 type stuff. And she
            took eight subjects, and she had them. she took them through a bunch of examples and then she gave them an
            examination. So eight subjects, and so they divide into two groups. The bottom half and the top half. The
            ones who did better than average and the ones who did worse than average.</p>
        <h2 id="unknown-328">未知</h2>
        <h2>Unknown</h2>
        <p>那么你可以说，好吧，那是什么意思？你可以说，他们自言自语了多少次？嗯，这是通过让他们大声说话来衡量的，因为他们在考试中解决问题。所以我们可以问一下，聪明的人和不太聪明的人做了多少自我解释？结果如下。最差的。最差的四个人自言自语了大约
            10 件事。</p>
        <p>So then you can say, well, OK, what did that mean? You can say, how much did they talk to themselves? Well,
            that was measured by having them talk out loud as they solved the problems on an examination. So we could
            ask how much self explanation was done by the smart ones versus the less smart ones? And here are the
            results. The worst ones. the worst four said about 10 things to themselves.</p>
        <p>最优秀的四人对自己说了大约 35
            件事。这是一个相当显著的差异。以下是更直接形式的数据。顺便说一句，这表明聪明的人得分是不太聪明的人的两倍。当我们查看他们在两个类别中给出的解释数量时，聪明的人对自己说的话是不太聪明的人的三倍。</p>
        <p>The best four said about 35 things to themselves. That’s a pretty dramatic difference. Here’s the data in a
            more straightforward form. This, by the way, points out that the smart ones scored twice as high as the less
            smart ones. And when we look at the number of explanations they gave themselves in two categories, smart
            ones said three times as much stuff to themselves as the less smart ones.</p>
        <p>因此，如您所见，解释分为两类。一些与监控有关，与物理完全无关。它们就像是，哦，天哪，我被困住了。或者，我不知道该怎么办。其他与物理有关。比如，好吧，也许我应该画一个力图。或者让我写下 f 等于
            ma，或者类似的东西，作为物理知识。</p>
        <p>So, as you can see, the explanations break down into two groups. Some have to do with monitoring and not with
            physics at all. They’re things like, oh hell, I’m stuck. Or, I don’t know what to do. And the others have to
            do with physics. Things like, well, maybe I should draw a force diagram. Or let me write down f equals ma,
            or something like that, as physics knowledge.</p>
        <h2 id="unknown-329">未知</h2>
        <h2>Unknown</h2>
        <p>我认为有趣的是，这个平均分数相差两倍，而自言自语的平均分数相差三倍。现在这还不完全正确，因为不清楚的是，如果你鼓励某人自言自语，他们自言自语的次数比平时多，这会让他们得分更高吗？我们只知道，自言自语越多的人得分越高。
        </p>
        <p>I think it’s interesting that this average score is different by a factor of two, and the average talking to
            oneself differed by a factor of three. Now this isn’t quite there, because what’s not clear is if you
            encourage somebody to talk to themself, and they talk to themselves more than they would have ordinarily,
            does that make them score better? All we know is that the ones who talk to themselves more do score better.
        </p>
        <p>但据传闻，我和一些 6.034
            的老学员交谈过，他们在解决问题时开始更多地自言自语，他们认为这样能让他们更聪明。现在我要提醒你不要在公共场合做太多这样的事。因为如果你自言自语太多，人们可能会产生误解。但事实上，这似乎确实有帮助。</p>
        <p>But anecdotally, talking to some veterans of 6.034, they’ve started talking to themselves more when they
            solve problems, and they think that it makes them smarter. Now I would caution you not to do this too much
            in public. Because people can get the wrong idea if you talk to yourself too much. But it does seem. it
            does, in fact, seem to help.</p>
        <p>上次我告诉你们如何成为一名优秀的科学家。现在我要告诉你们如何让自己变得更聪明。在本次讲座的最后，我想告诉你们如何包装自己的想法，让它们产生更大的影响。我想我本可以说如何让自己更出名，但我只说了如何更好地包装你的想法。
        </p>
        <p>Now what I did last time is I told you how to be a good scientist. What I’m telling you now is how to make
            yourself smarter. And I want to conclude this hour by telling you about how you can package your ideas so
            that they have greater impact. So I guess I could have said, how to make yourself more famous, but I’ve
            limited myself to saying how to package your ideas better.</p>
        <h2 id="unknown-330">未知</h2>
        <h2>Unknown</h2>
        <p>你之所以想更好地包装你的想法，是因为如果你比下一个笨蛋包装得更好，那么你就能得到教职，而他们得不到。如果你对我说，我要成为一名企业家，也是一样。如果你更好地包装你的想法，你就能得到风险投资家的钱，而下一个笨蛋则不会。
        </p>
        <p>And the reason you want to package your ideas better is because if you package your ideas better than the
            next slug, then you’re going to get the faculty position and they’re not. If you say to me, I’m going to be
            an entrepreneur, same thing. You’re going to get the venture capitalist money and the next slug won’t if you
            package your ideas better.</p>
        <p>因此，这件关于拱门的小作品比我想象的要出名得多。我做这件事的时候还很年轻，很愚蠢，根本不知道一件作品会有什么品质让它出名。直到很久以后我才明白过来。</p>
        <p>So this little piece of work on the arch business got a whole lot more famous than I ever expected. I did it
            when I was young and stupid, and didn’t have any idea what qualities might emerge from a piece of work that
            would make it well known. I only figured it out much later.</p>
        <p>但回想起来，它有五个特质，当你决定你的想法的包装形式是否能让这个想法广为人知时，你可以考虑这些特质。既然有五个特质，那么把它们都放在星号上就很方便了。所以这是第一个特质。我把这些都写成了单词，只是为了让它们更容易记住。
        </p>
        <p>But in retrospect, it has five qualities that you can think about when you’re deciding whether your packaging
            of your idea is in a form that will lead to that idea becoming well known. And since there are five of them,
            it’s convenient to put them all on the points of a star like so. So quality number one. I’ve made these all
            into s words just to make them easier to remember.</p>
        <h2 id="unknown-331">未知</h2>
        <h2>Unknown</h2>
        <p>第一个品质是作品要有某种符号。某种视觉处理，人们会用它来记住你的想法。那么这里的视觉符号是什么呢？嗯，这很容易弄清楚，对吧？那就是拱门。多年来，我无意中称之为拱门学习。所以你需要一个符号。然后你还需要一个口号。这是一种口头处理。
        </p>
        <p>Quality number one is that there’s some kind of symbol associated with a work. Some kind of visual handle
            that people will use to remember your idea. So what’s the visual symbol here? Well, that’s astonishingly
            easy to figure out, right? That’s the arch. For years without my intending it, this was called arch
            learning. So you need a symbol. Then you also need a slogan. That’s a kind of verbal handle.</p>
        <p>它并没有解释这个想法，但就像明斯基所说的那样，它足以让你回到你最初理解这个想法时的精神状态。那么这项工作的口号是什么呢？有人有什么想法吗？很明显。这个过程的运作需要什么？提出一个例子的能力与构成模型的能力非常相似，但不是其中之一。
        </p>
        <p>It doesn’t explain the idea, but it’s enough of a handle to, as Minsky would say, put you back in the mental
            state you were in when you understood the idea in the first place. So what is the slogan for this work?
            Anybody have any ideas? Pretty obvious. What’s essential to this process working? The ability to present an
            example is very similar that constitutes a model but isn’t one of those.</p>
        <p>学生：帕特里克·温斯顿教授：所以这是一次侥幸逃脱。如果你的工作要出名，你接下来需要的就是一个惊喜。这个东西有什么惊喜呢？嗯，惊喜。在此之前，人工智能领域与学习有关的一切都是神经网络的前身。学习任何东西都需要成千上万的例子。
        </p>
        <p>STUDENT: PROFESSOR PATRICK WINSTON: So it’s a near miss. The next thing you need if your work is going to
            become well known is a surprise. What’s the surprise with this stuff? Well, the surprise. everything that
            had been done in artificial intelligence having to do with learning before this time was precursors to
            neural nets. Thousands of examples to learn anything.</p>
        <h2 id="unknown-332">未知</h2>
        <h2>Unknown</h2>
        <p>所以最大的惊喜是，机器可以从每个例子中学到一些确定的东西。所以现在这被称为一次性学习。令人惊讶的是，计算机可以从一个例子中学到一些确定的东西。让我们看看。我们几乎完成了我们的星星。但上面还有更多的点。所以这一点是突出的。什么是突出的。什么是突出的想法？
        </p>
        <p>So the big surprise was that it was possible for a machine to learn something definite from each of the
            examples. So that now goes by the name of one shot learning. That was the surprise, that a computer could
            learn something definite from a single example. So let’s see. We’ve almost completed our star. But there are
            more points on it. So this point is the salient. What’s a salient. what’s a salient idea?</p>
        <p>何塞，你知道什么是突出的想法吗？他太害羞了，不敢告诉我。什么是突出的想法？啊，谁说重要？答案不对，但很好。你不害羞。那么它到底是什么意思？是的。学生：相对于某人已经在思考的东西？帕特里克·温斯顿教授：相对于某人正在思考的东西。不完全是。如果你有。如果你是某方面的专家。是吗？学生：帕特里克·温斯顿教授：非常接近。我们越来越接近了。是吗？
        </p>
        <p>Jose, do you know what a salient idea is? He’s too shy to tell me. What’s a salient idea? Ah, who said
            important? Wrong answer, but very good. You’re not shy. So what does it really mean? Yes. STUDENT: Relative
            to what somebody’s already thinking about? PROFESSOR PATRICK WINSTON: Relative to what somebody’s thinking
            about. Not quite. If you have a. if you’re an expert in. yes? STUDENT: PROFESSOR PATRICK WINSTON: Really
            close. We’re getting closer. Yes?</p>
        <p>学生：也许这个想法并不明显，但随着人们开始理解，它逐渐变得明显？帕特里克·温斯顿教授：我们正在集中精力。我们在这里围成一个圆圈，集中精力。是吗？学生：如果我抢先说了你接下来要说的话，它就像是通往理解这个想法的大门。帕特里克·温斯顿教授：它是什么？对不起。学生：它就像是通往理解这个想法的大门。
        </p>
        <p>STUDENT: Maybe an idea that wasn’t obviously apparent, but becomes apparent gradually as somebody starts to
            understand? PROFESSOR PATRICK WINSTON: We’re zeroing. we’re circling the wagons here and zeroing in on it.
            Yes? STUDENT: If I’m preempting what you’re about to say, it has sort of a doorway of how you can understand
            the idea. PROFESSOR PATRICK WINSTON: It’s what? Sorry. STUDENT: It’s sort of like a doorway of how you can
            grasp the idea.</p>
        <h2 id="unknown-333">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿教授：这倒也差不多，但如果你研究军事史，堡垒的突出部分是什么？嗯，这是一个值得你加入词汇表的词，因为它的意思大致如此，但它真正的意思是突出的东西。所以在堡垒上，如果这是堡垒，这些都会是突出部分，因为它们很突出。所以突出的概念通常很重要，因为它很突出。
        </p>
        <p>PROFESSOR PATRICK WINSTON: That’s sort if it, too, but if you study military history, what’s the salient on a
            fort? Well, this is a good word to have in your vocabulary because it sort of means all of those things, but
            what it really means is something that sticks out. So on a fort, if this were a fort, these would all be
            salients because they stick out. So the salient idea is usually important because it sticks out.</p>
        <p>但事实并非如此。其含义不是“重要”，而是“突出”。因此，如果一件作品有突出之处，它就会变得更加出名。这很有趣。麻省理工学院的论文中有太多好主意。怎么会有太多好主意呢？</p>
        <p>But it’s not. the meaning is not “important,” the meaning is “stick out.” So a piece of work becomes more
            famous if it has something that sticks out. It’s interesting. There are theses that have been written at MIT
            that have too many good ideas. And how can have too many good ideas?</p>
        <p>好吧，如果没有一个想法脱颖而出，成为人们在想到你时想到的想法，那么你可能会有太多的好想法。我们教员中有些人如果论文中的想法较少，他们可能会更出名。这真是太神奇了。所以这项工作确实有一个突出之处。突出的想法是你可以通过利用近乎失败的案例获得一次性学习。这就是突出的想法。
        </p>
        <p>Well, you can have too many good ideas if no one idea rises above and becomes the idea that people think
            about when they think about you. We have people on the faculty who would have been more famous if their
            theses had fewer ideas. It’s amazing. So this piece of work did have a salient. And the salient idea was
            that you could get one shot learning via the use of near misses. That was the salient idea.</p>
        <h2 id="unknown-334">未知</h2>
        <h2>Unknown</h2>
        <p>第五件事，啊。在 1
            月份的“如何演讲”讲座中，我会进一步讨论这个问题。我希望人们尝试融入演讲的第五件事是故事。因为我们人类不知何故喜欢故事。我们喜欢别人给我们讲故事。我们喜欢把事物包装成故事。相信我，我认为所有教育本质上都是关于讲故事和理解故事。
        </p>
        <p>The fifth thing, ah. Talk more about this in my “How to Speak” lecture in January. The fifth thing I like
            people to try to incorporate into their presentations is a story. Because we humans somehow love stories. We
            love people to tell us stories. We love things to be packaged in stories. And believe me, I think all of
            education is essentially about storytelling and story understanding.</p>
        <p>因此，如果你想要将自己的想法推销给风险投资家，如果你想要获得教职，如果你想要将自己的书卖给出版商，如果你想要将某样东西卖给客户，那么请问问自己，你的演讲是否具备这些品质。如果具备所有这些品质，那么它就更有可能有效。最终，你将会成名。
        </p>
        <p>So if you want your idea to be sold to the venture capitalist, if you want to get the faculty job, if you
            want to get your book sold to a publisher, if you want to sell something to a customer, ask yourself if your
            presentation has these qualities in it. And if it has all of those things, it’s a lot more likely to be
            effective than it doesn’t. And you’ll end up being famous.</p>
        <p>现在你对我说，嗯，出名。这听起来像斯隆学院那种概念。想出名难道不道德吗？也许这是你可以做出的决定。但每当我思考这个问题时，我总会想到这样的想法：你的想法就像你的孩子。你想确保他们过上最好的生活。所以，如果包装不当，他们就不会过上最好的生活。
        </p>
        <p>Now you say to me, well, being famous. that sounds like the Sloan School type of concept. Isn’t it immoral to
            want to be famous? Maybe that’s a decision you can make. But whenever I think about the question, I somehow
            think of the idea that your ideas are like your children. You want to be sure that they have the best life
            possible. So if they’re not packaged well, they won’t.</p>
        <h2 id="unknown-335">未知</h2>
        <h2>Unknown</h2>
        <p>我也想起了与朱莉娅·查尔德一起参加晚会的那个晚上。朱莉娅，还有我。我不知道我怎么会坐在朱莉娅·查尔德旁边。我想他们认为我是温斯顿家族的富豪之一。温斯顿花，或者海瑞·温斯顿钻石之类的。我就坐在朱莉娅·查尔德旁边。有趣的是。顺便问一下，你有没有注意到我现在在讲一个故事？
        </p>
        <p>I’m also reminded of an evening I spent at a soiree with Julia Child. Julia, and there’s me. And I have no
            idea how come I got to sit next to Julia Child. I think they thought I was one of the rich Winstons. The
            Winston flowers, or the Harry Winston diamonds or something like that. There I was, sitting next to Julia
            Child. And the interesting thing. by the way, did you notice I’m now telling a story?</p>
        <p>这次经历的有趣之处在于，人流不断。碰巧全是女性。人们从蔡尔德女士身边走过，说她真是太棒了，为她们的生活带来了如此巨大的改变。肯定有 10 个人。这太神奇了。人流源源不断。最后我向她靠过去，说蔡尔德女士，出名很有趣吗？
        </p>
        <p>The interesting thing about this experience was that there was a constant flow of people. happened to be all
            women. people going past Ms.&nbsp;Child saying how wonderful she was to have made such an enormous change in
            their life. Must have been 10 of them. It was amazing. Just steady flow. So eventually I leaned over to her
            and I said, Ms.&nbsp;Child, is it fun to be famous?</p>
        <p>她想了一会儿说，你会习惯的。这对我影响很大，因为你总是说，好吧，相反的情况是怎样的？被忽视好玩吗？答案是，不，被忽视并不好玩。所以是的，这是你可以习惯的事情，但你永远无法习惯别人忽视你的东西，尤其是好东西。</p>
        <p>And she thought about it a second and said, you get used to it. And that had a profound effect on me, because
            you always say, well, what’s the opposite like? Is it fun to be ignored? And the answer is, no, it’s not
            much fun to be ignored. So yeah, it’s something you can get used to, but you can never get used to having
            your stuff ignored, especially if it’s good stuff.</p>
        <h2 id="unknown-336">未知</h2>
        <h2>Unknown</h2>
        <p>这就是为什么我向你们推荐这个关于包装创意的行业。现在你们知道 6034 不仅仅是关于人工智能。它还关于如何做好科学。它关于如何让自己变得更聪明，如何让自己更出名。</p>
        <p>So that’s why I commend to you this business about packaging ideas. And now you see that 6034 is not just
            about AI. It’s about how to do good science. It’s how to make yourself smarter, and how to make yourself
            more famous.</p>
        <h1 id="learning-support-vector-machines">16.学习：支持向量机</h1>
        <h1>16. Learning: Support Vector Machines</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:34 <a
                href="https://youtube.com/watch?v=_PwhiWxHK8o">https://youtube.com/watch?v=_PwhiWxHK8o</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:34 <a
                href="https://youtube.com/watch?v=_PwhiWxHK8o">https://youtube.com/watch?v=_PwhiWxHK8o</a></p>
        <h2 id="unknown-337">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：那么我们现在讲到了哪里？我们从简单的学习方法开始。然后，我们谈到了学习的本质，我们对此有所启发。我们的大脑里充满了神经元，我们似乎是从灵长类动物进化而来的。然后，我们谈到了如何看待问题、解决问题以及如何学习概念。
        </p>
        <p>PATRICK WINSTON: So where are we? We started off with simple methods for learning stuff. Then, we talked a
            little about a purchase of learning that we’re vaguely inspired by. The fact that our heads are stuffed with
            neurons, and that we seemed to have evolved from primates. Then, we talked about looking at the problem and
            address the issue of and how it’s possible to learn concepts.</p>
        <p>但现在，我们又回到了起点，思考如何用决策边界划分空间。但是，你可以用神经网络、最近邻或 ID
            树来实现。这些都是非常简单的想法，而且经常奏效。今天，我们将讨论一个仍然有实现的非常复杂的想法。所以这需要放在每个文明人的工具包里。</p>
        <p>But now, we’re coming full circle back to the beginning and thinking about how to divide up a space with
            decision boundaries. But whereas, you do it with a neural net or a nearest neighbors or a ID tree. Those are
            very simple ideas that work very often. Today, we’re going to talk about a very sophisticated idea that
            still has a implementation. So this needs to be in the tool bag of every civilized person.</p>
        <p>这是关于支持向量机的，这是一个被开发出来的想法。好吧，今天我想跟你们谈谈想法是如何发展的。因为当你在书中看到类似的东西时，你会想，嗯，弗拉基米尔·瓦普尼克 (Vladimir Vapnik)
            只是在一个星期六下午想到了这个，当时天气太糟糕了，不能出门。但事情并不是这样的。事情的发生方式非常不同。我想跟你们谈谈这个。</p>
        <p>This is about support vector machines, an idea that was developed. Well, I want to talk to you today about
            how ideas develop, actually. Because you look at stuff like this in a book, and you think, well, Vladimir
            Vapnik just figured this out one Saturday afternoon when the weather was too bad to go outside. That’s not
            how it happens. It happens very differently. I want to talk to you a little about that.</p>
        <h2 id="unknown-338">未知</h2>
        <h2>Unknown</h2>
        <p>接下来，关于那些仍然健在的人所做出的伟大成就，你可以问他们是如何做到的。但你无法问傅立叶。你不能问傅立叶，你是怎么做到的？你是在星期六下午想到的吗？但可以打电话给瓦普尼克，向他提问。这就是我在这个小时快结束时要谈论的内容。
        </p>
        <p>The next thing about great things that were done by people who are still alive is you can ask them how they
            did it. You can’t do that with Fourier. You can’t say to Fourier, how did you do it? Did you dream it up on
            a Saturday afternoon? But can call Vapnik on the phone and ask him questions. That’s the stuff I’m going to
            talk about toward the end of the hour.</p>
        <p>好吧，这都是关于决策边界的。现在，我们有几种技术可以用来绘制一些决策边界。这是同样的问题。如果我们在这里绘制决策边界，我们可能会得到类似这样的结果。如果我们采用最近邻方法，如果我们采用 ID
            树，我们只需画一条这样的线。</p>
        <p>Well, it’s all about decision boundaries. And now, we have several techniques that we can use to draw some
            decision boundaries. And here’s the same problem. And if we drew decision boundaries in here, we might get
            something that would look like maybe this. If we were doing a nearest neighbor approach, and if we’re doing
            ID trees, we’ll just draw in a line like that.</p>
        <p>如果我们在做神经网络，那么，你可以在神经网络的任何地方放置很多直线，这取决于它是如何训练的。或者你只是简单地进入那里并设计它，所以如果你愿意的话，你可以这样做。你会认为，在人们研究这种东西 50 或 75
            年后，就不会再有任何技巧了。</p>
        <p>And if we’re doing neural nets, well, you can put in a lot of straight lines wherever you like with a neural
            net, depending on how it’s trained up. Or if you just simply go in there and design it, so you could do that
            if you wanted. And you would think that after people have been working on this sort of stuff for 50 or 75
            years that there wouldn’t be any tricks in the bag left.</p>
        <h2 id="unknown-339">未知</h2>
        <h2>Unknown</h2>
        <p>这时，每个人都感到惊讶，因为大约在 90 年代初期，弗拉基米尔·瓦普尼克 (Vladimir Vapnik)
            提出了我即将要讲的思想。瓦普尼克说的是这样的。这里有一个空间，有一些反面例子，也有一些正面例子。你如何区分正面例子和反面例子？他说我们要做的是画一条直线。</p>
        <p>And that’s when everybody got surprised, because around the early ’90s Vladimir Vapnik introduced the ideas
            I’m about to talk to you about. So what Vapnik says is something like this. Here you have a space, and you
            have some negative examples, and you have some positive examples. How do you divide the positive examples
            from the negative examples? And what he says that we want to do is we want to draw a straight line.</p>
        <p>但问题是哪条直线。好吧，我们想画一条直线。那么，这是一条好的直线吗？一条像那样向上的直线？可能不太好。一条就在这儿的直线怎么样？好吧，那可能会把它们分开，但它似乎非常接近负面例子。所以也许我们应该做的是，我们应该在这里画一条直线，有点像这样。
        </p>
        <p>But which straight line is the question. Well, we want to draw a straight line. Well, would this be a good
            straight line? One that went up like that? Probably not so hot. How about one that’s just right here? Well,
            that might separate them, but it seems awfully close to the negative examples. So maybe what we ought to do
            is we ought to draw our straight line in here, sort of like this.</p>
        <p>这条线是为了在正样本和负样本之间划出一条最宽的路而画的。这就是我称之为最宽路方法的原因。这样就为决策边界的划定提供了方法。就是划一条直线，但与 ID 树划一条直线的方式相反。</p>
        <p>And that line is drawn with a view toward putting in the widest street that separates the positive samples
            from the negative samples. That’s why I call it the widest street approach. So that makes way of putting in
            the decision boundary. is to put in a straight line but in contrast with the way ID tree puts in a straight
            line.</p>
        <h2 id="unknown-340">未知</h2>
        <h2>Unknown</h2>
        <p>它试图以某种方式将正例和反例区分开来。这条路尽可能宽。好的。所以你可能会想在 UROP 项目中这样做，然后就这样了。有什么大不了的？所以我们要做的是，我们必须弄清楚为什么这是一件大事。</p>
        <p>It tries to put the line in such a way as the separation between the positive and negative examples. That
            street is as wide as possible. All right. So you might think to do that in the UROP project, and then, let
            it go with that. What’s the big deal? So what we’ve got to do is we’ve got to go through why it’s a big
            deal.</p>
        <p>首先，我们想思考一下如何制定使用该决策边界的决策规则。所以我要请你想象一下，我们有一个任意长度的向量，限制为垂直于中线，或者如果你愿意，垂直于排水沟。它垂直于街道的中线。好吧，它是这样画出来的，这是正确的。</p>
        <p>So first of all, we like to think about how you would make a decision rule that would use that decision
            boundary. So what I’m going to ask you to imagine is that we’ve got a vector of any length that you like,
            constrained to be perpendicular to the median, or if you like, perpendicular to the gutters. It’s
            perpendicular to the median line of the street. All right, it’s drawn in such a way that’s true.</p>
        <p>我们还不知道它的长度。然后，我们还有一些未知数，比如说，就在这里。我们有一个通过 Excel 指向它的向量。所以现在，我们真正感兴趣的是这个未知数是在街道的右侧还是在街道的左侧。所以我们要做的就是将这个向量 u
            投影到与街道垂直的向量上。</p>
        <p>We don’t know anything about it’s length, yet. Then, we also have some unknown, say, right here. And we have
            a vector that points to it by excel. So now, what we’re really interested in is whether or not that unknown
            is on the right side of the street or on the left side of the street. So what we’d what to do is want to
            project that vector, u, down on to one that’s perpendicular to the street.</p>
        <h2 id="unknown-341">未知</h2>
        <h2>Unknown</h2>
        <p>因为这样，我们就会得到这个方向的距离，或者与这个方向成比例的数字。我们走得越远，我们就越接近街道的右侧，而街道的右侧并不是正确的一侧，而是街道的右侧。</p>
        <p>Because then, we’ll have the distance in this direction or a number that’s proportional to this in this
            direction. And the further out we go, the closer we’ll get to being on the right side of the street, where
            the right side of the street is not the correct side but actually the right side of the street.</p>
        <p>因此，我们可以做的是，取 w 并乘以 u，然后测量该数字是否等于或大于某个常数 c。请记住，点积已将投影取到 w 上。投影越大，投影沿这条线的位置就越远。</p>
        <p>So what we can do is we can say, let’s take w and dot it with u and measure whether or not that number is
            equal to or greater than some constant, c.&nbsp;So remember that the dot product has taken the projection
            onto w. And the bigger that projection is, the further out along this line the projection will lie.</p>
        <p>最终，它会变得非常大，以至于投影会穿过街道的中线，我们会说它一定是正样本。或者我们可以说，不失一般性，点积加上某个常数 b 等于或大于 0。如果这是真的，那么它就是正样本。这就是我们的决策规则。</p>
        <p>And eventually it will be so big that the projection crosses the median line of the street, and we’ll say it
            must be a positive sample. Or we could say, without loss of generality that the dot product plus some
            constant, b, is equal to or greater than 0. If that’s true, then it’s a positive sample. So that’s our
            decision rule.</p>
        <h2 id="unknown-342">未知</h2>
        <h2>Unknown</h2>
        <p>这是我们要理解支持向量机这一概念所必须了解的几个要素中的第一个。这就是决策规则。问题是我们不知道要使用什么常数，也不知道要使用哪个 w。我们知道 w 必须垂直于街道的中线。</p>
        <p>And this is the first in several elements that we’re going to have to line up to understand this idea called
            support vector machines. So that’s the decision rule. And the trouble is we don’t know what constant to use,
            and we don’t know which w to use either. We know that w has to be perpendicular to the median line of the
            street.</p>
        <p>但是有很多 w 垂直于街道的中线，因为它的长度可以是任意的。所以我们这里没有足够的约束来固定一个特定的 b 或特定的 w。到目前为止你明白了吗？好的。顺便说一下，我们只要说 c 等于减 b 就可以得到这个。</p>
        <p>But there’s lot of w’s that are perpendicular to the median line of the street, because it could be of any
            length. So we don’t have enough constraint here to fix a particular b or a particular w. Are you with me so
            far? All right. And this, by the way, we get just by saying that c equals minus b.</p>
        <p>我们接下来要做的是施加一些额外的约束，无论你是否要对情况施加足够的约束，以便我们能够实际计算 ab 和 a w。</p>
        <p>What we’re going to do next is we’re going to lay on some additional constraints whether you’re toward
            putting enough constraint on the situation that we can actually calculate a b and a w.</p>
        <h2 id="unknown-343">未知</h2>
        <h2>Unknown</h2>
        <p>所以我们要说的是，如果我们检查这个量是否大于或小于 0 以做出决定，那么我们要做的就是，如果我们取该向量 w，并将其与某个 x 加起来的点积，现在就是某个正样本。这不是未知数。这是一个正样本。</p>
        <p>So what we’re going to say is this, that if we look at this quantity that we’re checking out to be greater
            than or less than 0 to make our decision, then, what we’re going to do is we’re going to say that if we take
            that vector w, and we take the dot product of that with some x plus, some positive sample, now. This is not
            an unknown. This is a positive sample.</p>
        <p>如果我们取这两个向量的点积，并且我们有 b，就像在我们的决策规则中一样，我们希望它等于或大于 1。换句话说，你可以在这条街的任何地方都是未知的，并且比 0
            稍大一点或稍小一点。但如果你是一个正样本，我们会坚持这个决策函数给出 1 或更大的值。</p>
        <p>If we take the dot product of those two vectors, and we had b just like in our decision rule, we’re going to
            want that to be equal to or greater than 1. So in other words, you can be an unknown anywhere in this street
            and be just a little bit greater or just a little bit less than 0. But if you’re a positive sample, we’re
            going to insist that this decision function gives the value of one or greater.</p>
        <p>同样，如果 w 认为它提供了一些负样本，那么我们会说它必须等于或小于 -1。好吧。所以如果你是一个负样本，比如这两个中的一个或任何可能在这里的负样本，这个给我们决策规则的函数必须返回 -1
            或更小。所以这里有一个距离分离。</p>
        <p>Likewise, if w thought it was some negative sample is provided to us, then we’re going to say that has to be
            equal to or less than minus 1. All right. So if you’re a minus sample, like one of these two guys or any
            minus sample that may lie down here, this function that gives us the decision rule must return minus 1 or
            less. So there’s a separation of distance here.</p>
        <h2 id="unknown-344">未知</h2>
        <h2>Unknown</h2>
        <p>对所有样本进行减 1 到加 1
            的操作。这很酷。但我们还没有完成，因为像这样携带两个方程式很麻烦。所以我们要做的是引入另一个变量，让事情变得更容易一些。就像我们做的许多事情一样，当我们开发这种东西时，引入这个变量并不是上帝说必须要做的事情。它是什么？
        </p>
        <p>Minus 1 to plus 1 for all of the samples. So that’s cool. But we’re not quite done, because carrying around
            two equations like this, it’s a pain. So what we’re going to do is we’re going to introduce another variable
            to make like a little easier. Like many things that we do, and when we develop this kind of stuff,
            introducing this variable is not something that God says has to be done. What is it?</p>
        <p>我们引入这些额外的内容是为了做什么？为了让数学更方便，也就是数学方便。所以我们要做的是引入一个变量 y sub i，这样 y sub I 等于正样本的正 1，负样本的负
            1。好的。因此，对于每个样本，我们将为引入的这个新量 y 赋予一个值。</p>
        <p>We introduced this additional stuff to do what? To make the mathematics more convenient, so mathematical
            convenience. So what we’re going to do is we’re going to introduce a variable, y sub i, such that y sub I is
            equal to plus 1 for plus samples and minus 1 for negative samples. All right. So for each sample, we’re
            going to have a value for this new quantity we’ve introduced, y.</p>
        <p>y 的值将由它是正样本还是负样本决定。如果是正样本，则对于上面这种情况，它必须为正 1，而对于下面这种情况，它必须为负 1。</p>
        <p>And the value of y is going to be determined by whether it’s a positive sample or negative sample. If it’s a
            positive sample it’s got to be plus 1 for this situation up here, and it’s going to be minus 1 for this
            situation down here.</p>
        <h2 id="unknown-345">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们要对第一个等式进行的操作是将其乘以 y sub i，现在 x of i 加上 b 等于或大于 1。然后，你知道我们要做什么吗？我们还要将这个等式的左边乘以 y sub i。因此，第二个等式变为 y sub I
            乘以 x sub I 加 b。</p>
        <p>So what we’re going to do with this first equation is we’re going to multiply it by y sub i, and that is now
            x of i, plus b is equal to or greater than 1. And then, you know what we’re going to do? We’re going to
            multiply the left side of this equation by y sub i, as well. So the second equation becomes y sub I times x
            sub I plus b.</p>
        <p>现在，这在这里起什么作用？我们将这个数乘以负 1。所以以前的情况是小于负 1。所以如果我们将它乘以负 1，那么它必须大于正 1。这两个等式是相同的，因为这引入了这个小小的数学便利。所以现在，我们可以说 y sub I
            乘以 x sub I 加 b。好吧，我们要做的就是。</p>
        <p>And now, what does that do over here? We multiplied this guy times minus 1. So it used to be the case that
            was less than minus 1. So if we multiply it by minus 1, then it has to be greater than plus 1. The two
            equations are the same, because that introduces this little mathematical convenience. So now, we can say
            that y sub I times x sub I plus b. Well, what we’re going to do.</p>
        <p>布雷特？学生：w 怎么了？帕特里克·温斯顿：哦，我漏掉了 aw 吗？抱歉。谢谢。是的，那样的话我没法解释清楚。所以就是 w 点，w
            点。谢谢，布雷特。这些都是矢量。我很快就会忘记在上面放上小矢量标记，但你知道我的意思。所以就是 w 加 b。</p>
        <p>Brett? STUDENT: What happened to the w? PATRICK WINSTON: Oh, did I leave out a w? I’m sorry. Thank you. Yeah,
            I wouldn’t have gotten very far with that. So that’s dot it with w, dot it with w. Thank you, Brett. Those
            are all vectors. I’ll pretty soon forget to put the little vector marks on there, but you know what I mean.
            So that’s w plus b.</p>
        <h2 id="unknown-346">未知</h2>
        <h2>Unknown</h2>
        <p>现在，让我把 1 移到左边，它等于或大于 0。好的。有了 Brett 的修正，我认为一切都没问题。但我们还要再进一步，我们要说 y sub I 乘以 x sub I 乘以 w 加 b 减 1。它总是必须等于或大于
            0。但我要说的是，如果我们在边线上 x sub I。</p>
        <p>And now, let me bring that 1 over to the left side, and that’s equal to or greater than 0. All right. With
            Brett’s correction, I think everything’s OK. But we’re going to take one more step, and we’re going to say
            that y sub I times x sub I times w plus b minus 1. It’s always got to be equal to or greater than 0. But
            what I’m going to say is if we’re for x sub I in a gutter.</p>
        <p>因此，它总是大于 0，但我们将添加额外的约束，即对于最终落入街道排水沟的所有样本，它都将恰好为 0。因此，该表达式的值对于该样本恰好为 0，对于该样本和该样本恰好为 0，而不是对于该样本恰好为 0。它必须大于
            1。好吗？所以这是第二步。</p>
        <p>So there’s always going to be greater than 0, but we’re going to add the additional constraint that it’s
            going to be exactly 0 for all the samples that end up in the gutters here of the street. So the value of
            that expression is going to be exactly 0 for that sample, 0 for this sample and this sample, not 0 for that
            sample. It’s got to be greater than 1. All right? So that’s step number two.</p>
        <p>这是第一步。好的。现在，我们只需要讨论一些表达式，一些约束。现在，我们在这里要做什么？我忘了。哦，我现在想起来。我们正在尝试弄清楚如何安排街道上将正号和负号分开的线尽可能宽。所以也许我们最好弄清楚如何表达两个排水沟之间的距离。
        </p>
        <p>And this is step number one. OK. So now, we’ve just got some expressions to talk about, some constraints.
            Now, what are we trying to do here? I forgot. Oh, I remember now. We’re trying to figure out how to arrange
            for the line to be such at the street separating the pluses from the minuses as wide as possible. So maybe
            we better figure out how we can express the distance between the two gutters.</p>
        <h2 id="unknown-347">未知</h2>
        <h2>Unknown</h2>
        <p>让我们重复一下我们的绘图。我们这里有一些减号，这里有一些加号，这里还有一些向下的排水沟。现在，我们这里有一个指向减号的向量，这里还有一个指向加号的向量。所以我们称其为 x 加号，而这个为 x
            减号。那么街道的宽度是多少？我还不知道。</p>
        <p>Let’s just repeat our drawing. We’ve got some minuses here, got pluses out here, and we’ve got gutters that
            are going down here. And now, we’ve got a vector here to a minus, and we’ve got a vector here to a plus. So
            we’ll call that x plus and this x minus. So what’s the width of the street? I don’t know, yet.</p>
        <p>但我们可以做的是取这两个向量的差，然后得到一个像这样的向量，对吧？也就是 x 加减 x 减。</p>
        <p>But what we can do is we can take the difference of those two vectors, and that will be a vector that looks
            like this, right? So that’s x plus minus x minus.</p>
        <p>那么现在，如果我只有一个单位法线，它垂直于街道的中线，如果它是一个单位法线，那么我只需取该单位法线和这个差向量的点积，这就是街道的宽度，对吗？换句话说，如果我有一个该方向的单位向量，那么我只需将两者点在一起，这就是街道的宽度。
        </p>
        <p>So now, if I only had a unit normal that’s normal to the median line of the street, if it’s a unit normal,
            then I could just take the dot product or that unit normal and this difference vector, and that would be the
            width of the street, right? So in other words, if I had a unit vector in that direction, then I could just
            dot the two together, and that would be the width of the street.</p>
        <h2 id="unknown-348">未知</h2>
        <h2>Unknown</h2>
        <p>在我忘记之前，让我把它记下来。宽度等于 x 加减 x 减。好的。这是差值向量。现在，我必须将它乘以单位向量。但等一下。我说 w 是法线，对吧？w 是法线。</p>
        <p>So let me write that down before I forget. So the width is equal to x plus minus x minus. OK. That’s the
            difference vector. And now, I’ve got to multiple it by unit vector. But wait a minute. I said that w is a
            normal, right? The w is a normal.</p>
        <p>所以我能做的是，我可以将其乘以 w，然后除以 w
            的大小，这样它就会变成一个单位向量。所以那个点积，不是乘积，那个点积实际上是一个标量，它是街道的宽度。它没有多大用处，因为看起来我们不会从中获得太多东西。哦，但我不知道。</p>
        <p>So what I can do is I can multiply this times w, and then, we’ll divide by the magnitude of w, and that will
            make it a unit vector. So that dot product, not a product, that dot product is, in fact, a scalar, and it’s
            the width of the street. It doesn’t do as much good, because it doesn’t look like we get much out of it. Oh,
            but I don’t know.</p>
        <p>让我们看看，我们能从中得到什么？哦，天哪，我们这里有一个方程，这个方程限制了位于边缘的样本。因此，如果我们有一个正样本，那么这是加 1，我们有这个方程。所以它说 x 加乘以 w 等于，哦，1 减
            b。看，我只是取这里的这个部分，这个向量，我用 x 加点。</p>
        <p>Let’s see, what can we get out of it? Oh gee, we’ve got this equation over here, this equation that
            constrains the samples that lie in the gutter. So if we have a positive sample, for example, then this is
            plus 1, and we have this equation. So it says that x plus times w is equal to, oh, 1 minus b. See, I’m just
            taking this part here, this vector here, and I’m dotting it with x plus.</p>
        <h2 id="unknown-349">未知</h2>
        <h2>Unknown</h2>
        <p>这就是这部分。对于这种样本，y 为 1。所以我只需将 1 和 b 移回另一侧，就得到了 1 减 b。好吗？好吧，我们可以对 x 减法做同样的操作。如果我们得到的是负样本，那么 y sub I 为负。这给了我们负 w
            乘以点除以 x sub i。</p>
        <p>So that’s this piece right here. y is 1 for this kind of sample. So I’ll just take the 1 and the b back over
            to the other side, and I’ve got 1 minus b. OK? Well, we can do the same trick with x minus. If we’ve got a
            negative sample, then y sub I is negative. That gives us our negative w times dot over x sub i.</p>
        <p>但现在，我们把这个东西放回到右边，我们得到 1 加 b。所以所有人都可以把这个东西重写为 2 除以 w
            的量级。我是怎么做到的？好吧，我决定要强制执行这个约束。我注意到街道的宽度必须是这个差向量乘以一个单位向量。然后，我使用这个约束在这里重新插入一些值。</p>
        <p>But now, we take this stuff back over to the right side, and we get 1 plus b. So that all licenses to rewrite
            this thing as 2 over the magnitude of w. How did I get there? Well, I decided I was going to enforce this
            constraint. I noted that the width of the street has got to be this difference vector times a unit vector.
            Then, I used the constraint to plug back some values here.</p>
        <p>我惊喜地发现，街道的宽度是 w 的 2 倍。是的，布雷特？学生：所以你的第一个 x 加是减 b，x 减是 1 加 b。帕特里克·温斯顿：是的。学生：所以你要减去它？帕特里克·温斯顿：让我们看看。</p>
        <p>And I discovered to my delight and amazement that the width of the street is 2 over the magnitude of w. Yes,
            Brett? STUDENT: So your first x plus is minus b, and x minus is 1 plus b. PATRICK WINSTON: Yeah. STUDENT: So
            you’re subtracting it? PATRICK WINSTON: Let’s see.</p>
        <h2 id="unknown-350">未知</h2>
        <h2>Unknown</h2>
        <p>如果这里是负数，那么它就是负数，然后，b 就是负数，当我把 b 移到另一边时，它就变成正数。学生：是的，所以如果你用右边的减去左边的，帕特里克·温斯顿：不。不，对不起。这里的表达式是 1 加
            b。相信我，它有效。我的腿还没有像上周五那样缠在一起，至少现在还没有。这是可能的。</p>
        <p>If I’ve got a minus here, then that makes that minus, and then, the b is minus, and when I take the b over to
            the other side it becomes plus. STUDENT: Yeah, so if you subtract the left with the right PATRICK WINSTON:
            No.&nbsp;No, sorry. This expression here is 1 plus b. Trust me it works. I haven’t got my legs all tangled
            up like last Friday, well, not yet, anyway. It’s possible.</p>
        <p>这里最终会涉及很多代数运算。所以这个量是第三个奇迹。这个量是街道的宽度。我们要做的是尽量使其最大化，对吧？所以如果我们要在决定要使用的约束条件下得到最宽的街道，我们希望使 2 除以 w 的量级。好的。</p>
        <p>There’s going to be a lot of algebra here eventually. So this quantity here, this is miracle number three.
            This quantity here is the width of the street. And what we’re trying to do is we’re trying to maximize that,
            right? So we want to maximize 2 over the magnitude of w if we’re to get the widest street under the
            constraints that we’ve decided that we’re going to work with. All right.</p>
        <p>所以这意味着可以取 1/w 的最大值。我们只需去掉常数。这意味着可以最小化 w 的量级，对吧？这意味着可以最小化 w 的量级的 1/2 倍的平方。对吧，布雷特？我为什么要这样做？我为什么要乘以 1/2
            然后求平方？学生：因为这样在数学上很方便。帕特里克·温斯顿：这样在数学上很方便。谢谢。</p>
        <p>So that means that it’s OK to maximize 1 over w, instead. We just drop the constant. And that means that it’s
            OK to minimize the magnitude of w, right? And that means that it’s OK to minimize 1/2 times the magnitude of
            w squared. Right, Brett? Why did I do that? Why did I multiply by 1/2 and square it? STUDENT: Because it’s
            mathematically convenient. PATRICK WINSTON: It’s mathematically convenient. Thank you.</p>
        <h2 id="unknown-351">未知</h2>
        <h2>Unknown</h2>
        <p>所以这是开发中的第三点。那么我们要去哪里呢？我们决定将其作为我们的决策规则。我们将看看我们处于哪一边。我们决定限制这种情况，因此决策规则的值在正样本的边距中为正 1，在负样本的边距中为负 1。</p>
        <p>So this is point number three in the development. So where do we go? We decided that was going to be our
            decision rule. We’re going to see which side of the line we’re on. We decided to constrain the situation, so
            the value of the decision rule is plus 1 in the gutters for the positive samples and minus 1 in the gutters
            for the negative samples.</p>
        <p>然后，我们发现，最大限度地扩大街道的宽度会让我们产生这样的表达，我们希望将其最大化。我们应该休息一下吗？我们应该喝杯咖啡吗？太糟糕了，在这种情况下我们不能这样做。但如果可以的话，我们会的。我敢肯定，当瓦普尼克说到这一点时，他出去喝咖啡了。
        </p>
        <p>And then, we discovered that maximizing the width of the street led us to an expression like that, which we
            wish to maximize. Should we take a break? Should we get coffee? Too bad, we can’t do that in this kind of
            situation. But we would if we could. And I’m sure when Vapnik got to this point, he went out for coffee.</p>
        <p>现在，我们回过头来说，好吧，让我们让这些表达式开始发展成一首歌。不是那样，那样太无聊了，说到 Vapnik。它会唱什么歌？我们这里有一个表达式，我们想找到它的最小值，极值。我们这里有一些我们想遵守的约束。我们该怎么办？
        </p>
        <p>So now, we back up, and we say, well, let’s let these expressions start developing into a song. Not like
            that, that’s vapid, speaking of Vapnik. What song is it going to sing? We’ve got an expression here that
            we’d like to find the minimum of, the extremum of. And we’ve got some constraints here that we would like to
            honor. What are we going to do?</p>
        <h2 id="unknown-352">未知</h2>
        <h2>Unknown</h2>
        <p>让我以谜题的形式来回答你我们要做的事情。这和勒让德有关吗？这和拉普拉斯有关吗？或者这和拉格朗日有关吗？她说拉格朗日。实际上，据说这三人都是傅立叶博士答辩委员会的成员。这一定是个很好的例子。但我们想谈谈拉格朗日，因为我们遇到了一个情况。这是
            1801.1802 吗？</p>
        <p>Let me put what we’re going to do to you in the form of a puzzle. Is it got something to do with Legendre?
            Has it got something to do with Laplace? Or does it have something to do with Lagrange? She says Lagrange.
            Actually, all three were said to be on Fourier’s Doctoral Defense Committee. must have been quite an
            example. But we want to talk about Lagrange, because we’ve got a situation here. Is this 1801.1802?</p>
        <ol class="incremental" start="1802" type="1">
            <li>我们在 1802
                年就学到，如果我们要找到一个有约束的函数的极值，那么我们就必须使用拉格朗日乘数。这将给我们一个新的表达式，我们可以最大化或最小化它而不必再考虑约束。这就是拉格朗日乘数的工作原理。所以这给我们带来了第四个奇迹，第四个发展部分。它的工作原理是这样的。我们说
                L。</li>
            <li>We learned in 1802 that if we going to find the extremum of a function with constraints, then we’re
                going to have to use Lagrange multipliers. That would give us a new expression, which we can maximize or
                minimize without thinking about the constraints anymore. That’s how Lagrange multipliers work. So this
                brings us to miracle number four, developmental piece number four. And it works like this. We’re going
                to say that L.</li>
        </ol>
        <p>为了最大化街道的宽度，我们要尝试最大化的值为 1/2 乘以该向量 w 的平方减去。现在，我们必须对所有约束求和。每个约束都有一个乘数，alpha sub i。然后，我们写下约束。当我们写下约束时，它就在那里。</p>
        <p>The thing we’re going to try to maximize in order to maximize the width of the street. is equal to 1/2 times
            the magnitude of that vector, w, squared minus. And now, we’ve got to have a summation over all the
            constraints. And each or those constraints is going to have a multiplier, alpha sub i. And then, we write
            down the constraint. And when we write down a constraint, there it is up there.</p>
        <h2 id="unknown-353">未知</h2>
        <h2>Unknown</h2>
        <p>我必须非常小心，否则，我会迷失在代数中。所以约束是 y sub I 乘以向量 w，加上向量 x sub I 加 b，现在，我有一个右括号，a 减
            1。这就是我的约束的结尾，就像这样。我当然希望我做对了，因为如果做错了，我就会陷入大麻烦。有人看到其中有什么错误吗？看起来是对的。不是吗？</p>
        <p>And I’ve got to be hyper careful here, because, otherwise, I’ll get lost in the algebra. So the constraint is
            y sub I times vector, w, dotted with vector x sub I plus b, and now, I’ve got a closing parenthesis, a minus
            1. That’s the end of my constraint, like so. I sure hope I’ve got that right, because I’ll be in deep
            trouble if that’s wrong. Anybody see any bugs in that? That looks right. doesn’t it?</p>
        <p>我们已经得到了要处理的原始数据。现在，我们已将拉格朗日乘数全部相乘。这又回到上面的约束，其中每个约束都被限制为 0。好吧，这里有一点数学上的小技巧，因为最终，将为 0 的那些是这里的拉格朗日乘数。</p>
        <p>We’ve got the original thing we’re trying to work with. Now, we’ve got Lagrange multipliers all multiplied.
            It’s back to that constraint up there, where each constraint is constrained to be 0. Well, there’s a little
            bit of mathematical slight of hand here, because in the end, the ones that are going to be 0, the Lagrange
            multipliers here.</p>
        <p>非 0 的那些将是与位于边缘的向量相连的那些。其余的将是 0。但无论如何，我们可以假装这就是我们正在做的事情。我不在乎它是最大值还是最小值。我记不清了。但我们要做的是尝试找到它的极值。那么我们该怎么做呢？</p>
        <p>The ones that are going to be non 0 are going to be the ones connected with vectors that lie in the gutter.
            The rest are going to be 0. But in any event, we can pretend that this is what we’re doing. I don’t care
            whether it’s a maximum or minimum. I’ve lost track. But what we’re going to do is we’re going to try to find
            an extremum of that. So what do we do?</p>
        <h2 id="unknown-354">未知</h2>
        <h2>Unknown</h2>
        <p>1801 教会了我们什么？找到最大值。好吧，我们必须找到导数并将它们设置为 0。然后，完成之后，进行一些操作，我们将看到一首美妙的歌曲开始出现。所以让我们看看我们是否能做到。让我们取 L
            的偏函数，即拉格朗日量，关于向量 w。天哪，你如何对向量进行微分？</p>
        <p>What does 1801 teach us about? Finding the maximum. well, we’ve got to find the derivatives and set them to
            0. And then, after we’ve done that, a little bit of that manipulation, we’re going to see a wonderful song
            start to emerge. So let’s see if we can do it. Let’s take the partial of L, the Lagrangian, with respect to
            the vector, w. Oh my God, how do you differentiate with respect to a vector?</p>
        <p>事实证明，它的形式与对标量求导完全相同。而向自己证明这一点的方法就是，你只需根据向量的所有分量展开所有内容。你对求导的对象求导，结果一切都一样。</p>
        <p>It turns out that it has a form that looks exactly like differentiating with respect to a scalar. And the way
            you prove that to yourself is you just expand everything in terms of all of the vector’s components. You
            differentiate those with respect to what you’re differentiating with respect to, and everything turns out
            the same.</p>
        <p>因此，当您对矢量 w 求导时，得到的结果是 2 下降，并且我们只得到了 w 的幅值。它是 w 的幅值吗？是的，就像这样。它是 w 的幅值吗？哦，它不是 w 的幅值。它只是 w，就像这样，没有幅值。然后，我们在这里得到了
            aw，所以我们也必须对 w 求导这部分。</p>
        <p>So what you get when you differentiate this with respect to the vector, w, is 2 comes down, and we have just
            magnitude of w. Was it the magnitude of w? Yeah, like so. Was it the magnitude of w? Oh, it’s not the
            magnitude of w. It’s just w, like so, no magnitude involved. Then, we’ve got a w over here, so we’ve got to
            differentiate this part with respect to w, as well.</p>
        <h2 id="unknown-355">未知</h2>
        <h2>Unknown</h2>
        <p>但那部分要简单得多，因为我们只有 w。没有量级。它没有任何幂。那么 w 乘以什么呢？嗯，它乘以 x 和 y 乘以 i 和 alpha 乘以 i。好的。</p>
        <p>But that part’s a lot easier, because all we have there is a w. There’s no magnitude. It’s not raised to any
            power. So what’s w multiplied by? Well, it’s multiplied by x and y sub I and alpha sub i. All right.</p>
        <p>所以这意味着这个表达式，这个拉格朗日导数，关于 w 等于 w 减去 alpha i、y i 和 x i 之和，并且必须将其设置为 0。这意味着 w 等于某些 alpha i、某些标量之和乘以这个值减 1 或加上 1
            个变量乘以 x i 除以 i。</p>
        <p>So that means that this expression, this derivative of the Lagrangian, with respect to w is going to be equal
            to w minus the sum of alpha sub i, y sub i, x sub i, and that’s got to be set to 0. And that implies that w
            is equal to the sum of some alpha i, some scalars, times this minus 1 or plus 1 variable times x sub I over
            i.</p>
        <p>现在，数学开始发挥作用了。因为它告诉我们，向量 w 是样本的线性和，可以是所有样本或部分样本。它不必是那样。它可以被提升为幂。它可以是对数。当我们这样做时，可能会发生各种可怕的事情。</p>
        <p>And now, the math is beginning to sing. Because it tells us that the vector w is a linear sum of the samples,
            all the samples or some of the sample. It didn’t have to be that way. It could have been raised to a power.
            It could have been a logarithm. All sorts of horrible things could have happened when we did this.</p>
        <h2 id="unknown-356">未知</h2>
        <h2>Unknown</h2>
        <p>但是当我们这样做时，我们发现 w 将等于这里的一些向量的线性。样本集中的一些向量，我说一些，因为对于一些向量，alpha 将为 0。好的。所以这是我们要注意到的重要一点。</p>
        <p>But when we did this, we discovered that w is going to be equal to a linear some of these vectors here. Some
            of the vectors in the sample set, and I say some, because for some alpha will be 0. All right. So this is
            something that we want to take note of as something important.</p>
        <p>现在，当然，我们必须对 L 求导，使其相对于其他可能变化的因素，所以我们也必须对 L 求导，使其相对于 b。那么，它会等于什么呢？嗯，这里没有 b，所以它没有贡献。这部分没有 ab，所以它没有贡献。这里没有
            b，所以它没有贡献。</p>
        <p>Now, of course, we’ve got to differentiate L with respect to anything else it might vary, so we’ve got to
            differentiate L with respect to b, as well. So what’s that going to be equal to? Well, there’s no b in here,
            so that makes no contribution. This part here doesn’t have a b in it, so that makes no contribution. There’s
            no b over here, so that makes no contribution.</p>
        <p>因此，我们得到了 alpha I 乘以 y sub I 乘以 b。这有一个贡献。因此，这将是 alpha I 乘以 y sub i 的总和。然后，我们对 b 进行区分，这样它就消失了。这里有一个减号，它等于
            0，或者意味着 alpha I 乘以 y sub I 的总和等于 0。嗯，这看起来可能在某些方面有用。</p>
        <p>So we’ve got alpha I times y sub I times b. That has a contribution. So that’s going to be the sum of alpha I
            times y sub i. And then, we’re differentiating with respect to b, so that disappears. There’s a minus sign
            here, and that’s equal to 0, or that implies that the sum of the alpha I times y sub I is equal to 0. Hm,
            that looks like that might be helpful somewhere.</p>
        <h2 id="unknown-357">未知</h2>
        <h2>Unknown</h2>
        <p>现在，该喝咖啡了。顺便说一句，这些咖啡时间要持续数月。你盯着它。你做其他事情。你得担心你的期末考试。你再考虑一下。最后，你喝完咖啡回来，做下一件事。哦，下一件事是什么？好吧，我们仍然有这个表达式，我们正在尝试找到它的最小值。
        </p>
        <p>And now, it’s time for more coffee. By the way, these coffee periods take months. You stare at it. You work
            on something else. You’ve got to worry about your finals. And you think about it some more. And eventually,
            you come back from coffee and do the next thing. Oh, what is the next thing? Well, we’ve still got this
            expression that we’re trying to find the minimum for.</p>
        <p>你会对自己说，这其实是数值分析师的工作。他们知道这类东西。因为里面有一点力量，那个平方。这是一个所谓的二次优化问题。所以在这一点上，你会倾向于把这个问题交给数值分析师。他们会在几周后带着一个算法回来。你实施这个算法。也许事情会成功。也许它们不会收敛。
        </p>
        <p>And you say to yourself, this is really a job for the numerical analysts. Those guys know about this sort of
            stuff. Because of that little power in there, that square. This is a so called quadratic optimization
            problem. So at this point, you would be inclined to hand this problem over to a numerical analysts. They’ll
            come back in a few weeks with an algorithm. You implement the algorithm. And maybe things work. Maybe they
            don’t converge.</p>
        <p>但无论如何，你不必为此担心。但我们不会这样做，因为我们想做更多的数学运算，因为我们对这类东西感兴趣。我们感兴趣的是决策向量是样本的线性和。所以我们会在这方面多下点功夫。</p>
        <p>But any case, you don’t worry about it. But we’re not going to do that, because we want to do a little bit
            more math, because we’re interested in stuff like this. We’re interested in the fact that the decision
            vector is a linear sum of the samples. So we’re going to work a little harder on this stuff.</p>
        <h2 id="unknown-358">未知</h2>
        <h2>Unknown</h2>
        <p>具体来说，现在我们已经得到了 w 的表达式，就是这个，我们要把它代入那里，再把它代入这里，看看我们试图找到极值的那个东西会发生什么。大家放松一下，深呼吸一下？实际上，这是最简单的部分。这只是做一点代数运算。</p>
        <p>And in particular, now that we’ve got an expression for w, this one right here, we’re going to plug it back
            in there, and we’re going to plug it back in here and see what happens to that thing we’re trying to find
            the extremum of. Is everybody relaxed, taking deep breath? Actually, this is the easiest part. This is just
            doing a little bit of the algebra.</p>
        <p>因此，我们试图最大化或最小化的那个值等于 1/2。现在，我们必须将这个向量乘以两次。对吧？因为我们将两者相乘。让我们看看。从上面的表达式中，我们得到其中一个 w 就是 alpha I 乘以 y sub I 乘以向量 x
            sub i 之和。然后，我们得到另一个。</p>
        <p>So the think we’re trying to maximize or minimize is equal to 1/2. And now, we’ve got to have this vector
            here in there twice. Right? Because we’re multiplying the two together. So let’s see. We’ve got from that
            expression up there, one of those w’s will just be the sum of the alpha I times y sub I times the vector x
            sub i. And then, we’ve got the other one, too.</p>
        <p>所以这只是 alpha 的总和。现在，实际上，我最终要将这两个总和压缩成一个双重总和，所以我必须保持索引的正确性。所以我要将其写为 alpha sub j、y sub j、x sub
            j。所以这些是我的两个向量，我要对它们进行点积。这是第一部分，对吧？天哪，这很难。</p>
        <p>So that’s just going to be the sum of alpha. Now, I’m going to, actually, eventually, squish those two sums
            together into a double summation, so I have to keep the indexes straight. So I’m just going to write that as
            alpha sub j, y sub j, x sub j. So those are my two vectors and I’m going to take the dot product of those.
            That’s the first piece, right? Boy, this is hard.</p>
        <h2 id="unknown-359">未知</h2>
        <h2>Unknown</h2>
        <p>减去，现在，下一个项看起来像 alpha i、y sub i、x sub I 乘以 w。所以，你得到了一大堆。我们得到了 alpha I 乘以 y sub I 乘以 x sub i 的总和，然后，将其乘以
            w。所以我们会像这样把 alpha j、y sub j、x sub j 的总和放进去。然后，这就是点积。</p>
        <p>So minus, and now, the next term looks like alpha i, y sub i, x sub I times w. So you’ve got a whole bunch of
            these. We’ve got a sum of alpha I times y sub I times x sub i, and then, that gets multiplied times w. So
            we’ll put this like this, the sum of alpha j, y sub j, x sub j in there like that. And then, that’s the dot
            product like that.</p>
        <p>这并不像我想象的那么糟糕。现在，我必须处理下一个项，alpha I 乘以 y sub I 乘以 b。所以这是 alpha I 乘以 y sub I 乘以 b 的负 sub。然后，为了完成它，我们加上 alpha sub
            I 减 1 的总和，在总和前面减 1，例如 alpha 的总和。到目前为止，你明白了吗？</p>
        <p>That wasn’t as bad as I thought. Now, I’ve got to deal with the next term, the alpha I times y sub I times b.
            So that’s minus sub of alpha I times y sub I times b. And then, to finish it off, we have plus the sum of
            alpha sub I minus 1 up there, minus 1 in front of the summation, such as the sum of the alphas. Are you with
            me so far?</p>
        <p>只是一点代数。看起来不错。我想我还没有搞砸它。让我们看看。alpha I 乘以 y sub I 乘以 b。b 是一个常数。所以把它拉出来，然后，我得到了 alpha sub I 乘以 y sub i
            的总和。哦，那很好。那是 0。现在，对于这些项中的每一个，我们用整个表达式对其进行点运算。</p>
        <p>Just a little algebra. It looks good. I think I haven’t mucked it, yet. Let’s see. alpha I times y sub I
            times b. b is a constant. So pull that out there, and then, I just got the sum of alpha sub I times y sub i.
            Oh, that’s good. That’s 0. Now, so for every one of these terms, we dot it with this whole expression.</p>
        <h2 id="unknown-360">未知</h2>
        <h2>Unknown</h2>
        <p>所以这就像把这个东西放在一起，然后把这两个东西点在一起，对吧？哦，但这和我们这里得到的是一样的。所以现在，我们可以做的是，我们可以说我们可以将这个拉格朗日量重写为。我们得到了 alpha i
            的总和。这是正元素。然后，我们得到了其中一个和一半。</p>
        <p>So that’s just like taking this thing here and dotting those two things together, right? Oh, but that’s just
            the same thing we’ve got here. So now, what we can do is we can say that we can rewrite this Lagrangian as.
            we’ve got that sum of alpha i. That’s the positive element. And then, we’ve got one of these and half of
            these.</p>
        <p>所以是减 1/2。现在，我将把整个结果转换为对 I 和 j 的双重求和，即 alpha I 乘以 alpha j 乘以 y 乘以 I 乘以 y 乘以 j 乘以 x 乘以 I 除以 j 的
            x。我们确实经历了很多麻烦才到达那里，但现在，我们得到了它。我们知道我们试图做的是试图找到该表达式的最大值。</p>
        <p>So that’s minus 1/2. And now, I’ll just convert that whole works into a double sum over both I and j of alpha
            I times alpha j times y sub I times y sub j times x sub I dotted with x of j. We sure went through a lot of
            trouble to get there, but now, we’ve got it. And we know that what we’re trying to do is we’re trying to
            find a maximum of that expression.</p>
        <p>这就是我们要交给数值分析师的。所以，如果我们无论如何都要交给数值分析师，我为什么要这么麻烦呢？好问题。你知道我为什么要这么麻烦吗？因为我想找出这个表达式的依赖关系。万达正在告诉我。我边说边翻译。她用罗马尼亚语告诉我。
        </p>
        <p>And that’s the one we’re going to had off to the numerical analysts. So if we’re going to had this off to the
            numerical analysts anyway, why did I go to all this trouble? Good question. Do you have any idea why I went
            to all this trouble? Because I wanted to find out the dependence of this expression. Wanda is telling me.
            I’m translating as I go. She’s telling me in Romanian.</p>
        <h2 id="unknown-361">未知</h2>
        <h2>Unknown</h2>
        <p>我想找出这个最大化相对于这些向量、x、样本向量的依赖关系。我发现优化仅取决于样本对的点积。这是我们要记住的。这就是我把它放在皇家紫色中的原因。现在，在这里，让我们看看。我们把上面的那个叫做什么？那是二。我想，我们把这里的这个叫做三。
        </p>
        <p>I want to find what this maximization depends on with respect these vectors, the x, the sample vectors. And
            what I’ve discovered is that the optimization depends only on the dot product of pairs of samples. And
            that’s something we want to keep in mind. That’s why I put it in royal purple. Now, up here, so let’s see.
            What do we call that one up there? That’s two. I guess, we’ll call this piece here three.</p>
        <p>这一块是四。现在，还有一块。因为我想把那个 w 放回去，不仅要把它放回到拉格朗日量中，还要把它放回到决策规则中。所以现在，我的决策规则用这个 w 表达式将 w 插入那个东西中。</p>
        <p>This piece here is four. And now, there’s one more piece. Because I want to take that w, and not only stick
            it back into that Lagrangian, I want to stick it back into the decision rule. So now, my decision rule with
            this expression for w is going to be w plugged into that thing.</p>
        <p>因此，决策规则看起来就像 alpha I 乘以 y sub I 乘以 x sub I 加上未知向量，像这样。我想，我们要加上 b。我们会说，如果它大于或等于
            0，那么就加上。所以你现在明白为什么数学开始对我们产生吸引力了。</p>
        <p>So the decision rule is going to look like the sum of alpha I times y sub I times x sub I dotted with the
            unknown vector, like so. And we’re going to, I guess, add b. And we’re going to say, if that’s greater than
            or equal to 0, then plus. So you see why the math is beginning to sing to us now.</p>
        <h2 id="unknown-362">未知</h2>
        <h2>Unknown</h2>
        <p>因为现在我们发现，决策规则也只取决于这些样本向量和未知数的点积。所以所有数学运算都依赖于点积。好吧。现在，我听到了悄悄话。有人说，我不相信数学家能做到这一点。我不认为那些数值分析师能找到最优化。我想确定这一点。给我亲眼所见的证据。
        </p>
        <p>Because now, we discover that the decision rule, also, depends only on the dot product of those sample
            vectors and the unknown. So the total of dependence of all of the math on the dot products. All right. And
            now, I hear a whisper. Someone is saying, I don’t believe that mathematicians can do it. I don’t think those
            numerical analysts can find the optimization. I want to be sure of it. Give me ocular proof.</p>
        <p>所以我想演示一下。好的。这是我们的示例问题。我一开始就遇到过这个问题。现在，如果优化算法没有陷入局部最大值之类的问题，它应该会找到一条很好的直线将这两个人分开，从而找到负数和正数之间最宽的街道。</p>
        <p>So I’d like to run a demonstration of it. OK. There’s our sample problem. The one I started the hour out
            with. Now, if the optimization algorithm doesn’t get stuck in a local maximum or something, it should find a
            nice, straight line separating those two guys to finding the widest street between the minuses and the
            pluses.</p>
        <p>因此，只需几个步骤，您就可以看到第 11 步。它决定尽可能多地进行优化。它有三个
            alpha。您可以看到两个负样本都算在解决方案中，拉格朗日乘数的权重由那些小黄条给出。因此，两个负样本作为正样本之一参与解决方案，但另一个正样本不参与。</p>
        <p>So in just a couple of steps, you can see down there in step 11. It’s decided that it’s done as much as it
            can on the optimization. And it’s got three alphas. And you can see that the two negative samples both
            figure into the solution, the weights on the Lagrangian multipliers are given by those little yellow bars.
            So the two negatives participate in the solution as one of the positives, but the other positive doesn’t.
        </p>
        <h2 id="unknown-363">未知</h2>
        <h2>Unknown</h2>
        <p>所以它的权重为 0。所以一切都很顺利。现在，我说，只要它不陷入局部最大值，猜猜看，我们的数学朋友可以告诉我们并向我们证明这个东西是一个凸空间。这意味着它永远不会陷入局部最大值。</p>
        <p>So it has a 0 weight. So everything worked out well. Now, I said, as long as it doesn’t get stuck on a local
            maximum, guess what, those mathematical friends of ours can tell us and prove to us that this thing is a
            convex space. That means it can never get stuck in a local maximum.</p>
        <p>因此，与神经网络等存在局部最大值的算法相比，这个算法永远不会陷入局部最大值。让我们尝试一些其他示例。这是两个垂直点。没什么好惊讶的，对吧？好吧，你可能会说，也许它不能处理对角点。当然可以。这个算法怎么样？是的，它只需要两个点，因为任何两个点，正负两个点，都可以定义街道。
        </p>
        <p>So in contrast with things like neural nets, where you have a plague of local maxima, this guy never gets
            stuck in a local maxima. Let’s try some other examples. Here’s two vertical points. no surprises there,
            right? Well, you say, well, maybe it can’t deal with diagonal points. Sure it can. How about this thing
            here? Yeah, it only needed two of the points since any two, a plus or minus, will define the street.</p>
        <p>让我们试试这个。哦。你怎么看？发生了什么？好吧，我们完蛋了，对吧？因为它是线性不可分的。坏消息。因此，在线性不可分的情况下，机制会挣扎，最终，它会变慢，你会截断它，因为它没有取得任何进展。你看红点是它出错的。所以你会说，好吧，对我们这边来说太糟糕了。看起来反正也不是那么好。
        </p>
        <p>Let’s try this guy. Oh. What do you think? What happened here? Well, we’re screwed, right? Because it’s
            linearly inseparable. bad news. So in situations where it’s linearly inseparable, the mechanism struggles,
            and eventually, it will just slow down and you truncate it, because it’s not making any progress. And you
            see the red dots there are ones that it got wrong. So you say, well, too bad for our side. doesn’t look like
            it’s all that good anyway.</p>
        <h2 id="unknown-364">未知</h2>
        <h2>Unknown</h2>
        <p>但是，当陷入困境时，一个强有力的想法会拯救我们，让我们换个角度看问题。所以，如果我们不喜欢我们所处的空间，因为它给出的例子不是线性可分的，那么我们可以说，哦，该死。这是我们的空间。这里有两个点。这里是另外两个点。我们无法将它们分开。
        </p>
        <p>But then, a powerful idea comes to the rescue, when stuck switch to another perspective. So if we don’t like
            the space that we’re in, because it gives examples that are not linearly separable, then we can say, oh,
            shoot. Here’s our space. Here are two points. Here are two other points. We can’t separate them.</p>
        <p>但是如果我们能以某种方式将它们放入另一个空间，也许我们可以将它们分开，因为它们在另一个空间中看起来像这样，而且它们很容易分开。所以我们需要的是一个变换，它将我们从我们所在的空间带到一个更方便的空间，所以我们将这个变换称为
            phi，向量为 x。这就是变换。现在，这就是所有魔法的原因。</p>
        <p>But if we could somehow get them into another space, maybe we can separate them, because they look like this
            in the other space, and they’re easy to separate. So what we need, then, is a transformation that will take
            us from the space we’re in into a space where things are more convenient, so we’re going to call that
            transformation phi with a vector, x. That’s the transformation. And now, here’s the reason for all the
            magic.</p>
        <p>我说过，最大化只依赖于点积。所以我要做的最大化就是将一个向量的变换与另一个向量的变换相乘，就像这样。这就是我需要最大化的，或者说找到最大值的。然后，为了识别。它去哪儿了？在黑板下面。哦，是的。在这里。为了识别，我所需要的也是点积。
        </p>
        <p>I said, that the maximization only depends on dot products. So all I need to do the maximization is the
            transformation of one vector dotted with the transformation of another vector, like so. That’s what I need
            to maximize, or to find the maximum on. Then, in order to recognize. where did it go? Underneath the
            chalkboard. Oh, yes. Here it is. To recognize, all I need is dot products, too.</p>
        <h2 id="unknown-365">未知</h2>
        <h2>Unknown</h2>
        <p>因此，对于这个，我需要 x 的 phi 乘以 u 的 phi。为了使这个更一致，我将这个符号称为 xj，并将这个称为 x sub i。这就是 x sub i。这些就是我需要的数量。</p>
        <p>So for that one I need phi of x dotted with phi of u. And just to make this a little bit more consistent, the
            notation, I’ll call that x j and this x sub i. And that’s x sub i. Those are the quantities I need in order
            to do it.</p>
        <p>所以这意味着，如果我有一个函数，我们称它为 x sub I 和 x sub j 的 k，它等于 x sub I 的 phi 除以 x sub j 的
            phi。然后，我就完成了。这就是我需要的。我实际上不需要这个。我需要的只是那个函数 k，它恰好被称为核函数，它为我提供了另一个空间中这两个向量的点积。</p>
        <p>So that means that if I have a function, let’s call it k of x sub I and x sub j, that’s equal to phi of x sub
            I dotted with phi of x sub j. Then, I’m done. This is what I need. I don’t actually need this. All I need is
            that function, k, which happens to be called a kernel function, which provides me with the dot product of
            those two vectors in another space.</p>
        <p>我不需要知道其他空间的变换。这就是这个东西是个奇迹的原因。那么，哪些内核很流行呢？一个是线性内核，它表示 u 加 v 加 1 的 n 次方，就是这样一个内核，因为它里面有 u 和
            v，这两个向量。这就是其他空间中的点积。</p>
        <p>I don’t have to know the transformation into the other space. And that’s the reason that this stuff is a
            miracle. So what are some of the kernels that are popular? One is the linear kernel that says that u dotted
            with v plus 1 to the n th is such a kernel, because it’s got u in it and v in it, the two vectors. And this
            is what the dot product is in the other space.</p>
        <h2 id="unknown-366">未知</h2>
        <h2>Unknown</h2>
        <p>这是一种选择。另一种选择是像这样的核，e 减去 e。我们取这两个数的差的点积。我们取它的幅度，然后除以某个
            sigma。这是我们可以使用的第二种核。所以让我们回过头来看看我们是否可以通过将其变换到另一个空间来解决这个问题，从另一个角度来看。就是这样。</p>
        <p>So that’s one choice. Another choice is a kernel that looks like this, e to the minus. Let’s take the dot
            product of the difference of those two guys. Let’s take the magnitude of that and divide it by some sigma.
            That’s a second kind of kernel that we can use. So let’s go back and see if we can solve this problem by
            transforming it into another space where we have another perspective. So that’s it.</p>
        <p>这是另一个核。当然，我们可以。这就是转换回原始空间时的答案。我们也可以尝试使用所谓的径向基核来实现这一点。这是其中带有指数的核。我们可以从中学习。砰。没问题。所以我们得到了一个凸的通用方法，并保证能产生全局解决方案。我们有一个机制，可以轻松地将其转换到另一个空间。
        </p>
        <p>That’s another kernel. And so sure, we can. And that’s the answer when transformed back into the original
            space. We can also try doing that with a so called radial basis kernel. That’s the one with the exponential
            in it. We can learn on that one. Boom. No problem. So we’ve got a general method that’s convex and
            guaranteed to produce a global solution. We’ve got a mechanism that easily allows us to transform this into
            another space.</p>
        <p>所以它就像魔法一样有效。当然，它并不能消除所有可能的问题。看看这里的指数。如果我们选择一个足够小的 sigma，那么这些 sigma
            基本上会在样本点周围缩小，我们可能会过度拟合。所以它不能使我们免受过度拟合的影响，但它确实使我们免受局部最大值的影响，并为我们提供了一种通用机制，以便以更好的视角转换到另一个空间。</p>
        <p>So it works like a charm. Of course, it doesn’t remove all possible problems. Look at that exponential thing
            here. If we choose a sigma that is small enough, then those sigmas are essentially shrunk right around the
            sample points, and we could get overfitting. So it doesn’t immunize us against overfitting, but it does
            immunize us against local maxima and does provide us with a general mechanism for doing a transformation
            into another space with a better perspective.</p>
        <h2 id="unknown-367">未知</h2>
        <h2>Unknown</h2>
        <p>现在，历史课，所有这些东西感觉相当新。感觉它比你实际年龄还年轻。这是它的历史。瓦普尼克于 1991 年左右从苏联移民到美国。在他移民之前，没有人听说过这些东西。实际上，他在 60
            年代早期在莫斯科大学的博士论文中就基本支持向量思想进行了这项工作。</p>
        <p>Now, the history lesson, all this stuff feels fairly new. It feels like it’s younger than you are. Here’s the
            history of it. Vapnik immigrated from the Soviet Union to the United States in about 1991. Nobody ever heard
            of this stuff before he immigrated. He actually had done this work on the basic support vector idea in his
            Ph.D.&nbsp;thesis at Moscow University in the early ’60s.</p>
        <p>但他无法用它做任何事情，因为他们没有可以尝试任何事情的计算机。因此，他在接下来的 25 年里一直在苏联的一家肿瘤研究所做应用。贝尔实验室的某个人发现了他，邀请他去美国，随后他决定移民。1992 年左右，Vapnik
            向《神经信息处理系统》杂志提交了三篇论文。</p>
        <p>But it wasn’t possible for him to do anything with it, because they didn’t have any computers they could try
            anything out with. So he spent the next 25 years at some oncology institute in the Soviet Union doing
            applications. Somebody from Bell Labs discovers him, invites him over to the United States where,
            subsequently, he decides to immigrate. In 1992, or thereabouts, Vapnik submits three papers to NIPS, the
            Neural Information Processing Systems journal.</p>
        <p>所有这些申请都被拒绝了。他至今仍为此感到难过，但这激励了他。因此，大约在 1992 年、1993 年，贝尔实验室开始对手写字符识别和神经网络产生兴趣。Vapnik
            认为神经网络……用什么词比较好？我能想到一些俗语，但他认为神经网络不是很好。因此，他与同事打赌，说支持向量机最终会在手写识别方面比神经网络做得更好。</p>
        <p>All of them were rejected. He’s still sore about it, but it’s motivating. So around 1992, 1993, Bell Labs was
            interested in hand written character recognition and in neural nets. Vapnik thinks that neural nets. what
            would be a good word to use? I can think of the vernacular, but he thinks that they’re not very good. So he
            bets a colleague a good dinner that support vector machines will eventually do better at handwriting
            recognition then neural nets.</p>
        <h2 id="unknown-368">未知</h2>
        <h2>Unknown</h2>
        <p>而且这只是一顿晚餐的赌注，对吧？这没什么大不了的。但正如拿破仑所说，士兵为了一条丝带能做出什么事，真是令人惊奇。因此，正在研究手写识别问题的同事决定尝试使用核函数的支持向量机，其中 n 等于
            2，只是略微非线性，效果非常好。这是第一次有人尝试使用核函数吗？</p>
        <p>And it’s a dinner bet, right? It’s not that big of deal. But as Napoleon said, it’s amazing what a soldier
            will do for a bit of ribbon. So that makes colleague, who’s working on this problem with handwritten
            recognition, decides to try a support vector machine with a kernel, in which n equals 2, just slightly
            nonlinear, works like a charm. Was this the first time anybody tried a kernel?</p>
        <p>Vapnik 实际上在他的论文中提出了这个想法，但他从未认为它非常重要。在 90 年代初期，当它被证明可以解决手写识别问题时，Vapnik
            重新提出了内核的想法，开始开发它，并成为使用支持向量机的整个方法的重要组成部分。因此，关于这一点的要点是，从这个概念到有人听说过它，中间间隔了 30 年。</p>
        <p>Vapnik actually had the idea in his thesis but never though it was very important. As soon as it was shown to
            work in the early ’90s on the problem handwriting recognition, Vapnik resuscitated the idea of the kernel,
            began to develop it, and became an essential part of the whole approach of using support vector machines. So
            the main point about this is that it was 30 years in between the concept and anybody ever hearing about it.
        </p>
        <p>从瓦普尼克理解内核到意识到内核的重要性，他用了 30
            年的时间。事情往往就是这样，伟大的想法诞生后，很长一段时间内都没有进展，然后突然顿悟，最初的想法似乎只需要一点点改变就能产生巨大的力量。然后，世界就再也没有回头了。直到 90
            年代初，没人听说过瓦普尼克，如今，他因机器学习领域众所周知的事情而出名。</p>
        <p>It was 30 years between Vapnik’s understanding of kernels and his appreciation of their importance. And
            that’s the way things often go, great ideas followed by long periods of nothing happening, followed by an
            epiphanous moment when the original idea seemed to have great power with just a little bit of a twist. And
            then, the world never looks back. And Vapnik, who nobody ever heard of until the early ’90s, becomes famous
            for something that everybody knows about today who does machine learning.</p>
        <h1 id="learning-boosting">17.学习：提升</h1>
        <h1>17. Learning: Boosting</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 51:40 <a
                href="https://youtube.com/watch?v=UHBmv7qCey4">https://youtube.com/watch?v=UHBmv7qCey4</a></p>
        <p> 11 years ago (Jan 11, 2014) — 51:40 <a
                href="https://youtube.com/watch?v=UHBmv7qCey4">https://youtube.com/watch?v=UHBmv7qCey4</a></p>
        <h2 id="introduction-4">介绍</h2>
        <h2>Introduction</h2>
        <p>帕特里克·温斯顿：现在我们几乎完成了旅程。我们将讨论几种学习方法。最古老的方法，即最近邻和识别树类型的学习。如果没有理由不做简单的事情，它们仍然有用，仍然是正确的做法。然后我们有受生物启发的方法。神经网络。如果速率常数太大，就会出现各种局部最大值、过度拟合和振荡问题。遗传算法。
        </p>
        <p>PATRICK WINSTON: We’ve now almost completed our journey. This will be it for talking about several kinds of
            learning. the venerable kind, that’s the nearest neighbors and identification tree types of learning. Still
            useful, still the right thing to do if there’s no reason not to do the simple thing. Then we have the
            biologically inspired approaches. Neural nets. All kinds of problems with local maxima and overfitting and
            oscillation, if you get the rate constant too big. Genetic algorithms.</p>
        <p>就像神经网络一样，它们在试图模仿自然方面都非常幼稚。所以也许它们可以解决一类问题。它们确实各有一类擅长解决的问题。但作为通用的首选方法，我不推荐它。但现在理论家们已经站出来做了一些非常了不起的事情。最后，你不得不说，哇，这些想法太强大了。
        </p>
        <p>Like neural nets, both are very naive in their attempt to mimic nature. So maybe they work on a class of
            problems. They surely do each have a class of problems for which they’re good. But as a general purpose
            first resort, I don’t recommend it. But now the theorists have come out and done some things are very
            remarkable. And in the end, you have to say, wow, these are such powerful ideas.</p>
        <p>我想知道大自然是否也发现了它们？大脑中是否存在基于良好科学的良好工程？或者考虑到进化的本质，是否只是随机的垃圾才是做任何事情的最佳方式？谁知道呢？</p>
        <p>I wonder if nature has discovered them, too? Is there good engineering in the brain, based on good science?
            Or given the nature of evolution, is it just random junk that is the best ways for doing anything? Who
            knows?</p>
        <p>但今天，我们要讨论一个我敢打赌肯定存在的想法，因为它很容易实现，而且功能极其强大，是任何人学习机制中必不可少的一部分。如果你只理解公式，你永远无法解决测验中的问题，这是肯定的。</p>
        <p>But today, we’re going to talk about an idea that I’ll bet is in there somewhere, because it’s easy to
            implement, and it’s extremely powerful in what it does, and it’s the essential item in anybody’s repertoire
            of learning mechanisms. It’s also a mechanism which, if you understand only by formula, you will never be
            able to work the problems on the quiz, that’s for sure.</p>
        <p>因为从表面上看，模拟这种方法似乎非常复杂。但是，一旦你理解了它的工作原理，并了解了一些数学知识，让它为你唱歌，你会发现它变得非常简单。</p>
        <p>Because on the surface, it looks like it’d be very complicated to simulate this approach. But once you
            understand how it works and look at a little bit of the math and let it sing songs to you, it turns out to
            be extremely easy.</p>
        <h2 id="binary-classification">二元分类</h2>
        <h2>Binary Classification</h2>
        <p>所以这是为了让多种方法为您服务。到目前为止，我们一直在讨论只使用一种方法来做某事。</p>
        <p>So it’s about letting multiple methods work in your behalf. So far, we’ve been talking about using just one
            method to do something.</p>
        <p>我们现在要做的是看看群体是否能比群体中的个体更聪明。但在我们深入探讨这个抽象问题之前，我想先说一下，整个过程与分类和二元分类有关。我手里拿着的是一支粉笔还是一枚手榴弹？那是一杯咖啡还是茶？这些都是二元分类问题。</p>
        <p>And what we’re going to do now is we’re looking to see if a crowd can be smarter than the individuals in the
            crowd. But before we get too far down that abstract path, let me just say that the whole works has to do
            with classification, and binary classification. Am I holding a piece of chalk in my hand, or a hand grenade?
            Is that a cup of coffee or tea? Those are binary classification problems.</p>
        <p>因此，我们今天将严格讨论二元分类。我们不会讨论如何在页面上的字母表中找到正确的字母。这是一个 26 种选择。我们讨论的是二元选择。因此，我们假设有一组分类器可供我们利用。这是一个。h。它会产生负 1 或正
            1。这就是分类的完成方式。</p>
        <p>And so we’re going to be talking today strictly about binary classification. We’re not going to be talking
            about finding the right letter in the alphabet that’s written on the page. That’s a 26 way choice. We’re
            talking about binary choices. So we assume that there’s a set of classifiers that we can draw on. Here’s
            one. h. And it produces either a minus 1 or a plus 1. So that’s how the classification is done.</p>
        <p>如果是咖啡，加 1。如果是茶，减 1。这是粉笔，加 1。如果是手榴弹，减 1。这就是分类的工作原理。现在，对我们来说太糟糕了，通常情况下，世界不会给我们提供很好的分类器。</p>
        <p>If it’s coffee, plus 1. If it’s tea, minus 1. Is this chalk, plus one. If it’s a hand grenade, minus 1. So
            that’s how the classification works. Now, too bad for us, normally the world doesn’t give us very good
            classifiers.</p>
        <p>因此，如果我们查看此分类器或任何其他分类器的错误率，该错误率的范围为 0 到 1，即样本集上出错的情况的比例。因此，您希望错误率在这里非常低。如果错误率在那边，您就完蛋了。但是中间呢？如果它就在那里呢。比抛硬币好一点。
        </p>
        <p>So if we look at the error rate of this classifier or any other classifier, that error rate will range from 0
            to 1 in terms of the fraction of the cases got wrong on a sample set. So you’d like your error rate to be
            way down here. You’re dead if it’s over there. But what about in the middle? What if it’s, say, right there.
            Just a little bit better than flipping a coin.</p>
        <p>如果结果比抛硬币好一点点，那就是弱分类器。问题是，你能通过组合几个弱分类器并让它们投票，制作出一个强分类器吗？那么你会怎么做呢？</p>
        <p>If it’s just a little bit better than flipping a coin, that’s a weak classifier. And the question is, can you
            make a classifier that’s way over here, like there, a strong classifier, by combining several of these weak
            classifiers, and letting them vote? So how would you do that?</p>
        <p>你可能会说，好吧，让我们制作一个大分类器大写 H，它对某个样本 x 进行处理，并且其输出取决于各个分类器输出的总和。所以我们有 H1 对 x 进行处理。我们有 H2 对 x 进行处理。我们还有 H3 对 x
            进行处理。我们假设有三个，只是为了开始。</p>
        <p>You might say, well, let us make a big classifier capital H, that works on some sample x, and has its output
            produces something that depends on the sum of the outputs of the individual classifiers. So we have H1
            working on x. We have H2 working on x. And we have H3 also working on x. Let’s say three of them, just to
            start us off.</p>
        <p>现在让我们把这些数字加起来，然后取输出的符号。所以如果这三个数字中有两个数字一致，那么我们将得到正 1 或负 1。如果三个数字一致，我们将得到正 1 或负 1。因为我们只是取符号。我们只是取这些数字之和的符号。</p>
        <p>And now let’s add those guys up, and take the sign of the output. So if two out of the three of those guys
            agree, then we’ll get an either plus 1 or minus 1. If all three agree, we’ll get plus 1 or minus 1. Because
            we’re just taking the sign. We’re just taking the sign of the sum of these guys.</p>
        <p>所以这意味着只要另外两个人是对的，一个人就可能是错的。但我认为，如果你考虑一些样本空间，你会更容易理解这一切是如何运作的，你说，好吧，让我们让这里的那个区域是 H1 错误的地方，这里的这个区域是 H2
            错误的地方。然后这里的这个区域是 H3 错误的地方。</p>
        <p>So this means that one guy can be wrong, as long as the other two guys are right. But I think it’s easier to
            see how this all works if you think of some space of samples, you say, well, let’s let that area here be
            where H1 is wrong, and this area over here is where H2 is wrong. And then this area over here is where H3 is
            wrong.</p>
        <p>所以如果情况是这样的，那么这个公式总是能给你正确的样本答案。我现在就不说这个了，因为我想先介绍一下样本集的背景知识。我们正在讨论如何将这些东西包装到样本集上。稍后，我们会问，好吧，假设你在样本集上训练了这个东西，它在一些新的例子上表现如何？
        </p>
        <p>So if the situation is like that, then this formula always gives you the right answers on the samples. I’m
            going to stop saying that right now, because I want to be kind of a background thing on the samples set.
            We’re talking about wrapping this stuff over the sample set. Later on, we’ll ask, OK, given that you trained
            this thing on a sample set, how well does it do on some new examples?</p>
        <p>因为我们想问自己关于过度拟合的问题。但现在，我们只想看看，如果我们相信这种安排，即每个 H 都产生正 1 或负 1，我们将它们相加并取符号，这是否会比单独进行测试得到更好的结果？</p>
        <p>Because we want to ask ourselves about overfitting questions. But for now, we just want to look and see if we
            believe that this arrangement, where each of these H’s is producing plus 1 or minus 1, we’re adding them up
            and taking the sign, is that going to give us a better result than the tests individually?</p>
        <p>如果它们在覆盖样本集时看起来像这样，那么很明显我们每次都会得到正确答案，因为这里没有任何两个测试会给出错误答案。因此，得到正确答案的两个，在这个 H1
            的小圆圈中，其他两个也得到了正确答案。所以他们会投票否决它，你每次都会得到正确答案。</p>
        <p>And if they look like this when draped over a sample set, then it’s clear that we’re going to get the right
            answer every time, because there’s no area here where any two of those tests are giving us the wrong answer.
            So the two that are getting the right answer, in this little circle here for H1, these other two are getting
            the right answer. So they’ll outvote it, and you’ll get the right answer every time.</p>
        <p>但事情没必要这么简单。可能看起来像这样。可能存在这样一种情况：这是 H1，错误答案。这是 H2，错误答案。这是
            H3，错误答案。现在情况变得有点模糊，因为我们必须问自己，三个人中有三个人答错的区域是否足够大，以至于比其中 1 个单独的测试更糟糕。</p>
        <p>But it doesn’t have to be that simple. It could look like this. There could be a situation where this is H1,
            wrong answer. This is H2, wrong answer. And this is H3, wrong answer. And now the situation gets a little
            bit more murky, because we have to ask ourselves whether that area where three out of the three get it wrong
            is sufficiently big so as to be worse than 1 of the individual tests.</p>
        <p>因此，如果你看一下维恩图，并长时间注视它，并尝试一些事情，你可以说，好吧，没有任何情况会给出更糟糕的答案。</p>
        <p>So if you look at that Venn diagram, and stare at it long enough, and try some things, you can say, well,
            there is no case where this will give a worse answer.</p>
        <p>或者，你最终可能会得出这样的结论：在某些情况下，我们可以安排这些圆圈，使得投票方案给出的答案比单独测试更差，但我不会告诉你答案，因为我认为我们会把它变成一个测验问题。好主意？好的。所以我们会把它变成一个测验问题。这看起来是个好主意。
        </p>
        <p>Or, you might end up with the conclusion that there are cases where we can arrange those circles such that
            the voting scheme will give an answer that’s worst than an individual test, but I’m not going to tell you
            the answer, because I think we’ll make that a quiz question. Good idea? OK. So we’ll make that a quiz
            question. So that looks like a good idea.</p>
        <p>我们可以构建一个小算法，帮助我们选择特定的弱分类器来插入这里。我们有一整套分类器。我们有 H1、H2、H55。我们有很多可供选择。</p>
        <p>And we can construct a little algorithm that will help us pick the particular weak classifiers to plug in
            here. We’ve got a whole bag of classifiers. We’ve got H1, we’ve got H2, we’ve got H55. We’ve got a lot of
            them we can choose from.</p>
        <p>因此，我们要做的就是使用未受干扰的数据来生成 H1。我们将对数据进行所有测试，看看哪一个测试的错误率最小。这就是好方法，所以我们将使用它。然后，我们将使用夸大 H1 误差的数据。换句话说，这是一个关键的想法。</p>
        <p>So what we’re going to do is we’re going to use the data, undisturbed, to produce H1. We’re just going to try
            all the tests on the data and see which one gives us the smallest error rate. And that’s the good guy, so
            we’re going to use that. Then we’re going to use the data with an exaggeration of H1 errors. In other words.
            this is a critical idea.</p>
        <p>我们要做的是再次运行这个算法，但我们要做的不是只看错误样本的数量，而是看一组扭曲的样本，其中我们做得不好的样本对结果的影响被夸大了。</p>
        <p>What we’re going to do is we’re going to run this algorithm again, but instead of just looking at the number
            of samples that are got wrong, what we’re going to do is we’re going to look at a distorted set of samples,
            where the ones we’re not doing well on has exaggerated effect on the result.</p>
        <p>因此，我们要对它们进行加权或乘法，或者做一些事情，以便我们更加关注 H1 产生误差的样本，这将给我们 H2。然后我们再做一次，因为在这个特定的小探索方案中，我们有三件事要做。这一次，我们将夸大这些样本。</p>
        <p>So we’re going to weight them or multiply them, or do something so that we’re going to pay more attention to
            the samples on which H1 produces an error, and that’s going to give us H2. And then we’re going to do it one
            more time, because we’ve got three things to go with here in this particular little exploratory scheme. And
            this time, we’re going to have an exaggeration of those samples.</p>
        <p>我们现在要夸大哪些样本？我们不妨寻找那些 H1 给出与 H2 不同的答案的样本，因为我们想站在好人一边。所以我们可以说我们要夸大那些 H1 给出与 H2 不同的结果的样本。这将给我们 H3。好的。</p>
        <p>Which samples are we going to exaggerate now? We might as well look for the ones where H1 gives us a
            different answer from H2, because we want to be on the good guy’s side. So we can say we’re going to
            exaggerate those samples four which H1 gives us a different result from H2. And that’s going to give us H3.
            All right.</p>
        <p>因此，我们可以将整个作品视为多部分想法的第一部分。让我们看看。我不知道，第二步是什么？嗯，这是一个好主意。然后我们可以很容易地从中得出一棵小树，如下所示。</p>
        <p>So we can think of this whole works here as part one of a multi part idea. So let’s see. I don’t know, what
            might be step two? Well, this is a good idea. Then what we’ve got that we can easily derive from that is a
            little tree looked like this.</p>
        <p>我们可以说 x 的 H 取决于 H1、H2 和 H3。但是现在，如果这是一个好主意，并且它比任何单个测试都给出更好的答案，也许我们可以将这个想法稍微递归一下，然后说，好吧，也许 H1
            实际上不是原子测试。但也许是其他三个测试的投票。所以你可以创建一个看起来像这样的树结构。</p>
        <p>And we can say that H of x depends on H1, H2, and H3. But now, if that’s a good idea, and that gives a better
            answer than any of the individual tests, maybe we can make this idea a little bit recursive, and say, well,
            maybe H1 is actually not an atomic test. But maybe it’s the vote of three other tests. So you can make a
            tree structure that looks like this.</p>
        <p>所以这是 H11、H12、H13，然后是 3。然后是 H31、H32、H33。所以这是一种拉票的想法。我们正试图将一大堆单独的测试纳入法案中。所以我想直到大约 10
            年前才发现这一点的原因是因为你必须让这么多桌子都排好，这样这个想法才能通过漫长的想法筛选。</p>
        <p>So this is H11, H12, H13, and then 3 here. And then this will be H31, H32, H33. And so that’s a sort of get
            out the vote idea. We’re trying to get a whole bunch of individual tests into the act. So I guess the reason
            this wasn’t discovered until about ’10 years ago was because you’ve got to get so many of these desks all
            lined up before the idea gets through that long filter of ideas.</p>
        <p>所以这是众多想法中唯一的第二个。好吧，我们接下来可能会想到的是，我们继续讨论这些分类器。我们在谈论什么样的分类器？我有。哦，该死，我花光了我最后的五分钱。我没有硬币可以抛。但这是一个分类器，对吧？这个分类器的问题在于它是一个弱分类器，因为它给我的正确概率是
            50/50。</p>
        <p>So that’s the only idea number two of quite a few. Well, next thing we might think is, well, we keep talking
            about these classifiers. What kind of classifiers are we talking about? I’ve got. oh, shoot, I’ve spent my
            last nickel. I don’t have a coin to flip. But that’s one classifier, right? The trouble with that classifier
            is it’s a weak classifier, because it gives me a 50/50 chance of being right.</p>
        <h2 id="decision-trees">决策树</h2>
        <h2>Decision Trees</h2>
        <p>我想在某些情况下抛硬币比弱分类器更好。如果两个结果的概率不相等，那么抛硬币就是一个非常好的弱分类器。但我们要做的是，从另一组分类器的角度来思考。我们将它们称为决策树。现在，你还记得决策树，对吧？但我们不会构建决策树。
        </p>
        <p>I guess there are conditions in which a coin flip is better than a. it is a weak classifier. If the two
            outcomes are not equally probable, than a coin flip is a perfectly good weak classifier. But what we’re
            going to do is we’re going to think in terms of a different set of classifiers. And we’re going to call them
            decision tree. Now, you remember decision trees, right? But we’re not going to build decision trees.</p>
        <p>我们将使用决策树桩。因此，如果我们有一个像这样的二维空间，那么决策树桩就是一个单一的测试。它不是一棵完整的树，不会将样本划分为同质组。它只是你用一个测试就能做的事情。因此，每个可能的测试都是一个分类器。我们从中得到了多少个测试？12，对吗？是的。在我看来，它也不像是
            12。</p>
        <p>We’re going to use decision tree stumps. So if we have a two dimensional space that looks like this, then a
            decision tree stump is a single test. It’s not a complete tree that will divide up the samples into
            homogeneous groups. It’s just what you can do with one test. So each possible test is a classifier. How many
            tests do we get out of that? 12, right? Yeah. It doesn’t look like 12 to me, either.</p>
        <p>但是，这就是你得到 12
            的方法。你可以放进去的一个决策树测试就是那个测试。那将是一个完整的决策树桩。但是，当然，你也可以放进去这个。那将是另一个决策树桩。现在，对于右边的这个，我可以说，右边的一切都是一个减号。或者，我可以说，右边的一切都是一个加号。
        </p>
        <p>But here’s how you get to 12. One decision tree test you can stick in there would be that test right there.
            And that would be a complete decision tree stump. But, of course, you can also put in this one. That would
            be another decision tree stump. Now, for this one on the right, I could say, everything on the right is a
            minus. Or, I could say, everything on the right is a plus.</p>
        <p>它可能恰好是错误的，但它是一个有效的测试，具有有效的结果。这就是我们将测试行数加倍的方法。你知道吗？甚至可以进行一种测试，说一切都是正数，或者一切都是错数。</p>
        <p>It would happen to be wrong, but it’s a valid test with a valid outcome. So that’s how we double the number
            of test that we have lines for. And you know what? can even have a kind of test out here that says
            everything is plus, or everything is wrong.</p>
        <p>因此，对于每个维度，决策树桩的数量是我可以放入的行数乘以 2。然后我这里有两个维度，这就是我得到十二的原因。所以有三条线。我可以在左侧或右侧放置加号。所以是六个。然后我有两个维度，所以总共是
            12。这就是决策树桩的想法。</p>
        <p>So for each dimension, the number of decision tree stumps is the number of lines I can put in times 2. And
            then I’ve got two dimensions here, that’s how I got to twelve. So there are three lines. I can have the
            pluses on either the left or the right side. So that’s six. And then I’ve got two dimensions, so that gives
            me 12. So that’s the decision tree stump idea.</p>
        <p>这是其他决策树的边界，显然就是这样。所以这是一种可以生成一批测试来尝试使用大量测试来帮助你完成工作的想法的方法。学生：你不能在右侧也有一个决策树吗？帕特里克·温斯顿：问题是，你也可以在右侧进行测试吗？</p>
        <p>And here are the other decision tree boundaries, obviously just like that. So that’s one way can generate a
            batch of tests to try out with this idea of using a lot of tests to help you get the job done. STUDENT:
            Couldn’t you also have a decision tree on the right side? PATRICK WINSTON:. The question is, can you also
            have a test on the right side?</p>
        <p>瞧，这只是一个说法，一切都是正数或一切都是负数。所以你把线放在哪里并不重要。它可以在右边，也可以在左边，也可以在底部，也可以在顶部。或者你不必把线放在任何地方。这只是一个额外的测试，是对你在样本之间进行的测试的补充。所以整个提升的想法，是今天的主要想法。
        </p>
        <p>See, this is just a stand in for saying, everything’s plus or everything’s minus. So it doesn’t matter where
            you put the line. It can be on the right side, or the left side, or the bottom, or the top. Or you don’t
            have to put the line anywhere. It’s just an extra test, an additional to the ones you put between the
            samples. So this whole idea of boosting, the main idea of the day.</p>
        <p>它是否依赖于使用决策树桩？答案是否定的。不要混淆。您可以将增强与任何类型的分类器一起使用。那么今天我为什么要使用决策树桩呢？因为它让我的生活变得轻松。我们可以查看它，我们可以看到它在做什么。但我们可以将一堆神经网络放入其中。我们可以将一堆真正的决策树放入其中。
        </p>
        <p>Does it depend on using decision tree stumps? The answer is no. Do not be confused. You can use boosting with
            any kind of classifier. so why do I use decision tree stumps today? Because it makes my life easy. We can
            look at it, we can see what it’s doing. But we could put bunch of neural nets in there. We could put a bunch
            of real decision trees in there.</p>
        <p>我们可以把一堆最近邻的东西放进去。提升的想法并不关心。我只是用这些决策树桩，因为我和其他人都用它们来举例说明。好的。我们正在取得进展。现在，我们画的这些测试和线的错误率是多少？嗯，我猜错误率等于 1/n
            的总和。这就是点的总数，样本的数量。</p>
        <p>We could put a bunch of nearest neighbor things in there. The boosting idea doesn’t care. I just used these
            decision tree stumps because I and everybody else use them for illustration. All right. We’re making
            progress. Now, what’s the error rate for any these tests and lines we drew? Well, I guess it’ll be the error
            rate is equal to the sum of 1 over n.&nbsp;That’s the total number of points, the number of samples.</p>
        <p>总结我们犯错的情况。所以，我们要努力结合这些想法。我们有了夸大的概念。在我们在这里所做的某个阶段，我们希望能够夸大某些错误相对于其他错误的影响。</p>
        <p>Summed over the cases where we are wrong. So gee, we’re going to work on combining some of these ideas. And
            we’ve got this notion of exaggeration. At some stage in what we’re doing here, we’re going to want to be
            able to exaggerate the effect of some errors relative to other errors.</p>
        <p>因此，我们可以做的一件事是假设，或者规定，或者断言每个样本都有一个与之相关的权重。这是 W1，这是 W2，这是 W3。一开始，没有理由假设其中任何一个比其他任何一个更重要或更不重要。所以一开始，时间 1 时的 W
            sub I 等于 1 / n。&nbsp;</p>
        <p>So one thing we can do is we can assume, or we can stipulate, or we can assert that each of these samples has
            a weight associated with it. That’s W1, this is W2, and that’s W3. And in the beginning, there’s no reason
            to suppose that any one of these is more or less important than any of the other. So in the beginning, W sub
            I at time one is equal to 1 over n.&nbsp;</p>
        <p>因此，误差只是将错误样本的数量相加。这就是不正确样本的比例。这就是错误率。</p>
        <p>So the error is just adding up the number of samples that were got wrong. And that’ll be the fraction of
            samples to that you didn’t get right. And that will be the error rate.</p>
        <p>因此，我们想要做的是，我们不想用这个作为所有时间的错误率，而是想把它移开，并说错误率等于当前步骤中出错项的总和，乘以出错项的权重。所以在第一步中，所有项都有相同的权重，这并不重要。</p>
        <p>So what we want to do is we want to say, instead of using this as the error rate for all time, what we want
            to do is we want to move that over, and say that the error rate is equal to the sum over the things you got
            wrong in the current step, times the weights of those that were got wrong. So in step one, everything’s got
            the same weight, it doesn’t matter.</p>
        <p>但是如果我们找到一种方法来改变它们的权重。例如，为了高度夸大第三个样本，那么 W3 相对于 W1 和 W2 会上升。我们要确保的一件事是，无论我们如何调整权重，整个空间的权重总和等于
            1。换句话说，我们希望选择权重，使它们强调一些样本，但我们也希望对权重施加约束，使所有权重加在一起等于 1。</p>
        <p>But if we find a way to change their weights going downstream. So as to, for example, highly exaggerate that
            third sample, then W3 will go up relative to W1 and W2. The one thing we want to be sure of is there is no
            matter how we adjust the weights, that the sum of the weights over the whole space is equal to 1. So in
            other words, we want to choose the weights so that they emphasize some of the samples, but we also want to
            put a constraint on the weights such that all of them added together is summing to one.</p>
        <p>我们会说这强制执行了分布。分布是一组权重，总和为 1。好吧，这只是一个不错的想法。所以我们取得了一点进展。我们有这样的想法，我们可以将一些加/减 1
            的分类器加在一起，这样就可以得到一个更好的分类器。我们对如何做到这一点有了一些想法。我们突然想到，也许我们想以某种方式让很多分类器参与进来。</p>
        <p>And we’ll say that enforces a distribution. A distribution is a set of weights that sum to one. Well, that’s
            just a nice idea. So we’re make a little progress. We’ve got this idea that we can add some plus/minus 1
            classifiers together, you get a better classifier. We got some idea about how to do that. It occurs to us
            that maybe we want to get a lot of classifiers into the act somehow or another.</p>
        <p>也许我们想考虑使用决策树桩来深入思考所有这些东西。</p>
        <p>And maybe we want to think about using decision tree stumps so as to ground out thinking about all this
            stuff.</p>
        <h2 id="classifiers">分类器</h2>
        <h2>Classifiers</h2>
        <p>那么下一步就是说，好吧，我们实际上应该如何结合这些东西？你会发现，在文献库中，有很多这样的论文。这是多年来最先进的技术。</p>
        <p>So the next step is to say, well, how actually should we combine this stuff? And you will find, in the
            literature libraries, full of papers that do stuff like that. And that was state of the art for quite a few
            years.</p>
        <p>但后来人们开始说，好吧，也许我们可以分多个步骤构建这个分类器，即 x 的 H，并让许多分类器参与其中。所以也许我们可以说分类器是 H 的符号。这是我们首先选择的那个。这是我们首先选择的分类器。这就是查看样本。</p>
        <p>But then people began to say, well, maybe we can build up this classifier, H of x, in multiple steps and get
            a lot of classifiers into the act. So maybe we can say that the classifier is the sign of H. that’s the one
            we picked first. That’s the classifier we picked first. That’s looking at samples.</p>
        <p>然后我们得到了 H2。然后我们得到了
            H3。然后我们得到了我们可能想要多少个其他分类器，或者我们可能需要多少个分类器才能正确地对样本集中的所有内容进行分类。因此人们开始思考是否有一种算法可以以这种方式一步一步地开发分类器。</p>
        <p>And then we’ve got H2. And then we’ve got H3. And then we’ve got how many other classifiers we might want, or
            how many classifiers we might need in order to correctly classify everything in our sample set. So people
            began to think about whether there might be an algorithm that would develop a classifier that way, one step
            at a time.</p>
        <p>这就是为什么我将步数放在指数中，因为我们首先选择这个，然后将其扩展为两个，然后将其扩展为三个，依此类推。每个单独的分类器都会单独查看样本。但当然，很自然地会认为仅仅将所有内容加起来是不够的。事实并非如此。</p>
        <p>That’s why I put that step number in the exponent, because we’re picking this one at first, then we’re
            expanding it to have two, and then we’re expanding it to have three, and so on. And each of those individual
            classifiers are separately looking at the sample. But of course, it would be natural to suppose that just
            adding things up wouldn’t be enough. And it’s not.</p>
        <p>所以想出下一个点子并不难，就是通过什么来稍微修改一下这个东西？它看起来很像一个得分多项式，不是吗？那么我们要怎么做才能让它更漂亮一点呢？学生：帕特里克·温斯顿：再来一次？做什么？学生：帕特里克·温斯顿：有人在某处低声议论。学生：添加。帕特里克·温斯顿：增加重量！学生：重量。是的。帕特里克·温斯顿：很好。好主意。
        </p>
        <p>So it isn’t too hard to invent the next idea, which is to modify this thing just a little bit by doing what?
            It looks almost like a scoring polynomial, doesn’t it? So what would we do to tart this up a little bit?
            STUDENT: PATRICK WINSTON: Come again? Do what? STUDENT: PATRICK WINSTON: Somewhere out there someone’s
            murmuring. STUDENT: Add. PATRICK WINSTON: Add weights! STUDENT: weights. Yeah. PATRICK WINSTON: Excellent.
            Good idea.</p>
        <p>因此，我们要做的是，让每个分类器都具有
            alpha，然后确定是否有人可以构建这种公式来完成这项工作。所以，在我走得太远之前，也许我应该修改这个金星想法。我们不会平等对待人群中的每个人。我们会等待一些意见，而不是其他意见。</p>
        <p>So what we’re going to do is we’re going to have alphas associated with each of these classifiers, and we’re
            going to determine if somebody can build that kind formula to do the job. So maybe I ought to modify this
            gold star idea before I get too far downstream. And we’re not going to treat everybody in a crowd equally.
            We’re going to wait some of the opinions more than others.</p>
        <p>顺便说一句，他们都会在这个领域的不同部分犯错。所以也许这不是一群有影响力的群体的智慧，而是一群专家的智慧。他们每个人都擅长这个领域的不同部分。所以无论如何，我们有了这个公式，而且有几件事可以说出来了。但首先，让我们写下一个算法，看看这个算法应该是什么样的。
        </p>
        <p>And by the way, they’re all going to make errors in different parts of the space. So maybe it’s not the
            wisdom of even a weighted crowd, but a crowd of experts. Each of which is good at different parts of the
            space. So anyhow, we’ve got this formula, and there are a few things that one can say turn out. But first,
            let’s write down the an algorithm for what this ought to look like.</p>
        <p>在我用完空间之前，我想我会利用这里右边的板子，并将整体算法放在这里。</p>
        <p>Before I run out of space, I think I’ll exploit the right hand board here, and put the overall algorithm
            right here.</p>
        <h2 id="algorithm">算法</h2>
        <h2>Algorithm</h2>
        <p>因此，我们首先让时间 1 的所有权重等于 1/n。这只是说它们一开始都是相等的，并且等于 1/n。n 是样本数。然后，当我得到它时，我想以某种方式计算 alpha。</p>
        <p>So we’re going to start out by letting of all the weights at time 1 be equal to 1 over n.&nbsp;That’s just
            saying that they’re all equal in the beginning, and they’re equal to 1 over n.&nbsp;And n is the number of
            samples. And then, when I’ve got that, I want to compute alpha, somehow.</p>
        <p>让我们看看。不，我不想这样做。我想选择一个最小化错误率的分类器。然后 m，i，zes，时间 t 时的错误。这将是时间
            t。我们将回到这里。这就是为什么我们在那里放置一个步骤索引。因此，一旦我们选择了一个产生错误率的分类器，我们就可以使用错误率来确定 alpha。</p>
        <p>Let’s see. No, I don’t want to do that. I want to I want to pick a classifier the minimizes the error rate.
            And then m, i, zes, error at time t. And that’s going to be at time t. And we’re going to come back in here.
            That’s why we put a step index in there. So once we’ve picked a classifier that produces an error rate, then
            we can use the error rate to determine the alpha.</p>
        <p>所以我想要这里的 alpha。这将是选择该测试的副产品。有了所有这些，也许就足以计算 Wt 加
            1。所以我们将使用我们刚刚选择的分类器来获得一些修正权重，然后我们将进行循环，直到这个分类器在所有样本数据上产生一组完美的结论。</p>
        <p>So I want the alpha over here. That’ll be sort of a byproduct of picking that test. And with all that stuff
            in hand, maybe that will be enough to calculate Wt plus 1. So we’re going to use that classifier that we
            just picked to get some revised weights, and then we’re going to go around that loop until this classifier
            produces a perfect set of conclusions on all the sample data.</p>
        <p>这就是我们的总体战略。如果我们要对这些事情进行编号，那么我们可能已经想到了第四个大创意。这里的安排是第五个大创意。然后我们还有第六个大创意。第六个大创意是这样的。</p>
        <p>So that’s going to be our overall strategy. Maybe we’ve got, if we’re going to number these things, that’s
            the fourth big idea. And this arrangement here is the fifth big idea. Then we’ve got the sixth big idea. And
            the sixth big idea says this.</p>
        <p>假设第 1 个样本在时间 t 加 1 时的权重等于同一样本在时间 t 时的权重除以某个归一化因子，乘以时间 t 时的 e 减去 alpha，乘以时间 t 时的 h，乘以某个函数 y，该函数是 x
            的函数，但不是时间的函数。现在你会说，这是从哪里来的？</p>
        <p>Suppose that the weight on it ith sample at time t plus 1 is equal to the weight at time t on that same
            sample, divided by some normalizing factor, times e to the minus alpha at time t, times h at time t, times
            some function y which is a function of x, But not a function of time. Now you say, where did this come from?
        </p>
        <p>答案是，在数学家看到这个问题的最初 10
            分钟里，他并没有从内心产生这种想法。事实上，当我问他这是如何实现的时，他说，他大约有一年的时间每个星期六都坐在沙发上思考这个问题，他的妻子非常生气，但他最终找到了答案，挽救了他们的婚姻。那么，这种想法是从哪里来的呢？
        </p>
        <p>And the answer is, it did not spring from the heart of mathematician in the first 10 minutes that he looked
            at this problem. In fact, when I asked how this worked, he said, well, he was thinking about this on the
            couch every Saturday for about a year, and his wife was getting pretty sore, but he finally found it and
            saved their marriage. So where does stuff like this come from?</p>
        <p>实际上，它来自于对大量数学知识的了解，以及对大量情况的观察，以及知道像这样的东西在数学上可能很方便。 像这样的东西在数学上可能很方便。 但我们必须稍微回顾一下，让它为我们唱歌。 y 是什么？ 我们上次看到了 y。
            支持向量机。 那只是一个函数。</p>
        <p>Really, it comes from knowing a lot of mathematics, and seeing a lot of situations, and knowing that
            something like this might be mathematically convenient. Something like this might be mathematically
            convenient. But we’ve got to back up a little and let it sing to us. What’s y? We saw y last time. The
            support vector machines. That’s just a function.</p>
        <p>这是正 1 还是负 1，取决于输出应该是正 1 还是负 1。所以如果这个人给出了正确的答案，而正确的答案是正的，那么这个人也会是正
            1，因为它总是给你正确的答案。所以在这种情况下，当这个人给出正确的答案时，它们将具有相同的符号，所以这将是一个正 1 的组合。</p>
        <p>That’s plus 1 or minus 1, depending on whether the output ought to be plus 1 or minus 1. So if this guy is
            giving the correct answer, and the correct answer is plus, and then this guy will be plus 1 too, because it
            always gives you the correct answer. So in that case, where this guy is giving the right answer, these will
            have the same sign, so that will be a plus 1 combination.</p>
        <p>另一方面，如果那个人答错了，那么这个组合就会给你减 1。所以即使正确答案应该是减号，这也是正确的，对吧？所以如果正确答案应该是减号，而这个是加号，那么这个就是减 1，整个组合就会给你减 1。</p>
        <p>On the other hand, if that guy’s giving the wrong answer, you’re going to get a minus 1 out of that
            combination. So it’s true even if the right answer should be minus, right? So if the right answer should be
            minus, and this is plus, then this will be minus 1, and the whole combination well give you minus 1 again.
        </p>
        <p>换句话说，如果答案错误，y 会翻转符号，无论错误答案是加 1 还是减 1。这些 alpha。哎呀，它们与上面公式中的 alpha 相同。然后是 z，那是干什么用的？</p>
        <p>In other words, the y just flips the sign if you’ve got the wrong answer, no matter whether the wrong answer
            is plus 1 or minus 1. These alphas. shoot, those are the same alphas that are in this formula up here,
            somehow. And then that z, what’s that for?</p>
        <p>好吧，如果你只看之前的权重，以及它的指数函数来为下一代产生这些 W，那么它就不是一个分布，因为它们的总和不会是 1。所以这里的这个东西，这个 z，是一种标准化器。它使整个新权重的组合加起来等于
            1。所以它是你把所有这些加起来，然后除以这个数字得到的结果。</p>
        <p>Well, if you just look at the previous weights, and its exponential function to produce these W’s for the
            next generation, that’s not going to be a distribution, because they won’t sum up to 1. So what this thing
            here, this z is, that’s a sort of normalizer. And that makes that whole combination of new weights add up to
            1. So it’s whatever you got by adding up all those guys, and then dividing by that number.</p>
        <p>嗯，哎呀。我不知道。现在有些结果就是这样。我们假设有人做了和我们对支持向量机所做的一样的事情。我们要找到一种方法来最小化错误。我们要最小化的错误是上面 4
            中整个表达式产生的错误。我们将在进行过程中最小化整个表达式的错误。</p>
        <p>Well, phew. I don’t know. Now there’s some it turns out thats. We’re going to imagine that somebody’s done
            the same sort of thing we did to the support vector machines. We’re going to find a way to minimize the
            error. And the error we’re going to minimize is the error produced by that whole thing up there in 4. We’re
            going to minimize the error of that entire expression as we go along.</p>
        <p>当我们进行适当的微分等操作时，我们会发现。你知道，这就是我们在微积分中所做的。我们发现，如果 alpha 等于 1 减去时间 t 的误差率，再除以时间 t
            的误差率，那么整个过程的误差最小。现在让我们取它的对数，然后乘以一半。这就是我们努力寻找的。</p>
        <p>And what we discover when we do the appropriate differentiations and stuff. you know, that’s what we do in
            calculus. what we discover is that you get minimum error for the whole thing if alpha is equal to 1 minus
            the error rate at time t, divided by the error rate at time t. Now let’s take the logarithm of that, and
            multiply it by half. And that’s what was struggling to find.</p>
        <p>但是我们还没有完全弄对。所以让我把它分成几个部分，这样我们就不会混淆了。这是上面那个表达式的界限。这是该表达式产生的错误率的界限。有趣的是，这意味着当你在这个公式中添加项时，错误率实际上会上升。</p>
        <p>But we haven’t quite got it right. And so let me add this in separate chunks, so we don’t get confused about
            this. It’s a bound on that expression up there. It’s a bound on the error rate produced by that expression.
            So interestingly enough, this means that the error rate can actually go up as you add terms to this formula.
        </p>
        <p>您所知道的是，错误率将受指数衰减函数的限制。因此，它最终一定会收敛到零。因此，这是一个最小错误界限。结果是指数的。好了，就是这样。我们讲完了。您想看演示吗？好的。因为您看到它，会说，好吧，这样的事情怎么可能奏效？答案是，令人惊讶的是，事情就是这样发生的。
        </p>
        <p>All you know is that the error rate is going to be bounded by an exponentially decaying function. So it’s
            eventually guaranteed to converge on zero. So it’s a minimal error bound. It turns out to be exponential.
            Well, there it is. We’re done. Would you like to see a demonstration? Yeah, OK. Because you look at that,
            and you say, well, how could anything like that possibly work? And the answer is, surprisingly enough,
            here’s what happens.</p>
        <p>这是一个简单的小例子。这是第一个选择测试。绿色是加号，红色是减号，所以它仍然有错误。仍然有错误。砰。好了，分两步。现在有了。我们可以看看右上角。我们看到它使用了三个分类器，我们看到其中一个分类器说每个人都属于一个特定的类别，三个不同的权重。
        </p>
        <p>There’s a simple little example. So that’s the first test chosen. the greens are pluses and the reds are
            minuses, so it’s still got an error. Still got an error. boom. There, in two steps. It now has. we can look
            in the upper right hand corner. we see its used three classifiers, and we see that one of those classifiers
            says that everybody belongs to a particular class, three different weights.</p>
        <p>错误率已经收敛到
            0。让我们看看其他几个。这是我用来调试这个东西的。我们让它运行。看看它有多快？砰。它收敛到非常快地获得所有样本的正确性。这是另一个。这是我们几年前在考试中给出的。第一次测试。哦，我让它运行，所以它立即得到了正确的一切。
        </p>
        <p>And the error rate has converged to 0. So let’s look at a couple of other ones. Here is the one I use for
            debugging this thing. We’ll let that run. See how fast it is? Boom. It converges to getting all the samples
            right very fast. Here’s another one. This is one we gave on an exam a few years back. First test. Oh, I let
            it run, so it got everything instantaneously right.</p>
        <p>让我们一步一步地来看一下。这是第一个，第二个。仍然有很多错误。啊，错误率在下降。然后趋于平稳，趋于平稳，最后变为
            0。很酷，你不觉得吗？但你会对我说，呸，谁在乎那些东西？让我们尝试一些更有趣的东西。有一个。那也相当快。好吧，这里没有太多样本。所以我们可以试试这个。</p>
        <p>Let’s take that through step at a time. There’s the first one, second one. Still got a lot of errors. Ah, the
            error rate’s dropping. And then flattened, flattened, and it goes to 0. Cool, don’t you think? But you say
            to me, bah, who cares about that stuff? Let’s try something more interesting. There’s one. That was pretty
            fast, too. Well, there’s not too many samples here. So we can try this.</p>
        <p>所以这里有一系列的正负值。砰。你可以看到错误率是如何受到指数限制的？所以在底部的图表中，你得到了所涉及的分类器的数量，最终总数达到
            10。通过查看这个特定的选项卡，你可以看到每个添加的分类器的正负值。这只是显示了它们如何随着时间的推移而演变。</p>
        <p>So there’s an array of pluses and minuses. Boom. You can see how that error rate is bounded by an
            exponential? So in a bottom graph, you’ve got the number of classifiers involved, and that goes up to a
            total, eventually, of 10. You can see how positive or negative each of the classifiers that’s added is by
            looking at this particular tab. And this just shows how they evolve over time.</p>
        <p>但这里的进展是最有趣的。现在你会问我，机器是怎么做到的？一切都在这里。我们使用一个像这样的 alpha。这使我们能够计算新的权重。它说我们已经进行了初步计算。我们必须找到进行标准化的
            az。我们最好带上计算器，因为首先我们要计算错误率。</p>
        <p>But the progress thing here is the most interesting. And now you say to me, well, how did the machine do
            that? And it’s all right here. We use an alpha that looks like this. And that allows us to compute the new
            weights. It says we’ve got a preliminary calculation. We’ve got to find a z that does the normalization. And
            we sure better bring our calculator, because we’ve got, first of all, to calculate the error rate.</p>
        <p>然后我们必须取它的对数，除以
            2，将其代入公式，取指数，这样我们就可以得到新的权重。这就是程序的工作原理。如果你尝试这样做，我保证你会考试不及格。现在，我不关心我的电脑。我真的不关心。它是一个从属机器，它可以计算这些对数和指数，直到它变成蓝色，我不在乎。
        </p>
        <p>Then we’ve got to take its logarithm, divide by 2, plug it into that formula, take the exponent, and that
            gives us the new weight. And that’s how the program works. And if you try that, I guarantee you will flunk
            the exam. Now, I don’t care about my computer. I really don’t. It’s a slave, and it can calculate these
            logarithm and exponentials till it turns blue, and I don’t care.</p>
        <p>因为我有四个核心，谁在乎呢？最好这样做，而不是坐在那里发热量。但你不想这样做。所以你想要做的是，你想知道如何更迅速地完成这类事情。所以我们必须让他们给我们一点数学知识，以便找到更好的方法来做这类事情。</p>
        <p>Because I’ve got four cores or something, and who cares. Might as well do this, than sit around just burning
            up heat. But you don’t want to do that. So what you want to do is you want to know how to do this sort of
            thing more expeditiously. So we’re going to have to let them the math sing to us a little bit, with a view
            towards finding better ways of doing this sort of thing.</p>
        <p>那么我们就这样做吧。我们很快就会用完这里的空间，所以让我尽可能地收回这块板子。所以我要做的是，好吧，现在我们有了这个将 alpha t 与错误联系起来的 alpha 公式，然后我可以将其代入这里的公式 6。我会得到的是
            t 的权重加 1 等于 t 时的权重除以该归一化因子，乘以某个值，该值取决于它是否被正确分类。</p>
        <p>So let’s do that. And we’re going to run out of space here before long, so let me reclaim as much of this
            board as I can. So what I’m going to do is I’m going to say, well, now that we’ve got this formula for alpha
            that relates alpha t to the error, then I can plug that into this formula up here, number 6. And what I’ll
            get is that the weight of t plus 1 is equal to the weight at t divided by that normalizing factor,
            multiplied times something that depends on whether it’s categorized correctly or not.</p>
        <p>这就是 y 的用途，对吧？所以我们这里有一个对数，我们有一个符号翻转器，根据 x 和 y 组合的 H。所以，如果整个负 alpha 处的符号和 y H 组合的符号为负，那么我们就必须翻转这个对数中的分子和分母，对吧？
        </p>
        <p>That’s what that y’s in their for, right? So we’ve got a logarithm here, and we got a sign flipper up there
            in terms of that H of x and y combination. So if the sign of that whole thing at minus alpha and that y H
            combination turns out to be negative, then we’re going to have to flip the numerator and denominator here in
            this logarithm, right?</p>
        <p>哦，顺便说一下，因为我们这里得到的是半数，所以它实际上是对数中该项的平方根。因此，当我们仔细计算时，我们会发现，这取决于它是否正确。但它实际上是平方根的乘数。这里最好小心一点。什么的平方根？学生：帕特里克·温斯顿：好吧，让我们看看。
        </p>
        <p>And oh, by the way, since we’ve got a half out here, that turns out to be the square root of that term inside
            the logarithm. So when we carefully do that, what we discover is that it depends on whether it’s the right
            thing or not. But what it turns out to be is something like a multiplier of the square root. Better be
            careful, here. The square root of what? STUDENT: PATRICK WINSTON: Well, let’s see.</p>
        <p>但我们必须小心。假设我们有 4 项是正确的。如果正确，那么我们将从 x 和 y 的 H 中得到相同的符号。我们得到一个负号，所以我们要翻转分子和分母。如果正确的话，我们将得到 t 的 e 除以 1 减去 t 的
            epsilon 的平方根。</p>
        <p>But we have to be careful. So let’s suppose that this is 4 things that we get correct. So if we get it
            correct, then we’re going to get the same sign out of H of x and y. We’ve get a minus sign out there, so
            we’re going to flip the numerator and denominator. So we’re going to get the square root of e of t over 1
            minus epsilon of t if that’s correct.</p>
        <p>如果错了，那就只是相反的结果。所以就是 1 减去错误率的平方根除以错误率。大家同意我的观点吗？我想是对的。如果错了，我就得像去年一样上吊自杀，头上戴个纸袋子。但让我们看看这次我们能否正确完成。</p>
        <p>If it’s wrong, it’ll just be the flip of that. So it’ll be the square root of 1 minus the error rate over the
            error rate. Everybody with me on that? I think that’s right. If it’s wrong, I’ll have to hang myself and
            wear a paper bag over my head like I did last year. But let’s see if we can make this go correctly this
            time.</p>
        <p>现在，我们得到了这个，我们把所有东西都插入了，我们知道现在应该选择这个 z，这样它就等于这个数字乘以这些东西的总和，无论它是否正确。因为最终我们希望所有这些 w 加起来等于 1。所以让我们看看如果没有
            z，它们加起来会是多少。</p>
        <p>So now, we’ve got this guy here, we’ve got everything plugged in all right, and we know that now this z ought
            to be selected so that it’s equal to the sum of this guy multiplied by these things as appropriate for
            whether it’s correct or not. Because we want, in the end, for all of these w’s to add up to 1. So let’s see
            what they add up to without the z there.</p>
        <p>所以我们知道，如果我们把正确的数相加，我们得到的一定是错误率除以 1 减去 Wt 的比率加 1 的平方根。再加上现在我们得到的是 1 减去错误率除以错误率之和，乘以时间 t 时错误的 Wi
            之和。所以这就是我们把所有这些相加而不加 z 得到的结果。</p>
        <p>So what we know is that it must be the case that if we add over the correct ones, we get the square root of
            the error rate over 1 minus the rate of the Wt plus 1. Plus now we’ve got the sum of 1 minus the error rate
            over the error rate, times the sum of the Wi at time t for wrong. So that’s what we get if we added all
            these up without the z.</p>
        <p>因此，由于所有数字加起来必须等于 1，因此 z 应该等于这个和。这看起来很糟糕，直到我们意识到，如果我们将这些错误的数字加到权重上，这就是错误率。这是 e。因此，z 等于错误率的平方根乘以 1
            减去错误率。这就是这个术语的贡献。现在，让我们看看。</p>
        <p>So since everything has to add up to 1, then z ought to be equal to this sum. That looks pretty horrible,
            until we realize that if we add these guys up over the weights that are wrong, that is the error rate. This
            is e. So therefore, z is equal the square root of the error rate times 1 minus the error rate. That’s the
            contribution of this term. Now, let’s see.</p>
        <p>正确权重的总和是多少？嗯，那一定是 1 减去错误率。啊，所以这个东西给你的结果和这个一样。所以 z 等于 2
            倍。这是件好事。现在我们有点进展了。因为现在，写下一些东西变得容易一些了。好吧，我们已经远远超过了这个，所以让我们摆脱它吧。</p>
        <p>What is the sum of the weights over the ones that are correct? Well, that must be 1 minus the error rate. Ah,
            so this thing gives you the same result as this one. So z is equal to 2 times that. And that’s a good thing.
            Now we are getting somewhere. Because now, it becomes a little bit easier to write some things down. Well,
            we’re way past this, so let’s get rid of this.</p>
        <p>现在我们可以把一些东西放在一起。让我指出我所放在一起的是什么。我这里有一个 z 的表达式。我这里有一个新 w 的表达式。所以让我们把它们放在一起，并说 w of t 加 1 等于 w of t。我想我们要把它除以
            2。然后我们得到这个平方根乘以那个表达式。</p>
        <p>And now we can put some things together. Let me point out what I’m putting together. I’ve got an expression
            for z right here. And I’ve got an expression for the new w’s here. So let’s put those together and say that
            w of t plus 1 is equal to w of t. I guess we’re going to divide that by 2. And then we’ve got this square
            root times that expression.</p>
        <p>因此，如果我们取正确的那个，然后除以那个，然后抵消，我得到 1/1 减去错误率。就是这样。这是正确的。如果它不正确，那么就是 Wt/2。然后进行数学运算。如果错误，则为
            1/epsilon。我们觉得我们取得了任何进展吗？没有。因为我们还没有让它为我们唱得足够好。</p>
        <p>So if we take that correct one, and divide by that one, then the cancel out, and I get 1 over 1 minus the
            error rate. That’s it. That’s correct. And if it’s not correct, then it’s Wt over 2. and working through the
            math. 1 over epsilon, if wrong. Do we feel like we’re making any progress? No.&nbsp;Because we haven’t let
            it sing to us enough yet.</p>
        <p>所以我想提请大家注意业余攀岩者在攀爬到半山腰的险峻悬崖时会发生什么。他们通常不会，有时也不会。如果他们不会，他们就会害怕得要死。偶尔，当他们即将跌落时，他们会找到一个小洞，把指甲插进去，这样他们就不会跌落。这被称为感谢上帝的洞。
        </p>
        <p>So I want to draw your attention to what happens to amateur rock climbers when they’re halfway up a difficult
            cliff. They’re usually sometimes they’re not. If they’re not, they’re scared to death. And every once in a
            while, as they’re just about to fall, they find some little tiny hole to stick a fingernail in, and that
            keeps them from falling. That’s called a thank god hole.</p>
        <p>所以我要介绍的是那些你可以把指甲伸进去的小地方的类似物。这是解决增强问题的感谢上帝洞。</p>
        <p>So what I’m about to introduce is the analog of those little places where you can stick your fingernail in.
            It’s the thank god hole for dealing with boosting problems.</p>
        <h2 id="scaling">扩展</h2>
        <h2>Scaling</h2>
        <p>那么，如果我把分类器得出正确答案的所有这些加起来会发生什么？好吧，它将是 1/2，1/1 减去 epsilon，乘以答案正确的 Wt 的总和。这个总和是多少？哦！</p>
        <p>So what happens if I add all these up for the ones that the classifier where produces a correct answer on?
            Well, it’ll be 1 over 2, and 1 over 1 minus epsilon, times the sum of the Wt for which the answer was
            correct. What’s this sum? Oh!</p>
        <p>我的女神。1 减去 epsilon。所以我刚刚发现，如果我将新的 w 加到那些我得到正确答案的样本上，它等于 1/2。你猜怎么着？这意味着如果我将它们加到错误的样本上，它也等于
            1/2。所以这意味着我把上一次测试中得到正确答案的所有权重都加起来，这些方法加起来就会有一定结果。</p>
        <p>My goddess. 1 minus epsilon. So what I’ve just discovered is that if I sum new w’s over those samples for
            which I got a correct answer, it’s equal to 1/2. And guess what? That means that if I sum them over wrong,
            it’s equal to 1/2 half as well. So that means that I take all of the weight for which I got the right answer
            with the previous test, and those ways will add up to something.</p>
        <p>为了得到下一代的权重，我所要做的就是对它们进行缩放，使它们相等一半。开发这个东西的人没有注意到这一点。几年前，Luis Ortiz 注意到了这一点，他是 6.034
            的讲师。这些权重的总和将是它们之前权重的缩放版本。因此，您要取这个新分类器的所有权重。</p>
        <p>And to get the weights for the next generation, all I have to do is scale them so that they equal half. This
            was not noticed by the people who developed this stuff. This was noticed by Luis Ortiz, who was a 6.034
            instructor a few years ago. The sum of those weights is going to be a scaled version of what they were
            before. So you take all the weights for which this new classifier.</p>
        <p>你选择的这个会给你重新加权的东西的最小权重。你取它给出正确答案的那些，然后取所有这些权重，然后缩放它们，使它们加起来等于 1/2。那么，你需要计算对数吗？不需要。你需要计算指数吗？不需要。你需要计算 z
            吗？不需要。你需要计算 alpha 来获得新的权重吗？不需要。&nbsp;</p>
        <p>This one you selected to give you the minimum weight on the re weighted stuff. you take the ones that it
            gives a correct answer for, and you take all of those weights, and you just scale them so they add up to
            1/2. So do you have to compute any logarithms? No.&nbsp;Do you have to compute any exponentials? No.&nbsp;Do
            you have to calculate z? No.&nbsp;Do you have to calculate alpha to get the new weights? No.&nbsp;</p>
        <p>你所要做的就是对它们进行缩放。这是一个很好的谢天谢地洞。所以这是谢天谢地洞一号。现在，对于谢天谢地洞二号，我们需要回过头来思考一下我们将要给你的概率问题，这些问题涉及决策树桩。你可能要从许多决策树桩中挑选。</p>
        <p>All you have to do is scale them. And that’s a pretty good thank god hole. So that’s thank god hole number
            one. Now, for thank god hole number two, we need to go back and think about the fact that were going to give
            you problems in probability that involve decision tree stumps. And there are a lot of decision tree stumps
            that you might have to pick from.</p>
        <p>因此，我们需要一个洞来决定如何处理这个问题。我在哪里可以找到一些空间？就在这里怎么样？假设你有一个看起来像这样的空间。我只是随机编造的。有多少个。让我们看看。1,2.3,4.5,6.7,8.9,10.11。我必须在那个维度上考虑多少个测试？11。它是
            1 加上样本数量。那太可怕了。我不知道。我真的计算过这个吗？</p>
        <p>So we need a thank god hole for deciding how to deal with that. Where can I find some room? How about right
            here. Suppose you’ve got a space that looks like this. I’m just makings this up at random. So how many.
            let’s see. 1,2.3,4.5,6.7,8.9,10.11. How many tests do I have to consider in that dimension? 11. It’s 1 plus
            the number of samples. That would be horrible. I don’t know. Do I have actually calculate this one?</p>
        <p>那怎么可能比那个更好呢？它又错了一个地方。所以那个是有道理的。另一个没有道理。所以最后，两个正确分类的样本之间的测试都不会有什么用处。所以那个是好人，那个也是好人。而这个是坏人。坏人，坏人坏人，坏人。坏人，坏人，坏人。
        </p>
        <p>How could that possibly be better than that one? It’s got one more thing wrong. So that one makes sense. The
            other one doesn’t make sense. So in the end, no test that lies between two correctly classified samples will
            ever be any good. So that one’s a good guy, and that one’s a good guy. And this one’s a bad guy. Bad guy,
            bad guy bad guy, bad guy. Bad guy, bad guy, bad buy.</p>
        <p>所以实际的测试次数是三次。同样，在另一个维度上也是如此。好吧，我在这里没有画得那么好，但这个测试会是一个好的测试吗？不。那个？不。实际上，在我得出太多结论之前，我最好看看右边，看看我得到了什么。让我们看看这个，因为我不想过多地考虑另一个维度发生了什么。
        </p>
        <p>So the actual number of tests you’ve got is three. And likewise, in the other dimension. well, I haven’t
            drawn it so well here, but would this test be a good one? No.&nbsp;That one? No.&nbsp;Actually, I’d better
            look over here on the right and see what I’ve got before I draw too many conclusions. Let’s look over this,
            since I don’t want to think too hard about what’s going on in the other dimension.</p>
        <p>但人们的想法是，这些测试中只有极少数真正重要。现在，你会对我说，还有最后一件事。过度拟合怎么办？因为所有这些只是将解决方案覆盖在样本上。并且像支持向量机过度拟合、神经图过度拟合、识别树过度拟合。猜猜怎么着？这似乎没有过度拟合。这是一个文献中令人困惑的实验结果。它又回到了提供解释的问题。
        </p>
        <p>But the idea is that very few of those tests actually matter. Now, you say to me, there’s one last thing.
            What about overfitting? Because all this does is drape a solution over the samples. And like support vector
            machines overfit, neural maps overfit, identification trees overfit. Guess what? This doesn’t seem to
            overfit. That’s an experimental result for which the literature is confused. It goes back to providing an
            explanation.</p>
        <p>因此，这种方法在各种问题上都得到了尝试，比如手写识别、语音理解，各种方法都使用了增强技术。与其他方法不同，由于某些尚未完全理解的原因，这种方法似乎不会过度拟合。但最终，他们在 6.034
            中不遗余力。每次我们这样做时，我们都会做一些额外的实验。下面是我留给你们的一个例子。这是一个 10 维空间的情况。</p>
        <p>So this stuff is tried on all sorts of problems, like handwriting recognition, understanding speech, all
            sorts of stuff uses boosting. And unlike other methods, for some reason as yet imperfectly understood, it
            doesn’t seem to overfit. But in the end, they leave no stone unturned in 6.034. Every time we do this, we do
            some additional experiments. So here’s a sample that I’ll leave you with. Here’s a situation in which we
            have a 10 dimensional space.</p>
        <p>我们制作了一个假分布，然后我们放入了那个带框的异常值。它只是随机放入空间中，因此可以将其视为错误点。所以现在我们要做的就是看看当我们运行这个家伙时会发生什么。果然，在 17
            步中，它找到了解决方案。但也许这个小家伙是过度拟合的，它是一个错误。</p>
        <p>We’ve made a fake distribution, and then we put in that boxed outlier. That was just put into the space at
            random, so it can be viewed as an error point. So now what we’re going to do is we’re going to see what
            happens when we run that guy. And sure enough, in 17 steps, it finds a solution. But maybe it’s overfit that
            little guy who’s an error.</p>
        <p>但有一件事你可以做，那就是你可以说，好吧，所有这些分类器都将这个空间分成块，我们可以计算任何样本所占空间的大小。所以我们可以做一件事。唉，我必须进行一次新的演示。</p>
        <p>But one thing you can do is you can say, well, all of these classifiers are dividing this space up into
            chunks, and we can compute the size of the space occupied by any sample. So one thing we can do. alas, I’ll
            have to get up a new demonstration.</p>
        <p>我们可以做的一件事是，现在这个人在这里，我们可以切换卷选项卡，观察错误点所占卷在我们解决问题时如何演变。看看会发生什么。这当然是随机生成的。我指望它能正常工作。以前从未失败过。所以它最初占据总卷的
            26%。它最终占据了卷的 1.4 乘以 10 的负 3%。</p>
        <p>One thing we can do, now that this guy’s over here, we can switch the volume tab and watch how the volume
            occupied by that error point evolves as we solve the problem. So look what happens. This is, of course,
            randomly generated. I’m counting on this working. Never failed before. So it originally starts out as
            occupying 26% of the total volume. It ends up occupying 1.4 times 10 to the minus 3rd% of the volume.</p>
        <p>因此，通常情况下，这些决策树树桩会紧紧地缠绕在错误点上，没有过度拟合的空间，因为没有其他东西可以容纳在相同的体积中。所以，我认为这东西往往会产生不会过度拟合的解决方案。总而言之，这是神奇的。你总是想使用它。它可以与你想要的任何类型的分类器一起使用。你应该非常彻底地理解它，因为在维度学习中任何东西都是有用的，这就是它。
        </p>
        <p>So what tends to happen is that these decision tree stumps tend to wrap themselves so tightly around the
            error points, there’s no room for overfitting, because nothing else will fit in that same volume. So that’s
            why I think that this thing tends to produce solutions which don’t overfit. So in conclusion, this is magic.
            You always want to use it. It’ll work with any kind of classifiers you want. And you should understand it
            very thoroughly, because of anything is useful in the subject in dimension learning, this is it.</p>
        <h1 id="representations-classes-trajectories-transitions">18. 表示：类别、轨迹、转换</h1>
        <h1>18. Representations: Classes, Trajectories, Transitions</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 48:58 <a
                href="https://youtube.com/watch?v=bQI0OmJPby4">https://youtube.com/watch?v=bQI0OmJPby4</a></p>
        <p> 11 years ago (Jan 11, 2014) — 48:58 <a
                href="https://youtube.com/watch?v=bQI0OmJPby4">https://youtube.com/watch?v=bQI0OmJPby4</a></p>
        <h2 id="intro">简介</h2>
        <h2>Intro</h2>
        <p>帕特里克·温斯顿：我们或许会对人类智能的本质产生一些疑问，或许我们会对过去几周我们讨论的智能类型进行一些思考。这是一种智能。那些程序、支持向量机、增强技术，它们可以做非常聪明的事情。但使用这些方法的系统的特殊之处在于，它们不知道自己在做什么。它们什么都不知道。
        </p>
        <p>PATRICK WINSTON: We might wonder a little bit about the nature of human intelligence, and we might reflect a
            little bit on the kind of intelligence we’ve been talking about in the past few weeks. It’s been an
            intelligence of sorts. Those programs, support vector machines, boosting, they can do really smart things.
            But the peculiar thing about systems that use those methods is that those systems don’t have any idea about
            what they’re doing. They don’t know anything.</p>
        <p>所以它们并没有让我们深入了解人类智力的本质。毕竟，我希望有一个人类智力模型，因为，让我们面对现实吧，我们是世界上最聪明的生物。所以有很多方法可以解决这个问题。我们首先从进化的角度来探讨这个问题。一些科学家，比如我，认为我们的家谱大致如下。
        </p>
        <p>So they don’t give us very much of an insight into the nature of human intelligence. And, after all, I’d like
            to have a model of human intelligence because, let’s face it, we’re the smartest things around. So there are
            lots of ways to approach that question. And we’ll approach that question first from a evolutionary point of
            view. Some scientists believe, me for instance, that we have a family tree that looks about like this.</p>
        <p>太小了，看不清其中的很多细节，但关键是我们出现的时间并不长。我们人类出现的时间可能只有 20 万年，而恐龙在 6000
            万年前就灭绝了。所以转眼间，我们似乎或多或少已经接管了整个世界。当你以可见的规模查看这棵家谱时，其中一个特征就是脑容量不断增大。</p>
        <p>Too small to see much of that, but the main point is that we haven’t been around very long. We humans have
            been around maybe 200,000 years, and the dinosaurs died out 60 million years ago. So in the blink of an eye,
            we seem to have, more or less, taken over. When you look at that family tree on a scale where you can see
            it, one of the characteristics is increasing brain size.</p>
        <p>左边是人类，右边是黑猩猩。显然，大部分都是嘴巴，脑子不多。下面这张是 400 万年前左右的古猿类双足猿的复原图。所以我们在大脑发育很久之前就变成了双足动物。所以你可能会想，也许脑子大小与此有很大关系，我想确实如此。
        </p>
        <p>There we are on the left, chimpanzee on the right. Clearly, mostly mouth, not to much brain in there. That
            one down below is a reconstruction of one of those pithecus type bipedal apes from about 4 million years ago
            or so. So we became bipedal a long time before we had much of a brain. So you might think, well, maybe brain
            size has a lot to do with it, and I suppose it does.</p>
        <p>因此，我们可以绘制出我们祖先的脑容量与时间的关系图。我刚才给你们看的图片大概是 300
            万年前的。然后在右上角，哦，那不仅仅是我们，还有尼安德特人。他们的脑可能比我们的略大。所以这不仅仅是脑容量的问题。这是那个人的样子。当然，左边的那个是尼安德特人。</p>
        <p>So we can plot brain volume of our ancestors versus time. So the picture I just showed you was from about 3
            million years ago, I guess. And then on the upper right hand corner, oh, that’s not just us, that’s also the
            Neanderthals. Their brains might have been slightly bigger than ours. So it isn’t just brain size. Here’s
            what that guy looks like. That’s a Neanderthal, of course, on the left.</p>
        <p>右边的是我们其中一人。他们有一些明显的不同，大头，肋骨呈圆锥形，骨盆很大。人们喜欢对他们如何移动进行大量推测。但有一点很清楚，他们并不是什么了不起的人。他们可以制造石器，但这些石器在几万年里没有太大变化。</p>
        <p>And that’s one of us on the right. Some conspicuous differences, big heads, the rib cage is kind of conical
            in shape, a large pelvis. People like to make a lot of speculations about how they must have moved around.
            But one thing is plain, they didn’t amount to much. They could make stone tools, but those stone tools
            didn’t change much over tens of thousands of years.</p>
        <p>我们的情况也差不多，直到发生了一些事情，可能是在南部非洲，可能是在一群人身上，可能不到 1,000 人。有什么证据可以证明这一点？证据大多是。</p>
        <p>And that was pretty much the story with us too, until something happened, probably in southern Africa,
            probably in a group of individuals, maybe less than 1,000. And what’s the evidence for that? The evidence
            for that is mostly.</p>
        <p>来自具有大量概率假设的 DNA 研究和蒙特卡罗模拟，但在关于我们如何开始在世界上繁衍生息的相互竞争的假设中，似乎有一群我们，即智人，在南部非洲获得了别人所没有的东西，而最高概率的情况是我们迅速占领了这个世界。</p>
        <p>Comes from DNA studies with a lot of probabilistic assumptions, and Monte Carlo simulations, but it seems
            among the competing hypothesis for how we came to populate the world, it seems that there was a group of us,
            homo sapiens, in Southern Africa that got something that nobody else had and the highest probability
            scenario is that we quickly took over.</p>
        <p>那个智人种群统治了其余的种群，走出了非洲，在眨眼间就完成了这样的事情。那是什么样的事情？这两幅画作来自大约 25,000 年前的拉斯科。古人类学家，如塔特索尔，认为这是当时的人，我们智人，有象征性思维的明显证据。头部是
            25,000 年前女性乳齿象獠牙的雕刻。</p>
        <p>That population of homo sapiens dominated the rest, went out of Africa, and within the blink of an eye did
            that sort of stuff. What’s that sort of stuff? Those two paintings are from Lascaux about 25,000 years ago.
            Paleoanthropologists, like Tattersall, take that as plain evidence that there was symbolic thought on the
            people who were around at that time, us homo sapiens. The head is a carving of a mastodon tusk of a woman
            25,000 years ago.</p>
        <p>此外，从象征意义上讲，人们制作了大量珠宝，并进行自我装饰。尼安德特人似乎从未这样做过。珠宝制作似乎可以追溯到南部非洲，大约 7
            万年前。人们显然在刺穿贝壳并将其用作项链。所以发生了一些事情，而那些写出引人入胜的东西的古人类学家不知道该如何谈论它，只能说我们似乎变得具有某种象征意义。</p>
        <p>Also, plainly symbolic, people are making a lot of jewelry and doing self adornment. The Neanderthals never
            seemed to do that. That jewelry making seems to have gone back to Southern Africa, maybe 70,000 years ago.
            People were puncturing seashells and using them as necklaces, apparently. So something happened, and the
            paleoanthropologists, who write fascinating stuff, don’t quite know how to talk about it other than to say
            it looks like we became somehow symbolic.</p>
        <p>这与语言有某种关系。所以如果你和诺姆·乔姆斯基交谈，他会说。让我说得更准确一些，这是我能找到的最接近他的引文。他认为，能力就是将两个概念组合在一起形成第三个概念的能力，不会干扰原有的概念，也不会受到限制，而其中的每一部分都很重要。
        </p>
        <p>And somehow that has something to do with language. So if you talk to Noam Chomsky he will say. let me get
            this precise, this is near his quotation as I can get. He thinks it was the ability to take two concepts and
            put them together to form a third concept without disturbing the original concepts and without limit, and
            each part of that’s important.</p>
        <p>不受限制是我们与那些可能能够做到这一点的物种的区别，但我们可以在没有任何明显限制的情况下做到这一点。所以这是一位语言学家在说话。他谈论了很多关于合并操作、组合器和语言的事情。使用的术语对我们来说很陌生。最好不要使用“组合器”这个词，它是一种计算机科学术语。但不管它是什么，它似乎发生在那个时候。
        </p>
        <p>The without limit part is what separates us from species that might be able to do that a little bit, but we
            can do it without any apparent limit. So that’s a linguist speaking. He talks a lot about the merge
            operation, and combinators and language. using terms foreign to us. Better not use the term “combinator,”
            it’s kind of a computer science term. But whatever it is, it seemed to happen about that time.</p>
        <p>这种能力并不是慢慢形成的，也不是与大脑大小成比例的，而似乎是大脑长大到足以产生能力后突然形成的，但这种能力并不是推动进化朝那个方向发展的因素。所以，我相信，不管那是什么，那种能力使人类，我们人类，能够讲述和理解故事，这就是我们与其他灵长类动物的区别。那种能力。
        </p>
        <p>It didn’t happen slowly and proportion to brain size, it seemed to happen all of a sudden in consequence of a
            brain that had grown big enough to be an enablement but the capability was not what pulled evolution in that
            direction. So, I believe, that whatever that was, that capability, enabled humans, us humans, to tell and
            understand stories, and that’s what separates us from the other primates. That ability to.</p>
        <p>无论象征能力是什么，它都使讲故事和理解成为可能。这就是所有教育的意义所在，也是我们人类如此特别的原因。所以今天我们要讨论的可能是这个假设的一个实例，一种思考方式。这是一种思考语言学家称之为内在语言的方式。</p>
        <p>The symbolic ability, whatever it is, enabled storytelling and understanding. And that’s what all education
            is about and that’s why our species is special. So what we’re going to talk about today is something you
            might think of as an instantiation of that hypothesis, one way of thinking about it. And it’s a way of
            thinking about what the linguists would call the inner language.</p>
        <p>这不是我们用来交流的语言，而是我们用来思考的语言，这与我们用来交流的语言密切相关，但可能并不完全相同。你们中很多人都会说两种语言。克里斯就是双语者。克里斯，你有没有这样的经历，记得有人对你说了什么，但不记得他们用了什么语言？学生：帕特里克·温斯顿：你有过这种经历吗？学生：帕特里克·温斯顿：什么？
        </p>
        <p>It’s not the language with which we communicate, it’s the language with which we think, which is closely
            related to the language with which we communicate, but may not be quite the same thing. So many of you are
            bilingual. Chris is bilingual. Chris, have you ever had the experience of remembering that someone said
            something to you, but not remembering what language they used? STUDENT: PATRICK WINSTON: How about have you
            had that experience? STUDENT: PATRICK WINSTON: What?</p>
        <p>学生：我总是有这样的经历。大卫，你有没有这样的经历：记得一些对话，但不记得对话所使用的语言？ 学生：嗯，你记得一些你通常不知道它是什么语言。 帕特里克·温斯顿：你通常不记得。这是一种普遍的看法。</p>
        <p>STUDENT: I always PATRICK WINSTON: experience of having. David, have you ever had that experience of
            remembering some conversation but not remembering the language in which it was cast? STUDENT: Well, you
            remember something you usually don’t know which language it was in anyway. PATRICK WINSTON: You usually
            don’t remember. That’s a common view.</p>
        <p>你记得有人说过一些话，记得有一段对话是有内容的，但如果是和说你母语的人交谈，而你身处另一个地方，你经常记不住对话是用什么语言进行的。对吗，你记得这样的事情吗？不确定？学生：说某种语言的人，所以。</p>
        <p>You remember something was said, that there was a conversation that had some content, but if it’s with a
            speaker of your own language and your embedded in another place, you often don’t remember what language the
            conversation was in. Is that right, you remember things like that? Not sure? STUDENT: People who speak in a
            certain language, so.</p>
        <p>帕特里克·温斯顿：她说，有时你不会感到困惑，因为你总是用特定的语言和特定的人交谈。但许多人报告说，他们有过不记得自己说的是哪种语言的经历。</p>
        <p>PATRICK WINSTON: Sometimes you don’t have that confusion, she says, because you always speak to particular
            people in a particular language. But many people report that they have that experience of not remembering
            which language something was said in.</p>
        <h2 id="inner-language">内在语言</h2>
        <h2>Inner Language</h2>
        <p>好吧，那我们该怎么做呢？我们需要一种内在语言，也许我们可以这样开始：让我们找一些看起来熟悉的东西。</p>
        <p>Well, OK, so what are we going to do? We need an inner language, and maybe we can start just by saying, let’s
            have something that looks sort of familiar to us.</p>
        <p>我们有一个对象，它由其他一些对象支持，所以这些是支持关系。这就是我们所谓的语义网络的一个例子。它是一个有节点和链接的网络，它有一定的意义。这就是“语义”一词的由来。好吧，我们可能还有另一个看起来像这样的例子。</p>
        <p>We have an object and it’s supported by some other objects, so those are support relations. That’s one
            example of what we might call a semantic net. It’s a network that’s got nodes and links, it’s got. it has
            some meaning. That’s where the word “semantic” comes from. Well, we might have another example that looks
            like this.</p>
        <p>有麦克白，有邓肯，麦克白谋杀了邓肯，然后我们也知道，不知何故，结果涉及杀戮，最终，邓肯拥有一个属性。这个属性就是死亡的属性。所以还有另一个语义网络记录了莎士比亚的麦克白情节中发生的事情。现在，我们可以稍微修饰一下。这样就可以让其他几个概念发挥作用。
        </p>
        <p>There’s Macbeth, there’s Duncan, and Macbeth murders Duncan, then we also know, somehow, there’s a kill
            involved as a consequence, and then ultimately, Duncan has a property. And that property is the property of
            being dead. So there’s another semantic net recording something that happens in Shakespeare’s Macbeth plot.
            Now, we can decorate that a little bit. So as to get a couple of other concepts in play.</p>
        <p>首先，我们已经有了组合子。然后是组合子。嗯，这是连接节点的链接的别名。我们得到的另一件事是连接链接本身的机会。因此，谋杀在某种程度上意味着杀戮，而杀戮使我们得出受害者已死的结论。因此，这是将链接本身视为可以成为其他链接的主体或客体的对象。
        </p>
        <p>First of all, the thing we’ve got already is we’ve got combinators. Then combinators. Well, a fancy name for
            those links that connect the nodes. Another thing we’ve got is an opportunity for connecting the links
            themselves. So the murder sort of implies the kill, and the kill leads us to conclude that the victim is
            dead. So that is treating the links themselves as objects that can be the subject or object of other links.
        </p>
        <p>因此我们将这个过程称为“具体化”。现在，在人工智能领域，语义网络的早期工作已经完成，但如果你有一个覆盖整个网络的大型网络，你需要某种方式来突出它的某些部分。因此，马文·明斯基将大量技术内容融入到这个想法中，并创造了这个概念。这个概念值得在这里用另一种颜色来表达。
        </p>
        <p>So we call that process “reification.” Now, in artificial intelligence, semantic nets we’re all over the
            early work, but if you have a big network that covers the wall you need some way of putting a spotlight on
            some pieces of it. So Marvin Minsky put a lot of technical content into that idea and created the notion of.
            a notion that deserves another color here.</p>
        <p>他建议我们需要一个本地化过程，所以我们有框架，即所谓的框架或模板。这个谋杀动作的框架可能是有一个谋杀框架，其中有一个代理人和一个受害者，代理人是麦克白，受害者是邓肯。所以这是一种在我们目前所得到的基础上添加本地化层的方法。稍后，我将在该列表中添加序列。
        </p>
        <p>He suggested that we need a localization process, so we have frames, so called frames or templates. And a
            frame for this murder action might be that there’s a murder frame that has an agent and has a victim, and
            the agent is Macbeth, and the victim is Duncan. So that’s a way of putting a localization layer on top of
            what we’ve got so far. Later on, I’ll add sequence to that list.</p>
        <p>所以它停留在这里很长一段时间，从某种意义上说，它仍然停留在那里，因为一旦你有了组合子，你就得到了某种几乎通用的东西。你可以用它做任何事情。问题是它有点处于位级别。它就像汇编代码。作为一个概念，它没有足够的组织来帮助你达到下一个成就水平。
        </p>
        <p>So this is where it rested for a long time, and in some sense, still rests there because as soon as you’ve
            got combinators, you’ve got something that’s pretty much universal. You can do anything with it. The trouble
            is it’s sort of down at the bit level. It’s like assembly code. It doesn’t have, as a concept, enough
            organization to help you go to the next level of achievement.</p>
        <p>这里还有一个小问题值得一提，那就是整个事情都存在寄生语义学问题。寄生语义学。整个概念都充满了丑陋，因为当我们看这样的图表时，我们会说，哦，麦克白谋杀了邓肯，这意味着邓肯是受害者。我们知道肯定有动机，也许麦克白想当国王。
        </p>
        <p>There’s also a little problem here that deserves also, some mention, and that is that we have, over this
            whole thing, the problem of parasitic semantics. Parasitic semantics. A kind of ugliness that surrounds this
            whole concept because when we look at a diagram like that, and we say, oh, Macbeth murdered Duncan, that
            means Duncan’s the victim. We know there must have been a motive, maybe Macbeth wanted to be king.</p>
        <p>好吧，我们知道所有这些东西，而且有一种倾向，就是把这些知识投射到机器身上。如果你要玩手机，请走开。所以，如果我们把意义投射到我们的理解中，那不是机器的理解。很多意义可以说是寄生的。我们是寄生虫，我们把意义投射到那个东西上。把那个图表放到某种机器形式中并不意味着机器知道任何事情。
        </p>
        <p>Well, we know all that stuff, and there’s a tendency to project that knowing into the machine. If you’re
            going to play with your telephone, please leave. So if we project meaning into that’s our understanding
            that’s not the machine’s understanding. So much of the meaning can be said to be parasitic. We’re the
            parasite, and we’re projecting the into that thing. Putting that diagram into some machine form doesn’t mean
            the machine knows anything.</p>
        <p>它也许能够得出一些结论，但它的理解并非建立在与物质世界的任何接触之上。因此，我们必须对此深感担忧，哲学家们会止步于此，然后就这个主题写几本书。但我们不是哲学家，所以我们只是提到这个问题，然后继续前进。</p>
        <p>It might be able to conclude some things, but it’s understanding is not grounded in any kind of contact with
            the physical world. So we have to worry a lot about that and where philosophers would stop there and go off
            and write a few books on the subject. But we’re not philosophers so we’re going to just mention the problem
            and go barreling ahead.</p>
        <h2 id="classification">分类</h2>
        <h2>Classification</h2>
        <p>因此，我们需要使用语义网络的概念，我们必须问自己一些问题，关于内部语言的哪些元素最有用，尽管这可能非常复杂，但这是第一个有用之处。分类的概念。所以我们了解事物，我们了解钢琴。我们了解工具。我们了解地图。但我们在不同的层面上了解这些东西。
        </p>
        <p>So we need to use this notion of semantic net, and we have to ask ourselves some questions about what
            elements of the inner language are most useful and yet it might be very complicated, but here’s usefulness
            number one. The notion of classification. So we know about stuff, and we know about, for example, pianos.
            And we know about tools. And we know about maps. But we know about those things on different levels.</p>
        <p>那么，如果我说我在想一个工具，你对我所说的东西有一个很好的印象吗？答案一定是否定的，因为工具的概念非常模糊，所以很难形成一幅关于工具的图画。另一方面，如果我说我在想一台 Mac。嗯，这很有趣，因为那里存在词汇歧义。
        </p>
        <p>So say I’m thinking about a tool, do you have a very good image of what I’m talking about? The answer has to
            be no because the notion of a tool is very vague, so it’s hard to form a picture of what that’s all about.
            On the other hand, if I said I’m thinking about a mac. Well, this is interesting because there’s lexical
            ambiguity there.</p>
        <p>您不知道我说的是苹果型 Mac 还是苹果型
            Mac，或者我应该说水果还是电脑？因此，在两个或更多层面上存在词汇歧义。但让我们稍微补充一下。如果我知道我在谈论钢琴，你就可以形成一幅画面，这似乎处于更详细的层面，你可以在那里产生幻觉。</p>
        <p>You don’t know if I’m talking about the Apple type Mac or the apple type Mac, or should I say the fruit or
            the computer? So there’s lexical ambiguity there at two levels or more. But let’s fill this in a little bit.
            If I know I’m talking about a piano, you can form a picture of that so that seems to be on a more detailed
            level where you can do hallucination.</p>
        <p>在更高层次上，你拥有的只是一件乐器。我可以通过写下锤子来给你一个思考的工具。如果我要买一台
            Mac，那它就是苹果。在这种情况下，我希望你考虑一种水果。在这里，我也可以更具体地说明这些事情。我可以稍微细化一下细节，然后说，我正在考虑其中之一。</p>
        <p>At a higher level you have just a musical instrument. And I can give you a tool to think about by writing
            hammer. And if I’m going to have a mac, it’s going to be an apple. And in this case, I want you to think
            about a fruit. And down here, I can be more specific about these things too. I can add a slight refinement
            of detail and say, I’m thinking about one of these.</p>
        <p>你知道这是什么吗？
            学生：帕特里克·温斯顿：不，它不是普通的锤子，而是圆头锤。在某些圈子里，它被称为女士锤。我不知道为什么。但它是。它是干什么用的？大多数人买它主要是因为它小巧轻便。但事实上，它是用于金属加工的。它用于取一块金属板并将其敲成烟灰缸或类似的东西，或者用于固定铆钉。
        </p>
        <p>Do you know what this is? STUDENT: PATRICK WINSTON: No, it’s not a mere hammer, it’s a ball peen hammer. In
            some circles, it’s called a ladies hammer. I don’t know why. But it’s. What’s it for? Most people buy it
            mostly because it’s small and light weight. But, in fact, it’s for metal working. It’s for taking a piece of
            sheet metal and pounding it out into an ashtray or something or for seating rivets.</p>
        <p>这是金属工人的锤子。所以你以前可能不知道，但现在，至少，你有一个词来巩固这些知识。这是一把圆头锤。所以我们这里有不同的层次，从非常具体到非常一般。我们甚至可以说我们有一台 Bosendorfer，具体到钢琴。为什么
            Bosendorfer 很特别？我的意思是，它像鲍德温吗？Yoka 就是其中之一。</p>
        <p>It’s a metal worker’s hammer. So you might not have known about that before but now, at least, you have a
            word to hang that knowledge on. It’s a ball peen hammer. So we have various levels here, going from very
            specific to very general. And we can even go to a level of specificity for pianos by saying we’ve got a
            Bosendorfer. Why is a Bosendorfer special? I mean, is it like a Baldwin? Something’s Yoka.</p>
        <p>钢琴种类繁多，有什么特别的？你看，你不知道，因为除非你弹钢琴，除非你是一个认真的钢琴演奏者，否则你不知道
            Bosendorfer。Ariel，你知道。学生：我认为它应该在底部有一个额外的八度，黑键。很酷。帕特里克·温斯顿：它在底部有一些额外的键。大多数人都不知道这一点，除非他们对钢琴很认真。</p>
        <p>Lots of piano types, what’s special? You see, you don’t know because unless you play the piano, and probably
            unless you’re a serious piano player you don’t know that a Bosendorfer. Ariel, you know. STUDENT: I think
            its supposed to have an extra octave at the bottom, black keys. Pretty cool. PATRICK WINSTON: It’s got some
            extra keys at the bottom. And most people don’t know that unless they’re serious about the piano.</p>
        <p>有些专业钢琴演奏者在面对贝森朵夫钢琴时，必须让别人遮住那些琴键，因为这样会干扰他们的周边视觉，从而按错琴键。因为他们不习惯在底部有那些额外的琴键。</p>
        <p>Some professional piano players, when they’re confronted with a Bosendorfer have to have someone cover those
            keys because it screws up their peripheral vision, and hit the wrong key. Because they’re not used to having
            those extra keys at the bottom.</p>
        <h2 id="graphs">图表</h2>
        <h2>Graphs</h2>
        <p>所以这只是 Bosendorfer 的一个小细节。所以你可以制作一种图表，然后你可以说，让我们从低级、非常笼统的层次，到基础层次，再到特定层次。</p>
        <p>So that’s a little detail but the Bosendorfer. So you can make a kind of graph, and you can say, let’s go
            from low, very general, to a basic level, to a specific level.</p>
        <p>因此，在人类知识中，图表往往看起来像这样。这是工具，这是锤子，这是圆头锤。因此，在那个层次上，有一个大的跳跃，这是从一般到基本层次的过渡。所以那个基本层次可能存在，因为那是我们存放大量知识的层次。</p>
        <p>So it is the case in human knowledge that graph has a tendency to look sort of like this. So here’s tool,
            here’s hammer, here’s ball peen. So that level, where you have a big jump, that’s the general to basic level
            of transition. So that basic level is probably there because that’s the level on which we hang a huge amount
            of our knowledge.</p>
        <p>我们对钢琴了解很多，一切似乎都取决于“钢琴”这个词，这个词赋予了我们力量。</p>
        <p>We know a lot about pianos, and it all seems to be hanging on that word piano, which gives us power with the
            concept.</p>
        <h2 id="vocabulary-of-change">变化的词汇</h2>
        <h2>Vocabulary of Change</h2>
        <p>这就是我们内在语言元素的一个例子。将事物组装成这样的层次结构，并将关于这些对象的知识挂在该层次结构中的元素上的能力。那么，假设你在层次结构中有元素，你如何谈论它们呢？</p>
        <p>So that’s example number one of an element of our inner language. The ability to assemble things into
            hierarchies like that, and hang knowledge about those objects on the elements in that hierarchy. Well, given
            that you have elements in the hierarchy, how do you talk about them?</p>
        <p>好吧，为了便于说明，我想考虑一下这种可能性，假设您正在考虑一辆汽车撞到墙上。因此，您需要考虑一些因素，例如汽车的速度、与墙壁的距离以及汽车的状况。并且您需要考虑碰撞前、碰撞中和碰撞后的时间段。</p>
        <p>Well, I like to consider the possibility, just for the sake of illustration, that you’re thinking about a car
            crashing into a wall. So you’ve got things to think about like the speed of the car, the distance to the
            wall, and the condition of the car. And you’ve got the period before the crash, during the crash, and after
            the crash.</p>
        <p>所以你可能想思考如何谈论这三个时期的物体。所以我们可以用变化的词汇来做到这一点，我们这样做是因为我们相信大多数人类思维都在思考变化导致变化。这与我们作为工程师所学到的知识背道而驰。</p>
        <p>So you might want to think about how to talk about those objects in those three time periods. So we can do
            that with a vocabulary of change and we do that because we believe that most of human thinking is thinking
            about change causing change. And that flies in the face of what we learned as engineers.</p>
        <p>因为在工程学中，我们学习的是状态，一旦你知道了系统的状态，你就知道了预测未来所需的一切。问题是，在我们的头脑中，思考世界上的一切，包括月亮的当前相位，都是太多的东西了。所以，我们认为，我们的思维大多围绕着“变化导致变化”这一理念。这就是为什么我们有变化的词汇。
        </p>
        <p>Because in engineering, we learn about state, and once you know the state of the system, you know everything
            you need to know in order to predict the future. The trouble is, in our heads, thinking about everything
            there is in the world, including the current phase of the moon, is too much stuff. So mostly, our thinking,
            we think, is hinging on the idea that change leads to change. So that’s why we have a vocabulary of change.
        </p>
        <p>因此，在碰撞发生前的一段时间内，汽车的速度没有变化。没有变化，没有增量。到墙壁的距离在减少。汽车的状况没有变化。然后，汽车撞上了墙壁。因此，汽车的速度消失了，到墙壁的距离消失了，汽车的状况将发生巨大变化。</p>
        <p>So in the period before the crash, the speed of the car is not changing. There’s a little notation for not
            change, no delta. The distance to the wall, that’s decreasing. The condition of the car, that’s not
            changing. Then, the car hits the wall. So the speed of the car disappears, the distance to the wall
            disappears, and the condition of the car will change dramatically.</p>
        <p>最后，碰撞结束后，汽车的速度没有出现。与墙壁的距离没有变化，汽车的状况也没有变化。所以这暗示着词汇的变化及其使用，这将是我们开发词汇的第二个要素，这些词汇是我们可能构建内在语言的方式。</p>
        <p>Finally, after the crash is over, the speed of the car does not appear. The distance to the wall does not
            change, and the condition of the car also does not change. So that’s hinting at a vocabulary of change, and
            its use, which will be the second element in our development of a vocabulary of ways in which we might have
            constructed are inner language.</p>
        <h2 id="transition">过渡</h2>
        <h2>Transition</h2>
        <p>这是一个特殊的想法，这就是分类。</p>
        <p>So this a particular idea, that’s classification.</p>
        <p>这是过渡，一个旨在理解故事的系统，它非常重视过渡的概念，我们认为，也就是说，我认为，词汇必须有减少、增加、改变、出现和消失。所以在这样的图表中你可以有 10
            种东西。我做了五个。这是因为，对于每一个，都有一个不变的变化。</p>
        <p>This is transition, and a system that purports to understand stories with a heavy emphasis on this notion of
            transition and we believe, that is to say, I think, that vocabulary has to have decrease, increase, change,
            appear, and disappear. So there are 10 things you can have in such diagrams. I’ve done five. That’s because,
            for each of those, there’s a not variation on that.</p>
        <p>因此，拥有 10 个事物的词汇表可以在很大程度上帮助描述正在变化和转变的事物。我们的词汇表中有很多这样的词。我们经常使用这些词。它们似乎与视觉密切相关。我们的朋友出现了。猫消失了。速度增加了。所以这是对碰撞的描述。
        </p>
        <p>So with a vocabulary of 10 things can go a long way toward helping to describe things that are in process of
            change and making transition. And we have a lot of those words in our vocabulary. We use those words a lot
            in our vocabulary. They seem heavily connected with vision. Our friend appeared. The cat disappeared. The
            speed increased. So this is a description of a crash.</p>
        <p>就这些元素而言，现在我要问你，相机是如何工作的？好吧，我可以说相机之所以工作是因为光子撞击了光受体。那么当我说光子撞击了光受体时，我为什么这么说，它有什么帮助呢？</p>
        <p>In terms of those kinds of elements, now, I say to you, how does a camera work? Well, I could say the camera
            works because a photon crashes into a photo receptor. So when I say a photon crashes into a photo receptors,
            why am I saying that, and how does it help?</p>
        <p>我这么说是有帮助的，因为这和你在谈论汽车撞墙时已经知道的变化模式是一样的。这是怎么回事？光子的速度、光子到受体的距离以及受体的状况。所以像这样的类比是我们一直思考的核心。实际上，还有第二个表示。</p>
        <p>I’m saying that and it helps because it’s the same pattern of change you already know about when you talk
            about a car crashing into a wall. How does that work? The speed of the photon, the distance of the photon to
            the receptor, and the condition of the receptor. So analogies like that are very much of the core of what we
            think about all the time. Really then, there is representation number two.</p>
        <p>第一是类别，第二是过渡，现在，你准备好学习第三，即轨迹。研究句子的语言学家经常谈论基本模式，这些模式似乎存在于我们所说的很多内容中，而我们所说的很多内容都是关于物体沿着轨迹移动的。</p>
        <p>Number one is class, number two is transition, and now, you’re ready for number three, which is trajectory.
            Linguists, who study sentences, often talk in terms of fundamental patterns that seemed to be in a lot of
            what we say, and a lot of what we say is about objects moving along trajectories.</p>
        <h2 id="trajectories">轨迹</h2>
        <h2>Trajectories</h2>
        <p>因此我们可以讨论轨迹框架。轨迹框架将包含如下元素。</p>
        <p>So we can talk about a trajectory frame. And a trajectory frame will have elements like this.</p>
        <p>它有一个物体沿着轨迹移动，最终到达目的地。你可能从源头开始。它可能是由某种代理安排的，代理可能会用某种工具帮助自己完成动作。这里可能有人在帮忙，一个共同代理。那么，我们还能有什么呢？受益人，有人通过行动得到了帮助。有时，动作是由运输工具安排的。
        </p>
        <p>It has an object moving on a trajectory that ends up at a destination. You might start out at a source. It’s
            probably been arranged by some kind of agent, and the agent may assist himself making the motion happen with
            some kind of instrument. There might be somebody helping out over here, a co agent. Well, what else can we
            have? A beneficiary, someone is helped out by the action. Sometimes, the motion is arranged by a conveyance.
        </p>
        <p>因此，这些是大量的槽位、有限槽位和动作描述，其中许多涉及轨迹上的运动。我们倾向于在语言中以某种方式修饰这些东西，这取决于语言。因此，在许多语言中，修饰是通过句子中的位置进行的。在英语中，它通常是通过介词进行的。</p>
        <p>So these are a lot of slots, finite slots, and descriptions of actions, many of which involve motion on a
            trajectory. We have a tendency in language to decorate these things in one way or another, depending on the
            language. And so in many languages, the decoration is by way of position in the sentence. In English, it’s
            often by way of a preposition.</p>
        <p>它用于帮助集中注意轨迹场景中物体的特定角色。所以如果我说，我和朋友一起烤了一个蛋糕，有一个介词 with。如果我为朋友烤了一个蛋糕，朋友就是受益者。如果我用烤箱烤了一个蛋糕，那就是一个工具。</p>
        <p>It’s used to help zero in on a particular role of an object in the trajectory scenario. So if I say, I baked
            a cake with a friend, there’s a with preposition. If I baked a cake for a friend, the friend is the
            beneficiary. If I baked a cake with an oven, that’s an instrument.</p>
        <p>客体可能从源头移动到目的地，如果我要坐火车去纽约，我会在前面加上一个“by”。如果主语不在主语位置，我会说“哦，我看所有的工作都是学生做的”。所以这些介词倾向于帮助我们放大特定客体在整个包裹、整个框架中的实际作用。
        </p>
        <p>The object may be moving to a destination from a source, and if I’m going to New York by train, I put a by on
            top of that. If the agent isn’t in subject position, I would say something like, oh, I see all the work was
            done by a student. So those prepositions have a tendency to help us zoom in on the actual role of particular
            objects in this whole package, the whole frame.</p>
        <p>这是第三个。还有一个变体，其中没有实际轨迹，在这种情况下，我们将其称为角色框架。因为如果没有轨迹，我们仍然可以拥有诸如工具、共同代理人和受益人之类的东西。所以现在，我们有了三种表示。你可能会说，好吧，它们有什么用呢？
        </p>
        <p>So this is number three. There’s a variation on this in which there’s no actual trajectory in which case
            we’ll just call that a role frame. Because if there’s no trajectory, we can still have things such as an
            instrument, a co agent, and a beneficiary. So now, we’ve got three representations. You might say, well,
            what good are they?</p>
        <p>现在你可以判断它们有什么用处，因为现在更容易查看已建立的语料库，并判断这些语料库中的句子中有多少涉及分类、过渡或轨迹。其中最著名的就是所谓的华尔街日报语料库。它包含 50,000
            个句子，取自某个时期，所有句法语言类型都可以与该语料库很好地配合使用。</p>
        <p>And you can determine what good they are these days because it’s easier to go over established corpuses, and
            say, what fraction of those, of the sentences, in such a corpus involve classification or a transition or a
            trajectory. The most well known of these is the so called Wall Street Journal Corpus. It has 50,000
            sentences in it, drawn from some period of time, all the syntactical language types work with that corpus a
            great deal.</p>
        <p>我们也对其进行了一些研究，以查看句子的比例或这些句子中的轨迹和转换的密度。所以我必须更谨慎地说，因为发现在 100 个句子中你会发现大约 25
            个转换或轨迹。所以它们非常密集地表示。它们通常非常抽象。价格上涨，经济有所发展，但仍然有一些词表示转换或轨迹。</p>
        <p>And we worked with it a little bit too, to see what fraction of the sentences or what the density of
            trajectories and transitions are in those sentences. So I have to say that a little more carefully, because
            the finding is that in 100 sentences you’ll find about 25 transitions or trajectories. So they’re very
            densely represented. They’re often very abstract. Prices rose, the economy went to someplace, but there are
            still words that denote transition or trajectory.</p>
        <p>当然，一旦你有了所有这些东西，你就会有把它们组合在一起的愿望。所以我们接下来要讨论的是故事序列。所以一个故事序列可以是一个句子，我想用我最喜欢的一个句子来说明这一点。以下是句子。我想我选择了一个中性的名字，这样就不会惹上麻烦。
        </p>
        <p>Of course, once you have all this stuff, that you have then have a desire to put it together. So the next
            thing we need to talk about is story sequences. So a story sequence can be a single sentence, and I want to
            illustrate that with one of my favorites. Here’s the sentences. I think I’ve chosen a gender neutral name so
            as not getting in any trouble.</p>
        <p>所以 Pat，但我不叫自己 Pat，因为我 18 岁时就决定 Pat 是黄油的计量单位。无论如何，当 Pat 得到安慰时，玛丽，你对发生的事情有什么印象吗？可能不是很坚定的印象。你知道 Pat
            做了一些事情，但你不知道具体是什么。尽管如此，当 Pat 安慰 Chris 时，你可以构建一些看起来像角色框架的东西。</p>
        <p>So Pat, but I don’t call myself Pat because I decided when I was 18 years old that pat is a unit of measure
            for butter. In any case, with Pat comforted, Mary, do you have an image of what happened? Probably not a
            very firm image. You know that Pat did something, but you don’t know exactly what. Nevertheless, when Pat
            comforted Chris, you can construct something that looks like a role frame.</p>
        <p>因为角色框架会有一个代理人，那就是帕特。这是一个行动。我们要在那里打个问号，因为我们对行动是什么没有一个非常明确的印象。再说一次，我们正在建立一个墙框架，就像这样。对象是克里斯。哦，你知道那是对象。我们还能说什么吗？哦，是的，我们可能还能说些什么。
        </p>
        <p>Because the role frame for that would have an agent, and that would be Pat. There’s an action. We’re going to
            put a question mark in there because we don’t have a very firm image of what the action is. Then again,
            we’re building a wall frame, like so. The object is Chris. Oh, you know that is the object. Is there
            anything else we can say? Oh, yes, we can probably say something more.</p>
        <p>当你看到帕特安慰克里斯时，你会想到其他东西。这是一种结果，结果是一个过渡框架。过渡框架涉及一个对象，即克里斯，克里斯的情绪可能有所改善。情绪上升。你有什么想法吗，埃利奥特？学生：我想，你能把帕特安慰克里斯比作帕特安慰克里斯，因为克里斯是目的地吗？
        </p>
        <p>Something else comes to mind when you see Pat comforted Chris. There’s a sort of result, and the result is a
            transition frame. And the transition frame involves an object, which is Chris, and Chris has a mood, which
            presumably, is improved. It goes up. Did you have something, Elliott? STUDENT: Could you, I guess, analogize
            the Pat comforted Chris to something like Pat gave comfort to Chris as Chris is the destination?</p>
        <p>这难道不是帕特里克·温斯顿：艾略特正在进入一个非常有趣的领域，你难道不能换个角度思考这个问题，把它看作是沿着轨迹移动的东西吗？舒适感正在移动，如果不是从帕特那里，至少是到克里斯那里。这是一种非常重要的观察，因为它表明，以多种方式、多种表现形式思考事物是有用的。
        </p>
        <p>And couldn’t this be PATRICK WINSTON: Elliott is wandering into a very interesting area having to do with,
            couldn’t you think about this in another way and think of it as something moving along a trajectory.comfort
            is moving, if not from Pat, at least to Chris. And that’s a very important kind of observation because what
            it would suggest is there can be a utility in thinking of things in multiple ways, multiple representations.
        </p>
        <p>马文·明斯基有一句很棒的格言，如果你只能以一种方式思考某件事，那么当你陷入困境时，你将无计可施。因此，多种表示意味着你有多种方式从世界中收集规律并收集它们，因此这会让你变得更聪明。所以是的，你可以这样做，这将是对我现在所做的事情的赞美。让我继续我现在正在做的事情。那么我做了什么？
        </p>
        <p>Marvin Minsky has a wonderful aphoristic phrase, which is, if you can only think about something in one way,
            you have no recourse when you get stuck. So multiple representations mean you have multiple ways of
            gathering regularity from the world, and collecting it and therefore that’ll make you smarter. So yes, you
            could do that, and that would be a compliment to what I’m doing now. Let me continue what I’m doing now. So
            what have I done?</p>
        <p>我有一个角色框架和一个传统框架，过渡框架是角色框架中结果槽的目标。现在，我们可以稍微修改一下。也许想说，不是安慰，而是恐吓。这会如何改变事情？我们不知道帕特到底做了什么，所以行动仍然未知。代理和对象是相同的。但这里的结果大概是情绪低落了。
        </p>
        <p>I’ve got a role frame and a tradition frame, and the transition frame is the target of the result slot in the
            role frame. Now, we can modify this a little bit. And maybe want to say, instead of comforted, terrorized.
            And how would that change things? We don’t know exactly what Pat did so the action remains unknown. The
            agent and the object are the same. But the result here is presumably that the mood went down.</p>
        <p>顺便说一句，仅凭我们目前所掌握的知识，我们就可以回答很多问题。一旦我们理解了这些术语，我们就可以说出是谁做的？这到底是怎么回事？答案是 Pat。Pat 做了什么？安慰，恐吓。他们也对谁做了这件事？对象是谁？是
            Chris。结果如何？Chris 感觉好多了。Chris 感觉更糟了。</p>
        <p>With just what we’ve got so far, we can answer a lot of questions, by the way. Once we’ve got the sentence
            understood in these terms, we can say who did the thing? What’s this all about? And the answer is Pat. What
            did Pat do? Comfort, terrorized. Who do they do it too? Who was the object? It’s Chris. What was the result?
            Chris felt better. Chris felt worse.</p>
        <p>因此，这些表示已经为我们提供了问答能力，使我们能够理解句子。但是，我们仍然没有非常具体，因此我们的下一步是将同一个句子带向更具体的方向。所以这是一种方法。亲吻克里斯。现在，你开始意识到发生了什么，你形成了心理图像，所以你可能会产生幻觉。
        </p>
        <p>So these representations already give us a question and answering capability that makes for an understanding
            of the sentence. But still, we haven’t been very specific, so our next step takes this same sentence in a
            more specific direction. So here’s a way that goes. Kissed Chris. Now, you begin to get a sense of what’s
            going on, you form a mental image, so you could have a hallucination.</p>
        <p>幻觉也是一种框架，但在这种情况下，它将是一种轨迹框架。对象将是，我不知道，帕特的嘴唇。目的地将是克里斯的嘴唇。我不知道，对吗？这取决于情况。这取决于情况。你们都对这里发生的事情有印象了吗？</p>
        <p>And that hallucinations will also be a kind of frame, but, in this case, it’ll be a trajectory frame. And the
            object would be, I don’t know, Pat’s lips. And the destination will be Chris’ lips. I don’t know, is that
            right? It depends. It depends. Have you all formed a picture of what’s going on here?</p>
        <p>所以，根据克里斯是帕特的女朋友还是帕特的女儿，或者克里斯是青蛙而帕特是公主，情况会有所不同，我想，故事通常就是这样的。所以，不知何故，我们的头脑里有各种各样的图书馆，当我们看到诸如亲吻之类的东西时，它们可以帮助我们在脑海中形成事物的图像。最后再举一个例子来展示多样性。
        </p>
        <p>So it will be different depending on whether Chris is Pat’s girlfriend or if Chris is Pat’s daughter or if
            Chris is a frog and Pat is a prince ess, I guess, the way the story usually goes. So somehow we have, in our
            heads, all kinds of libraries that help us to form mental pictures of things when we see things like kissed.
            So one final one just to show the variety.</p>
        <p>我们可以说 Pat 刺伤了 Chris。有什么变化？让我们看看，在亲吻的情况下，情绪会上升。在刺伤的情况下，情绪会下降。你也可以说健康状况会下降。目的地是 Chris 的身体。移动的物体是 Pat
            的刀。这两个句子都得到了相同的模式。</p>
        <p>We could say that Pat stabbed Chris. What changes? Let’s see, in the case of kissed, the mood is going up. In
            the case of stabbed, the mood is going down. You can also probably say that the health is going down. And
            the destination is Chris’ body. And the object that’s moving is Pat’s knife. We get the same pattern with
            both of those sentences.</p>
        <p>因此，它们都涉及一个从动作开始，移动到过渡和轨迹的序列，所有这些都排列成一条线。相对于语义网络，这条线让我们能够控制局面。所以我要再一次修饰一下，说我们从内部语言中获得的另一个元素是序列。</p>
        <p>So both of them involve a sequence that starts off with the action, moves to a transition and a trajectory
            and those are all arranged in a line. And that line is something that gives us a lot of power over the
            situation relative to a semantic net. So I’m going to decorate that one more time, and say that another
            element we get out of our internal language is sequence.</p>
        <p>为了让任何事物看起来像是内在语言的描述，我们需要一个元素，那就是序列。因为如果你考虑事物排列在一个庞大的网络中，处理它们会很困难。但如果​​你考虑事物沿着一条线排列，按照一系列动作或事件排列，那么这就施加了足够的约束来掌握正在发生的事情。
        </p>
        <p>An element we need in order to have anything that looks like an account of an inner language is sequence.
            Because if you think about things being arrayed in a vast spreading network, it’s hard to deal with them.
            But if you thing about things being arrayed along a line, in a sequence of actions or events, like so, then
            that imposes enough constraint to get a handle on what’s going on.</p>
        <p>所以我们要称之为表现，我想我能想到一、二、三、四。这是故事序列的表现。所以即使这是一种微故事，它仍然是故事序列的一个例子，因为我们通过把所有东西排成一行来发挥它的力量。我认为，如果你演奏乐器，你会感觉到我们对序列的依赖程度。
        </p>
        <p>So what we’re going to call this is the representation, and I guess I’m up to one, two, three, four. This is
            a representation of story sequence. so even though that’s a kind of micro story, it’s still an example of a
            story sequence because we get the power out of it by arranging everything in a line. You have a sense, I
            think, especially if you play a musical instrument, on how dependent we are on sequence.</p>
        <p>因此，如果你会演奏乐器，你可能知道从小节的中间开始重放一段乐曲有多难。你至少要回到小节的开头，如果不是乐曲的开头，可能还要回到乐句的开头。因此，我们的记忆似乎至少在音乐方面非常根植于序列的概念。在讲故事方面也常常如此。
        </p>
        <p>So if you play a musical instrument, you probably know how difficult it is to start replaying a piece of
            music from the middle of a measure. You have to go back to at least the beginning of the measure, and
            probably to the beginning of the phrase, if not the beginning of the piece. So our memory seems to be, at
            least in music, very rooted in the idea of sequences. And that’s often true of storytelling too.</p>
        <p>我们必须回到至少一个场景的开头，因为这些东西以某种方式排列成序列，并以某种方式形成了序列的实用性。因此，我们还可以讨论一件事，那就是不仅仅是这些序列是如何构建的以及它们是由什么构建的，我们还可以谈谈故事库。</p>
        <p>We have to go back to the beginning of at least a scene, because somehow these things are arranged in
            sequences that form, somehow, usefulness out of their sequentialness. So there’s one more thing we can talk
            about and that is the idea of not just the idea of how these sequences are constructed and what they’re
            constructed out of, but we can also talk a little bit in terms of libraries of stories.</p>
        <h2 id="libraries-of-stories">故事图书馆</h2>
        <h2>Libraries of Stories</h2>
        <p>当我们谈论故事库时，我们可以思考一下我们拥有的标准故事类型以及它们是如何排列的，以及我们如何通过它的超类来了解很多事情。所以这是从超类中学习东西的主题的变体。所以这是一个事件框架。然后，除了事件框架之外，还有灾难框架。然后还有派对框架。
        </p>
        <p>And when we talk about libraries of stories, we can think about kind of the sort of standard stories that we
            have and how they’re arranged, and how we can know a lot about something by what it’s super class is. So
            it’s a variation on the theme of learning stuff from the super classes. So here’s an event frame. And then,
            in addition to event frames, there’s disaster frames. And then there are party frames.</p>
        <p>派对和灾难都是事件。当我们谈论灾难时，我们知道，它们会以各种形式出现。我们有地震灾难，也许还有飓风。在派对世界中，我们有生日派对，我们有婚礼。每一种类型的框架都邀请我们填补特定的空缺。</p>
        <p>And parties and disasters are both events. And when we talk about disasters, we know, in turn, they break up
            in a variety of things. We have earthquake disasters, and maybe we have hurricanes. And in the party world
            we have, I don’t know, birthday parties, and we have weddings. And each of these types of frames invites us
            to fill in particular slots.</p>
        <p>因此，如果我们在报纸上读到一篇关于婚礼的报道，我们知道我们将了解到关于任何派对的相同信息，只是我们期望会了解到关于新娘和新郎的额外信息。如果我们有一个原始事件，而我们对它一无所知，那么就会有一个时间和地点。</p>
        <p>So if we’re reading a newspaper story about a wedding, we know that we’re going to be learning the same sorts
            of things we will learn about any party except there is the additional information that we expect that says
            something about the bride and the groom. If we have a raw event and we don’t know anything more about it
            than that, there’s a time and place.</p>
        <p>如果是灾难框架，这里就会有灰熊，但如果是灾难框架，我们可能会有死亡人数，以及损失金额。如果是地震框架，我们需要知道地震的震级和断层的名称。如果是飓风，我们有类别和名称。</p>
        <p>If it’s a disaster frame, it gets a grizzly over here, but if there’s a disaster frame, we might have the
            fatalities, and how much it cost. If it’s an earthquake frame, we need to know the magnitude of the quake,
            and the name of the fault. If it’s a hurricane, we have the category and the name.</p>
        <p>因此，上面的每一件事都可以看作不仅仅是某件事的例子，而是一个全新的框架。随着我们逐渐成熟，我们将前四件事作为构建模块，然后我们自我教育，并得到所有那些有助于我们理解世界的框架。但如何从报纸故事中填补这些内容有时可能是一个相当大的挑战。
        </p>
        <p>So each of these things up there can be viewed, not just an example of something, but as a new frame all by
            itself. As we mature, we have these first four things as building blocks, and then, we educate ourselves,
            and we get all those kinds of frames that help us to understand the world. But how to fill those in from a
            newspaper story can be sometimes, quite a challenge.</p>
        <p>事实上，最难理解的是儿童故事，因为，这是在人们试图理解儿童故事时通过实验确定的，因为事实证明，儿童故事并不比我们为成年人写的故事简单。在很多情况下，它们更难理解。如果你读过莎士比亚的情节，你会发现里面全是阴谋、谋杀、嫉妒、贪婪，但当你试图写儿童故事时，它可以是关于任何事情的。
        </p>
        <p>Actually the worst thing to understand is children’s stories because, and this was determined experimentally
            when people tried to understand children’s stories, because it turns out that children’s stories are not
            simpler than the stories we write for adults. In many cases, they’re harder. If you read about Shakespearean
            plots, it’s all about intrigue, murder, jealousy, greed, but when you’re trying to write a children’s story
            it can be about anything.</p>
        <p>更糟糕的是，儿童故事常常会引发一些你在报纸故事中看不到的问题。让我来举例说明。</p>
        <p>And worse yet, the children’s story often raises problems that you don’t see in newspaper stories. Let me
            illustrate that for you.</p>
        <h2 id="pronouns">代词</h2>
        <h2>Pronouns</h2>
        <p>你想读这个故事吗？当然，你很容易就能理解，但想想一台可怜的机器。它很难理解任何事情。问题是什么？它会很难理解那些代词先行词。看看它们。它们很复杂。帕特里克·温斯顿：闭嘴哦，就是这样。</p>
        <p>You want to read that story? You have no trouble figuring it out, of course, but think about a poor machine.
            It’s struggling to understand anything. What’s going to be the problem? It’s going to have trouble figuring
            out those pronoun antecedents. Look at them. They’re complicated. PATRICK WINSTON: Shut Oh, that’s the way.
        </p>
        <p>这是代词。其中一个人想买一只风筝。他说他有一只。他会让你把它拿回去。所以这很难。实际上，我在这里要说的原则是，当你有一个新故事或任何故事时，如果你有一个老故事但你想读它。</p>
        <p>Here are the pronouns. One of them wanted to buy a kite. He has one, he said. He will make you take it back.
            So that’s pretty hard. Actually, the principal here that I’m driving at is, when you have a new story or any
            story, if you have an old story but you want to read it.</p>
        <p>当你想快速理解它、快速实例化它时，你需要确保，如果你是讲故事的人，你不会给理解增加任何句法难度的负担。所以这是一个讲故事时增加句法难度的例子。没有报纸记者会写这样的故事。他们会这样写。</p>
        <p>When you want to understand it quickly, the instantiation of it quickly, you need to be sure that, if you’re
            the storyteller, you don’t add to the burden of the understanding any syntactic difficulty. So that’s an
            example of telling a story with additional syntactic difficulty. No newspaper journalist would ever write
            the story like that. Here’s how they would write it.</p>
        <p>他们甚至会给你一些线索，告诉你某些信息你永远都不会得到，比如谁讲了这个故事。这就是可靠消息来源的问题。所以，这引出了我今天要讨论的最后一点，我要说的是，如果你今天上课时体重 97
            磅，写作能力很弱，那么你很快就会成为体重 250 磅的大作家。</p>
        <p>They would even give you a clue that there’s certain information you’re never going to get, like who told the
            story. It’s that reliable sources business. So this brings us to the final bit that I want to deal with
            today, and what I’m going to do is, if you came into this class today as a 97 pound writing weakling, you’re
            about to emerge as a 250 pound mountain of a writer.</p>
        <p>因为我想告诉你一些技巧，这些技巧通常会让你写得更好，特别是如果你是俄罗斯人或德国人。这就是它的工作原理。规则一。因为它们给读者理解故事增加了额外的句法负担。所以如果你告诉某人一些困难的新技术想法，你最不想让他们做的事情就是让他们的句法处理器负担代词先行词的负担。
        </p>
        <p>Because I want to tell you a few tricks that will make you usually better as a writer, especially if you’re
            Russian or German. And here’s how it works. Rule one. Because they place additional syntactic burden on the
            understanding of the story by the reader. So if you’re telling somebody about some difficult new technical
            idea, the last thing you want them to do is to burden their syntactic processor with figuring out pronoun
            antecedents.</p>
        <p>所以不要在写作中使用代词，至少技术写作会更清楚。顺便问一下，为什么这尤其适用于德国人和俄罗斯人？是因为。这是种族起源的诽谤还是他们语言的事实？这是他们语言的事实。事实在哪里？为什么他们可以逃避代词用法，而我们在英语中却不能逃避？是的，安德鲁？学生：性别，还有。
        </p>
        <p>So don’t use pronouns in you’re writing, technical writing, at least, will be much clearer. By the way, why
            does this especially apply to Germans and Russians? Is it because. is this an ethnic origin slur or is this
            a fact about their language? It’s a fact about their language. Where is the fact? Why can they get away with
            pronoun usages that we cannot get away with in English? Yes, Andrew? STUDENT: Gender and also.</p>
        <p>帕特里克·温斯顿：性别，因为如果你用性别修饰所有名词，那么代词先行词出现歧义的可能性就会降低三倍。因此，你会经常发现，德国和俄罗斯作家会使用各种代词，但这些代词对他们来说非常清楚，因为它们有性别。在翻译时，英语使用者可以理解这些代词，因为我们没有性别来帮助我们归纳。
        </p>
        <p>PATRICK WINSTON: Gender, because if you have all of your nouns decorated with gender, that reduces by three,
            the potential for ambiguity in the pronoun antecedent. So you’ll frequently find that German and Russian
            writers will have pronouns all over the place that are perfectly clear to them because of gender. They are
            interpretable by English speakers when translated because we don’t have the gender to help us zero in.</p>
        <p>因此，所有这些都与尽量减少读者的额外、多余、不必要的负担有关。第二点是不要使用 former 或
            later。你会看到这些词在技术写作中经常使用，你猜怎么着？没有人会在遇到这些词时不停下来回过头去弄清楚它们指的是什么。所以这是另一个不给读者带来不必要的句法负担的例子。</p>
        <p>So these things all have to do with minimizing extra, superfluous, gratuitous, unnecessary burden on the
            reader. Number two is don’t use former or latter. You see those words used frequently in technical writing,
            and guess what? No human being ever encounters those words without having to stop and go back to figure out
            what they refer to it. So that’s another example of not placing any unnecessary syntactic burden on the
            reader.</p>
        <p>最后，不要把铁锹说成铁锹。人们养成了一个习惯，那就是不要重复单词，这可能是由好心但被误导的高中老师灌输的，所以人们会不遗余力地使用不同的单词。问题是，读者不知道单词的变化是故意的，与某种微妙的意义变化有关，还是只是遵循高中老师不要再次使用同一个单词的告诫。
        </p>
        <p>And finally, don’t call a shovel a spade. There’s a habit, probably instilled by well meaning but misadvised
            high school teachers, that you shouldn’t repeat words, and so people go to great lengths to use some
            different word. The problem is that the reader doesn’t know if the shift in the word is deliberate and
            attached to some subtle meaning shift, or if it’s just adhering to some high school teacher’s admonition
            against using the same word again.</p>
        <p>所以你不想说，哦，挖这个特定洞的正确方法是用铁锹，然后换成铁锹，因为读者无法判断这是故意的、偶然的，还是只是不想使用同一个词的结果。</p>
        <p>So you don’t want to say, oh, the right way to dig this particular hole is with a spade and then switch to a
            shovel because the reader can’t tell if it’s deliberate, accidental, or a consequence of just the desire not
            to use the same word.</p>
        <p>因此，通过一些基于人工智能的非常简单的机制，你可以避免给阅读你作品的人带来不必要的句法负担，从而让自己成为一名更好的作家。</p>
        <p>So this is how you, with some very simple mechanisms grounded in AI, you can actually make yourself into a
            better writer by avoiding those kinds of things that put an unnecessary syntactic burden on the people who
            are reading your stuff.</p>
        <h1 id="architectures-gps-soar-subsumption-society-of-mind">19. 建筑：GPS、SOAR、Subsumption、心智社会</h1>
        <h1>19. Architectures: GPS, SOAR, Subsumption, Society of Mind</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:06 <a
                href="https://youtube.com/watch?v=PimSbFGrwXM">https://youtube.com/watch?v=PimSbFGrwXM</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:06 <a
                href="https://youtube.com/watch?v=PimSbFGrwXM">https://youtube.com/watch?v=PimSbFGrwXM</a></p>
        <h2 id="introduction-5">介绍</h2>
        <h2>Introduction</h2>
        <p>教授：“查拉图斯特拉如是说”在 2001
            年就已声名远扬，广受欢迎。当某个灵长类动物（可能是我们的祖先）突然有了想法时，就会播放这首神奇的音乐。那么我们如何解释这一切呢？我们已经准备好了所有材料。今天我想谈谈将这些材料组合在一起的各种方法。我们讨论了表示法，讨论了方法，今天我们将讨论架构。
        </p>
        <p>PROFESSOR: “Thus Spake Zarathustra” was made famous and popular by 2001. And that is music played at this
            magic moment when some primate suddenly gets an idea, presumably one of our ancestors. So how do we explain
            all that? We’ve got all of the ingredients on the table. And today I want to talk about various ways of
            putting those ingredients together. So we talked about representations, we’ve talked about methods, and
            today we’re going to talk about architectures.</p>
        <p>课程结束时，你将知道如何将这些东西组合在一起。实际上，没有人知道如何将这些东西组合在一起。但你将了解将这些东西组合在一起的一些替代方法，以便制造出可以与我们一样智能的东西。这就是我们今天的议程。我们还将进一步讨论故事。
        </p>
        <p>And by the end of the class you’ll know how to put one of those things together. Actually, no one knows how
            to put one of those things together. But what you will know is about some alternatives for putting those
            things together so as to make something that is arguably intelligent in the same way we are. So that is our
            agenda for today. We’ll also talk a little bit more about stories.</p>
        <p>我认为那是在 2007
            年，当时爱沙尼亚人将一座战争纪念碑从塔林市中心移至俄罗斯战争公墓。在此之前，爱沙尼亚人一直在建立国家计算机网络，因为他们认为计算是未来的潮流。网络等等。战争纪念碑移走后不久，有人破坏了爱沙尼亚国家网络。这是一次网络攻击。
        </p>
        <p>I think it was in 2007 when the Estonians moved a war memorial from the center of Tallinn off to a Russian
            war cemetery. Prior to that time the Estonians had been building up their national computer networks because
            they thought that computation was the wave of the future. networks and all of that. Shortly after the
            movement of that war memorial, someone brought the Estonian national network down. a cyber attack.</p>
        <p>人们普遍认为是俄罗斯人所为。爱沙尼亚有大量俄罗斯族人口。而战争纪念碑的搬迁激怒了俄罗斯人。所以每个人都认为是他们干的。但你知道吗？没有计算机能理解我刚刚讲的故事。它们可以浏览整个互联网，找到与之相关的信息，但没有计算机能理解我刚刚讲的故事，除了一台。
        </p>
        <p>It was widely believed to be the Russians. There’s a large Russian ethnic population in Estonia to start
            with. And the movement of that war memorial irritated the Russians. And so everybody thinks that they did
            it. But you know what? No computer can understand the story I just told. They can revel through all of the
            worldwide web finding information that’s relevant to that, but no computer can understand the story I just
            told, except one.</p>
        <p>今天晚些时候你们会看到一个演示。顺便说一句，如果你有兴趣了解智力的本质，这当然是本学期最重要的一堂课。我应该告诉你周三我们要做什么。因为出于某种原因，感恩节前一天的课程往往人很少，但这堂课除外。</p>
        <p>You’ll see a demonstration of that later on today. So by the way, if you’re interested in understanding the
            nature of intelligence, this is, of course, the most important lecture of the semester. And I should tell
            you a little bit about what we’re going to do on Wednesday. Because for some reason the day before
            Thanksgiving tends to be a lecture that’s lightly populated, except in this class.</p>
        <p>因为我要讲人工智能业务，以及从中可以学到什么，如何避免创业时破产。所以对你们中的许多人来说，这将是本学期最重要的一堂课。</p>
        <p>Because I’m going to talk about the artificial intelligence business and what can be learned from it about
            how to avoid going broke when you start your company. So for many of you that will be the most important
            lecture of the semester.</p>
        <h2 id="general-problem-solver">通用问题解决者</h2>
        <h2>General Problem Solver</h2>
        <p>这一切都始于人工智能的黎明时代。说来遗憾，其实这一切都始于卡内基梅隆大学。</p>
        <p>It all started back in the dawn age of artificial intelligence. And really, it all started at Carnegie
            Mellon, sad to say.</p>
        <p>因为卡内基梅隆大学的研究人员，尤其是纽厄尔和西蒙，是第一批想到将事物组合在一起以构建特定智能系统的结构或架构的通用方法的人。所以他们的想法被称为通用问题解决器。一个简单想法的长名字。简单的想法是，你从当前状态开始你的生活，称之为
            C。</p>
        <p>Because the people at Carnegie Mellon, notably Newell and Simon, were the first to think about sort of a
            general purpose way of putting things together so as to build a structure or architecture in which
            particular intelligent systems could be built. So their idea was called the general problem solver. A long
            name for a simple idea. And the simple idea is that you start your life out in a current state, call it C.
        </p>
        <p>你想要达到某个目标状态。称之为 S。而实现这一目标的一种方法就是以某种方式衡量你现在所处的位置和你想要达到的位置之间的象征性差异。这就是差异。我们将这种差异称为
            D。人们说，在这种解决问题的一般方法中，当你观察到这种差异时，这就足够了。</p>
        <p>And you want to get to some goal state. Call it S. And a way you do that is you measure somehow the symbolic
            difference between where you are and where you want to be. So that’s the difference. We’ll call that
            difference D. And when you observe that difference that’s enough, they say, in this general approach to
            problem solving.</p>
        <p>您可以选择一些操作，将您从当前状态转移到某个新状态，即中间状态。称之为 I。因此，I 或该操作符 O 由差值 D
            决定。然后，当然，接下来要做的是测量中间状态与您想要处于的状态之间的差异，并选择一些与减少该状态相关的操作符。</p>
        <p>For you to select some operation that will move you from your current state to some new state, an
            intermediate state. Call it I. So I, or that operator, O, is determined by the difference, D. And then, of
            course, the next thing to do is to measure the difference between that intermediate state and the state you
            want to be in, and choose some operator that’s relevant to reducing that state.</p>
        <p>因此我们将其称为 D2，将其称为 O2。D2 会引导您到达 02，如此循环往复。这就是想法。这通常被称为手段目的分析。为什么？因为您想要实现的结果是处于最终状态 S。手段就是操作符
            O。因此，您会对想要到达的位置以及当前位置和想要到达的位置之间的差异有所了解。</p>
        <p>So we’ll call that D2, and we’ll call this O2. And D2 is what leads you to 02, and so it goes. So that’s the
            idea. And that’s often called means ends analysis. Why? Because the end that you want to achieve is being in
            that final state, S. And the means is that operator, O. So you have some notion of where you want to be and
            the difference of where you are and where you want to be.</p>
        <p>然后你选择一个运算符来减少这种差异。所以这一切都非常抽象。让我们练习一下解决你们一两天后都会遇到的问题。对于你们中的许多人来说，我希望是大多数人，那就是回家的问题。所以你在这里。你在麻省理工学院。你想去的地方就在这里，在家里。
        </p>
        <p>And you pick an operator so as to reduce that difference. So this is all very abstract. Let’s exercise it in
            solving a problem that you will all be faced with here in a day or two. That is, for many of you. most Of
            you, I hope. the problem of going home. So here you are. You’re at MIT. And where you want to be is over
            here, at home.</p>
        <p>所以你要测量麻省理工学院和家之间的距离。对于你们中的许多人来说，这个距离比开车能到的要远，但也不是远到根本就到不了。所以你要做的是，你说，好吧，正确的运营商是乘坐飞机。所以有运营商，乘坐飞机。这就是差异，D。差异，D，足够大，你就会乘坐飞机。
        </p>
        <p>So you measure the difference between MIT and home. And for many of you it’s further than you can go by car
            and not so far that you can’t go at all. So what you do is, you say, well, the right operator is taking an
            airplane. So there is the operator, take an airplane. And this is the difference, D. And the difference, D,
            being sufficiently large, you take the plane.</p>
        <p>问题是，如果你正好坐在这里，你就没法乘坐飞机了，因为这里容不下这么多人。所以你还有另一个问题，那就是如何到达飞机。所以这里和洛根机场之间的距离是如此之远，正确的做法是乘坐马萨诸塞州湾交通管理局 (MBTA)。</p>
        <p>Trouble is, if you happen to be sitting here in there’s no way you can take an airplane, because they don’t
            fit in here. So you’ve got another problem, and that is to get to the airplane. So the distance between here
            and Logan is such that the right way to do that is to take the MBTA.</p>
        <p>这是确定的，因为你正在努力减少这里的差异，麻省理工学院和机场之间的差异。所以这个差异决定了你乘坐 MBTA。所以你看，你在咒骂。但你知道这里也没有 MBTA 车。所以还是有差异的。这个差异决定了你走路。</p>
        <p>And that’s determined because you’re working on this difference reduction right here, the difference from
            being at MIT and being at the airport. So that difference dictates that you take the MBTA. So you see,
            you’re cursing. But you know there are no MBTA cars in here either. So there’s still a difference like so.
            And that difference dictates that you walk.</p>
        <p>所以你有 D1、D2 和
            D3。当你排除与这三个差异相关的运营商时，你就到了洛根机场。然后你乘飞机，到达你的家乡，你面临的差异更小，那就是从那个机场到达你真正想去的地方。所以这就是一般的问题解决者的想法。这在当时是一个非常令人兴奋的想法。
        </p>
        <p>So you’ve got D1, D2, and D3. And by the time you’ve excised the operators relevant to those three
            differences, you’re at Logan. Then you take the airplane, you get over to your hometown, and your faced with
            the smaller difference of getting from that airport to where you actually want to go. So that’s the general
            problem solver idea. It was such an exciting idea at the time.</p>
        <p>当时这个想法非常令人兴奋，因为人们会对自己说，啊！这是一个通用的问题解决器，所以我们可以让它解决让自己变得更聪明的问题。因此，人们想象会发生一种连锁反应。这种架构的开发人员警告公众，在 10 年内。也就是说，到
            1970 年左右，计算机将与人类一样聪明。</p>
        <p>It was such an exciting idea at the time because people would say to themselves, ah! This is a general
            purpose problem solver, so we can set it onto the problem of making itself smarter. And so there was a kind
            of imagined chain reaction that would take place. And the developers of this architecture warned the public
            that within 10 years. that is to say, by about 1970. computers would be generally as smart as people.</p>
        <p>许多人嘲笑他们的预测。但实际上，科学家试图承担责任。因为他们认为，一场相当严重的混乱即将来临，人们应该知道它即将来临。因此，他们觉得，在那个科学责任时代，警告公众是他们的责任。</p>
        <p>And a lot of people made fun of them for that prediction. But it was actually scientists attempting to be
            responsible. Because they thought something, a quite serious dislocation was coming along, and that people
            should know that it was coming. And so they felt it was their responsibility in that age of scientific
            responsibility to warn the public.</p>
        <p>但事实并非如此，因为收集差异和查找运算符的问题超出了架构的范围。因此，在使用该架构之前，必须由人类解决这个问题。您必须确定可能遇到的差异和可能使用的运算符，并构建将两者关联在一起的表格。</p>
        <p>It didn’t turn out that way, because the problem of collecting the differences and finding the operators,
            that’s outside the scope of the architecture. So this is the problem that has to be solved by a human before
            this architecture can be used. You have to have identified the differences that you might encounter and the
            operators that you might use, and build this table which relates the two together.</p>
        <p>所以可能是那个，那个，一些非对角元素等等。但构建该表是一项艰巨的工作。</p>
        <p>So maybe that one, that one, some off diagonal elements, and so on. But building that table turned out to be
            a hard job.</p>
        <h2 id="soar">翱翔</h2>
        <h2>SOAR</h2>
        <p>因此，这个想法自然而然地发展起来了。最终，卡内基的研究人员开发了通用问题求解器。最著名的是纽厄尔和他的学生。他们开发了一种更新、更新颖、更复杂的架构，称为 SOAR。SOAR 的工作原理如下。首先，SOAR
            是什么意思？它没有任何意义。</p>
        <p>So not surprisingly, the idea evolved. And eventually the folks at Carnegie who developed the general problem
            solver. most notably Newell and his students. developed a newer, fresher, more elaborate architecture called
            SOAR. And here’s how SOAR works. First of all, what does SOAR mean? It doesn’t mean anything.</p>
        <p>它曾经的意思是状态操作符和结果。但出于某种原因，SOAR 架构的支持者决定他们不喜欢这个缩写词，并声称 SOAR 只是一个标签，不应该被视为缩写词。无论如何，SOAR
            由各个部分组成。它有长期记忆。它有短期记忆。它与外界有联系，可能是视觉系统和行动系统。</p>
        <p>It used to mean State Operator And Result. But for some reason the proponents of the SOAR architecture
            decided they don’t like that acronym, and have asserted that SOAR is merely a label that shouldn’t be
            thought of as an acronym. In any event, SOAR consists of various parts. It has a long term memory. It has a
            short term memory. And it has connections to the outside world, maybe a vision system and an action system.
        </p>
        <p>但 SOAR
            问题解决架构的大部分活动都发生在短期记忆中。因此，你可以将长期记忆的内容视为短期记忆的来来回回。因此，你可以立即看到，这种机制、这种架构受到某些认知心理学实验的严重影响，这些实验与你的短期记忆中可以容纳多少内容有关。
        </p>
        <p>But most of the activity of the SOAR problem solving architecture takes place in a short term memory. So you
            can view the contents of the long term memory as shuttling in and out of short term memory. So you can see
            right away that this mechanism, this architecture, is heavily influenced by certain cognitive psychology
            experiments having to do with how much you can hold in your short term memory.</p>
        <p>无意义的音节和所有那些当时流行的事物。所以这是由心理学家主要设计的一种架构。它的特点包括短期记忆和长期记忆。所以这是该架构的第一部分。那么长期记忆中有什么？嗯，断言和规则，又称制作。制作是卡内基规则的俗语。它只是基于规则的东西，就像你在开学第一天看到的那样。
        </p>
        <p>Nonsense syllables and all that sort of thing that was popular back in those days. So this was an
            architecture devised primarily by psychologists. And it had amongst its features a short term memory and a
            long term memory. So that’s part 1 of this architecture. So what’s in the long term memory? Well, assertions
            and rules, AKA productions. A production being the Carnegie vernacular for rule. It’s just the rule based
            stuff like you saw on almost the first day of class.</p>
        <p>因此，整个系统是一个巨大的基于规则的系统，其中的断言和规则在长期记忆和短期记忆之间来回穿梭，在短期记忆中进行处理。当你想到 SOAR
            架构时，你会想到的第三件事是它们有一个复杂的偏好系统。你还记得吗，当我们谈论基于规则的系统时，总会有一个问题，当不止一条规则起作用时，你会怎么做？</p>
        <p>So the whole thing is a gigantic rule based system with assertions and rules the shuttle back and forth from
            long term memory into short term memory where processing takes place. The third thing that comes to mind
            when you think of SOAR architecture is they had an elaborate preference system. You recall that when we
            talked about rule based systems there’s always a question of what do you do when more than one rule would
            work?</p>
        <p>你必须有某种方法来打破这些联系。SOAR 架构有一个精心设计的子系统来实现这一点。但我说这些是你首先想到的三件事，也许这并不正确。因为你接下来想到的可能是与 SOAR 架构相符的更好的东西。这就是问题空间的概念。
        </p>
        <p>You have to have some way of breaking those ties. The SOAR architecture has an elaborate subsystem for doing
            that. But I said that these are the first three things you think, and maybe that’s not right. Because the
            next thing you think about is perhaps a better thing to identify with the SOAR architecture. And that’s the
            idea of problem spaces.</p>
        <p>这个想法就是，如果你要解决问题，你必须开发一个空间并在该空间中进行搜索。就像我们谈论如何从这里回到家时所做的那样。有一个地方的空间，那就是我们的问题空间。我们可以搜索该空间以找到从一个地方到另一个地方的方法。这就是
            SOAR 关注的事情。</p>
        <p>And that’s the idea that if you’re going to solve a problem you have to develop a space and do a search
            through that space. Just like we did when we talked about how we can get from here to home. There’s a space
            of places, that’s our problem space. We can do a search through that space to find a way to get from one
            place to another. That’s the sort of thing that SOAR is focused on.</p>
        <p>最后，当您考虑 SOAR 时，您往往会想到的第五个要素是通用子目标的概念。这个想法是，每当您想不出下一步要做什么时，这就会成为您的下一个问题，它值得拥有自己的问题空间和自己的一套差异和运算符以及规则和断言。</p>
        <p>Finally, the fifth element that you tend to think about when you think about SOAR is the idea of universal
            subgoaling. And that’s the idea that whenever you can’t think of want to do next, that becomes your next
            problem that deserves it’s own problem space and its own set of differences and operators, and rules and
            assertions.</p>
        <p>因此，您从高层次开始，然后必须在较低层次上解决问题，就像您使用通用问题解决程序一样。因此，如果您拥有这两种架构，您就可以开始说，那么，它们以什么为中心？而这种架构，这种通用问题解决程序，以一切都与解决问题有关这一理念为中心。因为问题解决假设。没有人给它起这个名字。
        </p>
        <p>So you start off on a high level, then you have to solve problems at a lower level, just like you did up
            there with a general problem solver. So if you have these two architectures you can begin to say, well, what
            are they centered on? And this architecture, this general problem solver, is centered on the idea that
            everything is about problem solving. Because the problem solving hypothesis. no one gave it that name.</p>
        <p>但事实就是如此。这种建筑确实得名。纽厄尔一直说，它基于他所谓的符号系统假说。这个假说认为，我们人类就是符号操纵者。</p>
        <p>But that’s what it was. And this architecture did get its name. And it was said always, by Newell, to be
            based on what he called the symbol system hypothesis. The hypothesis that what we are as humans is symbol
            manipulators.</p>
        <p>我们可以通过向人们提供加密算术问题并让他们大声说出来，通过思考当你试图记住无意义的音节时会发生什么，通过这种架构首次提出时在心理学实验中流行的所有这些东西来揭示这一切是如何运作的。但当你观察架构时，你可以看到它们来自哪里以及它们的前身是什么。
        </p>
        <p>And we can uncover how that all works by giving people crypto arithmetic problems and having them talk out
            loud, by thinking about what happens when you try to remember nonsense syllables, by all that sort of stuff
            that was en vogue in terms of psychology experiments in the day when this architecture was first
            articulated. But when you look at architectures you can sort of see where they come from and what their
            antecedents are.</p>
        <p>它有短期记忆和长期记忆，因为 Newell 和他的同事是认知科学家。它有断言、规则和偏好，因为 Newell
            和他的同事也是人工智能专家。它还有问题空间和通用子目标，因为这些想法在通用问题解决器架构中已经以更原始的形式实现。这就是 SOAR 早期的样子。</p>
        <p>It has a short term memory and a long term memory, because Newell and his associates were cognitive
            scientists. It has assertions and rules and preferences, because Newell and his associates were also AI
            people. And it has problem spaces and universal subgoaling because those are ideas that had been work out in
            a more primitive form already in the general problem solver architecture. So that’s a glimpse of what SOAR
            looked like in its early days.</p>
        <p>许多聪明人已经将其高度发展。因此，尽管它以符号为中心，但他们将与情感和感知有关的东西附加在它上面，但一般认为，面对这种感知时要做的第一件事就是将其从那里取出并转化为符号形式。这是建筑所附带的偏见。</p>
        <p>It’s been very highly developed by a lot of smart people. So although it’s symbol centered, they’ve attached
            to it things having to do with emotion and perception, but generally with the view that the first thing to
            do when faced with this perception is to get it out of there and get it into a symbolic form. That’s sort of
            the bias that the architecture comes with.</p>
        <p>因此，这两种架构都严重偏向于认为我们所做的重要部分是解决问题。但也许最重要的是，至少从麻省理工学院的角度来看，这些以解决问题为导向的思考世界的方式是马文·明斯基的架构，他在他的书《情感机器》中对此进行了阐述。马文不仅关心解决问题，还关心如何分层解决问题。
        </p>
        <p>So those are two architectures that are heavily biased toward thinking that the important part of what we do
            is problem solving. But the most important, perhaps. at least from an MIT perspective. of these problem
            solving oriented ways of thinking about the world, is Marvin Minsky’s architecture, which he articulates in
            his book “The Emotion Machine.” And Marvin is not just concerned with problem solving, but also with how
            problem solving might come in layers.</p>
        <h2 id="marvin-minsky-1">马文·明斯基</h2>
        <h2>Marvin Minsky</h2>
        <p>那么让我给你举个例子，说明一下是什么问题激发了马文的某些思考。你可以读一下，这是一个简短的小故事。你不难理解它，对吧？不。这对我们人类来说并不难。对计算机来说却非常困难。部分原因是，你需要思考，理解这个故事的能力，需要你同时在多个层面上思考。
        </p>
        <p>So let me show you an example of the sort of problem what motivates some of Marvin’s thinking. So you can
            read that, it’s a short little vignette. You have no trouble understanding it, right? No.&nbsp;It’s not
            difficult for us humans. Awfully tough for a computer. In part, because the thinking you need, your ability
            to understand that story, requires you to think on many levels at the same time.</p>
        <p>首先，从根本上说，这是一种本能反应。你看到本能反应在哪里了吗？那就是她听到声音并摇头的那个部分。那是本能，对吧？这几乎是天生的。但她看到的是一辆车。所以这是我们没有天生的。在过去的 100
            年里，我们不太可能进化出对在路上疾驰的汽车的本能欣赏。</p>
        <p>First of all, there’s a sort of, at the bottom, instinctive reaction. You see where there’s instinctive
            reaction? That’s the part where she hears a sound and terns her head. That’s instinct, right? That’s
            practically built in. But then what she sees is a car. So that’s something that we don’t have wired in. It
            would be unlikely that we’ve evolved in the last 100 years to have an instinctive appreciation of cars
            barrelling down the road.</p>
        <p>因此，马文架构中的下一个层次是习得反应。这就是关于汽车的思考部分。现在遍布其中。好吧，让我们看看哪里是一个特别好的例子。她决定冲过马路。这就是她解决问题的地方。这就是深思熟虑的思考层次。它并不止于此，因为后来她反思了她的冲动决定。
        </p>
        <p>So the next level in Marvin’s architecture is learned reaction. So that’s the part about thinking about the
            car. Now spread throughout there. well, let’s see where is a particularly good example. She decides to
            sprint across the road. So that’s where she’s solving a problem. So that’s the deliberative thinking level.
            It doesn’t stop there, because later on she reflects on her impulsive decision.</p>
        <p>所以她不仅思考外面世界发生的事情，还思考这里发生的事情。所以，我们可以把这个层次称为反思性思考。你知道，反思性思考不止于此，因为在故事的另一部分，她还考虑了迟到带来的不安。</p>
        <p>So she thinks not only about stuff that’s happening out there in the world, but she also thinks about stuff
            that’s going on in here. So that’s a level which we can call reflective thinking. Well, you know, it doesn’t
            stop there, because she also considers, in another part of the story, something about being uneasy about
            arriving late.</p>
        <p>因此，她不仅在思考她脑海中正在发生的事情，而且还在思考与她制定的计划相关的正在发生的事情。有些人将之称为自我反思层。但这也不是全部，因为在故事的结尾，她开始担心她的朋友会怎么看她。因此，在更社会化的背景下，存在一种反思性思维。
        </p>
        <p>So she’s not only just thinking about events that are going on in her mind right now, but events that are
            going on right now relative to plans she’s made. Some Marvin calls that the self reflecting layer. But that
            isn’t the whole thing either, because toward the end of the story she starts to worry about what her friends
            would think of her. So there’s a kind of reflective thinking in a more social context.</p>
        <p>所以他称其为自我意识思考。正如卡内基的人们所认为的那样，SOAR
            架构主要关注解决问题，而明斯基的《情感机器》一书不仅考虑思考，还考虑了多层次的思考。阻碍这一切的因素可以说是常识的发展，而遗憾的是，计算机从来没有多少常识。所以可以说这是基于常识假设的。</p>
        <p>So he calls that self conscious thinking. So as the Carnegie folks think, the SOAR architecture focuses
            mostly on problem solving, Minsky’s “Emotion Machine” book considers not just thinking, but thinking on many
            layers. And the blocker to doing any of that can be said to be the development of common sense, which
            computers, alas, have never had much of. So this could be said to be based on the common sense hypothesis.
        </p>
        <p>常识假说认为，为了做所有这些事情，你必须拥有像人一样的常识。如果你必须拥有像人一样的常识，你就必须思考有多少常识，我们如何才能获得它？因此，这在受马文影响的人们中引发了媒体实验室的大量活动，这些活动与收集常识有关。
        </p>
        <p>And the common sense hypothesis holds that in order to do all of that stuff, you have to have common sense
            like people. And if you have to have common sense like people, you have to think about how much of that is
            there and how can we go get it? And so this spawned a lot of activity in the media lab amongst people
            influenced by Marvin, having to do with gathering common sense.</p>
        <p>开放思想项目是亨利·利伯曼等人的工作，它与从万维网上收集常识有关，以此作为填充系统的一种方式，为进行这种分层思考奠定基础。所以这是对一些机制的简要概述，有些机制比其他机制更古老，但除了 GPS 之外。GPS
            也是如此。让我们面对现实吧，很难想象在不涉及手段目的分析的情况下解决任何问题。</p>
        <p>The open mind project, the work of Henry Lieberman and others, having to do with the gathering of common
            sense from the world wide web as a way of populating systems that would lay the foundation for doing this
            kind of layered thinking. So that is a brief survey of some mechanisms, some older than others, but all but
            GPS. GPS too. Let’s face it, it’s hard to think of solving any problem without means ends analysis being
            involved.</p>
        <p>因此，GPS
            并没有错，只是它不是你思考该做什么时所需的唯一工具。因此，这些想法既有早期的，也有晚期的，而且仍然流行。但它并不是唯一的想法，因为有人反对这种关于智力发展的解决问题的思维方式。在这些反对意见和另类想法中，最突出的是罗德·布鲁克斯和他的包容架构。早期的想法就是这样。
        </p>
        <p>So GPS isn’t wrong, it’s just not the only tool you need to think about what to do. So these are early, and
            late, and still current. But it’s not the only thing there is, because there have been reactions against
            this problem solving way of thinking about the development of intelligence. And the most prominent of those
            counter currents, of those alternative ideas, belongs to Rod Brooks and his subsumption architecture. So
            along about the early.</p>
        <p>大约在 1990 年左右，布鲁克斯开始感到不安。因为机器人不能做太多事情。他们会在晚上打开它们，然后第二天早上它们会进入实验室，它们会移动 25
            英尺，也许可以很好地避开桌子。它们不能做太多事情，而且要花很长时间。所以他认为这是因为人们的思维方式错误。</p>
        <p>Along about the years surrounding 1990, Brooks became upset. subsumption. because robots couldn’t do much.
            They would turn them on at night, and then the next morning they’d come in the laboratory and they would
            have moved 25 feet, nicely avoiding a table perhaps. Not doing very much and taking a long time to do it. So
            he had decided that it’s because people were thinking in the wrong way.</p>
        <p>当时人们认为，制造机器人的方法是先构建视觉系统，然后构建推理系统，最后构建动作系统。机器人几乎什么都做不了，但还是能做点什么。因此，你要改进视觉系统，改进推理系统，改进实际系统。但现在你把它搞坏了，因为你以前能做的所有事情都不再管用了。
        </p>
        <p>In those days people thought that the way you build a robot is you build a vision system, and then you build
            a reasoning system, and then you build an action system. And it can do almost nothing, but it does
            something. So you improve the vision system, and improve the reasoning system, and improve the actual
            system. And now you’ve broken it, because all the stuff you used to be able to do doesn’t work anymore.</p>
        <p>那么替代方案是什么呢？正如布鲁克斯所阐述的那样，替代方案就是将这个想法反过来。因此，你拥有的不是封装的视觉系统、封装的推理系统和封装的行动系统，而是专注于感知、推理和行动的层，而是专门处理世界的层。</p>
        <p>So what’s the alternative? Well, the alternative, as articulated by Brooks, is to turn this idea on its side.
            So instead of having an encapsulated vision system, an encapsulated reasoning system, and an encapsulated
            action system, what you have is layers that are focused not on the sensing and the reasoning and the action,
            but layers that are specialized to dealing with the world.</p>
        <p>因此，按照布鲁克的思维方式，在最低层级，你可能拥有一个能够做到以下事情的系统。好吧，在我们讨论这个问题之前，先说躲避物体。也许下一个层级是漫游层。也许再下一个层级是探索。也许再下一个层级是寻找。</p>
        <p>So in Brook’s way of thinking about things, at the lowest level you might have a system that’s capable of.
            well, before we get to that, avoiding objects. And maybe the next level up is the wandering layer. And maybe
            the next level up after that is explore. And maybe the next level up after that is seek.</p>
        <p>以前人们考 6001 的时候，我很容易就回答了这个问题：这让你想起了 6001 中的什么？它不会让你想起 6001
            中的任何东西，因为你没有考过这门课。但它将编程思想视为一种概括，什么是编程思想？编程中只有少数几个强大的思想，这是其中之一的概括。它是什么？</p>
        <p>Now in the old days when people took 6001 I had no trouble getting an answer the question, what does this
            remind you of in 6001? It doesn’t remind you of anything in 6001 since you haven’t taken it. But it viewed,
            as a generalization of a programming idea, what is the programming idea? There are only a few powerful ideas
            in program, and this is a generalization of one of them. What is it?</p>
        <p>你有名字吗？是的，安德鲁？ 学生：抽象层？
            教授：抽象层和抽象屏障。这很好。因为这些家伙中的每一个都可以有自己的视野、行动和推理系统。如果你把它们看作抽象边界，那么当你让这个东西运转起来时，你就不再需要去摆弄它了。你在最上面构建这一层。</p>
        <p>Do you have a name? Yes, Andrew? STUDENT: Layers of abstraction? PROFESSOR: Layers of abstraction, and
            abstraction barriers. That nails it pretty well. Because each of these guys can have its own vision, action,
            and reasoning system. And if you think of these as abstraction boundaries, then when you got this thing
            working you don’t screw with it anymore. You build this layer on top.</p>
        <p>它可能时不时地深入大脑，但不会从根本上改变大脑。布鲁克斯的灵感部分来自于我们大脑的构造方式。我们与猪共有的所有旧东西都深藏在大脑深处，而我们则将新皮层覆盖在大脑皮层上。因此，新皮层看起来层次分明，令人自豪。</p>
        <p>And it may reach down in here from time to time, but it doesn’t fundamentally change it. Brooks was inspired
            in part by the way our brains are constructed. All that old stuff that we share with pigs is down in there
            deep, and we put the neocortex over it. So it looks layered in a way that would make proud.</p>
        <p>这就是布鲁克斯看待世界的方式，它与 SOAR
            一样具有一些特点。其中一个特点是没有代表性。所以这个细节可能恰好出现在布鲁克斯操作的水平上，而当你超越布鲁克斯操作的水平时，这个细节就非常值得怀疑了。但在我继续之前，让我先说一下这个假设是什么。这个假设是生物假设。
        </p>
        <p>So this then is the way that Brooks looks at the world, and it’s characterized by a few features just like
            SOAR is. One of those features is no representation. So this is a detail that’s probably right at the level
            that Brooks was operating, and very questionable when you get above the level that Brooks was operating. But
            before I go on, let me say what the hypothesis is. The hypothesis is the creature hypothesis.</p>
        <p>假设一旦你能让机器像昆虫一样聪明，那么剩下的事情就简单了。那么，你如何让一个生物像昆虫一样聪明呢？也许你不需要表征。我们在本课程中专注于表征，所以你可以看到有点紧张。接下来的事情是，如果你没有表征，你该怎么办？让我们看看。你的表征使模型成为可能。
        </p>
        <p>It’s the hypothesis that once you can get a machine to act as smart as an insect, then the rest will be easy.
            Well, how do you get a creature to be smart as an insect? Maybe you don’t need representation. We focused on
            representation in this course, so you can see there’s a little stress. Next thing is, what do you do if you
            don’t have a representation? Let’s see. Your representation makes a model possible.</p>
        <p>模型使预测、理解、解释和控制成为可能。那么如果你没有模型，你能做什么呢？布鲁克斯的答案是，你利用的是世界而不是模型。所以你所做的一切都是被动的。你脑子里没有任何关于这个房间的地图。但也许我不需要，因为我可以通过不断观察桌子来绕过它。
        </p>
        <p>Models make it possible to predict, to understand, to explain, and to control. So if you don’t have one what
            can you possibly do? Brooks’ answer is, you use the world instead of a model. So everything you do is
            reactive. You don’t have anything in your head that is a map of this room. But maybe I don’t need one
            because I can get around that table by constantly observing it.</p>
        <p>而且我们不必用这些信息填充内存，我只需对其做出反应即可。因此，没有表示，使用世界而不是模型，最纯粹的机制只是有限状态机。因此，布鲁克斯能够做到人们以前从未做到的事情。这种架构的现代性是什么？现在，据布鲁克斯称，美国有
            500 万个家庭在使用这种架构？学生：</p>
        <p>And we don’t have to fill up the memory with that information, I can just react to it. So no representation,
            use the world instead of a model, and the mechanisms in their purest form are just finite state machines. So
            with that, Brooks was able to do things that people were never able to do before. And what’s the modern of
            this architecture? Now, according to Brooks, in use in 5 million homes in the United States? STUDENT:.</p>
        <p>Roomba？教授：就是 Roomba。根据布鲁克斯的说法，Roomba 机器人大约是 iRobot 的第十三个商业计划。它让 iRobot 大获成功，因为 Rumba
            吸尘器非常成功。你想看一部关于它的处理器的电影吗？这是一部不久前拍摄的电影，从某种意义上说，它展示了该架构的概要。</p>
        <p>The Roomba? PROFESSOR: It’s the Roomba. The Roomba robot is, by Brooks’ account, approximately the thirteenth
            business plan of iRobot. And it’s the one that made it big, because the Rumba vacuum cleaner has been very
            successful. Would you like to see a movie of its processor? So this is a film made some time ago that shows,
            in some sense, the summa of that architecture.</p>
        <p>我想让你们简单想象一下，一个机器人在老房子的走廊和房间里四处游荡，碰碰可乐罐子。好，你们脑子里有这样的画面吗？因为我想让你们将现在脑海中那个四处游荡收集可乐罐子的机器人的画面与实际电影进行比较。</p>
        <p>What I want you to imagine very briefly is a robot that wanders around in the halls and rooms of the old
            clinking the Coke cans. Okay, you all got an image of that in your mind? Because I want you to compare the
            image you now have of that robot that’s wandering around collecting the Coke can, with the actual movie.</p>
        <h2 id="pervert">变态</h2>
        <h2>Pervert</h2>
        <p>赫伯特（Herbert），一款汽水罐收集移动机器人。</p>
        <p>Herbert, the soda can collecting mobile robot.</p>
        <p>他是 1989
            年在麻省理工学院人工智能实验室建造的。这项工作由约翰·坎内尔在罗德尼·布鲁克斯的监督下完成。赫伯特是一个由包容架构控制的机器人。这是影响机器人整体活动的一系列小行为。没有集中控制器，也没有世界模型。赫伯特通过使用身体周围的多个红外近距离传感器进行导航，基本上沿着墙壁和走廊行走。
        </p>
        <p>He was built at the MIT AI lab in 1989. Work was done by John Cannell under the supervision of Rodney Brooks.
            Herbert is a robot controlled by subsumption architecture. This is a collection of small behaviors that
            influence the overall activities of the robot. There are no centralized controllers and no world model.
            Herbert navigates by using a number of infrared proximity censors around its body and basically following
            walls and corridors.</p>
        <p>它还可以通过激光扫描器寻找罐头。现在它从办公室门口出来，沿着墙壁走，然后它的激光扫描器发现它前面的桌子上有一个罐头。当这种情况发生时，机器人就会展开手臂。你现在可以看到手臂伸出来了。手臂本身有多个检测器。</p>
        <p>It can also look for the can through a laser light striper. Right now it’s come out of the door of an office,
            followed along the wall, and then its laser light striper has seen a can on top of the desk in front of it.
            When this happens the robots and deploys its arm. You can see the arm going out now. The arm has a number of
            censors itself.</p>
        <p>机器人的爪子上装有指尖传感器、断梁，手掌前部装有两个红外近距离传感器。它以固定的方式抓取罐头。首先，机器人会降低身体，找到某个表面，然后沿着表面弹跳，直到看到前面的罐头。</p>
        <p>There are fingertip censors, a break beam in the jaws, and two infrared proximity sensors on the front of the
            hand. It grabs cans in a stereotype fashion. First, it lowers down to find a surface somewhere, then it
            bounces along the surface until it sees the can in front.</p>
        <p>它使用基于手的红外系统重新将手臂置于中心位置，方法是旋转机器人的身体，直到罐子进入夹钳的钳口之间，此时断线光束会感应到罐子。拿到罐子后，赫伯特会将手臂收回到正常的行进位置，并尝试返回。由于它没有中心，因此它没有任何地图来指示它来自哪里。
        </p>
        <p>It uses the hand based IRs to re center the arm by rotating the robot’s body until the can comes between the
            jaws of the gripper, at which point the break beam senses the can. After acquiring the can, Herbert will
            have tucked the arm back into its normal traveling configuration and attempt to go home. Since it has no
            central it doesn’t have any map of where it came from.</p>
        <p>相反，它有一个算法，使用磁罗盘来确定每次它穿过门时，它是否能找到门？它基本上有一个策略，每次出门时总是向北走。所以现在罐子被藏起来了。当机器人转动时，你会看到激光测距仪的红色条纹。</p>
        <p>Instead, it has an algorithm which uses a magnetic compass to determine every time it comes through a door,
            will it be able to find the door? It basically has a policy of always going north every time it exits the
            door. So now the can is being tucked away. As the robot turns you’ll see a red stripe from the laser range
            finder.</p>
        <p>现在它使用红外线导航回来，找到门，然后带着奖品穿过门。 教授：如果你留心观察，你会看到约翰·坎内尔的身影，他是开发该系统的学生。所以那真是一次杰作。那是一个神奇的时刻。那是你打开香槟的时候。</p>
        <p>And now it’s using the IR to navigate back, find the door, and go through the door with its prize. PROFESSOR:
            And there, if you were paying attention, you saw a little glimpse of John Cannell who was the student to
            develop that system. So that was a tour de force. That was a magic moment. That was when you open the
            champagne.</p>
        <p>当然，这不是你所期望的，因为当我说想象一个机器人四处游荡捡起可乐罐时，这留下了一个巨大的幻觉空间。通常，我们对这些事情的幻觉是这样的。我们想象的事情比实际情况更流畅、更自然、更令人印象深刻。但这令人印象深刻，因为之前没有机器人能做到这一点。
        </p>
        <p>It’s not what you expected, of course, because when I say imagine a robot wandering around in picking up Coke
            cans, that leaves open a huge envelope of possible hallucinations. And usually or hallucinations about these
            things are. we imagine things to be more fluid, more natural, and more impressive than they actually are.
            But that was impressive, because no robot came close to doing anything like that before.</p>
        <p>关于这一点，周三的商业讲座中还会有更多讨论。</p>
        <p>More to be said about that during the business lecture on Wednesday.</p>
        <h2 id="other-architectures">其他架构</h2>
        <h2>Other Architectures</h2>
        <p>这就是包容架构。顺便说一句，现在我们可以谈谈其他架构与明斯基所说的内容有何关联。这个审议思维层对应什么？这就是 SOAR 的意义所在，也许还有 GPS。那么包容是什么呢？它与下面的东西有关。它与本能反应和习得反应有关。
        </p>
        <p>So that’s the subsumption architecture. By the way, maybe at this point we can say something about how the
            other architectures relate to what Minsky was talking about. What’s this deliberative thinking layer
            correspond to? That’s what SOAR is about, and maybe GPS. So what’s subsumption about? It’s about stuff down
            here. It’s about instinctive reaction and learned reaction.</p>
        <h2 id="genesis">创世纪</h2>
        <h2>Genesis</h2>
        <p>但是，明斯基的其他层又怎么样呢？</p>
        <p>But shoot, what about Minsky’s other layers?</p>
        <p>如果我们要构建与这些东西一样智能的系统，那么我们也必须对这类事情有点担心。所以这让我们进入了创世纪架构。现在让我给你一个标准的警告，这在任何学术演讲的早期都应该被提及。</p>
        <p>If we’re going to be building systems that are as smart as those things then we have to worry a little bit
            about that sort of thing too. So that brings us to the genesis architecture. And now let me give you the
            standard caution that should be early in the presentation of any academic.</p>
        <p>我有时会说“我”，其实我指的是“我们”。有时我会说“我们”，其实我指的是“他们”。这个系统主要由我的学生开发，经过很长时间，他们说服我，他们的想法是正确的。但创世架构的工作原理如下。鉴于最近的讨论，这一切都以语言为中心，这并不奇怪。
        </p>
        <p>I will sometimes say “I,” and what I mean is “we.” And sometimes I’ll say “we,” and what I mean is “they.”
            This was a system that was developed mostly by students of mine who persuaded me, after a great deal of
            time, that they were thinking the right kinds of thoughts. But here’s how the genesis architecture works. As
            no surprise, given recent discussions, it’s all centered on language.</p>
        <p>而创世系统的语言部分有两个作用，一是引导、统领和与感知系统互动，二是实现对事件的描述。</p>
        <p>And the language part of the genesis system has two roles, one of which is to guide, and marshal, and
            interact with the perceptual systems. And the other is to enable the description of events.</p>
        <h2 id="perception">洞察力</h2>
        <h2>Perception</h2>
        <p>就是这样。那么感知重要吗？我不知道。我可能会问你一个问题，比如，坐在前排的有没有人穿着蓝色牛仔裤？</p>
        <p>That’s how it works. So is perception important? I don’t know. I might ask you a question like, is there
            anybody sitting in the front row wearing blue jeans?</p>
        <p>在这种情况下，你很难不去看问题并回答它。你的眼睛会回答问题。这不需要任何符号处理系统，除非我的语言系统与他们的语言系统进行交流，这会驱动你的运动系统和视觉系统去回答你的问题。但语言系统引导你注意的不仅仅是真实的东西。
        </p>
        <p>And it’s hard for you to resist, under those circumstances, your eyes from going over there and answering the
            question. Your eyes answer the question. No symbol processing system is involved, except in so far as my
            language system has communicated with their language system, which drives your motor system and your vision
            system to go over there and answer the question for you. But it’s not just the real stuff that the language
            system directs your attention to.</p>
        <p>这也是想象中的事情。这个学期过得很漫长。我跟你讲过我台锯的故事吗？可能没有。事情是这样的。我买了一台台锯。这是一台很棒的台锯。我和一个做橱柜的朋友一起安装它。他说，操作锯子时千万不要戴手套。“为什么？”我说。在他回答这个问题之前，我就明白了。
        </p>
        <p>It’s also the imagined stuff. It’s been a long semester. Have I told you the story about my table saw?
            Probably not. Here’s the deal. I bought a table saw. It’s a wonderful table saw. I was installing it with a
            friend of mine who’s a cabinet maker. He said, never wear gloves when you operate the saw. “Why?” I said.
            Before he could answer the question I figured it out.</p>
        <p>你能想明白为什么操作台锯时从不戴手套吗？你知道台锯是什么，对吧？它是一张中间有旋转刀片的桌子。你用它来切割木头。为什么你从不戴手套？是吗？学生：好吧。学生：嗯，你知道答案。哈，这不公平。那是老布雷特。他听过太多次这个故事了。是的，安德鲁，你明白了。
        </p>
        <p>Can you figure out why you never wear gloves when you operate a table saw? You know what a table saw is,
            right? It’s a table with a spinning blade in the middle. And you use it to cut wood. Why should you never
            wear gloves? Yes? STUDENT: Well. STUDENT: Well, you know the answer. Ha, that’s not fair. That’s old Brett
            up there. He’s heard the story too many times. Yes, Andrew, you got it.</p>
        <p>学生：我之前已经知道答案了。教授：你已经知道答案了。那么那些还没有知道答案的人呢？对吧？学生：因为手套可能会被夹住。教授：因为手套可能会被夹住，把你的手拉进刀片里。然后会发生什么？太可怕了。你的手会被弄伤，你的手指会被切断，这种情况在专业人士身上很常见。
        </p>
        <p>STUDENT: I’ve been told the answer before. PROFESSOR: You’ve been told the answer. How about somebody who
            hasn’t been told the answer. Yes? STUDENT: Because the gloves might get caught. PROFESSOR: Because the glove
            might get caught and pull your hand into the blade. And then what happens? It’s horrible. You’re hand gets
            mangled and your fingers get cut off, and this happens a lot to professionals.</p>
        <p>我买的那台台锯实际上不会发生这种情况，因为它的运动会检测肉并停止锯片，然后导致锯片停止，并在大约两微秒内让锯片退回到台面。两毫秒。所以，总的来说，这是一个坏主意，为了使用良好的安全措施，你总是必须假设机制无论如何都没有工作。
        </p>
        <p>It won’t actually happen with that table saw that I bought, because its play detects flesh and stops the
            blade, which then leads to stopping the blade and having the blade retreat into the table in about two
            microseconds. two milliseconds. So, in general though, it’s a bad idea, and you always have to suppose that
            the mechanism isn’t working anyway in order to use good safety practice.</p>
        <p>但这里有一个例子，说明没有人告诉过你，他能够通过想象会发生什么，并根据他想象的场景读出答案来弄清楚。所以没有人说过很多我们知道的事情，但我们无论如何都知道。这是另一个例子。想象一下提着满满一桶水在街上奔跑。会发生什么？水溅出来，弄湿了你的腿，对吧？
        </p>
        <p>But here’s an example of something that nobody ever told you that he was able to figure out, by imagining
            what would happen and reading the answers off of the scene that he imagined. So nobody ever says many of the
            things that we know, but we know them anyway. Here’s another example. Imagine running down the street with a
            full bucket of water. What happens? The water splashes out and gets your leg wet, right?</p>
        <p>你在 Open Minds
            数据库中找不到这个。网络上从来没有人说过这个。它没有被记录在任何地方。但你知道它。因为你，我们人类有能力想象感知事物，并利用感知器官从我们的想象中读出问题的答案。所以这是一个非常重要的联系。如果你有能力描述事件，那么你就有能力讲述和理解故事。
        </p>
        <p>You won’t find that in Open Minds database. Nobody ever said that over the web. It’s not written down
            anywhere. But you know it. Because you, we human beings have the capacity to imagine perceptual things and
            read the answers to questions off of our imaginations with that perceptual apparatus. So that’s a very
            important connection down there. And then if you’ve got the ability to describe events, then you’ve got the
            ability to tell and understand stories.</p>
        <p>如果你能做到这一点，那么你就可以开始掌握文化，包括宏观和微观文化。宏观文化指的是你成长的国家、你成长过程中的宗教信仰。微观文化指的是你的家庭和个人经历，以及介于两者之间的所有方面。所以我不知道是什么促使我和我的同事这样思考？
        </p>
        <p>And if you can do that, then you can start to get a handle on culture, both macro and micro. And by macro
            culture I mean the country you grew up in, the religion you grew up with. And by micro I mean your family
            and personal experience, and all shades in between. So I don’t know, what inspires me and my associates to
            think in these terms?</p>
        <p>上次我谈到进化论和人类物种在大约 5
            万年前的繁盛时，我们谈到了这一点，那时我们得到了一些东西。我相信我们得到了一些东西。这是这个特定假设的特征。我们得到的是讲述故事和理解故事的能力。所以如果我们想给这种表述贴上标签，那就是强故事假设。</p>
        <p>We talked about a little bit of it last time when I talked about evolution and the apparent flowering of our
            species about 50,000 years ago, at which time we got something. And I believe that what we got. and this is
            the characterization of this particular hypothesis. what we got is the ability to tell stories and
            understand them. So if we want to label this representation, it’s the label strong story hypothesis.</p>
        <p>那么弱故事假设是什么？弱故事假设是，这很重要。强故事假设是，这就是全部。</p>
        <p>So what’s the weak story hypothesis? The weak story hypothesis is, this is important. The strong story
            hypothesis is, this is all there is.</p>
        <h2 id="story-hypothesis">故事假设</h2>
        <h2>Story Hypothesis</h2>
        <p>但还有其他证据表明这一点真的、真的、真的很重要吗？所以我在课程开始前问了克里希纳，他说我还没有告诉你接下来的实验。在我看来，这是认知心理学、发展心理学中最重要的一系列实验。</p>
        <p>But is there any other evidence of this is really, really, really important? So I’ve queried Krishna here
            before the class starts, and he tells me I haven’t told you about the following experiment. This, in my way
            of thinking, is the most important series of experiments ever done in cognitive psychology, developmental
            psychology, actually.</p>
        <p>那么我们开始吧。如果你是人，房间是长方形的。如果你是老鼠，房间就是长方形的盒子。所有的墙都漆成了白色。你明白了吗？现在，每个角落都有一个篮子、布或其他东西，你可以在里面或下面放一些食物。现在，你把食物放在那里，让老鼠看着你。然后你稍微旋转一下老鼠，让它迷失方向。
        </p>
        <p>So here’s how we get started. There’s a rectangular room, if you’re a person. If you’re a rat, it’s a
            rectangular box. All the walls are painted white. Are you with me so far? So now, in each corner there’s a
            basket, or cloth, or something in which or under which you can put some food. Now, you put the food there
            while the rat watches you. And then you give the rat a little spin to disorient it.</p>
        <p>好吗？然后，剩下的老鼠就停下来寻找食物。你可以跟踪老鼠去哪里。老鼠以大致相等的概率主要去这两个角落。所以我敢打赌你不知道老鼠这么聪明。所以它们知道房间是长方形的，它们不会去没有食物的对角角落。这些老鼠是天才吗？</p>
        <p>All right? So then, the rest stops and goes for the food. And you can keep track of where the rat goes. And
            the rat goes with approximate equal probability predominantly to those two corners. So I’d have bet you
            didn’t know that rats were that smart. So they understand the rectangular nature of the room and they don’t
            go to the diagonal corners where the food cannot be. So are these genius rats?</p>
        <p>或者我们只是脑袋很大的老鼠。因为我们做同样的事情。所以如果你重复这个实验，用一个小孩代替老鼠，然后你把玩具代替食物放进去，老鼠就不是老鼠了。孩子通常被父母抱在怀里，通常是孩子的母亲。</p>
        <p>Or maybe we’re just rats with big brains. Because we do the same thing. So if you repeat this experiment and
            replace the rat with a small child, and then you put a toy in there instead of food, and the rat. not the
            rat. The child is usually held in a parent’s arms, usually the child’s mother.</p>
        <p>通常是因为他们认为如果他们参加哈佛的这些实验，他们的孩子将来会考上哈佛。所以孩子就像老鼠一样走到对角线角落。然后你接下来要做的就是尝试一个成年人，也许是麻省理工学院的学生。这样你就可以再次使用食物。你会得到同样的结果。谁会感到惊讶呢？
        </p>
        <p>Usually because they think that if they participate in these experiments up there at Harvard their kid will
            get into Harvard some day. So the kid goes to a diagonal corner just like a rat. And then the next thing you
            do is, you try an adult, maybe an MIT student. That way you can use food again. And you get the same result.
            Who could be surprised?</p>
        <p>因此，就这项实验而言，老鼠、儿童和人类成年人的表现几乎相同，直到你把一面墙漆成蓝色。如果你想知道的话，老鼠不是色盲。然后会发生什么？好吧，如果你把一面墙漆成蓝色，老鼠仍然会以相同的概率走向两个对角线角。如果你把一面墙漆成蓝色，孩子仍然会以大致相同的概率走向两个对角线角。
        </p>
        <p>So rats, children, and human adults, pretty much all the same with respect to this experiment, until you
            paint one wall blue. Rats are not colorblind, in case you’re wondering. Then what happens? Well, if you pay
            one wall blue the rat still goes with equal probability to the two diagonal corners. If you paint one wall
            blue, the child still goes to the two diagonal corners with approximate equal probability.</p>
        <p>只有我们这些天才成年人才会走到那个角落。因此，这引发了几个问题。其中之一是，孩子什么时候成为成年人？有什么想法吗？你怎么看？学生：教授：你可以选一个大于 1 小于 10
            的数字。你怎么看？学生：五？教授：这是一个相当不错的猜测。你在那个年龄有兄弟姐妹吗？这很令人惊讶，但为什么是五？</p>
        <p>It’s only us genius human adults who go only to that corner. So this invites a couple of questions. One of
            which is, when does a child become an adult? Any ideas? what do you think? STUDENT: PROFESSOR: You can pick
            a number greater than 1 and less than 10. what do you think? STUDENT: Five? PROFESSOR: It’s a pretty good
            guess. Do you have siblings at that age? It’s a surprise but, why is it five?</p>
        <p>是因为……它与什么有关？与这种能力的开始有什么关联吗？你可能会尝试一切，因为她非常小心。所以她尝试了性别，她尝试了语言的开始，音乐欣赏，惯用手，只有一件事很重要。那就是当孩子开始用左和右这个词来描述世界时，他们就成了成年人。
        </p>
        <p>Is it because. what does it relate to? Is there any correlate to the onset of that ability? You might try
            everything, as does, because she’s extremely careful. So she’s tried gender, she’s tried the onset of
            language, the appreciation of music, handedness, and there’s only one thing that matters. And that is that
            the child becomes adult at that time when they start to use the word left and right when they describe the
            world.</p>
        <p>我之所以这么说，是因为他们从小就懂得了左右，但他们在描述世界时，只有当他们开始打破这种对称性，走向正确的角落时，才开始使用左右这两个词。现在，对于下一个元素，我需要一些阅读材料。有人手边有教科书吗？啊，“中国，一部插图史”。现在我需要一个志愿者。好的。
        </p>
        <p>Now I said that very carefully because they understand left and right at an earlier age, but they only have
            started to use the words left and right when they describe the world at the time that they begin to break
            this symmetry and go to the correct corner. Now for the next element of this I need something to read. Has
            anyone got a textbook handy? Ah, “China, an Illustrated History.” Now I need a volunteer. OK.</p>
        <p>安德鲁，你想这样做吗？所以你要这样做。你可以待在那里。但你需要站起来。所以我要做的是，我要给你读这本书的一段话。我希望你在我读的时候也把它读给我听。这就像你在做同声传译，只不过是英语对英语。这个东西里有我不会发音的单词。
        </p>
        <p>Andrew, you want to do this? So here’s what you’re going to do. You can stay there. But you need to stand up.
            So what I’m going to do is, I’m going to read you a passage from this book. And I want you to say it back to
            me at the same time I read it. It’s as if you’re doing simultaneous translation, except it’s English to
            English. This things got words I can’t pronounce.</p>
        <p>好的，你准备好了吗？好的。“当他被他所解决的问题的规模压倒时，他开始怀疑其他人在密谋反对他或暗中嘲笑他。”非常感谢。太好了。所以你看，他可以做到。有些人做不到。至少需要一点练习。但他做到了。猜猜我对他做了什么？我把他的智力降低到了老鼠的水平。
        </p>
        <p>OK, are you ready to go? All right. “When overwhelmed by the magnitude of the problems he tackled, he began
            to suspect that others were plotting against him or secretly ridiculing him.” Thank you very much. That’s
            great. So you see, he could do it. Some people can’t do it. At least it take a little practice. But he did
            it. And guess what I’ve done to him? I’ve reduced his intelligence to that of a rat.</p>
        <p>因为如果你对一个成年人进行这个实验，让他同时进行英语到英语的翻译，他们会以相同的概率选择两个角落。那么发生了什么？发生了什么，因为你堵塞了他们的语言处理器。当他们的语言处理器被堵塞时，他们就无法将蓝色的墙和矩形形状拼凑在一起。所以似乎语言正是你构建描述所需的组合器的中介。
        </p>
        <p>Because if you do this experiment with an adult human who’s doing this simultaneous English to English
            translation, they go with equal probability to the two corners. So what’s happened? What’s happened is
            you’ve jambed their language processor. And when their language processor is jambed they can’t put the blue
            wall together with the rectangular shape. So it seems to be that language is the mediator of exactly the
            combinators you need in order to build descriptions.</p>
        <p>因为当他们的语言处理器被同声传译现象堵塞时，他们甚至无法把这些东西放在一起。所以这给我们带来了当今的两个黄金想法。一个是，如果你想让自己更聪明，你就要做这些事情。看、听、画和说。因为这些都是围绕这个区域的特殊机制，这是我们所做事情的中心。这是我们思考的中心。
        </p>
        <p>Because they can’t even put those things together when their language processor is jambed by the simultaneous
            translation phenomenon. So that brings us to the two gold star ideas of the day. One is, if you want to make
            yourself smarter you want to do those things. look, listen, draw, and talk. Because those are the particular
            mechanisms that surround this area down here, which is the center of what we do. which is the center of our
            thinking.</p>
        <p>那么你为什么要在课堂上记笔记呢？现在因为你以前会看笔记，但它迫使你的语言、你的运动和你的视觉器官参与进来。这会让你变得更聪明，因为它锻炼了这些东西。总而言之，你可以说的第二件事，特别是从这个实验中，是要警惕说话快的人。你为什么要警惕说话快的人。
        </p>
        <p>So why do you take notes in class? Now because you’ll ever look at them before, but because it forces the
            engagement of your linguistic. of your linguistic, your motor, and your visual apparatus. And that makes you
            smarter, because it’s exercising that stuff. The second thing you can say, in conclusion, especially from
            this experiment, is beware of fast talkers. Why do you want to be aware of fast talkers.</p>
        <p>这并不是因为他们会说服你做任何事情。这是因为当他们说话很快时，他们会堵塞你的语言处理器，让你无法思考。这就是为什么你要警惕说话很快的人。</p>
        <p>It’s not because they will talk you into anything. It’s because that when they talk fast they’re jambing your
            language processor and you can’t thing. That’s why you want to be aware of fast talkers.</p>
        <p>因为如果他们堵塞了你的语言处理器，你就不会思考，你会买那辆车，或者买那杯饮料，或者做任何你想让你做这些事情的人通过与堵塞你的处理器交谈学会做的事情。所以这就是我们今天要做的。我会在另一个场合向你演示其中的一些内容。
        </p>
        <p>Because if they jamb your language processor you won’t thinking and you’ll buy that car, or you’ll buy that
            drink, or you’ll do any manner of things that people who want you to do those things have learned to do by
            talking to jamb your processor. So that completes what we’re going to do today. And I’ll give you a
            demonstration of some of this stuff on another occasion.</p>
        <h1 id="probabilistic-inference-i">21. 概率推理 I</h1>
        <h1>21. Probabilistic Inference I</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 48:30 <a
                href="https://youtube.com/watch?v=A6Ud6oUCRak">https://youtube.com/watch?v=A6Ud6oUCRak</a></p>
        <p> 11 years ago (Jan 11, 2014) — 48:30 <a
                href="https://youtube.com/watch?v=A6Ud6oUCRak">https://youtube.com/watch?v=A6Ud6oUCRak</a></p>
        <h2 id="unknown-369">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：现在到了最后的冲刺阶段。还有三道题。我们将从最后三道题中选出一些，也许是最后三道题中的两道，来谈谈与概率方法有关的东西。概率在人工智能中的应用。现在，对于你们中的许多人来说，这将是一次回顾，因为我知道你们中的许多人从那以后每年都在桌子上学习概率。
        </p>
        <p>PATRICK WINSTON: Here we are, down to the final sprint. Three to go. And we’re going to take some of the last
            three, maybe two of the last three, to talk a little bit about stuff having to do with probabilistic
            approaches. use of probability in artificial intelligence. Now, for many of you, this will be kind of a
            review, because I know many of you learned about probability over the table and every year since then.</p>
        <p>但也许我们会再加一点小改动，尤其是在这个小时快结束的时候，当我们开始讨论所谓的信念网络时。但首先，我今天早上开车过来，看到这个东西时，我非常惊讶。我的第一反应是，天哪，这是世界上最伟大的黑客。然后我决定，好吧，也许这是一件艺术品。
        </p>
        <p>But maybe we’ll put another little twist into it, especially toward the end of the hour when we get into a
            discussion of that which has come to be called belief nets. But first, I was driving in this morning, and I
            was quite astonished to see, as I drove in, this thing here. And my first reaction was, oh my god, it’s the
            world’s greatest hack. And then I decided, well, maybe it’s a piece of art.</p>
        <p>所以我想谈谈我如何才能解决这个问题。这个东西很有可能是帽子的产物，可能是某种艺术展的结果。无论如何，某种雕像出现了，雕像通常不会这样出现。所以我有机会思考所有这些事情可能如何同时发生或不同时发生。</p>
        <p>So I’d like to address the question of how I could come to grips with that issue. There’s a distinct
            possibility that this thing is a consequence of a hat, possibly the result of some kind of art show. And in
            any event, some sort of statue appeared, and statues don’t usually appear like that. So I got the
            possibility of thinking about how all these things might occur together or not occur together.</p>
        <h2 id="unknown-370">未知</h2>
        <h2>Unknown</h2>
        <p>所以很自然地，我会给自己制作一个表格来记录我的观察结果。我的表格中有三列。我记录了雕像出现、黑客入侵和某种艺术展的可能性。因此，我可以制作一个表格，记录所有可能出现的组合。我碰巧已经猜到我的表格中有八行。</p>
        <p>So the natural thing is to build myself some sort of table to keep track of my observations. So I have three
            columns in my table. I’ve got the possibility of a statue appearing, a hack having occurred, and some sort
            of art show. And so I can make a table of all the combinations of those things that might appear. And I
            happen to have already guessed that there are going to be eight rows in my table.</p>
        <p>所以它看起来会像这样。这是这一行中没有出现任何组合的组合。下面是所有这些情况都发生的情况。毕竟，我们有可能举办一场艺术展，让一个黑客成为艺术展的合法参与者。这就是为什么我们有最后一行。所以我们有介于两者之间的各种组合。
        </p>
        <p>So it’s going to look like this. And this is the set of combinations in this row where none of that occurs at
            all. And down here is the situation where all of those things occur. After all, it’s possible that we can
            have an art show and have a hack be a legitimate participant in the art show. That’s why we have that final
            row. So we have all manner of combinations in between.</p>
        <p>所以这些就是这些组合。然后我们有 F、F、T、F、F、T、F、T、F、T、F、T、F、T、F、T。所以很明显，表中的行数或这些二进制可能性是变量数的 2
            倍。这可能是一个很大的数字。事实上，我很想做一个更大的例子，但我没有耐心去做。</p>
        <p>So those are those combinations. Then we have F, F, T, T, F, F, T, T, F, T, F, T, F, T, F, T. So it’s plain
            that the number of rows in the table, or these binary possibilities, is 2 to the number of variables. And
            that could be a big number. In fact, I’d love to do a bigger example, but I don’t have the patience to do
            it.</p>
        <h2 id="unknown-371">未知</h2>
        <h2>Unknown</h2>
        <p>但无论如何，为了弄清楚这些组合的可能性有多大，我们可能会在很长一段时间内观察学生中心外的区域和校园的其他地方，并跟踪 1,000 天、1,000 个月或 1,000
            年内发生的事情。我不知道。问题是，这些事件并不经常发生。</p>
        <p>But anyhow, what we might do is in order to figure out how likely any of these combinations are, is we might
            have observed the area outside the student center and rest of campus over a long period of time and keep
            track of what happens on 1,000 days. Or maybe 1,000 months or 1,000 years. I don’t know. The trouble is,
            these events don’t happen very often.</p>
        <p>因此，我用于测量的时间段需要相当长。一天可能不够短。但无论如何，我可以记录我看到这些不同组合的频率。例如，这个可能是 405，这个可能是 45，这个可能是 225，这个可能是 40，等等。</p>
        <p>So the period of time that I use for measurement needs to be fairly long. Probably a day is not short enough.
            But in any case, I can keep a tally of how often I see these various combinations. So this one might be, for
            example, 405, this one might be 45, this one might be 225, this one might be 40, and so on.</p>
        <p>因此，在完成所有这些测量并记录所有数据后，我可以说，在任何给定时间段内，这些事件发生的概率就是频率。计数数除以计数总数。所以这将是一个介于 0 和 1
            之间的数字。所以这就是每个事件的概率。而且它很容易根据我的数据计算出来。</p>
        <p>And so having done all those measurements, kept track of all that data, then I could say, well, the
            probability that at any given time period one of these things occurs will just be the frequency. the number
            of tallies divided by the total number of tallies. So that would be a number between 0 and 1. So that’s the
            probability for each of these events. And it’s readily calculated from my data.</p>
        <h2 id="unknown-372">未知</h2>
        <h2>Unknown</h2>
        <p>一旦我这样做了，我就可以说我得到了一个联合概率表，我可以用这个联合概率表来创造各种奇迹。那么，趁着这个机会，让我来创造一些奇迹吧。这是表格。现在，我想要做的是计算雕像出现的所有行中的概率。</p>
        <p>And once I do that, then I can say that I got myself a joint probability table, and I could perform all
            manner of miracles using that joint probability table. So let me perform a few of those miracles, while
            we’re at it. There’s the table. And now, what I want to do is I want to count up the probability in all the
            rows where the statue appears.</p>
        <p>这就是雕像出现的概率。所以我只需勾选那四个框即可。在我的模型中，雕像出现的概率似乎约为
            0.355。我认为它不是那么频繁，但这是课堂练习，对吧？所以我可以随意编造数字。现在，我可以说，假设有一场艺术展，雕像出现的概率是多少？</p>
        <p>So that’s going to be the probability of the statue appearing. So I’ll just check off those four boxes there.
            And it looks like the probability of the statue appearing is about 0.355 in my model. I don’t think it’s
            quite that frequent, but this is a classroom exercise, right? So I can make up whatever numbers I want. Now,
            I could say, well, what’s the probability of a statue occurring given that there’s an art show?</p>
        <p>好吧，我可以将我的计数限制在艺术展为真的计数上，就像这样。在这种情况下，概率就会大幅上升。所以如果我知道有艺术展，雕像出现的概率就会高得多。如果我知道有黑客活动和艺术展，这个概率就会上升到
            0.9。我们还可以做其他事情。</p>
        <p>Well, I can limit my tallies to those in which art show is true, like so. And in that case, the probability
            has just zoomed up. So if I know there’s an art show, there’s a much higher probability that a statue will
            appear. And if I know there’s a hack as well as an art show going on, it goes up higher still to 0.9. We can
            also do other kinds of things.</p>
        <h2 id="unknown-373">未知</h2>
        <h2>Unknown</h2>
        <p>例如，我们可以回到原始表格。我们不会像刚才那样计算雕像的概率，而是计算举办艺术展的概率。我想应该是那个和那个，不是那个，而是那个。所以举办艺术展的概率是十分之一。或者我们可以用黑客技术做同样的事情。</p>
        <p>For example, we can go back to the original table. And instead of counting up the probability we’ve got a
            statue, as we just did, we’re going to calculate the probability that there is an art show. I guess that
            would be that one and that one, not that one, but that one. So the probability there’s an art show is one
            chance in 10. Or we can do the same thing with a hack.</p>
        <p>在这种情况下，我们会得到那一个关闭，那一个打开，那一个关闭，那一个打开，那一个关闭，那一个打开，那一个关闭。因此，在任何给定时间段内发生黑客攻击的概率约为 50
            50。所以我制作了这个小演示，以便它可以完成所有这些事情的“和”。它也可以做“或”，只需多做一点工作。但这些只是这些不同组合的“和”。</p>
        <p>In that case, we get that one off, that one on, that one off, that one on, that one off, that one on, that
            one off. So the probability of a hack on any given time period is about 50 50. So I’ve cooked up this little
            demo so it does the “ands” of all these things. It could do “ors,” too, with a little more work. But these
            are just the “ands” of these various combinations.</p>
        <p>然后你可以问一些更复杂的问题，比如，你可以说，假设有一座雕像，那么发生黑客攻击的概率是多少？这会将计算限制在雕像事件为真的行。然后我得到的结果是
            0.781。现在，如果我知道有一场艺术展，发生黑客攻击的概率会怎样？这个数字会上升还是下降？好吧，让我们试试。</p>
        <p>Then you can ask more complicated questions, like for example, you could say, what is the probability of a
            hack given that there’s a statue? And that would be limiting the calculations to those rows in which the
            statue thing is true. And then what I get is 0.781. Now, what would happen to the probability that it’s a
            hack if I know that there’s an art show? Will that number go up or down? Well, let’s try it.</p>
        <h2 id="unknown-374">未知</h2>
        <h2>Unknown</h2>
        <p>啊，它倒下了。所以，艺术展的存在在某种程度上解释了为什么这座雕像可能在那里。现在，为了好玩，我要换一种情况，非常相似。这里的情况是邻居的狗经常叫。这可能是因为有小偷。也可能是因为浣熊。有时，有小偷和浣熊。有时，那只该死的狗只是叫。
        </p>
        <p>Ah, it went down. So that’s sort of because the existence of the art show sort of explains why the statue
            might be there. Now, just for fun, I’m going to switch to another situation, very similar. And the situation
            here is that a neighbor’s dog often barks. It might be because of a burglar. It might be because of a
            raccoon. Sometimes, there’s a burglar and a raccoon. Sometimes, the damn dog just barks.</p>
        <p>所以让我们做一些计算，计算出浣熊出现的概率，就像我们上次做的一样。看起来在任何一个晚上。这是一个树木繁茂的地方。浣熊出现的概率很高。然后我们可以问，那么，如果浣熊出现，狗叫的概率是多少？</p>
        <p>So let’s do some calculations there and calculate the probability that a raccoon is true, similar to what we
            did last time. Looks like on any given night. it’s kind of a wooded are. there’s a high probability of a
            raccoon showing up. And then we can ask, well, what is the probability of the dog barking given that a
            raccoon shows up?</p>
        <p>好吧，在这种情况下，我们只想将行数限制为浣熊或狗叫的行数。看起来，狗叫的概率，不知道其他的，大约是 但现在我们想知道浣熊的概率。这些家伙需要检查。这些都不对。所以这是浣熊的概率。我说得对吗？哦，这是小偷的概率。</p>
        <p>Well, in that case, we want to just limit the number of rows to those where a raccoon. or where the dog is
            barking. Looks like the probability of the dog barking, knowing nothing else, is about But now we want to
            know the probability of the raccoon. that’s these guys here need to get checked. These are off. So that’s
            the probability of a raccoon. Did I get that right? Oh, that’s probability of a burglar.</p>
        <h2 id="unknown-375">未知</h2>
        <h2>Unknown</h2>
        <p>抱歉，这太难了。让我回去计算一下。我想要得到浣熊出现的概率。答案是真、假、真、假、真、假、真。所以，正如我之前所说，浣熊出现的概率是
            0.5。现在，如果我知道狗在叫，这个概率会怎样？好吧，我需要做的就是将行限制在狗在叫的行，也就是最下面的四行。</p>
        <p>Sorry, that was too hard. So let me go back and calculate. I want to get the probability of a raccoon. That’s
            true, false, true, false, true, false, true. So the probability of a raccoon, as I said before is 0.5. Now,
            what happens to that probability if I know the dog is barking? Well, all I need to do is limit my rows to
            those where the dog is barking, those bottom four.</p>
        <p>我点击那里，你会注意到中点以上的所有计数都变为零，因为我们只考虑狗叫的情况。在这种情况下，浣熊出现的概率。只是计数数量除以总计数数量。哎呀，我猜是 225 加 50 除以 370。结果是 0.743。所以大约 75%
            的时间里，狗叫声被考虑在内。</p>
        <p>And I’ll click that there, and you’ll notice all these tallies up above the midpoint have gone to zero,
            because we’re only considering those cases where the dog is barking. In that case, the probability that
            there’s a raccoon. just the number of tallies over the total number of tallies. gee, I guess it’s 225 plus
            50 divided by 370. That turns out to be 0.743. So about 75% of the time, the dog barking is accounted for.
        </p>
        <p>那么，在这些情况下，浣熊出现的概率相当高。现在，我再次要问，假设狗在叫，而且有窃贼，浣熊出现的概率是多少？猜猜会发生什么？我们之前对雕像做过一次这样的实验。当我们看到雕像时，概率首先上升，然后当我们看到另一种解释时，概率下降。这是这个。哇，看看这个。
        </p>
        <p>Well, the probability of a raccoon under those conditions is pretty high. And now, once again, I’m going to
            ask, well, what is the probability of a raccoon, given that the dog is barking and there’s a burglar? Any
            guess what will happen there? We did this once before with the statue. Probability first went up when we saw
            the statue and then went down when we saw another explanation. Here’s this one here. Wow, look at that.</p>
        <h2 id="unknown-376">未知</h2>
        <h2>Unknown</h2>
        <p>它回到了原来的状态，即先验概率。因此，不知何故，窃贼的存在和狗叫声意味着浣熊出现的概率和我们开始这个游戏之前一样。所以这些都是很有趣的问题，有了这张表，我们就可以通过这些计算做很多事情。</p>
        <p>It went back to its original condition, its a priori probability. So somehow, the existence of the burglar
            and the dog barking means that the probability of a raccoon is just what it was before we started this game.
            So those are kind of interesting questions, and there’s a lot we can do when we have this table by way of
            those kinds of calculations.</p>
        <p>事实上，概率推理的整个奇迹就在我们面前。就是这张表。那么我们为什么不回家呢？因为这张表有点问题。我以示例的方式向你们展示的这两张表就是如此。问题是有很多行。我很难计算出这些数字。我没有耐心等待并进行观察。</p>
        <p>And in fact, the whole miracle of probabilistic inference is right in front of us. It’s the table. So why
            don’t we go home? Well, because there’s a little problem with this table. with these two tables that I’ve
            shown you by way of illustration. And the problem is that there are a lot of rows. And I had a hard time
            making up those numbers. I didn’t have the patience to wait and make observations.</p>
        <p>那会花太长时间。所以我不得不做一些猜测。我可以用八行来处理。那些在上面。我可以放一些计数。这不是什么大问题。所以我自己得到了所有上面的八个数字。同样，对于艺术展计算，产生了八个数字。但如果我添加其他东西呢？</p>
        <p>That would take too long. So I had to kind of make some guesses. And I could kind of manage it with eight
            rows. those up there. I could put in some tallies. It wasn’t that big of a deal. So I got myself all those
            eight numbers up there like that. And similarly, for the art show calculations, produced eight numbers. But
            what if I added something else to the mix?</p>
        <h2 id="unknown-377">未知</h2>
        <h2>Unknown</h2>
        <p>如果我加上星期几或早餐吃什么会怎么样？每一项都会使二进制变量的行数翻倍。所以如果我必须考虑 10 个影响因素，那么我就有 2 的 10 次方。我要处理 1,000
            个数字。这会很难。但如果​​我有联合概率表，那么我就可以创造这种奇迹。</p>
        <p>What if I added the day of the week or what I had for breakfast? Each of those things would double the number
            of rows of their binary variables. So if I have to consider 10 influences all working together, then I’d
            have 2 to the 10th. I’d have 1,000 numbers to deal with. And that would be hard. But if I had a joint
            probability table, then I can do these kinds of miracles.</p>
        <p>但是戴夫，请让我现在就用这个​​小型投影仪。我只想强调，虽然我们谈论的是概率推理，它是一个非常强大的工具，但它并不是我们包里唯一需要的工具。人工智能中大多数想法的问题在于，它们的铁杆支持者认为它们是唯一可以做的事情。概率推理在发展人类智能理论方面发挥着作用。
        </p>
        <p>But Dave, if I could have this little projector now, please. I just want to emphasize that although we’re
            talking about probabilistic inference, and it’s a very powerful tool, it’s not the only tool we need in our
            bag. Trouble with most ideas in artificial intelligence is that their hardcore proponents think that they’re
            the only thing to do. And probabilistic inference has a role to play in developing a theory of human
            intelligence.</p>
        <p>它当然有实用价值，但并非唯一。为了说明这一点，我想想象一下，麻省理工学院成立于公元前 1861 年，而不是公元 1861 年。如果是这样，那么可能会有一个关于漂浮物的研究项目。</p>
        <p>And it certainly has a practical value, but it’s not the only thing. And to illustrate that point, I’d like
            to imagine for a few moments that MIT were founded in 1861 BC instead of 1861 AD. And if that were so, then
            it might be the case that there would be a research program on what floats.</p>
        <h2 id="unknown-378">未知</h2>
        <h2>Unknown</h2>
        <p>当然，这会成为实验物理学的一个问题，我们可以想象，早期麻省理工学院的那些人会出于实验的考虑，尝试一些东西。哦，我不知道发生了什么。看起来粉笔会浮起来。这是一块石头。不，它没有浮起来。这是一些钱。不会浮起来。这是一支铅笔。不，它不会浮起来。这是一支钢笔。这是我从肯德拉那里得到的一块锡箔。它会浮起来。
        </p>
        <p>And this, of course, would be a problem in experimental physics, and we could imagine that those people back
            there in that early MIT would, being experimentally minded, try some things. Oh, I didn’t know that’s what
            happened. It looks like chalk floats. Here’s a rock. No, it didn’t float. Here’s some money. Doesn’t float.
            Here’s a pencil. No, it doesn’t float. Here’s a pen. Here’s a piece of tin foil I got from Kendra. That
            floats.</p>
        <p>那是金属。其他东西也是金属。现在我真的糊涂了。这是一小团纸。这是手机照片。不，实际上，我以前试过。它们不会漂浮。而且它们之后也不会起作用。我不需要从公元 1861
            年及以后的麻省理工学院做任何这些事情，因为我知道阿基米德已经解决了这一切。</p>
        <p>That’s a metal. The other stuff’s metal, too. Now I’m really getting confused. Here’s a little wad of paper.
            Here’s a cell ph. no, actually, I’ve tried that before. They don’t float. And they also don’t work
            afterward, either. I don’t need to do any of that in the MIT of 1861 AD and beyond, because I know that
            Archimedes worked this all out.</p>
        <p>我所要做的就是测量物体的体积，除以重量，如果这个比例足够大，物体就会浮起来。但在过去，我必须尝试很多东西，然后做一个大桌子，考虑各种因素，比如物体的硬度、大小、重量、是否是活的。大多数活的东西都会浮起来。</p>
        <p>And all I have to do is measure the volume of the stuff, divide that by the weight, and if that ratio is big
            enough, then the thing will float. But back in the old days, I would have to try a lot of stuff and make a
            big table, taking into account such factors as how hard it is, how big it is, how heavy it is, whether it’s
            alive or not. Most things that are alive float.</p>
        <h2 id="unknown-379">未知</h2>
        <h2>Unknown</h2>
        <p>有些则不然。例如，鱼就不是这样。所以这样做是愚蠢的。这是一种概率推理。另一方面，有很多事情我并不知道进行计算所需的全部信息。</p>
        <p>Some don’t. Fish don’t, for instance. So it would be foolhardy to do that. That’s sort of a probabilistic
            inference. On the other hand, there are lots of things where I don’t know all the stuff I need to know in
            order to make the calculation.</p>
        <p>我知道判断某件事是否可行所需的所有信息，但不知道判断共和党人的孩子是否可能成为共和党人所需的所有信息。这其中有很多微妙的影响，共和党人和民主党人的孩子更有可能与父母拥有相同的政党。</p>
        <p>I know all the stuff I need to know in order to decide if something floats, but not all the stuff I need to
            know in order, for example, to decide if the child of a Republican is likely to be a Republican. There are a
            lot of subtle influences there, and it is the case that the children of Republicans and the children of
            Democrats are more likely to share the political party of their parents.</p>
        <p>但我没有任何直接的方法来计算这是否属实。在这种情况下，我能做的就是我在这里所做的，即进行一些测量，获取一些频率，对世界现状进行一些快照，并将其纳入一组概率中，这可以帮助我确定任何一位父母是否是共和党人，因为我已经观察了他们孩子的投票行为。
        </p>
        <p>But I don’t have any direct way of calculating whether that will be true or not. All I can do in that case is
            what I’ve done over here, is do some measurements, get some frequencies, take some snapshots of the way the
            world is and incorporate that into a set of probabilities that can help me determine if any given parent is
            a Republican, given that I’ve observed the voting behavior their children.</p>
        <h2 id="unknown-380">未知</h2>
        <h2>Unknown</h2>
        <p>因此，概率有其存在的意义，但它并不是我们唯一需要的工具。这是我们今天要做的所有事情的重要前言。现在，我们真的结束了，因为这个联合概率表就是全部，除了我们无法记录所有这些数字，而且猜测它们很快就会变得很麻烦。有两种方法可以考虑这一切。
        </p>
        <p>So probability has a place, but it’s not the only tool we need. And that is an important preamble to all the
            stuff we’re going to do today. Now, we’re really through, because this joint probability table is all that
            there is to it, except for the fact we can’t either record all those numbers, and it becomes quickly a pain
            to guess at them. There are two ways to think about all this.</p>
        <p>我们可以把这些概率看作是通过观察某些数据得出的概率。这是频率论者对概率的看法。或者我们可以说，我们无法进行这些测量。所以我只能编造它们。这是对这些概率来源的主观看法。在某些情况下，有些人喜欢谈论自然倾向，比如在量子力学中。
        </p>
        <p>We can think about these probabilities as probabilities that come out of looking at some data. That’s a
            frequentist view of the probabilities. Or we could say, well, we can’t do those measurements. So I can just
            make them up. That’s sort of the subjective view of where these probabilities come from. And in some cases,
            some people like to talk about natural propensities, like in quantum mechanics.</p>
        <p>但为了达到我们的目的，我们要么编造它们，要么做一些统计。问题是，我们无法处理这种表格。因此，由于无法处理这种表格，出现了一个巨大的行业，用于处理概率，而不需要编制完整的表格。这就是我们接下来要做的事情。这是我们要走的路。
        </p>
        <p>But for our purposes, we either make them up, or we do some tallying. Trouble is, we can’t deal with this
            kind of table. So as a consequence of not being able to deal with this kind of table, a gigantic industry
            has emerged for dealing with probabilities without the need to work up this full table. And that’s where
            we’re going to go for the rest of the hour. And here’s the path we’re going to take.</p>
        <h2 id="unknown-381">未知</h2>
        <h2>Unknown</h2>
        <p>我们将讨论基本概率的一些基本概述。然后我们将逐步转向所谓的信念网络，这使得它成为一种实用工具成为可能。那么让我们开始吧。第一件事是基本概率。我们称之为基本概率。基本概率。所有概率都来自少数几个公理。</p>
        <p>We’re going to talk about some basic overview of basic probability. Then we’re going to move ourselves step
            by step toward the so called belief networks, which make it possible to make this a practical tool. So let
            us begin. The first thing is basic probability. Let us say basic. And basic probability. all probability
            flows from a small number of axioms.</p>
        <p>我们知道，某些事件 a 的概率必须大于 0 且小于 1。这是第一条公理。在二元世界中，事物有为真的概率。有些事物有为假的概率。但是，真事件除了真之外，没有任何其他可能性，因此真事件的概率等于 1，假事件的概率等于
            1，假事件的概率等于 1。</p>
        <p>We have the probability of some event a has got to be greater than 0 and less than 1. That’s axiom number
            one. In a binary world, things have a probability of being true. Some have a probability of being false. But
            the true event doesn’t have any possibility of being anything other than true, so the probability of true is
            equal to 1, and the probability of false. the false event, the false condition.</p>
        <p>没有为真的可能性，所以是 0。那么概率公理的第三条就是，a 的概率加上 b 的概率减去 a 和 b 的概率等于 a 或 b 的概率。是的，这说得通，对吧？我想如果我中途不改变符号的话会更有意义。a 和 b。</p>
        <p>Has no possibility of being true, so that’s 0. Then the third of the axioms of probability is that the
            probability of a plus the probability of b minus the probability of a and b is equal to the probability of a
            or b. Yeah, that makes sense, right? I guess it would make more sense if I didn’t switch my notation in
            midstream. a and b.</p>
        <h2 id="unknown-382">未知</h2>
        <h2>Unknown</h2>
        <p>所以这些是数学家喜欢用这种方式提出的公理，他们可以由此推导出一切。但我永远不能用这种方式处理事情。我必须画一幅图，用更直观的方式思考这些东西。</p>
        <p>So those are the axioms that mathematicians love to start up that way, and they can derive everything there
            is to derive from that. But I never can deal with stuff that way. I have to draw a picture and think of this
            stuff in a more intuitionist type of way.</p>
        <p>这就是处理概率的正式方法，它反映在与空间讨论有关的直觉中，例如，我们有圆圈或区域来表示 a 和
            b。为了保持符号的一致性，我将它们改为小写。因此，你可以将它们视为可能发生这些事情的所有可能世界的空间。或者你可以将它们视为样本空间。</p>
        <p>So that’s the formal approach to dealing with probability, and it’s mirrored by intuitions that have to do
            with discussions of spaces, like so, in which we have circles, or areas, representing a and b. And to keep
            my notation consistent, I’ll make those lowercase. So you can think of those as spaces of all possible
            worlds in which these things might occur. Or you can think of them as sample spaces.</p>
        <p>但无论如何，你把 a 的概率与这个区域相对于矩形总面积的大小联系起来。宇宙。所以 a 的概率是这个圆的大小除以这张图片中这个矩形的大小。所以现在所有这些公理都有意义了。</p>
        <p>But in any event, you associate with the probability of a the size of this area here relative to the total
            area in the rectangle. the universe. So the probability of a is the size of this circle divided by the size
            of this rectangle in this picture. So now all these axioms make sense.</p>
        <h2 id="unknown-383">未知</h2>
        <h2>Unknown</h2>
        <p>确定 a 的概率就是当 a 填满整个世界，并且没有其他地方可以放置样本时，这意味着它必须是 a。所以这个概率一直上升到 1。另一方面，如果 a 的大小只是一个无穷小的点，那么落在那个世界中的几率就是
            0。这就是另一端的界限。所以这是公理一。</p>
        <p>The probability that a is certain is just when that fills up the whole thing, and there’s no other place for
            a sample to be, that means it has to be a. So that probability goes all the way up to 1. On the other hand,
            if the size of a is just an infinitesimal dot, then the chances of landing in that world is 0. That’s the
            bound on the other end. So this. axiom number one.</p>
        <p>从那幅图来看，这是有道理的。同样，公理二也是如此。公理三呢？从所有这些方面来看，这是否有意义？答案是肯定的，因为我们只需用一点彩色粉笔就可以查看这些区域。因此，a 的概率就是这里的这个区域。b
            的概率就是这里的这个区域。</p>
        <p>Makes sense in terms of that picture over there. Likewise, axiom number two. What about axiom number three?
            Does that make sense in terms of all this stuff? And the answer is, sure, because we can just look at those
            areas with a little bit of colored chalk. And so the probability of a is just this area here. The
            probability of b is this area here.</p>
        <p>如果我们想知道我们处于 a 或 b
            的概率，那么我们只需将这些面积相加即可。但是当我们将这些面积相加时，这个交点部分会被加两次。所以我们必须减去它才能使这个东西成为一个合理的方程，这样才有意义。公理三是有意义的，就像公理一和公理二一样。所以这就是基本概率的全部内容。
        </p>
        <p>And if we want to know the probability that we’re in either a or b, then we just have to add up those areas.
            But when we add up those areas, this intersection part is added in twice. So we’ve got to subtract that off
            in order to make this thing make a rational equation, so that makes sense. And axiom three makes sense, just
            as axioms one and two did. So that’s all there is to basic probability.</p>
        <h2 id="unknown-384">未知</h2>
        <h2>Unknown</h2>
        <p>现在你可以用它做各种代数运算，它很优雅，因为它就像电路理论或电磁学，因为从非常少的几个公理。在这个例子中是三个。你可以建立一个优雅的数学系统。这就是概率学科所做的。但我们不会去那里，因为我们专注于找到一个可以处理我们目前无法处理的联合概率表的点。
        </p>
        <p>And now you could do all sorts of algebra on that, and it’s elegant, because it’s like circuit theory or
            electromagnetism, because from a very small number of axioms. in this case three. you can build an elegant
            mathematical system. And that’s what probability subjects do. But we’re not going to go there, because we’re
            sort of focused on getting down to a point where we can deal with that joint probability table that we
            currently can’t deal with.</p>
        <p>因此，我们不会深入研究这些代数。我们只需要研究网络所需的代数。因此，接下来我们要研究的是条件概率。虽然这些是公理，但这是一个定义。我们说，根据定义，给定 b 的 a 的概率等于 a 和 b 的概率。</p>
        <p>So we’re not going to go into a whole lot of algebra with these things. Just what we need in order to go
            through that network. So the next thing we need to deal with is conditional probability. And whereas those
            are axioms, this is a definition. We say that the probability of a given b is equal to, by definition, the
            probability of a and b.</p>
        <p>我使用这个常用符号来表示该领域的惯例。然后我们将用该符号除以 B 的概率。你可以将其视为定义，然后它只是一点神秘的代数。或者你可以像我们上面所做的那样，采取直觉主义的方法，并询问这些东西在圆形图和某种空间中的含义。
        </p>
        <p>I’m using that common notation to mean as is conventional in the field. And then we’re going to divide that
            by the probability of B. You can take that as a definition, and then it’s just a little bit of mysterious
            algebra. Or you could do like we did up there and take an intuitionist approach and ask what that stuff
            means in terms of a circle diagram and some sort of space.</p>
        <h2 id="unknown-385">未知</h2>
        <h2>Unknown</h2>
        <p>让我们看看，这是什么意思？这意味着我们试图将 a 的概率限制在已知 b 为真的情况下。我们会这样说。我们得到了这部分，然后我们得到了 a 与 b 的交集。</p>
        <p>And let’s see, what does that mean? It means that we’re trying to restrict the probability of a to those
            circumstances where b is known to be so. And we’re going to say that. we’ve got this part here, and then
            we’ve got the intersection of a with b.</p>
        <p>因此，从定义上来说，它确实是有意义的，因为它表示，如果你得到了 b，那么你得到 a 的概率就是交集的大小。粉色和橙色部分。除以整个 b。因此，这就像我们将考虑的宇宙限制在 b
            所覆盖的原始宇宙的那部分。因此，从定义上来说，这确实是有意义的。</p>
        <p>And so it does make sense as a definition, because it says that if you’ve got b, then the probability that
            you’re going to get a is the size of that intersection. the pink and orange stuff. divided by the whole of
            b. So it’s as if we restricted the universe of consideration to just that part of the original universe as
            covered by b. So that makes sense as a definition.</p>
        <p>当然，我们可以将其改写为 a 和 b 的 P 等于给定 b 的 a 的概率乘以 b
            的概率。这些都是基本知识。现在，我们确实需要在这里做一点代数运算，因为我想考虑的不只是两种情况，但如果我们将这个空间分成三部分会怎么样？那么我们会说 a、b 和 c 的概率等于多少？</p>
        <p>And we can rewrite that, of course, as P of a and b is equal to the probability of a given b times the
            probability of b. That’s all basic stuff. Now, we do want to do a little bit of algebra here, because I want
            to consider not just two cases, but what if we divide this space up into three parts? Then we’ll say that
            the probability of a, b, and c is equal to what?</p>
        <h2 id="unknown-386">未知</h2>
        <h2>Unknown</h2>
        <p>嗯，有很多种方法可以考虑这个问题。但其中一种思考方式是，我们将宇宙限制在 b 和 c 都为真的那部分世界。因此，假设 y 等于 b 和 c。b 和 c 的交集，其中 a 和 b 都为真。</p>
        <p>Well, there are lots of ways to think about that. But one way to think about it is that we are restricting
            the universe to that part of the world where b and c are both true. So let’s say that y is equal to b and
            c.&nbsp;the intersection of b and c, where a and b are both true.</p>
        <p>然后我们可以使用这个公式来表示 a、b 和 c 的概率等于 a 和 y 的概率，等于给定 y 的概率乘以 y 的概率。然后我们可以将其展开并表示给定 b 和 c 的 P 等于概率。</p>
        <p>Then we can use this formula over here to say that probability of a, b, and c is equal to the probability of
            a and y, which is equal to the probability of a given y times the probability of y. And then we can expand
            that back out and say that P of a given b and c is equal to the probability.</p>
        <p>抱歉，乘以 y 的概率，但 y 等于 b 和 c 的概率，就像这样。啊，但等一下。我们也可以把这个想法应用到那个上，我们可以说，这整个作品等于给定 b 和 c 的 a 的概率乘以给定 c 的 b 的概率乘以 c
            的概率。&nbsp;</p>
        <p>Sorry, times the probability of y, but y is equal to the probability of b and c, like so. Ah, but wait. we
            can run this idea over that one, too, and we can say that this whole works is equal to the probability of a
            given b and c times the probability of b given c times the probability of c.&nbsp;</p>
        <h2 id="unknown-387">未知</h2>
        <h2>Unknown</h2>
        <p>现在，当我们退后一步，让这一切为我们歌唱，我们可以看到一些魔法开始在这里发生，因为我们已经把所有事情都是这样的概率分解成三个概率的乘积。前两个是条件概率，所以它们实际上都是条件概率。最后一个没有任何条件。但看看我们从左到右会发生什么。a
            依赖于两件事。</p>
        <p>And now, when we stand back and let that sing to us, we can see that some magic is beginning to happen here,
            because we’ve taken this probability of all things being so, and we’ve broken up into a product of three
            probabilities. The first two are conditional probabilities, so they’re really all conditional probabilities.
            The last one’s conditional on nothing. But look what happens as we go from left to right. a is dependent on
            two things.</p>
        <p>B 只依赖于一件事，左边没有任何事。c 不依赖于任何事情，左边没有任何事。所以你可以感觉到一个概括即将到来。让我们把它写下来。让我们从这里到这里，说一大堆事情的概率。x1 到 x10。等于概率的某个乘积。</p>
        <p>B is only dependent on one thing and nothing to the left. c is dependent on nothing and nothing to the left.
            So you can sense a generalization coming. So let’s write it down. So let’s go from here over to here and say
            that the probability of a whole bunch of things. x1 through x10. is equal to some product of probabilities.
        </p>
        <p>我们让索引 I 从 n 到 1。x 到系列中最后一个的概率，取决于所有其他的。抱歉，这是 i 的概率，I 减 1 直到 x1，像这样。对于此产品中的第一个，I 将等于 n。&nbsp;</p>
        <p>We’ll let the index I run from n to 1. Probability of x to the last one in the series, conditioned on all the
            other ones. sorry, that’s probability of i, I minus 1 down to x1 like so. And for the first one in this
            product, I will be equal to n.&nbsp;</p>
        <h2 id="unknown-388">未知</h2>
        <h2>Unknown</h2>
        <p>对于第二个，我将等于 n 减 1。但是您会注意到，当我从 n 向 1
            移动时，这些条件会变得越来越小。条件中的东西的数量会变得越来越少，而且这些东西都不在左边。它们只是我在右边的东西。所以我的意思是，所有这些东西都有一个小于这个索引的索引。</p>
        <p>For the second one, I will be equal to n minus 1. But you’ll notice that as I go from n toward 1, these
            conditionals get smaller. the number of things on condition get smaller, and none of these things are on the
            left. They’re only stuff that I have on the right. So what I mean to say is all of these things have an
            index that’s smaller than this index.</p>
        <p>那些索引更高的项都没有出现在那个条件中。所以这是一种获取一系列事物结束的概率并将其写为条件概率乘积的方法。所以我们取得了很好的进展。我们完成了一个。我们完成了两个。现在我们完成了三个，因为这是链式法则。我们已经完成了图表的一半，距离我们可以做一些有趣的事情的一半了。
        </p>
        <p>None of the ones that have a higher index are appearing in that conditional. So it’s a way of taking a
            probability of the end of a whole bunch of things and writing it as a product of conditional probabilities.
            So we’re making good progress. We’ve done one. We’ve done two. And now we’ve done three, because this is the
            chain rule. And we’re about halfway through our diagram, halfway to the point where we can do something fun.
        </p>
        <p>但是我们还有另外几个概念要处理，下一个概念是条件概率的概念。这就是上面的所有内容。哎呀。这里的所有内容都是条件概率的定义。现在我想谈谈独立性的定义。这是另一个定义问题。但这也是一个定义问题，用图表来表达也有一定的意义。所以定义是这样的。
        </p>
        <p>But we still have a couple more concepts to deal with, and the next concept is the concept of conditional
            probability. So that’s all this stuff up here. oops. All this stuff here is the definition of conditional
            probability. And now I want to go to the definition of independence. So that’s another definitional deal.
            But it’s another definitional deal that makes some sense with a diagram as well. So the definition goes like
            this.</p>
        <h2 id="unknown-389">未知</h2>
        <h2>Unknown</h2>
        <p>如果 a 独立于 b，我们说给定 b 的 a 的 P 等于 a 的 P。也就是说，a 的概率不取决于 b 的情况。不管哪种情况都一样。所以它是独立的。b
            无关紧要。那么如果我们尝试制作一个直觉图，它会是什么样子？好吧，让我们看看。这是 a。这是 b。现在，给定 b 的 a 的概率。好吧，让我们看看。</p>
        <p>We say that P of a given b is equal to P of a if a independent of b. So that says that the probability of a
            doesn’t depend on what’s going on with b. It’s the same either way. So it’s independent. b doesn’t matter.
            So what does that look like if we try to do an intuitionist diagram? Well, let’s see. Here’s a. Here’s b.
            Now, the probability of a given b. well, let’s see.</p>
        <p>那一定是用这里的这个部分除以这里的这个部分。所以这些面积的比率就是给定 b 的 a 的概率。所以这是这种方式的概率除以两种方式的概率。那么，就这些面积而言，a 的概率是多少？嗯，就这些面积而言，a
            的概率就是概率。让我们看看，我说得对吗？我搞反了。</p>
        <p>That must be this part here divided by this part here. So the ratio of those areas is the probability of a
            given b. So that’s the probability of this way divided by the probability of both ways. So what’s the
            probability of a in terms of these areas? Well, probability of a in terms of these areas is the probability.
            let’s see, have I got this right? I’ve got this upside down.</p>
        <p>给定 b 时 a 的概率是交集处事物的概率。所以这是双向的。除以 b 中事物的概率，也就是这个方向。让我们看看，除了存在于这个宇宙中之外，不受任何条件影响的 a
            的概率就是所有这些井号，像这样，除以宇宙。所以当我们说某事是独立的时，这意味着这两个比率是相同的。</p>
        <p>The probability of a given b is the probability of the stuff in the intersection. so that’s both ways.
            divided by the probability of the stuff in b, which is going this way. And let’s see, the probability of a
            not conditioned on anything except being in this universe is all these hash marks, like so, divided by the
            universe. So when we say that something’s independent, it means that those two ratios are the same.</p>
        <h2 id="unknown-390">未知</h2>
        <h2>Unknown</h2>
        <p>从直觉主义者的角度来看，这就是它的全部含义。因此，它表示，这里的这个小区域除以整个区域，等于整个区域除以宇宙的大小。这就是独立性的含义。现在，这需要做大量的工作。但我们还没有完成独立性，因为我们要处理条件独立性。这也可以看作是一个定义。
        </p>
        <p>That’s all it means in the intuitionist’s point of view. So it says that this little area here divided by
            this whole area is the same as this whole area for a divided by the size of the universe. So that’s what
            independence means. Now, that’s quite a lot of work. But we’re not done with independence, because we’ve got
            conditional independence to deal with. And that, too, can be viewed as a definition.</p>
        <p>我们要说的是，给定 b 和 z，a 的概率等于给定 z 的概率。这是什么意思？这意味着如果你知道我们处理的是 z，那么 a 的概率就不依赖于 b。一旦你被限制在 z 中，b 就不再重要了。所以你可以这样看。这是
            a，这是 b，这是 z。</p>
        <p>And what we’re going to say is that the probability of a given b and z is equal to the probability of a given
            z. What’s that mean? That means that if you know that we’re dealing with z, then the probability of a
            doesn’t depend on b. b doesn’t matter anymore once you’re restricted to being in z. So you can look at that
            this way. Here’s a, and here’s b, and here is z.</p>
        <p>所以，我们说的是，我们将世界限制在 z 所在的宇宙部分。因此，给定 b 和 z 的概率是这里的这部分。给定 b 和 z 是那里的部分。给定 z 的概率是这里的这部分除以全部 z。</p>
        <p>So what we’re saying is that we’re restricting the world to being in this part of the universe where z is. So
            the probability of a given b and z is this piece in here. a given b and z is that part there. And the
            probability of a given z is this part here divided by all of z.</p>
        <h2 id="unknown-391">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们说的是，这里的这个小块与这个部分的比率，我会这样标记，这个与这个的比率与那个与那个的比率相同。所以这是条件独立性。因此，你可以从这些东西中推断出，用一点代数知识，给定 z 的 a 和 b 的 P 等于给定 z
            的 a 的 P 乘以 z 中 b 的 P。</p>
        <p>So we’re saying that the ratio of this little piece here to this part, which I’ll mark that way, ratio of
            this to this is the same as the ratio of that to that. So that’s conditional independence. So you can infer
            from these things, with a little bit of algebra, that P of a and b given z is equal to P of a given z times
            P of b in z.</p>
        <p>伙计，这真是一段漫长的旅程，但我们已经完成了一、二、三、四和五。现在，下一个话题是信念网络，我要请你们暂时忘记我所说的一切。然后我们再回来讨论。我想再次讨论狗、窃贼和浣熊。</p>
        <p>Boy, that’s been quite a journey, but we got all the way through one, two, three, four, and five. And now the
            next thing is belief nets, and I’m going to ask you to forget everything I’ve said for a minute or two. And
            we’ll come back to it. I want to talk about the dog and the burglar and the raccoon again.</p>
        <p>现在，抛开概率不谈，我可以说，看，如果浣熊出现，狗就会叫。如果窃贼出现，狗就会叫。窃贼不会出现是因为狗在叫。浣熊不会出现是因为狗在叫。因此，因果关系从窃贼和浣熊流向吠叫。因此，我们可以画一个图表。我们的图表将如下所示。
        </p>
        <p>And now, forgetting about probability, I can say, look, the dog barks if a raccoon shows up. The dog barks if
            a burglar shows up. A burglar doesn’t show up because the dog is barking. A raccoon doesn’t show up because
            the dog is barking. So the causality flows from the burglar and the raccoon to the barking. So we can make a
            diagram of that. And our diagram will look like this.</p>
        <h2 id="unknown-392">未知</h2>
        <h2>Unknown</h2>
        <p>这是窃贼，这是浣熊。它们与狗叫有因果关系。所以这是一个有趣的想法，因为现在我可以这么说。好吧，我现在还不能说什么，因为我想给它增加一点复杂性。我要再添加两个变量。你可能会报警，这取决于狗叫得有多厉害，我想。</p>
        <p>Here is the burglar, and here is the raccoon. And these have causal relations to the dog barking. So that’s
            an interesting idea, because now I can say that. well, I can’t say anything yet, because I want to add a
            little more complexity to it. I’m going to add two more variables. You might call the police, depending on
            how vigorous the dog is barking, I guess.</p>
        <p>浣熊有撞翻垃圾桶的倾向。所以现在，我有五个变量。我需要多大的联合概率表才能让我的计数保持准确？嗯，是 2 的 5 次方。也就是
            32。但我要说的是，这个图是一个声明，它中的每个节点都依赖于它的父节点，而不是其他不是后代的节点。</p>
        <p>And the raccoon has a propensity to knocking over the trash can. So now, I’ve got five variables. How big a
            joint probability table am I going to need to keep my tallies straight? Well, it’ll be 2 to the 5th. That’s
            32. But what I’m going to say is that this diagram is a statement, that every node in it depends on its
            parents and nothing else that’s not a descendant.</p>
        <p>现在，我需要说大约 50
            次，因为你必须说对。那里的每个节点都独立于除其父节点之外的每个非后代节点。不，这不完全正确。给定其父节点，每个节点都独立于所有其他非后代节点。那么，这是什么意思呢？这就是报警的情况。这是它的唯一父节点。</p>
        <p>Now, I need to say that about 50 times, because you’ve got to say it right. Every node there is independent
            of every non descendant other then its parents. No, that’s not quite right. Given its parents, every node is
            independent of all other non descendants. Well, what does that mean? Here’s the deal with calling the
            police. Here’s its one and only parent.</p>
        <h2 id="unknown-393">未知</h2>
        <h2>Unknown</h2>
        <p>因此，考虑到这位家长，他们报警的概率并不取决于 B、R 或 T 之类的因素。这是因为所有因果关系都通过这只狗叫声来流动。我不会以除了狗叫与否之外的任何其他因素来报警。</p>
        <p>So given this parent, the probability that they were going to call the police doesn’t depend on anything like
            B, R, or T. It’s because all of the causality is flowing through this dog barking. I’m not going to call the
            police in a way that’s dependent on anything else other than whether the dog is barking or not.</p>
        <p>因为这个家伙有这个作为父母，而这些不是报警的后代，所以这与 B、R 和 T
            无关。所以让我们来看看其他的。这是狗。狗的父母是汉堡包和浣熊。所以狗出现的概率与那边的垃圾桶无关，因为那不是后代。它取决于这些父母。垃圾桶怎么样？</p>
        <p>Because this guy has this as a parent, and these are not descendants of calling the police, so this is
            independent of B, R, and T. So let’s go walk through the others. Here’s the dog. The dog’s parents are
            burger appearing and raccoon appearing. So the probability that the dog appears is independent of that trash
            can over there, because that’s not a descendant. It is dependent on these parents. How about the trash can?
        </p>
        <p>它只依赖于浣熊。它不依赖于任何其他非后代，因此，它不依赖于 D、B 或 P。B 呢？它没有父母。所以它不依赖于任何其他东西，因为其他所有东西要么是非后代，因为 B 不依赖于 R 和 T，因为它们不是后代。有趣的是，B
            可能依赖于 D 和 P，因为它们是后代。</p>
        <p>It depends only on the raccoon. It doesn’t depend on any other non descendant, so therefore, it doesn’t
            depend on D, B, or P. How about B? It has no parents. So it depends on nothing else, because everything else
            is either a non descendant, because B does not dependent on R and T, because they’re not descendants. It’s
            interesting that B might depend on D and P, because those are descendants.</p>
        <h2 id="unknown-394">未知</h2>
        <h2>Unknown</h2>
        <p>因此，重要的是要理解，考虑到所有其他非后代的父母，存在着独立性。你马上就会明白为什么这种有趣、奇怪的语言很重要。但现在，让我们看看。我想建立一个模型来说明这里会发生什么。所以让我看看我需要弄清楚什么样的概率。这个家伙不依赖任何上游的东西。
        </p>
        <p>So it’s important to understand that there’s this business of independence given the parents of all other non
            descendants. And you’ll see why that funny, strange language is important in a minute. But now, let’s see. I
            want to make a model of what’s going to happen here. So let me see what kind of probabilities I’m going to
            have to figure out. This guy doesn’t depend on anything upstream.</p>
        <p>因此，我们可以说，我们所需要的只是小偷出现的概率。假设这是一个犯罪率相当高的社区。10 分之一的概率。每 10 天中就有 1 天，小偷就会出现。浣熊除了自己的倾向之外，不依赖于任何其他因素，因此，我们可以说它的概率是
            0.5。浣熊喜欢这个地方，所以它每两天就会出现 1 天。那么狗叫呢？</p>
        <p>So we could just say that all we need there is the probability that a burglar is going to appear. Let’s say
            it’s a fairly high crime neighborhood. 1 chance in 10. 1 day in 10, a burglar appears. The raccoon doesn’t
            depend on anything other than its own propensity, so its probability, we’ll say, is 0.5. Raccoons love the
            place, so it shows up about 1 day in 2. So what about the dog barking?</p>
        <p>这取决于是否有小偷，另一个父节点是是否有浣熊。所以我们需要跟踪这四种组合中狗叫的概率。所以这将是小偷，这将是浣熊。这将是假，假，真，真。哦。假，假，真，假，假，真，真，真。所以假设这是一只很棒的狗，如果有小偷，它就会叫。
        </p>
        <p>That depends on whether there’s a burglar, and the other parent is whether there’s a raccoon. So we need to
            keep track of the probability that the dog will bark for all four combinations. So this will be the burglar,
            and this will be the raccoon. This will be false, false, true, true. oops. false, false, true, false, false,
            true, true, true. So let’s say it’s a wonderful dog, and it always barks if there’s a burglar.</p>
        <h2 id="unknown-395">未知</h2>
        <h2>Unknown</h2>
        <p>所以这里的概率是 1.0，这里的概率是 1.0。如果没有小偷也没有浣熊，狗仍然喜欢吠叫，只是为了好玩。所以我们说这个概率是十分之一。如果有小偷，我们假设。没有小偷，但有浣熊。他厌倦了浣熊，所以他只有一半的时间吠叫。
        </p>
        <p>So that would say that the probability here is 1.0, and the probability here is 1.0. And if there’s neither a
            burglar nor a raccoon, the dog still likes to bark just for fun. So we’ll say that’s a chance of 1 in 10.
            And then in case there’s a burglar, let’s say this. There’s no burglar, but there is a raccoon. he’s tired
            of the raccoons, so he only barks half the time.</p>
        <p>顺便问一下，这些数字加起来必须等于 1 吗？显然不是。这些数字加起来不等于 1。加起来等于 1 的是狗叫的概率。然后另一个幻影概率就在这里。</p>
        <p>Do these numbers, by the way, have to add up to 1? They clearly don’t. These numbers don’t add up to one.
            What adds up to 1 is this is the probability that the dog barks. And then the other phantom probability is
            out here.</p>
        <p>这些数字加起来必须等于 1。所以那将是 0.9，那将是 0.0，那将是 0.5，而这将是 0.0。因为这些只是 1 减去这些列中的数字，所以我懒得把它们写下来。好吧，我们还有几件事要做。我们报警的概率只取决于狗。
        </p>
        <p>And these have to add up to 1. So that would be 0.9, that would be 0.0, that would be 0.5, and this would be
            0.0. So because those are just 1 minus the numbers in these columns, I don’t bother to write them down.
            Well, we still have a couple more things to do. The probability that we’ll call the police depends only on
            the dog.</p>
        <h2 id="unknown-396">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们将有一列表示狗，然后我们将有一个报警的概率。该概率为假，也为真。因此，如果狗不叫，我们报警的可能性就很小。因此，将该概率设为
            0,0.1。如果狗叫，并且叫得足够响亮，那么报警的概率可能只有十分之一。这里，我们有垃圾桶。</p>
        <p>So we’ll have a column for the dog, and then we’ll have a probability of calling the police. There’s a
            probability for that being false and a probability for that being true. So if the dog doesn’t bark, there’s
            really hardly any chance we’ll call the police. So make that 0,0.1. If the dog is barking, if he barks
            vigorously enough, maybe 1 chance in 10. Here, we have the trash can.</p>
        <p>我们需要考虑的最后一件事是垃圾桶；更确切地说，是浣熊。这是垃圾桶的概率。取决于浣熊是否存在。如果没有浣熊，垃圾桶被风吹倒的概率是千分之一。如果浣熊在，天哪，那家伙总是喜欢进去，所以这个概率是
            0.8。现在我完成了这个模型的指定。</p>
        <p>The final thing we have to think about. There’s the trash can; rather, the raccoon. And here’s the trash can
            probability. Depends on the raccoon being either present or not present. If the raccoon is not present, the
            probability the trash can is knocked over by, say, the wind is 1 in 1,000. If the raccoon is there, oh man,
            that guy always likes to go in there, so that’s 0.8. So now I’m done specifying this model.</p>
        <p>问题是，我必须指定多少个数字？好吧，让我们看看。我必须指定那个，那个，那个，那个，那个，那个。那是 6、7、8、9、10。所以我必须指定 10 个数字。如果我直接尝试构建一个联合概率表，我必须提供多少个数字？好吧，是
            2 的 n 次方。所以是 2 的 5 次方，也就是 32。节省了不少。</p>
        <p>And the question is, how many numbers did I have to specify? Well, let’s see. I have to specify that one,
            that one, that one, that one, that one, that one. that’s 6,7.8,9.10. So I had to specify 10 numbers. If I
            just try to build myself a joint probability table straightaway, how many numbers would I have to supply?
            Well, it’s 2 to the n.&nbsp;So it’s 2 to the 5th, that’s 32. Considerable saving.</p>
        <h2 id="unknown-397">未知</h2>
        <h2>Unknown</h2>
        <p>顺便问一下，你认为我是如何制作这张表格的？不是通过计算所有这些数字。而是通过建立这个信念网络，然后使用信念网络来计算这些数字。这就是为什么这是一个奇迹，因为有了这些数字，我就可以计算这些数字，而不是编造它们或进行大量的计数式测量。所以我想确保这是真的。
        </p>
        <p>By the way, how do you suppose I made that table? Not by doing all those numbers. By making this belief
            network and then using the belief network to calculate those numbers. And that’s why this is a miracle,
            because with these numbers, I can calculate those numbers instead of making them up or making a whole lot of
            tally type measurements. So I’d like to make sure that’s true.</p>
        <p>我可以使用这些东西来计算完整的联合概率表。这就是它的工作原理。我有一些组合的概率。假设是警察、狗、窃贼、垃圾桶和浣熊。所有可能的组合都会在表中给我一个条目。一行。但让我们看看。这里有一些奇迹。哦，这个链式法则。让我们使用链式法则。
        </p>
        <p>And I can use this stuff here to calculate the full joint probability table. So here’s how this works. I have
            the probability of some combination. let’s say the police, the dog, the burglar, the trash can, and the
            raccoon. All the combinations that are possible there will give me an entry in the table. one row. But let’s
            see. there’s some miracle here. Oh, this chain rule. Let’s use the chain rule.</p>
        <p>我们可以将其写成给定 d、b、t 和 r 时报警的概率。然后，我链中的下一个是给定 b、t 和 r 时 d 的概率。然后，我链中的下一个是给定 t 和 r 时 b 的概率。我链中的下一个是给定 r 时 t 的概率
            P。我链中的最后一个是 r 的概率 p。</p>
        <p>We can write that as a probability that we call the police given d, b, t, and r. And then the next one in my
            chain is probability of d given b, t, and r. Then the next one in the chain is the probability of b given t
            and r. And the next one in my chain is P of t given r. And the final one in my chain is p of r.</p>
        <h2 id="unknown-398">未知</h2>
        <h2>Unknown</h2>
        <p>现在，我们也有一些条件独立性知识，不是吗？我们知道这里的概率仅取决于 d，因为没有后代。因此，我们不必考虑这一点，这里我们需要的所有数字都由这个表生成。这里的这个怎么样？狗叫的概率仅取决于它的父母 b 和
            r，因此它不取决于 t。因此，b 反过来取决于。</p>
        <p>Now, we have some conditional independence knowledge, too, don’t we? We know that this probability here
            depends only on d because there are no descendants. So therefore, we don’t have to think about that, and all
            the numbers we need here are produced by this table. How about this one here? Probability that the dog barks
            depends only on its parents, b and r, so it doesn’t depend on t. So b, in turn, depends on.</p>
        <p>它依赖于什么？它不依赖于任何东西。所以我们可以忽略它们。给定 r 的 t 的概率，是的，那里有一个概率，但我们可以从表中得到。最后是 P 或
            r。这就是为什么我要研究所有这些概率垃圾，因为如果我们按照从下到上的扩展顺序排列事物，那么我们就可以排列事物，这样这些家伙在这个公式中都不依赖于后代。</p>
        <p>What does it depend on? It doesn’t depend on anything. So we can scratch those. Probability of t given r,
            yeah, there’s a probability there, but we can get that from the table. And finally, P or r. So that’s why I
            went through all that probability junk, because if we arrange things in the expansion of this, from bottom
            to top, then we arrange things so that none of these guys depends on a descendant in this formula.</p>
        <p>而且它所依赖的东西数量有限。这就是我们计算完整联合概率表的方法。今天的讨论到此结束。但我们要考虑的是，我们能从中真正节省多少钱？</p>
        <p>And we have a limited number of things that it depends on above it. So that’s the way we can calculate back
            the full joint probability table. And that brings us to the end of the discussion today. But the thing we’re
            going to think about is, how much saving do we really get out of this?</p>
        <h2 id="unknown-399">未知</h2>
        <h2>Unknown</h2>
        <p>在这个特殊情况下，我们只需要从 32 个数字中选出 10 个。如果我们有 10 处房产或 100 处房产会怎么样？那么我们可以节省多少钱？这是我们下次周三测验后要讨论的问题。</p>
        <p>In this particular case, we only had to devise 10 numbers out of 32. What if we had 10 properties or 100
            properties? How much saving would we get then? That’s what we’ll take up next time, after the quiz on
            Wednesday.</p>
        <h1 id="probabilistic-inference-ii">22. 概率推理 II</h1>
        <h1>22. Probabilistic Inference II</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 48:46 <a
                href="https://youtube.com/watch?v=EC6bf8JCpDQ">https://youtube.com/watch?v=EC6bf8JCpDQ</a></p>
        <p> 11 years ago (Jan 11, 2014) — 48:46 <a
                href="https://youtube.com/watch?v=EC6bf8JCpDQ">https://youtube.com/watch?v=EC6bf8JCpDQ</a></p>
        <h2 id="introduction-6">介绍</h2>
        <h2>Introduction</h2>
        <p>帕特里克·温斯顿教授：我这周大部分时间都在华盛顿寻找黄金。另一个副作用是，我忘了安排鲍勃·贝里克代替我做周四的朗诵。我可能会为此下地狱。无论如何，我们有很多解释，但没有一个是好的。但今天我们会试着回到正题上，你会学到一些有趣的东西。
        </p>
        <p>PROFESSOR PATRICK WINSTON: I was in Washington for most of the week prospecting for gold. Another byproduct
            of that was that I forgot to arrange a substitute Bob Berwick for the Thursday recitations. I shall probably
            go to hell for this. In any event, we have many explanations, none of them good. But today we’ll try to get
            back on track and you’ll learn something fun.</p>
        <p>特别是，你们将了解到我的研究生马克和暑期 UROP 学生 Brett van
            Zuiden（你们中的一员）如何成功完成了一项壮举，并在这两个描述中识别出我们人类通常称之为“复仇”的模式。它被发现了。当然，系统没有给它起名字。它只是知道那里有一个模式，并等待我们给它命名。</p>
        <p>In particular you will learn how a graduate student of mine Mark together with a summer UROP student, Brett
            van Zuiden, one of you. managed to pull off a tour de force and recognize in these two descriptions the
            pattern that we humans commonly call “revenge.” It was discovered. The system didn’t have a name for it, of
            course. It just knew that there was a pattern there and sat waiting for us to give a name to it.</p>
        <p>这就是我们最终要达到的目标。但要达到这个目标还需要一段路程，因为我们必须把大纲上的所有内容都过一遍。特别是，我们想从一点点回顾开始。因为上次做的一些事情很快就过去了。</p>
        <p>That’s where we’re going to end up. But it’ll be a bit of a journey before we get there, because we’ve got to
            go through all that stuff on the outline. And in particular, we want to start off by a little tiny bit of
            review. Because some of the stuff we did last time went by pretty fast.</p>
        <p>特别是，你可能还记得他们有一张很棒的联合概率表，它告诉我们所有我们想知道的信息。我们可以通过点击相应的框来决定在给定这个和那个等所有这些东西的情况下，警察被叫来的概率是多少。问题是，天哪，那里只有三个变量。</p>
        <p>In particular, you may remember they had this wonderful joint probability table, which tells us all we want
            to know, all we want to know. We can decide what the probability of the police being called is given the
            this and the that, and all that sort of stuff, by clicking the appropriate boxes. The trouble is, gee, there
            are only three variables there.</p>
        <p>当变量很多时，编制这些数字甚至收集这些数字就会变得非常困难。</p>
        <p>And when there are lots of variables it gets pretty hard to make up those numbers or to even collect them.
        </p>
        <h2 id="inference-nets">推理网络</h2>
        <h2>Inference Nets</h2>
        <p>所以我们不得不寻找替代方案。一周前节目结束时，我们找到了这个替代方案。我们开始定义这些推理网络，有时也被称为“贝叶斯网络”。我们研究的推理网络是这样的。</p>
        <p>So we’re driven to an alternative. And we got to that alternative just at the end of the show a week ago. And
            we got to the point where we were defining these inference nets, sometimes called “Bayes nets.” And the one
            we worked with looked like this.</p>
        <p>有小偷、浣熊、狗叫、警察被叫、垃圾桶被翻倒。所以变量比这还多。那个只有三个。这个有五个。但我们能够用它做一些神奇的事情，因为作为人类，当我们定义时。当我们画这张图时，我们会断言事物如何相互依赖或不相互依赖。</p>
        <p>There’s a burglar, a raccoon, the possibility of a dog barking, the police being called, and a trash can
            being overturned. So more variables than that. That only has three. This has got five. But we’re able to do
            some magic with this because we, as humans, when we define. when we draw this graph we’re making an
            assertion about how things depend or don’t depend on one another.</p>
        <p>尤其是，有些东西需要分解并记住，直到脱口而出。那就是，我认为这个图上的任何变量都独立于给定其父母的任何其他非后代。独立于给定其父母的任何非后代。所以这意味着，给定其父母，狗叫的概率不依赖于 T，即垃圾桶被翻倒。</p>
        <p>In particular, there’s something to break down and memorize to the point where it rolls off your tongue. And
            that is that any variable on this graph is said by me to be independent of any other non descendant given
            its parents. Independent of any non descendant given its parents. So that means that the probability of the
            dog barking, given its parents, doesn’t depend on T, the trash can being overturned.</p>
        <p>因为直觉告诉我们，所有的因果关系都通过父母流动，如果不通过父母，就无法到达这个变量
            D。所以这就是我们绘制的网格的属性。我们倾向于以一种反映因果关系的方式来绘制它们。所以这往往是有意义的。所以不知何故，这个东西会这样。我们将用这个东西代替那个东西。</p>
        <p>Because the intuition is all of the causality is flowing through the parents and can’t get to this variable D
            without going through the parents. So that is property of the nets that we draw. And we tend to draw them in
            a way that reflects causality. So it tends to make sense. So somehow this thing is going to be. we’re going
            to use this thing instead of that thing.</p>
        <h2 id="chain-rule">链式法则</h2>
        <h2>Chain Rule</h2>
        <p>但请稍等一下。</p>
        <p>But wait.</p>
        <p>我们可能需要那个东西来完成我们想要执行的所有计算。所以我们需要能够证明我们可以通过对这个东西进行计算来得到那个东西。那么该怎么办呢？好吧，我们将使用链式法则。请记住，链式法则是通过概率的基本公理、定义和一些彩色粉笔传给我们的。
        </p>
        <p>We may need that thing in order to do all the computations we want to perform. So we need to be able to show
            that we can get to that thing by doing calculations on this thing. So what to do? Well, we’re going to use
            the chain rule. And remember that the chain rule came to us by way of the basic Axioms of Probability plus
            the definition plus a little colored chalk.</p>
        <p>所以上一次我们到了某种程度，我们有点相信这一点。这真是一件神奇的事情。它说所有这些事情同时发生的概率是一系列条件概率的乘积。这个乘积中的条件概率是这样安排的：第一个人依赖于其他所有人。第二个人不依赖于第一个人，但依赖于其他一切。
        </p>
        <p>So we got to the point last time where we sort of believed this. It’s a really magical thing. It says that
            the probability of all this stuff happening together is given as the product of a bunch of conditional
            probabilities. And the conditional probabilities in this product are arranged such that this first guy
            depends on everybody else. The second guy doesn’t depend on the first guy but depends on everything else.
        </p>
        <p>因此，依赖关系列表会随着您往下走而变得越来越小，直到它只依赖于一件事。根本没有条件。所以这会帮到我们，因为它使我们能够从这里的计算转到整个表格。但首先我必须慢慢地向您展示这是如何实现的。</p>
        <p>So that list of dependencies gets smaller and smaller as you go down here until it depends only one thing.
            There’s no conditional at all. So that’s going to come to our rescue because it enables us to go from
            calculations in here to that whole table. But first I have to show you a little bit more slowly how that
            comes to be.</p>
        <p>在考虑概率之前，我要做的一件事是，我要列出所有这些变量的线性列表。我要这样做的方式是从底部开始逐一处理这些变量。我利用了这些网络的一个非常重要的特性。那就是没有循环。你可以按照任何方式跟随箭头，这样你就可以回到自己身上。
        </p>
        <p>One thing I’m going to do before I think about probability is I’m going to make a linear list of all these
            variables. And the way I’m going to make it is I’m going to chew away at those variables from the bottom.
            I’ve taken advantage of a very important property of these nets. And that is there no loops. You can follow
            the arrows in any way so as you get back to yourself.</p>
        <p>所以总会有一个底部。所以我要说的是，好吧，这里有两个底部，分别是 C 和 T。所以我有一个选择。我要选择 C。所以我要把它拿掉，假装它不再存在。然后我要拿走这个。它现在是一个底部，因为它下面什么都没有。我已经把 C
            拿走了。所以我们接下来要把它拿走。</p>
        <p>So there’s always going to be a bottom. So what I’m going to do is I’m going to say, well, there are two
            bottoms here, there’s C and T. So I have a choice. I’m going to choose C. So I’m going to take that off and
            pretend it’s not there anymore. Then I’m going to take this guy. That’s now a bottom because there’s nothing
            below it. I’ve already taken C out. So we’ll take that out next.</p>
        <p>现在我有了这个家伙、这个家伙和这个家伙。这个家伙下面不再有任何东西了。所以我可以把它列在下一个。现在这里有浣熊和垃圾桶。但是垃圾桶在最下面。所以我必须把它列在下一个，因为我是从下往上工作的。我想确保这个列表中没有在我之前的后代。所以最后我到了浣熊。
        </p>
        <p>And now I’ve got this guy, this guy, and this guy. This guy no longer has anything below it. So I can list it
            next. Now over here I’ve got raccoon and trashcan. But trashcan is at the bottom. So I’ve got to take it
            next because I’m working from the bottom up. I want to ensure that there are no descendants before me in
            this list. So finally I get to raccoon.</p>
        <p>因此，我构建此列表的方式确保此列表排列元素，以便对于任何特定元素，其后代都不会出现在其左侧。现在，这就是我想要使用链式法则的神奇顺序。所以现在我可以写了。我可以选择 C ​​作为我的变量 n。&nbsp;</p>
        <p>So the way I constructed this list like so ensures that this list arranges the elements so that for any
            particular element, none of its descendants appear to its left. And now that’s the magical order for which I
            want to use the chain rule. So now I can write. I can pick C to be my variable n.&nbsp;</p>
        <p>我可以说，链式法则表明，所有这些变量 P（C、D、B、T 和 R）的联合概率。这些变量的任何特定组合的概率都等于在所有其他变量的情况下 C 的概率。在所有其他变量的情况下，下一个是 D。在所有其他变量的情况下，下一个是
            T。在所有其他变量的情况下，下一个是 B。在所有其他变量的情况下，下一个是 T。最后，就是 R。</p>
        <p>And I can say that the chain rule says that the joint probability of all these variables P of C, D, B, T, and
            R. the probability of any particular combination of those things is equal to the probability of C given
            everybody else. Next in line is D given everybody else. Next in line is T. next in line is B given everybody
            else. And next in line is T given everybody else. And finally, just R.</p>
        <p>因此，这些事物的组合具有由该链式法则表达式给出的概率。啊。但是首先，这些表达式都不会将任何变量作为非后代以外的任何条件，好吗？这只是因为我排列变量的方式。我总是可以这样做，因为没有循环。我总是可以咀嚼底部。</p>
        <p>So this combination of things has a probability that is given by this chain rule expression. Ah. But first of
            all, none of those expressions condition any of the variables on anything other than non descendants, all
            right? That’s just because of the way I’ve arranged the variables. And I can always do that because are no
            loops. I can always chew away at the bottom.</p>
        <p>这确保了每当我写一个变量时，它都会受到其后代以外的其他东西的制约。因此，这些条件概率中的任何一个变量都是非后代。哦，等等。当我画出这个图时，我断言没有变量依赖于给定其父代的任何非后代。所以如果我知道一个变量的父代，我就知道这个变量与所有其他非后代无关。好吗？
        </p>
        <p>That ensures that whenever I write a variable, it’s going to be conditioned on stuff other than its
            descendants. So all of these variables in any of these conditional probabilities are non descendants. Oh
            wait. When I drew this diagram, I asserted that no variable depends on any non descendant given its parents.
            So if I know the parents of a variable I know that the variable is independent of all other non descendants.
            All right?</p>
        <p>现在我可以开始删除一些东西了。好吧，让我们看看。我知道从我的图表来看，C 只有一个父节点，即 D。因此，给定它的父节点，它就独立于所有其他非后代。所以我可以将它们删除。D 有两个父节点，即 B 和
            R。但考虑到这一点，我可以删除任何其他非后代。B 取决于 T 和 R。啊，但 B 没有父节点。因此，它实际上独立于这两个节点。</p>
        <p>Now I can start scratching stuff out. Well, let’s see. I know that C, from my diagram, has only one parent,
            D. So given its parent, it’s independent of all other non descendants. So I can scratch them out. D he has
            two parents, B and R. But given that, I can scratch out any other non descendant. B is conditional on T and
            R. Ah, but B has no parent. So it actually is independent of those two guys.</p>
        <p>是的，垃圾桶取决于 R。而这里的 R
            是链中的最后一个元素，它只是一个概率。所以现在我有办法计算该表中的任何条目，因为该表中的任何条目都是所有这些变量的值的组合。瞧。所以原则上，我能用表格做的任何事，我都可以用这个小网络做。好吗？</p>
        <p>The trashcan, yeah, that’s dependent on R. And R over here, the final thing in the chain, that’s just a
            probability. So now I have a way of calculating any entry in that table because any entry in that table is
            going to be some combination of values for all those variables. Voila. So anything I can do with a table, I
            can do in principle with this little network. OK?</p>
        <p>但现在的问题是，我有一些概率需要在这里计算出来。所以让我画一个稍微不同的版本。所以这里我们得到了 B 的先验概率。好吧，这只是 B 的概率。下面对于狗，我有一个更大的表格，因为我得到了取决于其父母值的概率。</p>
        <p>But now the question is, I’ve got some probabilities I’m going to have to figure out here. So let me draw a
            slightly different version of it. So up here we’ve got the a priori probability of B. Well, that’s just
            probability of B. Down here with the dog, I’ve got a bigger table because I’ve got probabilities that depend
            on the values of its parents.</p>
        <p>狗叫的概率取决于父母的状况，而不是其他因素。让我们看看。我必须有一列 B。我必须有一列小偷和浣熊。对于这两个家伙，有很多可能性。但是一旦我得到这些，我就能计算出狗叫的概率。所以有两个这样的变量。所以有四种组合。有 T。
        </p>
        <p>The probability of dog barking depends on the condition of the parents, nothing else. So let’s see. I’ve got
            to have a column for B. I’ve got to have a column for the burglar and the raccoon. And there are a bunch of
            possibilities for those guys. But once I get those then I’ll be able to calculate the probability of the dog
            barking. So there are two of these variables. So there are four combinations. There’s T.</p>
        <p>有 TR、RT，还有。哇哦，我在干什么？醒醒！T 为假。假为真。还有假。所以我真正想做的是计算所有这些概率，这些概率给出了窃贼和浣熊的狗条件的概率。同样，我想计算 B
            发生的概率，它不依赖于任何其他因素。所以我不知道该怎么做。</p>
        <p>There’s T R, R T, and. whoa, what am I doing? Wake up! T false. False true. And false. So what I really want
            to do is I want to calculate all of these probabilities that give the probability of the dog condition of
            the burglar and the raccoon. Similarly, I want to calculate the probability of B happening doesn’t depend on
            anything else. So I don’t know what to do.</p>
        <p>好吧，我实际上要做的是做我在上面必须做的事情。我要跟踪。</p>
        <p>Well, what I’m going to actually do is I’m going to do the same thing I had to do up there. I’m going to keep
            track of.</p>
        <h2 id="data-elements">数据元素</h2>
        <h2>Data Elements</h2>
        <p>我要尝试很多。我要收集一些数据。也许我会做很多实验。也许有人会把它交给我。</p>
        <p>I’m going to try a bunch of. I’m going to get myself together a bunch of data. Maybe I do a bunch of
            experiments. Maybe somebody hands it to me.</p>
        <p>但我要用这些数据来构建一堆计数，这些计数最终会给我所有这些事情的概率。所以我不知道，让我们看看。我们应该怎么开始？第一步，找到彩色粉笔。第二步，我要稍微扩展一下这些表格，这样我就可以跟踪计数了。所以这将是所有最终出现在特定行中的计数。
        </p>
        <p>But I’m going to use that data to construct a bunch of tallies which are going to end up giving me the
            probabilities for all of those things. So I don’t know, let’s see. How should we start? Step one, find
            colored chalk. Step two, I’m going to extend these tables a little bit so I can keep track of the tallies.
            So this is going to be all the ones that end up in a particular row.</p>
        <p>这些就是 dog 为真的那些。同样，我要把这个家伙延伸到这里，以便跟踪一些计数。这些就是 B 为真的那些。而这一个就是全部。这就是我的设置。现在假设我的第一个实验轰轰烈烈地到来了。而且全是 T。所以我有 T。</p>
        <p>And these are going to be the ones for which dog is true. Similarly, I’m going to extend this guy up here in
            order to keep track of some tallies. This is going to be the ones for which B is true. And this one will be
            all. So that’s my set up. And now suppose that my first experiment comes roaring in. And it’s all T’s. So I
            have T.</p>
        <p>这是我的第一个实验结果，我的第一个数据项。让我们看看。这里的排列是窃贼、浣熊、狗。所以窃贼为真。并且其中有一个计数。同样，T，即窃贼和浣熊，这让我回到第一行。所以这给了我一个计数，狗为真，所以这给了我一个勾号。好吗？到目前为止你明白了吗？
        </p>
        <p>That’s my first experimental result, my first data item. So let’s see. The arrangement here is burglar,
            raccoon, dog. So burglar as a true. And there’s one tally count in there. Likewise, the T, that’s the
            burglar and the raccoon, that brings me down to this first row. So that gives me one tally in there and dog
            is true so that gives me a tick mark in that one. All right? Are you with me so far?</p>
        <p>现在让我们假设接下来发生的事情全都是假的。那么，窃贼就是假的。但是有一个实验。每个人都是假的。所以我们来谈谈假。这就是我们要研究的行。我们在那里得到一个计数。我们要在这里放一个吗？不，因为那是假的。狗是假的。这就是我们的数据元素所说的。这很酷。也许再来一个。假设我们有
            T F。</p>
        <p>And now let’s suppose that the next thing happens be all false. Well, burglar is false. But there is one
            experiment. Everybody’s false. So we come down here to false. And that’s the row we’re going to work on. We
            get a tally in there. Do we put one in here? No, because that’s false. Dog is false. That’s what our data
            element says. So that’s cool. Maybe one more. Let’s suppose we have T F.</p>
        <p>好吧，在这种情况下，我们在这里有一个勾号，在这里也有一个勾号，因为 burglar 元素为真。然后我们有 T。这又把我们带到了第一行。所以我们在那里得到了一个勾号。但是 dog
            为假，所以那里没有勾号。这就是它的工作原理。我想你想看演示，对吧？总是喜欢看演示。所以它实际上看起来是这样的。</p>
        <p>Well in that case, we have a tick mark here and a tick mark here because the burglar element is true. Then we
            have T. That brings us to the first row again. So we get a tick mark there. But dog as false, so no tick
            mark there. That’s how it works. I suppose you’d like to see a demonstration, right? Always like to see a
            demonstration. So here’s what it actually looks like.</p>
        <h2 id="simulations">模拟</h2>
        <h2>Simulations</h2>
        <p>因此，在左侧，您可以看到我们构建的网络，其中包含一堆概率。我现在要做的是开始模拟，以便积累刻度标记、计数标记，并查看它们为表格指示了哪些类型的概率。我碰巧使用了一个过程，左侧的模型是该过程的正确反映。因此，有一个模拟。所以狗叫了。
        </p>
        <p>So on the left you see the network as we’ve constructed it, with a bunch probabilities there. And what I’m
            going to do now is I’m going to start simulating away so as to accumulate tick marks, tally marks, and see
            what kinds of probabilities that they indicate for the table. I happen to be using a process for which the
            model on the left is a correct reflection. So there’s one simulation. So the dog barking.</p>
        <p>让我们看看，窃贼是假的。浣熊是真的。我得到一个勾号。所以有 1 的概率。当然，我不会只选一个。我想把一大堆东西放进去。所以我会再运行一堆模拟。不，我这里甚至还没有 TF 的条目。那是因为我没有运行足够的数据。</p>
        <p>Let’s see, the burglar is false. The raccoon is true. I get one tick mark. So the probability there is one.
            Of course, I’m not going to just go with one. I want to put a whole bunch of stuff in there. So I’ll just
            run a bunch more simulations. No I don’t even have an entry at all yet for T F here. That’s because I
            haven’t run enough data.</p>
        <p>所以让我把它清除掉，而不是一次一个地做。让我运行 100 次模拟。看，它仍然不太好。因为它说这个 T
            概率为真。这只是因为我给它输入了数据，对吧？我正在跟踪数据元素告诉我特定组合出现的频率。是的，学生：所以当你做一个模拟时，那是变量吗？</p>
        <p>So let me clear it instead of doing it one at a time. Let me run 100 simulations. See, it’s still not too
            good. Because it says this T probability true. This just because I’m feeding it data, right? And I’m keeping
            track of what the data elements tell me about how frequently a particular combination appears. Yes, STUDENT:
            So when you’re doing one simulation, is that variables?</p>
        <p>帕特里克·温斯顿教授：当我进行一次模拟时，我只是在每个表中跟踪该组合。因为它会告诉我一些关于我想在这些表中反映的概率的信息。所以当我在这里看到窃贼时，很容易看出。如果我有很多数据元素，它们都会告诉我一些关于窃贼以及其他变量的信息。
        </p>
        <p>PROFESSOR PATRICK WINSTON: When I’m doing one simulation, I’m just keeping track of that combination in each
            of these tables. Because it’s going to tell me something about the probabilities that I want reflected in
            those tables. So it’s pretty easy to see when I go up here to burglar. If I have a lot of data elements,
            they’re all going to tell me something about the burglar as well as the other variables.</p>
        <p>所以，如果我只看那个窃贼的事情，它在所有数据元素中为真的时间分数就是它的概率。所以现在当我转到联合表时，我仍然可以得到这些概率数字。但现在它们取决于其父母的网状条件。这就是我得到这些概率的方法。</p>
        <p>So if I just look at that burglar thing, the fraction of time that it turns out true over all the data
            elements is going to be its probability. So now when I go down to the joint tables, I can still get these
            probability numbers. But now they’re conditioned on reticular condition of its parents. So that’s how I get
            these probabilities.</p>
        <p>所以我在这里做得不是很好，因为那个 T 组合给了我过高的概率。所以也许 100 次模拟还不够。让我们运行 10,000 次。所以通过这么多数据，我得到的概率。</p>
        <p>So I didn’t do too well here because that T combination gave me an excessively high probability. So maybe 100
            simulations isn’t enough. Let’s run 10,000. So with that much data running through, the probabilities I get.
        </p>
        <p>让我们看看，我这里得到的是 893，而不是 0.9，807 而不是 0.8，607 而不是 0.6。这个数字正好是
            0.01。因此，如果我运行足够多的这些模拟，我就能很好地知道，假设我得到了一个正确的模型，那么概率应该是多少。好的，这样就解决了这个问题。当然，我没有在这里画出其他的东西。但通过扩展，你可以看到它们是如何工作的。哦。
        </p>
        <p>Let’s see, I’ve got 893 here, instead of 0.9,807 instead of 0.8,607 instead of 0.6. And that one’s dead on at
            0.01. So if I run enough of these simulations, I get a pretty good idea what the probabilities ought to be
            given that I’ve got a correct model. OK, so that takes care of that one. And of course, I didn’t draw the
            other things in here. But by extension, you can see how those would work. Oh.</p>
        <p>但你知道吗？我想我会在这里放一点浣熊概率表。因为接下来我想做的是反过来。这是从某个过程中重新编码计数，以便我可以开发一个模型。但是，一旦我得到了这些概率，我就可以开始模拟模型会做什么。好吗？我该怎么做呢？</p>
        <p>But you know what? I think I will put a little probability of raccoon table in here. Because the next thing I
            want to do is I want to go the other way. This is recoding tallies from some process so I can develop a
            model. But once I’ve got these probabilities, of course, then I can start to simulate what the model would
            do. All right? How would I do that?</p>
        <p>那么，我想使用同一张表吗？我认为，为了保持整洁，我会再来一遍。这是 B。它有 B 的概率。这是 R。这是 R 的概率表。这归结为狗的联合表。它有四个元素。根据窃贼条件和浣熊条件，我们得到狗的概率。</p>
        <p>Well, do I want to use the same table? I think just to keep things sanitary, what I’ll do is I’ll go over
            here and do it again. Here’s B. It’s got a probability of B. Here’s R. Here’s a table probability of R. That
            comes down into a joint table for dog. And it’s got four elements. Depending on the burglar condition and
            the raccoon condition, we get a probability of dog.</p>
        <p>现在，假设这些都已填好。那么，如果我想模拟这个系统为所有变量生成一些值的组合，我该怎么做呢？好吧，我做了与我使用这个链式法则时相反的事情，表明我可以从表格中找到这些概率。现在我得到了概率。我要朝另一个方向走。</p>
        <p>And now, imagine these have all been filled in. So what do I want to do if I want to simulate this system
            generating some combination of values for all the variables? Well, I do the opposite of what I did when I
            was working around with this chain rule showing that I could go from the table to those probabilities. Now
            I’ve got the probabilities. I’m going to go the other direction.</p>
        <p>我不会从底部开始，而是从顶部开始。因为当我进入顶部并开始咀嚼时，抛硬币所需的所有信息都在那里。因此，具体来说，当我到这里时，我现在知道了盗窃的概率。所以我将使用该概率来抛硬币。假设它产生 T。这样就解决了这个家伙。
        </p>
        <p>Instead of chewing away from the bottom, I’m going to chew away from the top. Because when I go into the top
            and chew way, everything I need to know to do a coin flip is there. So in particular, when I go up in here,
            I’ve got the probability of burglar now. So I’m going to use that probability to flip a coin. Say it
            produces a T. So that takes care of this guy.</p>
        <p>现在我可以把它划掉，因为它不再被考虑。它不再是首要变量。所以现在我转到浣熊，做同样的事情。我取这个概率。我翻转。假设它产生
            F。无论它的概率是多少，我都会抛出一枚有偏差的硬币，这就是我碰巧得到的结果。但现在，处理完这两个家伙后，这揭示了这只狗的事情。</p>
        <p>And I can now scratch it off since it’s no longer in consideration. It’s no longer a top variable. So now I
            go over into raccoon and I do the same thing. I take this probability. I do a flip. And say it produces an
            F. Whatever its probability is, I flip a biased coin and that’s what I happen to get. But now, having dealt
            with these two guys, that uncovers this dog thing.</p>
        <p>而且我已经获得了足够的信息，因为我已经完成了上述所有操作，可以计算出狗是否会吠叫。但是等一下。我必须知道我有 T、T、T、F、F、T、F、F。因为我必须选择正确的行。所以我知道 B 是 T。我知道 R 是 F。</p>
        <p>And I’ve got enough information, because I’ve done everything above, to make the calculation for whether to
            dog is going to be barking or not. But wait. I have to know that I’ve got a T and a T and a T and an F and
            an F and a T and an F and an F. Because I have to select the right row. So I know that B is T. And I know
            that R is F.</p>
        <p>这样我就进入了表格的第二行。现在我得到了这个概率。我抛硬币，得到一些结果，比如
            T。瞧。我可以用其他两个变量来做到这一点。我自己进行了一次实验，该实验是根据表格中的概率进行的。好吗？当然。是的，事实上，我是如何得到这些数字的？</p>
        <p>So that takes me into the table into the second row. So now I get this probability. I flip that coin and I
            get some result, say, T. Voila. I can do that with the other two variables. And I’ve got myself an
            experimental trial that is produced in accordance with the probabilities of the table. OK? Of course. yeah,
            in fact, how did I get those numbers?</p>
        <p>实际上我所做的是使用左侧的模型来生成用于计算右侧概率的样本。所以您已经看到了这个演示。当然。我不知道，所有这些都取决于一切都正确。我写了一些东西，想再写一次。窃贼、浣熊、狗、报警、垃圾桶。但其他人可能会说，哦，你全都错了。
        </p>
        <p>Actually what I did is I used the model on the left to generate the samples that were used to compute the
            probabilities on the right. So you’ve seen that a demonstration of this already. Now of course. I don’t
            know, all of this sort of depends on having everything right. I’ve written a thing to write it one more
            time. Burglar, raccoon, dog, call the police, trashcan. But somebody else may say, oh, you’ve got it all
            wrong.</p>
        <p>这才是它的真实样子。狗根本不关心浣熊。所以这是一个正确的模型。现在当我进行模拟时，我可以填写任一模型中的表格，对吗？我相信你会想看一个演示。所以让我给你展示一下。</p>
        <p>This is what it really looks like. The dog doesn’t care about the raccoon at all. So that’s a correct model.
            Now when I do a simulation, I could fill in the tables in either model, right? I’m sure you’d like to see a
            demonstration. So let me show you a demonstration of that.</p>
        <h2 id="bayesian-inference">贝叶斯推理</h2>
        <h2>Bayesian Inference</h2>
        <p>所以有两张表。我也可以对它们进行 10,000 次模拟。现在，看看。</p>
        <p>So there are the two tables. And I can run 10,000 simulations on those guys, too. Now, look.</p>
        <p>左边的模型很好地反映了我用来生成数据的模型中的概率。但右边的模型并不知道更好的方法。它只是填写自己的表格。那么该怎么办？我说这个是正确的模型。你说那个是正确的模型。谁是对的？也许我们永远不会知道。</p>
        <p>The guy on the left is a pretty good reflection of the probabilities in a model I used to produce the data.
            But the guy on the right doesn’t know any better. it just fills in its own tables, too. So what to do? I say
            this one’s the right model. And you say that one’s the right model. Who’s right? Maybe we’ll never know.</p>
        <p>左边的人会在股市上发财，而右边的人会破产。如果我们真的能弄清楚谁是对的，那就太好了。那么你想知道如何找出谁是对的吗？是的，我也是。我们要做的是研究朴素贝叶斯推理。这是我们的下一个任务。所以它是这样运作的。</p>
        <p>And the guy on the left will get rich in the stock market and the guy on the right will go broke. I would be
            nice if we could actually figure out who’s right. So would you to see how to figure out who’s right? Yeah,
            so would I. What we’re going to do is we’re going to look at naive Bayesian inference. And that’s our next
            chore. So here’s how it works.</p>
        <p>我们知道，根据条件概率的定义，我们知道给定 B 时 A 的概率等于 A 和 B 的概率除以 B 的概率，对吗？根据定义等于。所以这意味着给定 B 时 A 的概率乘以 B 的概率。我只是把它乘以。它等于联合概率。</p>
        <p>We know, from the definition of conditional probability, we know that the probability of A given B is equal
            to the probability of A and B divided by the probability of B, right? Equal to by definition. So that means
            that the probability of A given B times the probability of B. I’m just multiplying it out. it equal to that
            joint probability.</p>
        <p>哦，但是根据对称性，我可以反过来说，给定 A 的概率乘以 B 的概率，B 的概率也等于联合概率，这样说也没什么坏处，对吧？我只是用不同的对称方式扩展了它。如果我必须在 B 上写 a、b，在 A 上写
            b、a。谢谢。谁在抱怨？干得好。那将是一场大联盟灾难。</p>
        <p>Oh, but by symmetry, there’s no harm in saying I can turn that around and say that the probability of B given
            A times the probability of B is also equal to that joint probability, right? I’ve just expanded it a
            different and symmetric way. If I’ve got to write a, b on B, b, a on A. Thank you. Who was complaining? Good
            work. That would have been a major league disaster.</p>
        <p>但现在，写完这些，我可以忘掉中间部分了。因为我真正感兴趣的是我如何在那个条件中改变概率。我为什么要关心这样做呢？顺便说一句，我们现在谈论的是贝叶斯牧师的工作。</p>
        <p>But now, having written that, I can forget about the middle. Because all I’m really interested in is how I’ve
            turned the probabilities around in that conditional. Why would I care about doing that? By the way, we’re
            now talking about the work of the Reverend Bayes.</p>
        <p>因为我们可以再次重写这个公式，即给定 B 时 A 的概率等于给定 A 时 B 的概率乘以 A 的概率除以 B
            的概率。这只是初等代数。但现在我要做一些神奇的事情。我要说我有一个分类问题。我想知道你得了哪种病。这是一个分类问题。也许你得了猪流感。也许你得了消化不良。谁知道呢。</p>
        <p>Because we can rewrite this yet again as the probability of A given B is equal to the probability of B given
            A times the probability of A divided by the probability of B. That’s just elementary algebra. But now I’m
            going to do something magical. I’m going to say I’ve got a classification problem. I want to know which
            disease you have. That’s a classification problem. Maybe you’ve got the swine flu. Maybe you’ve got
            indigestion. Who knows.</p>
        <p>但是我得到了所有这些症状。我得到了所有这些证据。你发烧了。你呕吐。哦，好吧，我们就不说得太详细了。但我要做的是，假设 A 等于我感兴趣的一个类，即你患的疾病。而 B 等于证据，即我观察到的症状。好了。</p>
        <p>But I get all these symptoms. I get all these pieces of evidence. You’ve got a fever. You’re throwing. oh,
            well, let’s not go into too much detail, there. But what I’m going to do is I’m going to say, well, let’s
            suppose that A is equal to a class that I’m interested in, the disease you’ve got. And B is equal to the
            evidence, the symptoms I observe. Voila.</p>
        <p>我可能很难确定给定证据的类别的概率。但给定类别，确定证据的概率可能并不那么难。让我再拿一块板子来告诉你我的意思。</p>
        <p>I may have a pretty hard time figuring out what the probability of the class is given the evidence. But
            figuring out the probability of the evidence given the class might not be so hard. Let me get another board
            in play and show you what I mean.</p>
        <p>通过将类别和证据代入贝叶斯规则，我得到的结果是，给定证据的某个类别的概率等于给定该类别的证据概率乘以该类别的概率除以证据的概率。现在你必须让它对你有一点启发。假设我有几个类别需要决定。我试图从这批类别中选出最好的一个。
        </p>
        <p>By plugging class and evidence into Bayes’ rule, what I get is the probability of some class given the
            evidence is equal to the probability of the evidence given the class times the probability of the class
            divided by the probability of the evidence. Now you’ve got to let that sing to you a little bit. Suppose
            I’ve got several classes that I’m trying to decide between. I’m trying to select the best out of that batch
            of classes.</p>
        <p>好了，我得到了证据。如果我知道每个类别的证据概率，并且我事先知道类别的初始概率，那么我就完成了。因为我得到了分子中的两个元素。为什么我完成了？因为分母对于所有类别都相同。它只是证据的概率。然后我可以把所有东西加起来。我知道它加起来是
            1。</p>
        <p>Well, I’ve got the evidence. And if I know the probability of the evidence given each of those classes, and
            if I know, a priori, the initial probability the class, then I’m done. Because I’ve got the two elements in
            the numerator. Why am I done? Because the denominator is the same for all the classes. It’s just the
            probability of the evidence. And then I could just sum everything up. I know it adds to 1 anyway.</p>
        <p>这很酷。但有时有证据。实际上有多个证据。假设有某个类。某个 i，我们试图找出它是否是正确的类。所以我们有 c sub I 和 c sub I。假设证据实际上是一堆证据。所以它可能是 e sub 1、e sub
            n，哎呀，右括号过早。</p>
        <p>So that’s cool. But sometimes there’s evidence. actually there’s more than one piece of evidence. Let’s say
            that there’s some class. some i, and we’re trying to figure out if that’s the correct class. So we’ve got c
            sub I there and c sub I there. And suppose that evidence is actually a bunch of pieces of evidence. So it
            could be e sub 1, e sub n, oops, premature right bracket.</p>
        <p>所有这些证据，给定 I 类乘以 I 类除以某个分母的概率，我们不关心这个分母，因为它对每个人都一样。所以我们就把它写成 d。&nbsp;</p>
        <p>All that evidence, given the class I times the probability of the class I over some denominator that we don’t
            care about because it’s going to be the same for everybody. So we’ll just write that as d.&nbsp;</p>
        <h2 id="independent-inference">独立推理</h2>
        <h2>Independent Inference</h2>
        <p>现在，如果这些证据在给定类别的情况下都是独立的，会怎么样呢？所以，如果你得了猪流感，你发烧的概率与你呕吐的概率是独立的。</p>
        <p>Now what if these pieces of evidence are all independent given the class? So if you have the swine flu, the
            probability you have a fever is independent of the probability you’re going to throw up, say.</p>
        <p>那么我们能用另一种方式来写吗？更简单的方式？当然可以。因为当事物独立时，联合概率等于各个概率的乘积。也就是说，写下来比直接说出来更容易看出来。</p>
        <p>Then can we write this another way? An easier way? Sure. Because when things are independent, the joint
            probability is equal to the product of the individual probabilities. So that is to say. it’s easier to see
            it if you write it down than if you just say it.</p>
        <p>这里这两个元素的概率等于在 c sub I 条件下 e sub 1 的概率乘以在 c sub i 条件下 e sub 2 的概率，一直到在 c sub I 条件下 e sub n 的概率除以我们不关心的某个分母。
        </p>
        <p>This probability here from these two elements here is equal to the probability of e sub 1 conditioned on c
            sub I times the probability of e sub 2 conditioned on c sub i, all the way down to the probability of e sub
            n conditioned on c sub I divided by some denominator we don’t care about.</p>
        <p>瞧，我要尝试做的是，我要遍历所有的 ci，看看哪一个是最大的。学生：那是
            ci，对吧？帕特里克·温斯顿教授：这是概率。学生：右边帕特里克·温斯顿教授：就是这里？哦，是的，你说得很对。哦，是的，谢谢。我不能一边写一边思考。谢谢。好的。所以我只是弄清楚了其中哪一个是最大的。</p>
        <p>See, what I’m going to try to do is I’m going to go through this for all the ci and see which one’s the
            biggest. STUDENT: That’s the ci, right? PROFESSOR PATRICK WINSTON: This is the probability of. STUDENT:
            right hand side PROFESSOR PATRICK WINSTON: Right here? Oh yes, you’re quite right. Oh yeah, thanks. I can’t
            write and think at the same time. Thanks. OK. So I’ve just figure out which one of these is the biggest.</p>
        <p>我已经确定了类别。现在你对我说，好吧，我想看一个例子。那么。我不知道，有人有零钱吗？五美分硬币，四分之一硬币。这不是因为麻省理工学院的加薪幅度极低。我只需要它来做个演示。我需要两枚硬币。别忘了把它们拿回来，我倾向于这样做。现在假设这两枚硬币不完全相同。
        </p>
        <p>And I’ve identified the class. Now you say to me, well, I would like to see an example. So. I don’t know,
            does anyone have any spare change? A nickel, a quarter. This is not because of infinitesimally low raises
            here at MIT. I just need it for a demonstration. I need two coins. Don’t forget to get these back, I tend to
            be. Now suppose these two coins are not exactly the same.</p>
        <p>其中一枚是真正的、价值连城的美国 25 美分硬币。另一枚是假的。而这一枚硬币正面朝上的概率，假设是 0.8，而不是
            0.5。所以我把它们都混合起来。我选了一枚。我开始抛硬币。我得到了正面。然后我又抛了一枚。我得到了反面。我选了哪枚硬币？好吧，我们将用这些东西来算出结果。</p>
        <p>One of these points is a legitimate, highly prized American quarter. The other one is a fake. And with this
            one, the probability of heads, let us say, is 0.8 instead of 0.5. So I mix these all up. And I pick one. And
            I start flipping it. And I get a head. Then I flip it again. And I get a tail. Which coin did I pick? Well,
            we’re going to use this stuff to figure it out.</p>
        <p>事情是这样的。在我忘记之前。非常感谢。所以我们所做的就是从我的手中选择这些东西。我不会画手。所以我在这里画一个小杯子。这里有两枚硬币。我们要选一枚。一枚正面朝上的概率是 0.8。这枚正面朝上的概率是
            0.5。所以这是抽签。我选一枚。</p>
        <p>Here’s what happens. Before I forget. Thank you very much. So what we’ve done is we’ve selected these things
            from my hands. And I can’t draw hands. So I’ll draw a little cup here. And there are two coins in here. And
            we’re going to pick one. And one has a probability of heads equal to 0.8. And this one has a probability of
            a head of 0.5. So here’s the draw. I pick one.</p>
        <p>每个硬币的概率都是 0.5。这个硬币正面朝上的概率是 0.8。这个硬币正面朝上的概率是 0.5。好吗？现在假设第一次抛硬币的结果为
            T。那么，这就是证据。这就是给出类的证据。那么，在抽出这枚有偏差的硬币的情况下，出现反面的概率。</p>
        <p>Each has a probability of 0.5. This one is the one with the 0.8 as the probability of head. And this one is
            the one with the probability of 0.5 as a head. OK? So now suppose the first flips as it was is T. Well,
            that’s a piece of evidence. That’s here. Probably of evidence given the class. Well in the case of having
            drawn this biased coin, the probability of coming up with a tail.</p>
        <p>啊，假设是正面，这样我的数字就简单多了。假设是正面，那么出现正面的概率是 0.8。假设是一枚公平硬币，那么出现正面的概率是 0.5。所以现在如果我们拿下一枚硬币，假设是反面，那么在有证据的情况下出现反面的概率是
            0.2。假设是反面，那么出现正面的概率是 0.8。</p>
        <p>Ah, let’s say a head, just to make my numbers a little easier. Probability of coming out there with a head is
            equal 0.8 given that it’s up here in this choice. The probability given that you have a fair coin is 0.5. So
            now if we take the next coin and take it to be a tail then the probability of this guy given that evidence
            is 0.2. And the probability of this guy given that evidence.</p>
        <p>这是一枚公平的硬币，所以它不在乎。它仍然是 0.5。那么现在，根据这个证据，这个类别的概率是多少？它是 0.5 乘以 0.8 乘以 0.2 的乘积。这个家伙的概率是多少？它是 05 乘以 0.5 乘以
            0.5，除以分母，分母在两种情况下都相同。所以让我们忘记这里的早期 0.5。因为在两种情况下它都相同。我们只需将这些数字相乘即可。</p>
        <p>It’s a fair coin, so it doesn’t care. It’s still 0.5. So now what’s the probability of this class given this
            evidence? It’s the product 0.5 times 0.8 times 0.2. And what’s the probability of this guy? It’s 05 times
            0.5 times 0.5, divided by a denominator which is the same in both cases. So let’s forget about this early
            0.5 here. Because it’s the same in both cases. And we just multiply those numbers together.</p>
        <p>这样我们就得到了 0.8 乘以 0.2。那是多少？0.16？还有这个，0.5 乘以 0.5，就是 0.25。所以看起来非常像。通过这种组合。我选的硬币是公平的。再抛一次？让我们再抛一次，假设我们得到正面。那么这里就是
            0.8。这里就是 0.5。将它们相乘就是 0.125。这是 0.128。所以它们差不多相等。</p>
        <p>That gives us 0.8 times 0.2. What’s that? 0.16? And this guy, 0.5 times 0.5, that’s 0.25. So it looks an
            awful lot like. with this combination. that I’ve picked the coin that’s fair. One more flip? So let’s flip
            it again, and suppose we come up with a head. So that puts a 0.8 in here. And 0.5 in here. When you multiply
            those out that’s 0.125. And this is 0.128. So it’s about equal.</p>
        <p>你明白这是怎么回事了吗？好吧。所以我们用硬币翻转作为证据，找出涉及哪个类别。好吧，我不知道，你可能也想看看这个演示，对吧？你会对我说，天哪，只有两种硬币。这不是很有趣。让我们试试五种硬币。所以我想向你展示的是所有这些硬币的概率。
        </p>
        <p>So you see how that works? All right. So we’re using the coin flips as evidence to figure out which class is
            involved. OK so I don’t know, you’d probably like to see a demonstration of this, too, right? You say to me,
            gosh, just two kinds of coins. That’s not very interesting. Let’s try five kinds of coins. So what I want to
            show you is how the probabilities for all these coins.</p>
        <p>一共有五个，用颜色编码。概率如何随着一系列的抛掷而变化。假设我掷出了正面。顺便说一下，灰线是正面的百分比。所以这将是 1。因为我只掷正面。你看到那条黑线在上升吗？应该看起来像火箭。这就是概率。</p>
        <p>There are five of them, color coded. how the probabilities vary with a series of flips. Let’s suppose I’ve
            got a head. the grey line, by the way, is the fraction of heads. so that’s going to be one. Because I’m just
            doing heads. You see that black line rising? Should look like a rocket. That’s the probability that the.</p>
        <p>那是只出现正面的硬币，正面的概率是 1。而我在这里抛出了很多正面。这不是很酷吗？现在如果我突然抛出反面会发生什么？顺便说一句，你肯定会在这里看到最左边的一个。P=0 硬币的初始概率是
            0.1。只要我抛出正面，概率就会降到 0。而且它永远不会降到 0，对吧？这很有道理。</p>
        <p>That’s the coin which only shows heads, the probability of head is 1. And I’m flipping a whole bunch of heads
            here. Isn’t that cool? Now what happens if I suddenly put in a tail? By the way, you’ll no doubt, here one
            the extreme left. the initial probability of the P=0 coin was 0.1. As soon as I flipped a head that went to
            0. And it will never get off 0, right? That makes sense.</p>
        <p>因为如果掷出正面的概率是 1，那么你永远也不会看到反面。如果你看到反面，那不是你的硬币。如果我打断一系列正面，然后掷出反面，会发生什么？学生：帕特里克·温斯顿教授：那是什么？学生：帕特里克·温斯顿教授：黑色的硬币会变成
            0。还会发生什么？顺便说一句，蓝色的硬币是掷出正面的概率最高的硬币。砰！</p>
        <p>Because if the probability that you’ll get a head is 1 you should never see a tail. If you ever do, that
            isn’t your coin. What happens now if I interrupt a series of heads and produce a tail? STUDENT: PROFESSOR
            PATRICK WINSTON: What’s that? STUDENT: PROFESSOR PATRICK WINSTON:. The black one will go to 0. What else
            happens? By the way, the blue one is the one with the highest probability of being a head. Boom!</p>
        <p>那个蓝色的硬币猛涨。不是慢慢涨的。而是猛涨。因为现在所有正面的硬币都表明，我抛硬币的概率是 0.75，倾向于正面。所以让我们来澄清一下。选择你想要的任何概率。0.25、0.5 等等。我不知道，我们选
            0.25，因为我们已经处于上限。</p>
        <p>That blue one shot up. Not going up slowly. It shot up. Because now the preponderance of evidence with all
            those heads is that I’ve flipped the coin with a bias of 0.75 towards heads. So let’s clear this. Pick any
            probability you want. 0.25,0.5, and so on. I don’t know, let’s pick 0.25 since we’ve been at the upper end.
        </p>
        <p>所以橙色是 0.25。果然，在最初的不规则之后，我选择 0.5 硬币的概率不断上升。大数定律开始发挥作用。我选择 0.25 硬币的概率非常接近
            1。好吧。这很酷。现在你会对我说，这真是太好了，但别这样。真是太好了，但不太符合现实世界。</p>
        <p>So orange is 0.25. And sure enough, the probability that I’ve selected the 0.5 coin is going up and up and up
            and up after the original irregularity. The Law of Large Numbers is setting in. And a probability that I’ve
            got that 0.25 coin in play is pretty close to 1. All right. So that’s cool. Now you say to me, that’s
            awfully nice but stop. Awfully nice, but not very real world ish.</p>
        <p>那么让我再给你一个问题。众所周知，你很可能与父母有相同的政治观点。所以如果我想知道父母属于哪个政党，我可以看看他们的孩子属于哪个政党，对吧？所以这就像抛硬币一样。我选择的特定硬币对应于父母。每次抛硬币对应于孩子所属的政党。
        </p>
        <p>So let me give you another problem. It’s well known that you are, with high probability, of the same
            political persuasion as your parents. So if I wanted to figure out which party a parent belongs to, I could
            look at the party that their children belong to, right? So it’s just like flipping coins. The particular
            coin I have chosen corresponds to the parent. Individual flips correspond to the political party that the
            child belongs to.</p>
        <p>所以让我们稍微振作起来。顺便说一句，我周末写了所有这些内容。所以谁知道其中是否有用。但让我们看看。父党分类器。这就是民主党和共和党。现在给出的共和党先验是
            0.5。但我不知道，这是一个有点民主党的州。所以让我们稍微调低一点。</p>
        <p>So let’s get up a little bit. by the way, I wrote all this stuff over the weekend. So who knows if any of it
            will work. But let’s see. A parent party classifier. There it is, Democrats and Republicans. And now the
            prior for being a Republican given here is 0.5. But I don’t know, this is a little bit Democratic state. So
            let’s adjust that down a little bit.</p>
        <p>某个地方可能差不多，但为了便于课堂演示，我们先往下看。现在仪表显示的是先验概率，因为到目前为止，这是公式中唯一的东西。我没有证据。现在我们假设第一个孩子是共和党人。回到中立。所以我得到的是父母的概率很低。</p>
        <p>Somewhere in there might be about right But let’s just, for the sake of a classroom illustration, go down
            here. So now the meter is showing the prior probability because that’s the only thing in the formula so far.
            I’ve got no evidence. So now let’s suppose that child number one is a Republican. Back to neutral. So I’ve
            got a low probability that the parent.</p>
        <p>父母是共和党人且孩子是共和党人的先验概率。我注意到 0.2 和 0.8，条件概率是 0.8。先验概率是
            0.2。这就是为什么结果会相互平衡，对吧？所以现在如果我们再有一位共和党人，这个概率就会大幅上升。如果我有一个民主党孩子，这个概率就会下降。</p>
        <p>A priori probability that the parent is a Republican and a child who’s a Republican. I notice that 0.2 and
            0.8, the conditional is 0.8. And the prior is 0.2. That’s why it comes out to balance each other, right? So
            now if we get another Republican in there it goes way up. If I have a Democratic child it goes back down.
        </p>
        <p>如果我的孩子数量均衡，那么由于先前的概率很低，这个数字就会大幅下降。所以如果我把这个数字调高，即使孩子数量均衡，我成为共和党人的概率仍然很高。现在让我们看看。如果我把那里的滑块（条件概率）移到左边。让我把它均衡。让我们把它变成一件事。我不知道。
        </p>
        <p>If I have an equal balance between children then it goes way back down because of that prior probability
            being low. So if I make that high, even though the children are balanced, I’m still going to have a high
            probability of being a Republican. Now let’s see. If I take that slider there, the conditional probability,
            and drive it to the left here. let me make that equally in. And let’s make that one thing. I don’t know.</p>
        <p>我现在在做什么？如果我让概率小于
            0.5，那意味着什么？这意味着你对父母感到不满，你想加入另一个政党。好吧，那么现在，下一步是什么？哦天哪。下一步是什么？这就是下一步。某个地方的下一步是什么？是的，这就是下一步。这里是。我们有两个模型。还记得我说过我们想在它们之间做出选择吗？
        </p>
        <p>What am I doing now? If I make the probability less than 0.5, what’s that mean? That means you’re sore at
            your parents and you want to belong to a different party. All right, so now, what’s next? Oh gosh. What’s
            next? This is what’s next. What’s next to somewhere? Yeah, this is what’s next. This here. We’ve got two
            models. Remember when I said we wanted to decide between them?</p>
        <p>我们可以使用贝叶斯算法来做到这一点吗？当然。因为我们有这两个模型。我们在它们中得到了概率。所以现在我可以获取我的数据并计算给定数据的左模型的概率和给定数据的右模型的概率，将其乘以它们的先验概率，我假设它们是相等的。
        </p>
        <p>Can we use that Bayesian hack to do that, too? Sure. Because we’ve got these two models. We’ve got the
            probabilities in them. So now I can take my data and calculate the probability of a left model given the
            data and the probability of the right model given the data, multiply that times their a priori
            probabilities, which I’ll assume are equal.</p>
        <p>然后我可以进行模型选择，这与我之前暗示的完全不同。让我们试试吧。哇哦。这是我的两个模型。是的，它们在那里。我们已经对它们进行了训练。它们得到了概率。现在我们要做的就是使用原始模型来模拟数据。</p>
        <p>Then I can do a model selection deal much in defiance to what I was hinting at before. so let’s try that.
            Whoa. There are my two models. Yes, there they are. We’ve already trained them up. And they’ve got their
            probabilities. Now what we’re going to do is we’re going to use the original model to simulate the data.</p>
        <p>因此，我们要做的就是使用一个类似于左侧的模型来模拟抽奖、模拟事件以及所有变量的类似组合，该模型就是左侧的模型，只是概率略有不同，好吗？然后，我们将进行贝叶斯分析，看看仪表的走向。因此，我们将运行一个数据点。哎呀，走错了方向。这让我很紧张。
        </p>
        <p>So what we’re going to do is we’re going to simulate draws, simulate events, similarly combinations of all
            variables using a model that looks like the one on the left, that is the one on the left except for the
            slight differences in probabilities, OK? Then we’re going to do this Bayesian thing and see where the meter
            goes. So we’ll run one data point. Oops, went the wrong way. Makes me nervous.</p>
        <p>我刚刚在 9:15 完成。可能有个错误。哎呀，两个数据点，向左摆动。三个数据点，又回到右边。当然，数据不多。所以让我们再输入一些数据。是的。砰，成功了。让我们再试一次。太酷了。让我们运行 1,000
            次模拟和一个数据点。它稍微晃动了一下，然后向左平移。</p>
        <p>I just finished this at 9:15. Maybe there’s a bug. Oops, two data points, swings to the left. Three data
            points, back to the right. Of course that’s not much data. So let’s put some more data in. Yeah. Boom, there
            it goes. Let’s try that again. That was cool. So let’s run 1,000 simulations and one data point. It bobbles
            around a little bit and goes flat over to the left.</p>
        <p>因为这个模型反映了数据生成的模型。所以现在我们有了贝叶斯分类，只不过现在分类又向前迈了一步，变成了结构发现。我们有两种结构选择。我们可以用这个贝叶斯方法决定这两种结构中哪一种最好。这难道不酷吗？好吧，如果你能做什么就太酷了？所以如果你有两个选择。
        </p>
        <p>Because that is the model that reflects the one that the data is generated from. So now we got Bayesian
            classification, except now the classification has gone one step more and it becomes structure discovery.
            We’ve got two choices of structure. And we can use this Bayesian thing to decide which of the two structures
            is best. Isn’t that cool? Well, it’s only cool if you could do what? So if you had two choices.</p>
        <p>您可以在它们之间进行选择，并挑选出最好的一个。但是有。天哪，对于这么多变量，有很多不同的网络满足无循环标准，并且没有太多的父节点。它们的数量非常多。事实上，如果您将此网络严格限制为两个父节点，则可能有成千上万种可能的结构。那么我要尝试所有结构吗？可能不会。
        </p>
        <p>You can select between them and pick the best one. but there are. gosh, for this number of variables, there
            are a whole lot of different networks that satisfy the no looping criteria and don’t have very many parents.
            There’s an awful lot of them. In fact, if you strict this network to two parents there are probably
            thousands and thousands of possible structures. So do I try them all? Probably not.</p>
        <p>当你得到 30 个变量或类似的东西时，工作量就太大了。那么你该怎么做呢？我们知道该怎么做，对吧？我们几乎是 6034
            的老手了。我们必须搜索！所以我们做的是，我们选择失败者并对其进行修改。然后我们再次修改它。我们不断修改它，直到我们失败或得到我们满意的东西。</p>
        <p>It’s too much work when you get 30 variables or something like that. So what do you do? We know what to do,
            right? We’re almost veterans a 6034. We have to search! So what we do is we take the loser and we modified
            it. And then we modify it again. And we keep modifying it until we drop dead or we get something that we’re
            happy with.</p>
        <p>让我们看看如果我们稍微改变一下这个问题并进行结构发现，会发生什么。我们从没有任何联系开始。我们将开始运行这个家伙。所以将会发生的情况是好人将获胜。而坏人将是好人的副本，但在某种程度上会受到干扰。所以这是一个随机搜索。你会注意到那个分数。它太小了，你看不清。
        </p>
        <p>So let’s see what happens if we change this problem a little bit and do structure discover. We’re starting
            out with nothing linked. And we’re going to just start running this guy. So what’s going to happen is that
            the good guy will prevail. And the bad guy will be a copy of the good guy perturbed in some way. So it’s a
            random search. You’ll notice that score. it’s too small for you to read.</p>
        <p>所有这些都太小了，无法读取。让我把它放大一点。太小了，无法读取，但右边的数字实际上不是概率的乘积。它是概率对数的总和。它们是相加的，对吧？你用这个而不是概率的原因是这些数字太小了，在 32 位机器上，你最终会输。
        </p>
        <p>All these things are too small to read. Let me make it a little bigger. Too small to read, but that number on
            the right there is not the product of the probabilities, actually. It’s the sum of the logarithms of the
            probabilities. They go together, right? And the reason you use this instead of the probabilities is because
            these numbers get so small that was a 32 bit machine, you eventually lose.</p>
        <p>因此，使用概率的对数而不是概率的乘积。使用对数的总和而不是概率的乘积。最终，你希望这个东西会收敛到正确的解释。但你知道吗？这个东西像空间一样平坦，像电线杆一样大，充满了局部最大值。所以这个程序偶尔会这样做。</p>
        <p>So use the log of the probabilities rather than the product of the probabilities. You use the sum of the logs
            instead of the product of the probabilities. And eventually, you hope that this thing converges on the
            correct interpretation. But you know what? This thing is so flat as a space and so a large and so telephone
            pole like that it’s full of local maxima. So what this program is doing is every once in awhile.</p>
        <p>我认为概率是 1 和
            10；我忘了我使用了什么参数。每隔一段时间，它就会对结构进行彻底的重新排列。换句话说，这是一个随机重启的过程。它会跟踪迄今为止最好的那个。每隔一段时间，它就会完全随机地重启，以搜索空间。所以这就是你从概率推理到结构发现的过程。现在这些东西什么时候才真正有用呢？
        </p>
        <p>I think with probability 1 and 10; I forgot what parameters I used. every once in awhile, it’ll do a total
            radical rearrangement of the structures. In other words, it’s a random restart. It keeps track of the best
            guy so far. And every once in awhile it does a totally random restart in its effort to search the space. So
            that’s how you go from probabilistic inference to structure discovery. Now when is this stuff actually
            useful?</p>
        <p>好吧，我暗示了医学诊断，对吧？在这种情况下，您出现了一些症状。您想知道是什么疾病。因此，只要您使用“诊断”这个关键词，您就会遇到一个问题，而这些东西就是候选对象。那么还有哪些其他类型的诊断问题？好吧，您可能在骗我。所以我可以给你装上测谎仪。
        </p>
        <p>Well, I hinted at a medical diagnosis, right? That’s a situation where you’ve got some symptoms. And you want
            to know what the disease is. So as soon as you use the keyword “diagnosis,” you’ve got a problem for which
            this stuff is a candidate. So what other kinds of diagnosis problems are there? Well, you might be lying to
            me. So I can put a lie detector on you.</p>
        <p>测谎仪测量的每一个变量都是一个独立的指标，可以判断你是否在说实话。所以这就是贝叶斯发现的东西。朴素贝叶斯分类。还有什么其他类型的问题与诊断有关？好吧，我们想知道你对材料的了解程度！所以我们可以使用测验作为证据。</p>
        <p>And each of those variables that are measured by the lie detector are an independent indication whether
            you’re telling the truth or not. So it’s this kind of Bayesian discovery thing. Naive Bayesian
            Classification. What other kinds of problems speak to the issue of diagnosis? Well, we like to know how well
            you know the material! So we can use quizzes as pieces of evidence.</p>
        <p>感谢上帝，我们没有使用简单的贝叶斯分类器，因为那样我们就无法进行这种组合。我们必须使用稍微复杂一点的贝叶斯网络来进行这种特殊的诊断。你可能有一艘航天器、一架飞机或其他设备，它们有各种各样的症状。你正在试图弄清楚下一步该做什么，原因是什么。
        </p>
        <p>Thank god we don’t use exactly a naive Bayesian classifier, because then we wouldn’t be able to do that
            combination. We have to use a slightly more complex. what you can think of as a slightly more complex
            Bayesian net to do that particular kind of diagnosis. You might have a spacecraft or an airplane or other
            piece of equipment with all sorts of symptoms. You’re trying to figure out what to do next, what the cause
            is.</p>
        <p>因此，利用证据追溯原因。所以也许你的某个程序不起作用。我经常遇到这种情况。所以我利用不当行为症状的证据来找出最可能的原因。但现在要结束这一天了。上次没有任何有力的想法。</p>
        <p>So using the evidence to go backward to the cause. So maybe you’ve got some program that doesn’t work.
            Happens to me a lot. So I use the evidence from the symptoms of the misbehavior to figure out what the most
            probable cause is. But now to conclude the day. last time there weren’t any powerful ideas.</p>
        <p>但是，如果你把上一讲和这一讲结合起来，作为金星创意的候选，这些就是我想要留给你的。我们得到的是。贝叶斯的东西，所有这些概率计算都是正确的。当你什么都不知道的时候，它们是正确的工作方式，这听起来好像你没什么用，因为你总是认为自己没什么用。
        </p>
        <p>But if you take the combination of the last lecture and this lecture to be a candidate for gold star ideas,
            these are the ones I’d like to leave you with. We got here is. this Bayesian stuff, all these probabilistic
            calculations are the right thing to do. They’re the right way to work when you don’t know anything, which
            would make it sound like you’re not very useful, because you think you always.</p>
        <p>事实上，在很多情况下，你要么无法了解所有事情，要么没有时间了解所有事情，要么不想花精力了解所有事情。所以在医学诊断中，你只有症状。你无法更准确地找出问题所在。所以你用症状来确定病因。然后就是我提到的所有其他类型的情况。
        </p>
        <p>Well, in fact, there are a lot of situations where you either can’t know everything, don’t have time to know
            everything, or don’t want to take the effort to know everything. So in medical diagnosis all you’ve got is
            the symptoms. You can’t go in there and figure out in a more precise way exactly what’s wrong. So you use
            the symptoms to determine what the cause is. And then all those other kinds of cases that I mentioned.</p>
        <p>但是现在，还有哪些其他类型的结构发现呢？好吧，我在开头暗示的结构发现类型将是我们周三下一次也是最后一次谈话的主题。</p>
        <p>But now, what other kinds of structure discovery are there? Well, the kind of structure discovery that I
            hinted at in the beginning will be the subject that we’ll begin with during our next and sadly final
            conversation here in on Wednesday.</p>
        <p>我们不仅会讨论如何使用这些东西来发现模式和故事，还会讨论最后的内容，以及接下来可以做什么，诸如此类的事情来结束这个主题。今天的故事就到此结束。</p>
        <p>It will feature not only a discussion of how this stuff can be used to discover patterns and stories, but
            we’ll also talk about what’s on the final, what kind of thing you could do next, that sort of thing to
            finish off the subject. And that’s the end of the story for today.</p>
        <h1 id="model-merging-cross-modal-coupling-course-summary">23. 模型合并、跨模态耦合、课程总结</h1>
        <h1>23. Model Merging, Cross-Modal Coupling, Course Summary</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:31 <a
                href="https://youtube.com/watch?v=XPEJg_6Cg6o">https://youtube.com/watch?v=XPEJg_6Cg6o</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:31 <a
                href="https://youtube.com/watch?v=XPEJg_6Cg6o">https://youtube.com/watch?v=XPEJg_6Cg6o</a></p>
        <h2 id="unknown-400">未知</h2>
        <h2>Unknown</h2>
        <p>帕特里克·温斯顿：好吧，别停下来。该死。我想我们必须停下来。我很快就会出现戒断症状，​​持续约六周。但另一方面，我们都开始出现一种疲惫和绝望的表情。也许把这个学期抛在脑后，进入冬至休眠期是件好事。不管怎样，今天有很多事情要做。
        </p>
        <p>PATRICK WINSTON: Well, don’t stop. Shoot. I guess we’ve got to stop. I will soon go into withdrawal symptoms
            that will last about six weeks. But, on the other hand, we all are beginning to develop a sort of tired and
            desperate look. And perhaps it’s a good thing to get the semester behind us and go into solstice
            hibernation. Anyway, there’s a lot to do today.</p>
        <p>我想总结几件事，谈谈接下来要做什么，也许会讨论一些大问题，也许是一个小小的 Genesis 演示，诸如此类。所以这是我们首先要做的。上次，我谈到了结构发现的整个想法。</p>
        <p>I want to wrap up a couple things, talk about what’s next, and maybe get into some big issues, perhaps a
            little Genesis demo, that sort of thing. So here’s what we’re going to do first. Last time, I talked about
            this whole idea of structure discovery.</p>
        <p>事实上，我之所以开始讨论基本方法，是因为将这个想法再进一步，并在你可能无法找到结构的情况下找到结构，可能会带来潜在的效用。这是否是最好的思考方式，这仍然是一个悬而未决的问题。但让我们开始吧。想象一下你有几个故事。这些圆圈代表故事中的事件。
        </p>
        <p>And really, the whole reason I cracked and started talking about basic methods is because of the potential
            utility of taking that idea one step further and finding structure in situations where you might not
            otherwise find it. It’s still an open question about whether that’s best way to think about it. But here it
            goes. Imagine you’ve got a couple of stories. And these circles represent the events in the story.</p>
        <h2 id="unknown-401">未知</h2>
        <h2>Unknown</h2>
        <p>现在，你想从这些故事中得到某种描述故事集合的有限状态图。因此，你可能会发现，例如，这两个事件非常相似。这两个事件非常相似。因此，你可以以此为基础推测，也许更紧凑的方式来表示故事中的内容会是这样的。这个。让我们看看。
        </p>
        <p>And now what you’d like to get out of these stories is some kind of finite state graph that describes the
            collection of stories. So you might discover, for example, that these two events are quite similar. And
            these two events are quite similar. So you might use that as a basis for speculating that maybe a more
            compact way of representing the stuff in the story would look like this. Where this one. let’s see.</p>
        <p>这个和这个相配，这个和这个相配，中间还有另一种状态的可能性。这就是贝叶斯故事合并的概念。现在我想向你们展示一个更有说服力的演示。它是这样进行的。所以有两个故事。这只是一个课堂演示，没什么大不了的。但你可以看到其中有一种平行结构。
        </p>
        <p>This one goes with this one, this one goes with this one and there’s a possibility of another state in
            between. So that’s the notion of Bayesian story merging. Now I’d like to show you a little bit more
            convincing demonstration of that. Here’s how it goes. So there are the two stories. This is just a classroom
            demonstration, no big deal. But you can see there’s a sort of parallel structure in there.</p>
        <p>这是研究生马克·芬莱森的作品，他处理了这些故事，产生了这些事件，并将这些事件组合成两个故事图。问题是，这是解释这些故事的最可能方式吗？当然，答案是否定的。</p>
        <p>So this is the work of a graduate student, Mark Finlayson, who processed those stories to produce those kinds
            of events and those kinds of events that get assembled into two story graphs. And the question is, is the
            most probable way of explaining that corpus of stories? And of course, the answer is no.</p>
        <h2 id="unknown-402">未知</h2>
        <h2>Unknown</h2>
        <p>如果你将追逐和停止等事件合并起来，那么你就会得到一个更简单的图表，一个更可能发生的图表，就像我们上次讨论的那样。然后你可以合并奔跑和逃跑，因为它们是类似的事件。最后，你需要思考和决定。砰。这就是你的故事图表。</p>
        <p>If you merge some things like chase and stop, then you get a simpler graph, one that is more probable in the
            same sense that we discussed last time. Then you can merge run and flee because they’re similar kinds of
            events. And finally, you’ve got think and decide. Boom. There is your story graph.</p>
        <p>而这正是同一想法的几个更高层次，它产生了在这两个故事中发现复仇概念的能力，正如我们上次讨论开始时所承诺的那样。所以有时贝叶斯方法是正确的做法，特别是当你什么都不知道的时候。但有时你确实知道一些东西。当你确实知道一些东西时，你有可能做一些更有效的事情。
        </p>
        <p>And this is the same idea taken several levels higher that produce the capacity to discover, in these two
            stories, the concept of revenge, as promised at the beginning of our last discussion. So sometimes the
            Bayesian stuff is the right thing to do, especially if you don’t know anything. But sometimes you do know
            stuff. And when you do know stuff it’s possible that you can do something very much more efficient.</p>
        <p>这种事情需要大量计算机来处理。但我们在开发过程中学到了很多东西，而这些东西并不是用大量计算机就能搞清楚的。例如，我们学会了如何将嘴部的动作与我们发出的声音联系起来。所以，我想花一两分钟谈谈一些工作，我想，这些工作将来有一天会成为几次讲座的主题。
        </p>
        <p>This sort of thing takes clouds of computers to process. But we learned a lot of stuff in the course of our
            development that we don’t use a cloud of computers to figure out. We learned how to associate the gestures
            of our mouth with the sounds that we make, for example. So I want to spend a minute or two talking about
            some work that someday will be the subject of a couple of lectures, I think.</p>
        <h2 id="unknown-403">未知</h2>
        <h2>Unknown</h2>
        <p>但问题是如何使用多种模态及其之间的对应关系来理清两种贡献模态。这听起来很矛盾。让我给你举个例子。这是一个例子，一只斑胸草雀。它向你展示了迈克尔·科恩（现为威斯康星大学教授）编写的程序的结果。所以雄性斑胸草雀从它的爸爸那里学会了唱一首好听的求偶歌。
        </p>
        <p>But it’s the question of how to use multiple modalities and correspondences between them to sort out both of
            the contributing modalities. That sounds contradictory. Let me show you an example. This is the example, a
            zebra finch. And it’s showing you the result of a program written by Michael Coen, now a professor at the
            University of Wisconsin. So the male zebra finch learns to sing a nice mating song from its daddy.</p>
        <p>这就是斑胸草雀的叫声。听起来不错，不是吗？这是一个程序所学到的，它根本不使用概率，而是使用跨模态耦合的概念。你能分辨出区别吗？目前还不知道这首特别的歌是否会吸引雌性斑胸草雀。但对于未经训练的人耳来说，它们听起来确实非常相似。那么这是如何运作的呢？下面是它的工作原理。
        </p>
        <p>And this is what one such zebra finch sounds like. Nice, don’t you think? And here’s what was learned by a
            program that uses no probabilistic stuff at all, but rather the notion of cross modal coupling. Can you tell
            the difference? It’s not known if this particular song turns on the female zebra finch. But to the untrained
            human ear, they sure sound a whole lot alike. So how does that work? Here’s how that works.</p>
        <p>好吧，我不会向你们展示它是如何工作的。我要向你们展示的是课堂上的例子，即科恩博士论文第一章的例子。事情是这样的。当我们说话时，我们会产生一个傅里叶变换，它会随着我们的讲话而移动。如果我们说一个元音，比如aah，你会得到一个相当恒定的傅里叶频谱。你可以说，那么，傅里叶频谱中的峰值在哪里？
        </p>
        <p>Well, I’m not going to show you how that works. What I’m going to show you is how the classroom example
            works, the first chapter example in Coen’s Ph.D.&nbsp;Thesis. Here’s what happens. When we talk, we produce
            a Fourier transform that moves along with our speech. And if we say a vowel like aah, you get a fairly
            constant Fourier spectrum. And you can say, well, where are the peaks in that Fourier spectrum?</p>
        <h2 id="unknown-404">未知</h2>
        <h2>Unknown</h2>
        <p>当我说出元音时，它们与我的嘴巴形状如何对应？这就是它的工作原理。这是特定元音的傅里叶频谱。当你平滑它时，这些峰值被称为共振峰。所以我们只需跟踪第一和第二共振峰。但当我说出这些词时，我也可以在嘴巴周围形成一个椭圆。
        </p>
        <p>And how do they correspond to the appearance of my mouth when I say the vowel? So here’s how that works. So
            here’s the Fourier spectrum of a particular vowel. And when you smooth that, those peaks are called
            formants. And so we’re just going to keep track of the first and second formant. But when I say those
            things, I also can form an ellipse around my mouth when I say them.</p>
        <p>当我说这些词时，嘴巴周围画一个椭圆，这就给了我第二种模态。那么问题是，有没有办法把产生声音的手势和声音本身联系起来？嗯，有各种各样的来源提供的人类数据，包括迈克尔·科恩的妻子，她提供了右边的唇形数据。所以这些都是根据英语中的特定元音标记和颜色编码的。
        </p>
        <p>And when I form an ellipse around my mouth when I say them, that gives me this second modality. So the
            question is, is there a way of associating the gestures that produce the sound with the sound itself? Well,
            there’s the human data conveniently provided by a variety of sources, including Michael Coen’s wife who
            produced the lip contour data on the right. So that’s all marked up and color coded according to the
            particular vowels in English.</p>
        <p>我想有十个。所以我们人类都是从中学习的。但你猜怎么着？我们不是从中学习的。因为我们没有使用任何标记数据。我们是从中学习的。我们以某种方式接触自然世界，然后挖掘出元音。我们这样做真是太棒了。但我们确实有跨模态耦合数据，也许这与此有关。
        </p>
        <p>I guess there are ten of them. So we humans all learn that. But guess what? We don’t learn it from this.
            Because we don’t get to work with any marked up data. We learn it from that. Somehow we’re exposed to the
            natural world, and we dig the vowel sounds out. It’s fantastic how we do that. But we do have cross modal
            coupling data and maybe that’s got something to do with it.</p>
        <h2 id="unknown-405">未知</h2>
        <h2>Unknown</h2>
        <p>这是一组特定的声音。我想知道的是，我能否将这两个音组中的任何一个合并起来，形成一个具有对应含义的更大的音组？所以我能做的是，我可以说，好吧，我可以观察这些音组。我知道发出特定声音时的唇形。所以我有这些对应关系。所以可能有四个这样的对应关系，就像这样。
        </p>
        <p>So here is a particular cluster of sounds. And what I want to know is, can I merge any of these two clusters
            to form a bigger cluster with a corresponding meaning? So what I can do is, I can say, well, I can watch
            these. I know what the lip form is when a particular sound is made. So I have these correspondences. So
            maybe there are four of those, like so.</p>
        <p>也许这个家伙会投射出几种色调。问题是，这个家伙能和这些家伙中的任何一个结合起来吗？答案是肯定的。如果它们在一侧靠得很近，也许这表明你应该把它们聚集在另一侧。但有一个问题，即“靠近”意味着什么。所以让我们假设我们也看看这些家伙，另外两个家伙，是如何投射的。
        </p>
        <p>And maybe this same guy projects a couple of tones into that. And the question is, can this guy be combined
            with any of these guys? And the answer is yes. If they’re close together on one side, maybe that suggests
            you ought to cluster them on the other side. But there’s a question about what close means. So let’s suppose
            that we also look at how these guys, these other two guys, project.</p>
        <p>假设这个家伙在这里投射两次，在这里投射一次。而这个家伙在这里疯狂地投射到那个家伙身上。哪个更接近？这两个还是这两个？好吧，我的图表更接近一点。但是如果你在我画的时候注意的话，你会看到这个家伙与那个家伙成比例地投射。所以如果我们看一下。
        </p>
        <p>And suppose this guy projects twice up here and once over here. And this guy down here just projects like
            crazy into that guy. Which are closer? These two or these two? Well, my diagram’s getting a little closer.
            But if you paid attention when I was drawing it, you would see that this guy projects in proportion to that
            guy. So if we look at the.</p>
        <h2 id="unknown-406">未知</h2>
        <h2>Unknown</h2>
        <p>如果我们将每个投影都视为一个向量的分量，那么这两个向量的方向是相同的。它们之间的余弦为零。因此，从这种度量的角度来看，这两个是最接近的。而那些就是被合并的。你想看演示吗？好的。好的，这是基于 Coen
            的工作的演示。所以这里有两个方面。</p>
        <p>If we take each of these projections as the components of a vector, then those two vectors are in the same
            direction. And the cosine between them is zero. So these are the two that are closest together from that
            sort of perspective of that kind of metric. And those guys are the ones who get combined. Would you like to
            see a demonstration? Yeah. OK, here’s a demonstration based on Coen’s work. So here we have two sides.</p>
        <p>我们可以将一侧视为元音，另一侧视为唇形轮廓或其他东西。但到目前为止，您在图表中看不到任何关于如何将这些事物分类的信息。所以如果我只采取一步，为什么，它会发现这两个人彼此具有相同的投影模式。</p>
        <p>We could think of one side as being vowel sounds and the other side as being lip contours or something. But
            you don’t see anything in the diagram so far about how these things ought to be sorted out into groups. So
            if I just take one step, why, it discovers that those two guys had the same projection pattern as each
            other.</p>
        <p>因此，如果我再进一步，在另一侧做同样的事情，现在在第三步中，之前合并的两个区域现在形成了一个超级区域。它们看起来与蓝色区域以相同的方式投影。因此，使用这种投影思想，我可以逐渐理解这些区域是如何组合在一起的。</p>
        <p>So if I take another step and do the same thing on the other side, and now in the third step, the two areas
            that were formerly combined now form a super area. And they’re seen to project in the same way as the blue
            area. So using this kind of projection idea, I can gradually build up an understanding of how those regions
            go together.</p>
        <h2 id="unknown-407">未知</h2>
        <h2>Unknown</h2>
        <p>我发现，在这个设计示例中，右侧的垂直排列与左侧的水平排列相对应。现在，你对我说，我想看到一些更像唇形轮廓的数据。我只是在这里一步步操作，直到得到我喜欢的东西。哦，听起来不错。这似乎不错。所以这里有一个对应关系。</p>
        <p>And I discover, in this contrived example, that there’s a vertical arrangement on the right side that
            corresponds to a horizontal arrangement on the left side. Now, you say to me, I’d like to see something a
            little bit more like the lip contour data. I’m just stepping through here until I get something I kind of
            like. Oh, that sounds good. That seems good. So there’s a correspondence here.</p>
        <p>这些都是虚构的数据，各种形状和方向的高斯分布。让我们看看当我运行聚类算法时会发生什么。每一步都会学到一些明确的东西。我们发现右边的粉色区域和左边的粉色区域之间存在对应关系。在某些情况下，当区域之间的联系比较模糊时，另一侧可以帮助系统弄清楚事物是如何组织的。
        </p>
        <p>This is all made up data, Gaussians of various shapes and orientations. Let’s see what happens when I run the
            clustering algorithm on that. Something definite was learned at every step. We find the correspondence
            between the pink region on the right and the pink region on the left. In some cases, where the regions are
            rather blurred together, the other side is the one that helps the system figure out how things are
            organized.</p>
        <p>所以我引用这个例子来说明我认为非常重要的一点。首先，我们有可能发现规律性，而不必过分关注贝叶斯概率。而且，人类智能中很可能有很多这样的事情发生。当我们出现并开始检查和探索我们周围的世界时，我们会面临大量未标记的数据，我们必须弄清楚它们。
        </p>
        <p>So I cite this as an example of something I think is very important. Number one, it’s possible to discover
            regularity without being obsessively concerned with Bayesian probability. And also, that there’s very likely
            a whole lot of this going on in human intelligence. When we emerge and begin to examine and explore the
            world around us, we’re presented with a lot of unlabeled data that we’ve got to make sense of.</p>
        <h2 id="unknown-408">未知</h2>
        <h2>Unknown</h2>
        <p>我相信这种跨模态耦合的理念很可能与我们对所呈现的世界的理解紧密相关。它快速、直接。它不需要数千个数据点。它自然发生。而且毫不费力。如果这不是内置的。如果这不是决心内置的，你可以在 15
            年后回到麻省理工学院，把我关进监狱。</p>
        <p>And I believe that this kind of cross modal coupling idea is very likely to be bound up in our understanding
            of that world that’s presented to us. It’s fast, it’s direct. It doesn’t take thousands of data points. It
            just happens. And it happens effortlessly. And if this isn’t built in. if this isn’t determined to be built
            in, you can come back to MIT in 15 years and put me and jail.</p>
        <p>因为我认为这确实是它的工作方式。就是这样。有几件事需要总结。现在，我想在最后一次以这种形式在一起的时间里与你们讨论各种事情。我将与往常不同，转到一些幻灯片。戴夫，我们可以看中间的屏幕吗？</p>
        <p>Because I think this is really the way it works. So there it is. There’s a couple of things to have wrapped
            up. And now the next thing I want to do for the rest of our last time together in this format is talk to you
            about a variety of things. And I’ll depart from my usual practice and move to some slides. So Dave, could we
            have the center screen, please?</p>
        <p>首先，我们来简单回顾一下我们过去和现在的情况。我记得在第一堂课上，我就讲了什么是人工智能。我还讲了如何从工程角度或科学角度来看待人工智能。我站在科学的角度。</p>
        <p>So first, a brief review of where we’ve been and where we’ve come. I think in the very first class, I talked
            about what artificial intelligence was. And I talked about how you could view it from either an engineering
            perspective or a scientific perspective. I’m on the scientific perspective side.</p>
        <h2 id="unknown-409">未知</h2>
        <h2>Unknown</h2>
        <p>我认为，我并不反对应用，但我认为如果我们不仅从工程角度看待构建事物，而且从科学角度看待理解事物，我们将能够制作出更复杂、更精彩的应用程序。所以这两种观点都很重要。在这种情况下，你可以看到它们都涉及表示、方法和架构。戴夫，我改变主意了。
        </p>
        <p>And I think, nothing against applications, but I think we’ll be able to make much more sophisticated and
            wonderful applications if we have not only the engineering perspective about building stuff but also the
            scientific perspective about understanding the stuff to begin with. So both perspectives are important. And
            in this case you can see that they all involve representations, methods, and architectures. Dave, I’ve
            changed my mind.</p>
        <p>你能把侧屏也给我，让我也看看吗？就这样。接下来是什么？感恩节时我们谈到的商业视角。重要的想法是，下意识地认为某物的商业价值在于取代人，这种预期首先是不合理的，其次被证明是不太可能的，也是不真实的。</p>
        <p>Could you also give me the side screen so I can see it too? So, that’s that. What’s next? The business
            perspective, which we talked about on Thanksgiving. The important idea being that the knee jerk expectation
            that the commercial value of something is in replacing people is something that is not sensible in the first
            instance, and demonstrated to be unlikely and untrue in the second instance.</p>
        <p>从应用的角度来看，让人们兴奋的不是取代人类，而是创造新的收入，创造新的能力。这同时也意味着，某些事情不是完全由计算机完成的，而是可以与人合作完成的。因此，人工智能的所有重要应用都涉及人与计算机的协同工作，每个人都做自己最擅长的事情。而不是取代人类。就是这样。
        </p>
        <p>The thing that turns people on from the point of view of applications is not replacing people but making new
            revenue, making new capability. And that at once licenses you to not have something done exclusively by a
            computer but something that can be done in partnership with a person. So all the important applications of
            artificial intelligence involve people and computers working in tandem, with each doing what they do best.
            not with replacing people. So that’s that.</p>
        <h2 id="unknown-410">未知</h2>
        <h2>Unknown</h2>
        <p>人工智能与其他试图促进理解智能的领域的区别在于，人工智能有以下特点。现在，我们有了程序语言。我们掌握了所有隐喻，这些隐喻是我们了解编程的结果。我们有垃圾收集的隐喻。</p>
        <p>Here’s what AI does that makes AI different from the rest of the fields that attempt to contribute to an
            understanding of intelligence. Now, we have the benefit of having a language for procedures. We have all the
            metaphors that we are the custodians of in consequences of knowing about programming. We have the metaphor
            of garbage collection.</p>
        <p>我们可以用编程隐喻来谈论各种各样的事情，而其他领域对心理学感兴趣的人则无法做到这一点。我们有一种建立模型的方法，因为我们可以编写程序。当我们编写程序时，不存在掩盖事实的问题。我们必须解决细节问题。</p>
        <p>We can talk about all sorts of things with programming metaphors that are unavailable to people in other
            fields that are interested in psychology. We have a way to make models because we can write programs. And
            when we write programs, there’s no question of sweeping things under the rug. We have to work out the
            details.</p>
        <p>一旦我们完成了所有这些，我们就有机会进行实验，而这些实验是大多数其他领域无法进行的。哦，如今在发展心理学和所有其他心理学分支领域都进行了精彩的实验，包括 MRI
            研究，可以探测你的大脑并观察它如何消耗糖分。但很难消除或拿走你的部分知识并观察你在没有它的情况下如何工作。</p>
        <p>And once we’ve done all that, then we have opportunities to experiment that are beyond the ability to
            experiment in most other fields. Oh, magnificent experiments are done these days in developmental psychology
            and all the rest of. all the other branches of psychology, including MRI studies that probe into your brain
            and see how it’s consuming sugar. But it’s very difficult to ablate or take away some piece of your
            knowledge and see how you work without it.</p>
        <h2 id="unknown-411">未知</h2>
        <h2>Unknown</h2>
        <p>我可以拿我们讨论过的线条绘制程序来说，如果它对分叉路口一无所知，它会怎么做？我们可以确定答案。但我无法通过外科手术进入塞巴斯蒂安的脑袋，并消除他对分叉路口的知识。这根本做不到。</p>
        <p>I can take the line drawing program that we talked about and say, how will it do if it doesn’t know anything
            about fork junctions? And we can determine an answer. But I can’t reach into Sebastian’s head here with a
            surgical procedure and take out his knowledge of fork junctions. It just can’t be done.</p>
        <p>最后，我们与众不同的另一个原因是，我们可以设定完成某项任务所需的知识量的上限。如今，随着推土机计算的发展，最常被问到的问题是，如何从网络上获取数十亿的东西并使用它们。我们，尤其是我，有时会问相反的问题，即你有多少知识才能理解一个故事？
        </p>
        <p>And finally, another reason why we’re different is because we can put upper bounds on how much knowledge is
            needed in order to perform a certain kind of task. These days, with bulldozer computing, the question most
            often asked is, how can you get billions of the things off the web and use them. We. I especially. sometimes
            ask the opposite question, which is how little knowledge can you have and still understand a story?</p>
        <p>这就是我感兴趣的地方。所以有一张方法论幻灯片，讨论了如何具体实现人工智能，我想，如何实现一般的科学和工程。在这个领域，人们倾向于爱上特定的方法。我们有一些人将整个职业生涯都奉献给了神经网络、遗传算法和贝叶斯概率。这就是机制嫉妒。在我看来，更好的方法是问，问题是什么？
        </p>
        <p>That’s what’s interesting to me. So there’s a methodological slide that talks to the question of how you do
            artificial intelligence in particular and, I suppose, science in general, engineering in general. There’s a
            great tendency in this field to fall in love with particular methods. And we’ve had people who’ve devoted
            entire careers to neural nets, genetic algorithms, Bayesian probability. And that’s mechanism envy. And a
            better way, in my judgment, is to say, what is the problem?</p>
        <h2 id="unknown-412">未知</h2>
        <h2>Unknown</h2>
        <p>科学方法。问题是什么？然后采用正确的机制来解决问题，而不是寻找与特定机制有关的东西。所以这是 David Marr
            首次以有力的方式阐述的方法。您需要从试图理解的能力开始，然后采用一种表示来表达它，这种表示可以揭示约束和规律。因为没有这些，您就无法创建模型。</p>
        <p>Scientific method. what’s the problem? And then bring the right machinery to bear on the problem, rather than
            looking for things to do with a particular kind of machinery. So this is the methodology that first
            articulated in a forceful way, by David Marr. You want to start with the competence you’re trying to
            understand, then bring a representation to bear on it, a representation that exposes the constraints and
            regularities. Because without those, you can’t make models.</p>
        <p>如果没有这些模型，你就无法理解、解释、预测或控制它。所以从 MIT
            的以模型为中心的观点来看，这似乎是有道理的。只有当你把所有这些都搞清楚了，你才能开始研究你的方法，实施实验，然后绕着这个循环走。我想这就是我想通过回顾的方式说的全部内容了。</p>
        <p>And without those models you can’t understand it, explain it, predict it, or control it. So it seems to make
            sense from a kind of MIT, model centered point of view. And only when you’ve got all that straight do you
            start working on your methods and implement an experiment and then go around that loop. So that’s all I want
            to say by way of review, I suppose.</p>
        <p>我想花一两分钟提醒一下你们期末考试的内容。没什么可提醒的，因为你们都知道期末考试的内容了。我们将有四个部分，对应四场考试。然后我们将有第五个也是最后一个问题，它将是其他所有内容。你们睡着了的所有内容都会在那里出现，还有一个关于贝叶斯推理的小问题。
        </p>
        <p>I want to take a minute or two and just remind you of what’s on the final. And there’s nothing to remind
            because you all know what’s on the final already. We’ll have four sections corresponding with four exams.
            Then we’ll have a fifth and final question that will be everything else. All that stuff you slept through
            will be featured there, as well a little problem on Bayesian inference.</p>
        <h2 id="unknown-413">未知</h2>
        <h2>Unknown</h2>
        <p>我们重新安排了主题，主要是为了让我能写出演示。这样贝叶斯的东西就不会在第四次测验之前出现。因此，你在之前的测验中看到的贝叶斯东西可能比我们在期末考试中问的东西更难，因为你在这方面的经验不如去年的人那么多。我在那里放了一些图标来提醒我告诉你一些事情。
        </p>
        <p>We rearranged the subject, mostly so I could write the demonstrations. So that the Bayesian stuff didn’t come
            before the fourth quiz. Therefore the Bayesian stuff that you see on those previous quizzes is likely to be
            harder than the stuff that we’ll ask on the final, because you haven’t had as much experience with it as
            people did last year. I’ve got a few icons on there to remind me to tell you a few things.</p>
        <p>一如既往，除了电脑之外，其他一切都可以打开。你可以穿戏服。你可以做任何你想做的事，只要它不会打扰你的邻居，在合理范围内。好吧，我想如果它不打扰邻居，那就是在合理范围内。所以也许这就是我需要说的全部。我不确定我们会在哪里。但从历史上看，确实没有可见的时钟。
        </p>
        <p>As always, open everything except for computers. You can wear a costume. You can do anything you like as long
            as it doesn’t disturb your neighbor, within reason. Well, I guess if it doesn’t disturb neighbor, it is
            within reason. So maybe that’s all I need to say. I’m not sure where we’re going to be. But it’s certainly
            the case that, historically, there are no visible clocks.</p>
        <p>因此，我们很快就用完了手机、手表和其他钟表。所以记得带上某种钟表是值得的，因为我们无法很好地传达时间。最后，我看到那里有一个小计算器。我不记得有哪场考试需要计算器。但有一台计算器就像是一条安全毯。</p>
        <p>So, we soon run out of all of our cellphones, wrist watches, and other time pieces as we hand them out. So it
            pays to remember to bring some kind of timepiece, because we won’t be able to convey the time very well. And
            finally, I see a little calculator there. I don’t recall any exam where you actually needed a calculator.
            But it’s sort of a security blanket to have one.</p>
        <h2 id="unknown-414">未知</h2>
        <h2>Unknown</h2>
        <p>人们有时会看到一道题，然后说，天哪，我该怎么办？我把计算器忘在家里了。为了避免这种焦虑，你可能要带一个，即使你不需要它。所以这是最后的。我敢肯定没有人会问问题。有吗？很明显。每个人都会做得很好。两枪，全部搞定。现在，下一步该怎么做？假设这个话题让你兴奋不已。
        </p>
        <p>People sometimes see a problem and say, oh my God, what am I going to do? I left my calculator at home. So as
            to avoid that anxiety, you might want to bring one even though you won’t need it. So that’s the final. I’m
            sure there are no questions. Are there? It’s obvious. Everybody will do well. Two shots, that whole thing.
            Now, what to do next? Suppose this subject has turned you on.</p>
        <p>下学期你应该考虑做很多事情。我想回顾一下其中的几件。其中之一就是马文·明斯基的课程《心智社会》。这门课与这门课非常不同。没有准备好的讲座。马文不排练。他不会提前考虑要说什么。这门课就像这样，只不过是和马文的一次对话。
        </p>
        <p>There are a variety of things that you should be thinking about doing next semester. And I wanted to review
            just a few of those. One of them is Marvin Minsky’s subject, Society of Mind. It’s very different from this
            class. There are no prepared lectures. Marvin doesn’t rehearse. He doesn’t think about what he’s going to
            say in advance. It’s like this except it’s just a conversation with Marvin.</p>
        <p>很多人听了三堂课，有两堂会觉得无聊透顶。但到了第三堂课，马文会说一些话，让你思考一年甚至一生。我也是这样的。听了三堂课，有两堂会觉得无聊透顶。但到了第三堂课，他又说了一些话，让我思考了至少一年，甚至永远。</p>
        <p>So many people find themselves bored stiff for two lectures out of three. But then in the third lecture,
            Marvin will say something that you’ll think about for a year or for the rest of your life. That’s what
            happens to me. I’m bored stiff two out three times. And then the third lecture he says something, and I
            think about it for at least a year and maybe permanently.</p>
        <h2 id="unknown-415">未知</h2>
        <h2>Unknown</h2>
        <p>所以这是一次机会，让我们看到麻省理工学院真正的天才们如何大声思考。所以这是一次你不想错过的经历，因为你来这里就是为了看到天才们如何大声思考。说到天才，还有鲍勃·贝里克。他英勇地在春季学习了两门课程。如果可以的话，我都会选修这两门课程。一门是语言理解课程。
        </p>
        <p>So it’s an opportunity to see one of MIT’s true geniuses think out loud. So it’s an experience that you don’t
            want to miss because that’s what you come here for, is to see the geniuses think out loud. Speaking of
            geniuses, then there’s Bob Berwick. And he heroically is doing two subjects in the spring. Both of which I’d
            take if I could. One is his subject on Language Understanding.</p>
        <p>我之所以会选择这个，是因为我相信语言是解释我们智力的核心。所以如果我是你，我想我会最倾向于选择这个主题。嗯，也许是明斯基的。很难说。顺便说一句，非常英勇的是，鲍勃还教授了一门关于进化如何运作的课程，它到底是如何运作的。就我们所知，它到底是如何运作的。
        </p>
        <p>And the reason I’d take that is because I believe that language is at the center of any explanation of our
            intelligence. So that’s the subject I would be, I suppose, most inclined to take if I were you. Well, maybe
            Minsky’s. It’s hard to say. And incidentally, very heroically, Bob is also teaching a course on how
            evolution works, how it really works. in so far as we know how it really works. as well.</p>
        <p>所以这两门课都会在春季开课。我不知道他是怎么做到的。我不知道他是如何同时开两门课的。当然，有很多地方可以上课，而且教师会同时教授五门课程。我只是觉得他们疯了。我不知道这是怎么回事。Gerry Sussman
            将教授他的“大规模符号系统”课程。</p>
        <p>So both of those will be offered in the spring. I don’t know how he does it. I don’t know how he does two all
            at the same time. Of course there are lots of places where you can go to school, and the faculty will be
            teaching five courses at the same time. I just think they’re crazy or something. I don’t know how that
            works. Gerry Sussman will be teaching his Large Scale Symbolic System subject.</p>
        <h2 id="unknown-416">未知</h2>
        <h2>Unknown</h2>
        <p>有时候就是这样。哦，我忘了他不能怎么称呼它，对于真正喜欢编程的人来说，编程在政治上是不正确的。这是一门关于如何构建真正大型系统的精彩课程。我们在我们的研究系统中使用了该主题中的想法，因为这是唯一的方法。理解它的工作原理是构建无法构建的大型系统的唯一方法。
        </p>
        <p>That’s sometimes. oh, I forgot what he wasn’t able to call it, something that wasn’t politically correct
            about programming for people who really, really like to program. It’s a splendid course on how to build
            really big systems. And we use the ideas in that subject in our research system, because it’s the only way.
            understanding how that works is the only way that you can build systems that are too big to be built.</p>
        <p>我稍后可能会谈一谈。所以这是我最喜欢的三/四个选择。但还有很多其他的东西，太多东西要讲了。媒体实验室、心理学。还有很多东西。我只想说，有这三个名字的课程一定很好。我认为这些同事对我有重要的看法。</p>
        <p>I may say a word about that a little later. So those are my favorite three/four picks. But there’s lots of
            other stuff, too many things to cover. the media lab, psychology. There’s tons of stuff out there. And I
            would only mention that courses that have those three names on them are bound to be good. These are
            colleagues that I think I have a important perspective.</p>
        <p>我不一定同意，但这是一个你应该理解的重要观点。理查兹、特南鲍姆和辛哈。现在我们来谈谈我的春季课程《人类智能企业》。它之所以是
            6.XXX，不是因为它有什么色情内容，而是因为很长一段时间以来，我都记不住它的数字。所以我养成了称它为 6.XXX 的习惯，而且似乎已经习惯了。这就是它的含义。是的，这可能很有趣。</p>
        <p>Not necessarily one I agree with, but an important perspective that you should understand. Richards,
            Tenenbaum, and Sinha. And now we come to my spring course, the Human Intelligence Enterprise. It’s 6.XXX not
            because there’s anything pornographic about it but because for a long time, I couldn’t remember it’s number.
            So I developed a habit of referring to it as 6.XXX and it seems to have stuck. Here’s what that’s about.
            Yeah, that might be interesting.</p>
        <h2 id="unknown-417">未知</h2>
        <h2>Unknown</h2>
        <p>不过，这门课的教学方式就像人文课程一样。没有讲座，我只是讲。所有的助教都是这门课的老手，所以如果你想知道是否应该这样做，你可以找到一些资源。你可以和他们谈谈。或者你可以看看我们讨论的内容。以下是我们通过包装讨论的一些内容。是的，我可以听到一些窃笑声，因为人们已经发现了最后一个元素。
        </p>
        <p>It’s taught like a humanities course, though. No lectures, I just talk. And all the TAs are veterans of that
            class so if you want to know if you should do it, you have several resources. You can talk to them. Or you
            could look at the sorts of things we talk about. Here are some things we talk about by way of packaging.
            Yeah, I can hear a little tittering there because people have discovered the last element.</p>
        <p>有些人会选择整个主题，因为他们想参加那个单元。我们会讨论所有这些事情。我们会看看所有这些包装问题中的共同点是什么，当你成年时，无论你做什么，你都会一次又一次地面对这些问题。</p>
        <p>Some people take the whole subject because they want to be present for that unit. And we talk about all those
            kinds of things. And we look to see what the common elements are in all those kinds of packaging problems of
            the sort that you will face over and over again when you become an adult, no matter what you do.</p>
        <p>如果你成为一名商人、企业家、军官、科学家或工程师，包装材料通常会决定你是否成功。然后，这是你可以决定是否要选修这门课的第二种方法。内容、助教，当然，你总是可以求助于地下指南。</p>
        <p>If you become a business person, an entrepreneur, a military officer, a scientist, or an engineer, that
            packaging stuff will often make the difference between whether you succeed or don’t. And then, that’s the
            second way you can figure out whether you want to take the subject. The content, the TAs, and then of course
            you can always appeal to the Underground Guide.</p>
        <h2 id="unknown-418">未知</h2>
        <h2>Unknown</h2>
        <p>这就是为什么很少有人因为阅读了《地下指南》而没有参加这堂期末讲座，而选择 6.XXX 的考生。这是几年前《地下指南》中出现的一个元素。没有考试。但有破解《地下指南》的传统。所以这是出现的另一个例子。</p>
        <p>And that’s why it’s very rare for someone that takes 6.XXX who hasn’t been at this final lecture because they
            read the Underground Guide. Here is an element that appeared in the Underground Guide a few years back.
            There are no exams. But there is a tradition of hacking the Underground Guide. So this is another example of
            something that appeared.</p>
        <p>这一切的起因是，在 6.XXX 教学初期，我向学生抱怨我在麻省理工学院待了很长时间，从大一开始。但我还没有收到任何一位我报告过关于我教学的评论。好的、坏的、无所谓的。什么都没有。一句话都没有。</p>
        <p>So it all came about because early in the teaching of 6.XXX, I was whining to the students about the fact
            that I’ve been at MIT for a long time, since I was freshman. And I still have yet to have any person I
            report to say anything about my teaching. good, bad, indifferent. Nothing. Not a word.</p>
        <p>因此，学生们决定，看看他们能否说出一些足够离谱的话，迫使系主任和我进行对话，这将会很有趣。到目前为止，他们完全没有成功。我什么办法都试过了。温斯顿即使来也迟到了。他是个好老师，但总是从棕色纸袋里啜饮。各种各样的东西。但就是这样。这很有趣。
        </p>
        <p>So the students decided that it would be interesting to see if they could say something sufficiently
            outrageous to force a conversation between the department chairman and me. And so far they’ve been totally
            unsuccessful. And I’ve tried everything. Winston shows up late if he shows up at all. Good instructor but
            constantly sipping from a brown paper bag. All kinds of stuff. But there it is. It’s a lot of fun.</p>
        <h2 id="unknown-419">未知</h2>
        <h2>Unknown</h2>
        <p>由于报名人数有点多，所以我们必须抽签，中奖几率大概是 50%，等等。但你们中的许多人会觉得这是件好事。哦，是的。现在我还想提醒自己，有一个 IP 活动已经成为麻省理工学院的传统。那就是我主持的“如何演讲”讲座。</p>
        <p>It’s a little oversubscribed so we have to have a lottery and there’s about 50% chance and so on and so
            forth. But many of you will find it a good thing to do. Oh, yeah. And now I also want to remind myself that
            there is a IP event that’s become kind of MIT tradition. It’s the How to Speak lecture that I give.</p>
        <p>今年将于 1 月 28 日在 6120 举行。6120 可容纳约 120 人，约有 250 人到场。所以如果你想去听那场讲座，你应该提前 15 分钟到场。这是我和 6034 名学生之间的小秘密。这也是关于包装的。这是
            6.XXX 的一堂讲座版本。</p>
        <p>This year it’ll be on January 28 in 6120.6120 holds about 120 people and about 250 show up. So if you want to
            go to that lecture you should show up 15 minutes early. That’s a little secret just between me and 6034
            students. It’s about packaging, too. It’s a one lecture version of 6.XXX.</p>
        <p>但它是非常非线性的，因为你在那一小时内学到的一件事可能会决定你是得到这份工作还是别人得到这份工作。所以这是那种可以在短时间内产生巨大影响的事情之一。</p>
        <p>But it’s very nonlinear because one thing that you pick up from that one hour may make the difference between
            you getting the job and some other slug getting the job. So it’s one of those sorts of things that can make
            a big difference in a short period of time.</p>
        <h2 id="unknown-420">未知</h2>
        <h2>Unknown</h2>
        <p>你可能在 55 分钟的讲座中睡了 50 分钟，但在那神奇的五分钟里，你学会了何时讲笑话、如何开始讲座、如何结束讲座、工作谈话或销售演示等。这会让你觉得很值得。当然，你也有可能。</p>
        <p>You may sleep through 50 minutes of the 55 minutes in that lecture and stay awake for that one magical five
            minutes when you learn something about when to tell a joke or how to open a lecture or how to conclude one
            or a job talk or a sales presentation or anything. And that will make it worthwhile for you. And then of
            course there’s the possibility of your.</p>
        <p>任何和我一起做事的人都可能对我最近所说的强故事假设感兴趣，这是我们不时谈论的事情。这就是我们人类的本质，而它们不是。它们是猩猩或黑猩猩，尽管我们共享 DNA。它时好时坏。一度是 96%，然后是
            98%。现在我认为又降到了 97%。</p>
        <p>Anybody who’s doing with me is likely to be interested in what I’ve recently come to call a strong story
            hypothesis, something that we’ve talked about from time to time. That’s what makes us humans, and that’s not
            what they are. They’re orangutans or chimpanzees, even though the DNA that we share is. it goes up and down.
            At one point it was 96%, then it went to 98%. Now I think it’s back down to 97%.</p>
        <p>但无论我们是什么，都不是因为我们和我们的表亲在 DNA 上有巨大的差异。因此，在我的团队中，我们建立了一个 Genesis 系统，这个系统被谦虚地称为。它看起来就是这样的。</p>
        <p>But whatever we are, it’s not because of a huge, massive difference in DNA between us and our cousins. So in
            my group we build a Genesis system, modestly called. And that’s what it looks like.</p>
        <h2 id="unknown-421">未知</h2>
        <h2>Unknown</h2>
        <p>它包含了我们在 6034
            中不时讨论过的所有东西。它即将进入特别有趣的领域，比如，你能否在灾难发生之前检测到可能的发生并进行干预？你能否根据更高级别的概念检索压力机？诸如此类的事情。你想看演示吗？好的。所以你之前已经看过一点了。</p>
        <p>And it has in it all the sorts of things that we’ve talked about from time to time in 6034. And it’s about to
            move into areas that are especially interesting, like can you detect the onset of a possible disaster before
            it happens and intervene? Can you retrieve presses based on higher level concepts? Things of that sort.
            Would you like to see a demonstration? OK. So you’ve see in a little bit of this before.</p>
        <p>事实上，我甚至不确定我已经向你展示了什么。让我过去看看这里发生了什么。我现在要做的就是阅读《麦克白》。情节的简短摘要。当然不是全部。只是几句话的情节。现在它正在做的是吸收英语并翻译成一种内部语言。</p>
        <p>In fact, I’m not even sure what exactly I’ve already shown you. Let me just get over there to see what goes
            on here. What I’m going to do right now is I’m just going to read about Macbeth. A short precis of the plot.
            Not the whole thing, of course. Just a few sentences about the plot. Right now what it’s doing is absorbing
            the English and translating into a sort of internal language.</p>
        <p>一种通用的内部语言，全是关于轨迹、转变和社会关系等等。因此，它由两个不同的角色阅读。你可能会说，每个角色都有不同的教育背景。他们可能代表不同的文化。所以最终他们构建了这样的图表。白色的东西都是故事中明确表达的内容。灰色的东西都是系统推断出来的东西。
        </p>
        <p>A sort of universal, internal language that’s all about trajectories and transitions and social relationships
            and all that. So it’s being read there by two different persona. Each of those personas has a different
            educational background, you might say. They might represent different cultures. So eventually they build up
            graphs like that. And everything in white is stuff that’s explicit in the story. And everything that’s in
            grey is stuff that’s been inferred by the system.</p>
        <h2 id="unknown-422">未知</h2>
        <h2>Unknown</h2>
        <p>所以理解有好几个层次。一个是明确存在的东西。另一个是可以轻易推断出来的东西。由于这些人物的教育背景不同，你可能会说他们对剧中麦克白的死有不同的看法。一个人认为这是一种疯狂的暴力行为，另一个人认为这是复仇的结果。</p>
        <p>So there are several layers of understanding. One is what’s there explicitly. And the other thing that’s
            there is stuff that’s readily inferred. And because these persona have different educational backgrounds,
            you might say they see the killing of Macbeth at the end of the play in a different light from one another.
            One sees it as an act of insane violence and the other sees it as a consequence of a revenge.</p>
        <p>所以一旦你有了这种能力，你就可以做各种各样的事情。例如，你可以问问题。让我安排一下问问题。顺便说一句，这是一个现场演示。到目前为止，我很高兴它真的有效。让我们看看。麦克白为什么要杀邓肯？你们都知道为什么，对吧？是的，你是对的。他实际上并没有这么做。从常识层面上讲，杰克博士和海德先生都没有意见。
        </p>
        <p>So once you’ve got that capability, you can do all sorts of things. For example, you can ask questions. So
            let me arrange it to ask. by the way, this is a live demonstration. I’m thrilled to pieces that it actually
            works so far. Let’s see. Why did Macbeth kill Duncan? You all know why, right? Yeah, you’re right. He didn’t
            actually do that. On a common sense level, neither Dr.&nbsp;Jeckll nor Mr.&nbsp;Hyde have an opinion.</p>
        <p>从反思的角度看，杰克博士和海德先生都没有意见。那是因为这根本就没有发生过。所以我们把这两个人称为杰克博士和海德先生。你马上就要抱怨杰克的拼写了，不是吗？嗯，那是因为语音生成器用这种拼写让它听起来更像我们说的杰基尔。但真正发生的是麦克德夫杀死了麦克白。
        </p>
        <p>On a reflective level neither Dr.&nbsp;Jeckll more Mr.&nbsp;Hyde have an opinion. That’s because it didn’t
            happen. So we call these two persona Doctor Jeckll and Mr.&nbsp;Hyde. And you’re ready to complain right
            away about the spelling of Jeckll, aren’t you? Well, that’s because with this spelling the speech generator
            makes it sound a little bit more like when we say Jekyll. But what really happened is that Macduff killed
            Macbeth.</p>
        <h2 id="unknown-423">未知</h2>
        <h2>Unknown</h2>
        <p>从常识层面来看，杰克尔博士认为麦克德夫杀死麦克白是因为麦克德夫疯了。海德先生认为麦克德夫杀死麦克白是因为麦克白激怒了麦克德夫。从反思层面来看，杰克尔博士认为麦克德夫杀死麦克白是疯狂暴力行为的一部分。海德先生认为麦克德夫杀死麦克白是错误行为、皮洛士式胜利和复仇行为的一部分。这不是很酷吗？
        </p>
        <p>On a common sense level, it looks like Dr.&nbsp;Jeckll thinks Macduff kills Macbeth because Macduff is
            insane. It looks like Mr.&nbsp;Hyde thinks Macduff kills Macbeth because Macbeth angers Macduff. On a
            reflective level, it looks like Dr.&nbsp;Jeckll thinks Macduff kills Macbeth as part of an act of insane
            violence. It looks like Mr.&nbsp;Hyde thinks Macduff kills Macbeth as part of acts of mistake, Pyrrhic
            victory, and revenge. Isn’t that cool?</p>
        <p>我敢打赌，如果你在八年级时说了这句话，你肯定会得
            A。但是一旦你有能力从多个角度理解这个故事，你就会开始想出各种你可以做的奇妙的事情。例如，你可以让杰克尔博士与海德先生谈判，因为杰克尔博士将能够理解海德先生的观点，并向海德先生证明他认为这个观点是合理的。</p>
        <p>I bet you’d get an A if you had said that in eighth grade. But once you’ve got this ability to understand the
            story from multiple points of view, you begin to think of all kinds of wonderful things you can do. For
            example, you can have Dr.&nbsp;Jeckll negotiate with Mr.&nbsp;Hyde, because Dr.&nbsp;Jeckll will be able to
            understand Mr.&nbsp;Hyde’s point of view and demonstrate to Mr.&nbsp;Hyde that he thinks that point of view
            is legitimate.</p>
        <p>或者，杰克博士可以教海德先生一个新领域的主题。或者杰克博士可以观察海德先生的思想，并在灾难发生之前避免它。那么，让我在这里向你展示另一种情况。我想打开开始检测器并阅读另一个小片段。这个是关于。我该怎么办？我们将进行俄罗斯和爱沙尼亚的网络战争。现在正在阅读背景知识。
        </p>
        <p>Or, Dr.&nbsp;Jeckll can teach Mr.&nbsp;Hyde the subject matter of a new domain. Or Dr.&nbsp;Jeckll can watch
            what’s happening in Mr.&nbsp;Hyde’s mind and avert disaster before it happens. So let me just show you
            another situation here. I want to turn on the onset detector and read another little snippet. This one is
            about. what should I do? We’ll do the Russia and Estonia cyber war. It’s reading background knowledge right
            now.</p>
        <h2 id="unknown-424">未知</h2>
        <h2>Unknown</h2>
        <p>但很快，在左上角，当它开始阅读故事时，你会看到它发现了潜在的复仇行动或潜在的惨胜，并表明它们的基础开始显现，并为系统提供了干预的机会。所以你可以看到它认为可能发生的事情。并非所有事情都会发生。但有些事情确实会发生。
        </p>
        <p>But pretty soon, in the upper left hand corner, as it begins to read the story, you’ll see it spotting the
            onset of potential revenge operations or potential Pyrrhic victories, and show their foundations begin to
            emerge and giving the system an opportunity to intervene. So there you can see all the things that it thinks
            might happen. Not all of them do happen. But some of them do.</p>
        <p>顺便说一句，你会注意到，这是杰克博士和海德先生具有不同文化观点的另一个例子。一个是俄罗斯的盟友，另一个是爱沙尼亚的盟友。一个认为这是无端的报复，另一个认为这是在教训。所以，我不知道。我们还有什么？哦，是的，总统回忆。很久以前，我们讨论过基于关键字计数向量进行信息检索。这很酷，但没有这么酷。
        </p>
        <p>And you’ll note, incidentally, that this is another case of Dr.&nbsp;Jeckll and Mr.&nbsp;Hyde having
            different cultural perspectives. One’s an ally of Russia and one’s an ally of Estonia. One sees it as
            unwarranted revenge and the other sees it as teaching a lesson. So, I don’t know. What else have we got
            here? Oh yeah, president recall. A long time ago, we talked about doing information retrieval based on
            vectors of keyword counts. That’s cool but not this cool.</p>
        <p>这是在故事中出现的概念向量上进行的，例如“复仇”，尽管“复仇”这个词根本没有出现。因此，由于我们能够从多个层面理解故事，因此我们可以使用那些完全不涉及故事中单词的更高层次来驱动检索过程。</p>
        <p>This is doing it on vectors of concepts that appear in the stories, such as revenge, even though the word
            revenge doesn’t appear anywhere. So because we’re able to understand the story on multiple levels, we can
            use those higher levels that don’t involve the words in the story at all to drive the retrieval process.</p>
        <h2 id="unknown-425">未知</h2>
        <h2>Unknown</h2>
        <p>所以这一切都是多种因素的结果，其中之一就是将英语翻译成内部语言的专家。顺便说一下，这也是我们使用 Gerry Sussman
            的传播器架构的结果。所以一个学生来到我们小组，说他想做某件事。我们说好的，这就是系统的组织方式。</p>
        <p>So all that is a consequence of a variety of things, one of which is the specialists that translate the
            English into an internal language. And it’s also, incidentally. I mentioned it a little before. it’s also a
            consequence of our use of Gerry Sussman’s propagator architecture. So a student comes into our group and
            says he wants to do something. And we say OK, here’s how the system is organized.</p>
        <p>这就像是一堆用电线连接在一起的盒子。所以你拿到一个盒子，我们会告诉你输入是什么样子的。我们会告诉你我们想要的输出是什么。如果你不喜欢输入，就忽略它们。如果我们不喜欢你的输出，我们也会忽略它。所以没有人会搞砸任何事情，因为他们只利用了系统的一个非常有限的部分。
        </p>
        <p>It’s like a bunch of boxes that are wired together. So you get a box and we’ll tell you what the inputs are
            going to look like. And we’ll tell you what we want on the outputs. And if you don’t like the inputs, just
            ignore them. And if we don’t like your outputs, we’ll just ignore that. So nobody can screw up anything
            because they have a very circumscribed piece of the system to work with.</p>
        <p>所以我可以说，例如，总统希望伊拉克走向民主。然后就成功了。这开始通过网络传播。在我看来，如果这一切最终没有与感知联系起来，那么这一切都是难以令人信服的。因为如果它最终没有与感知联系起来，那么它就是另一个系统，它展示了它在实际上一无所知的情况下看起来有多聪明。
        </p>
        <p>So I can say, for example, the president wanted Iraq to move toward democracy. And bingo. That starts a
            propagation through that network. All this would be unconvincing, in my view, if it weren’t eventually
            connected to perception. Because if it’s not eventually connected with perception, it’s yet another system
            that demonstrates how smart it can seem to be without actually knowing anything.</p>
        <h2 id="unknown-426">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们所做的另一半工作是早期尝试将语言与世界上正在发生的事情联系起来。所以我们说，想象一个跳跃动作。</p>
        <p>So another half of what we do is an early stage attempt to connect the language stuff with things that are
            going on in the world. So we say, imagine a jumping action.</p>
        <p>其中有一个跳跃动作，这是国防研究计划局为推动“心灵之眼”计划而开发的测试扫描的一部分，该计划的开发很大程度上是麻省理工学院工作的成果，重点在于如果我们想要了解智力的本质，我们就必须了解语言是如何与我们的感知系统耦合的，以及这些感知系统如何回答语言系统提出的问题。
        </p>
        <p>And there is a jumping action that’s part of a test sweep developed by the Defense Research Projects Agency
            to drive what they call the Mind’s Eye program, which was developed largely as a consequence of work done
            here at MIT, focused on the idea that if we’re going to understand the nature of intelligence, we have to
            understand how language is coupled into our perceptual systems and how those perceptual systems can answer
            questions posed to them by the language system.</p>
        <p>这是 Genesis
            系统的一个小演示。以下是我们试图探索的问题。没有什么太严重的问题，只是对人类思维的任何解释而言极其基本的东西的本质。现在，所有这些都可能让你兴奋。你会对我说，嗯，你厌倦了麻省理工学院。你想去别的地方读研究生。</p>
        <p>That’s a little demo of the Genesis system. Here are the issues that we’re trying to explore. Nothing too
            serious, just the nature of what is extraordinarily fundamental to any explanation of human thinking. Now,
            all of this might turn you on. And you say to me, well, you’re sick and tired of MIT. You’d like to go
            somewhere else for graduate school.</p>
        <h2 id="unknown-427">未知</h2>
        <h2>Unknown</h2>
        <p>现在我已经展示了我们在这里做的事情，这是我们在这里做的众多事情之一，我将谈谈你可以去的其他地方。这是一种以麻省理工学院为中心的世界观。它代表了我小时候可以去的所有地方。但是，虽然我这里有这个特定的图表，但我只是在测试我的麻省理工学院傲慢。
        </p>
        <p>So now that I’ve demonstrated what we do here, one of the many things we do here, I’ll talk a little bit
            about other places you can go. This is a sort of MIT centric view of the world. It represents all of the
            places you could go when I was a kid. But while I’ve got this particular diagram on here, I just. sort of
            testing my MIT arrogance.</p>
        <p>我记得我的同事 Peter Szolovits 经常讲的一个故事。他说，当他从加州理工学院来到麻省理工学院参加面试时，他在这里坐了三天，没有人跟他说话。最后他说，我必须做点什么。他走到一个研究生面前说，你好，我叫
            Peter Szolovits。我来自加州理工学院。研究生说，加州理工学院太烂了，然后走开了。</p>
        <p>I remember a story often told by my colleague Peter Szolovits. He says that when he came to a job interview
            from Caltech to MIT, he was sitting here for three days and nobody spoke to him. So eventually he said, I’ve
            got to do something. He walked up to a graduate student and said, hi my name is Peter Szolovits. I’m from
            Caltech. And the graduate student said, Caltech sucks, and walked away.</p>
        <p>无论如何，我们已经填满了您在这里看到的所有这些地方，甚至更多。这只是我今天早上草草列出的清单。我肯定我已经忘记了许多同样有权列入这份名单的地方。但最终，您去哪一所取决于您想成为谁的学徒。因为研究生院就是学徒制。</p>
        <p>Anyway, we’ve populated all these places now that you see here, and more. This is just a list that I
            scratched up this morning. I’m sure I’ve forgotten many that have equal right to be on this list. But in the
            end, which one you go to depends on who you want to apprentice yourself to. Because a graduate school is an
            apprenticeship.</p>
        <h2 id="unknown-428">未知</h2>
        <h2>Unknown</h2>
        <p>这意味着如果你去的地方只有一个人，如果你想拜他为师，那也没关系。每个地方都有不同的重点，因为他们有不同的人。所以你需要找出这些地方是否有人。不管是人工智能还是其他领域。理论物理学。</p>
        <p>And that means if you go to a place with just one person, it’s OK if that’s the person you want to apprentice
            yourself to. Each of these places has a different focus because they have different people. So you need to
            find out if there’s somebody at any of these places. It doesn’t matter if it’s AI or some other field.
            Theoretical physic.</p>
        <p>你必须弄清楚在那个地方是否有你想拜师的人。所以这些实地考察真的很重要。我还想强调的是，当你申请研究生院时，这与申请本科生院有很大不同。因为他们根本不关心他们的学校是否适合你。他们只关心一件事。你是否适合他们的学校。
        </p>
        <p>You’ve got to find out if there’s somebody at that place you want to apprentice yourself to. So those site
            visits are really important. And I would like to also stress that when you make your application to graduate
            school, it’s very different from applying to undergraduate school. Because they don’t care whether their
            school is good for you at all. They only care about one thing. whether you’re good for their school.</p>
        <p>所以不要感到困惑，说说这个专业如何非常适合你。因为他们感兴趣的是你是否会为他们的研究项目做出贡献。哦，我应该说，如果你申请的是人工智能，那就意味着你不会说，我对思维的各个方面都感兴趣。你需要集中注意力。</p>
        <p>So don’t get confused and talk about how it’s a wonderful fit for you. Because what they’re interested in is
            whether you’re going to contribute to their research program. Oh, I should say that if you’re applying to
            artificial intelligence that means you don’t say, I’m interested in all aspects of thinking. You need to be
            focused.</p>
        <h2 id="unknown-429">未知</h2>
        <h2>Unknown</h2>
        <p>你不说自己对思维的各个方面都感兴趣的另一个原因是人工智能职业选择的缺陷理论。尽管这看起来很奇怪，但事实似乎是这样的，人工智能领域的人们经常专注于他们自己做不好的事情。因此，研究语言的人，除了鲍勃·贝里克，经常很难说出连贯的句子。
        </p>
        <p>There’s another reason why you don’t say that you’re interested in all aspects of thinking and that is the
            defect theory of AI career selection. It seems to be the case, strange though it may seem, that people in
            artificial intelligence often specialize their research on the things that they don’t do very well
            themselves. So people who study language, with the exception of Bob Berwick, often have trouble getting out
            a coherent sentence.</p>
        <p>而做手眼协调的人是那种会洒咖啡的人。所以不要说你想研究所有的思考，因为。不过，最极端的情况是。如果你不介意的话，我会给你讲一个极端情况的故事。多年前，我们在旧人工智能实验室接待了一位来自日本的访客。他来了一年。我们叫他义明，只是为了取个名字。
        </p>
        <p>And people who do hand eye coordination are the sorts who spill their coffee. So don’t say you want to study
            all thinking because. The most extreme case of this, though, is. if you don’t mind, I’ll tell you a story
            about an extreme case in this. We had a visitor from Japan in the old artificial intelligence lab many years
            ago. He came for a year. Let’s call him Yoshiaki, just to pick a name.</p>
        <p>义昭在人工智能实验室工作了一年，然后把妻子留在了日本。原因是她怀孕了。当时，除非你接种了天花疫苗，否则无法获得美国签证。由于她怀孕了，她不想接种天花疫苗，因为怀孕期间接种天花疫苗对胎儿有一点危险。所以她留在了那里。
        </p>
        <p>Yoshiaki spent a year at the artificial intelligence lab, and he left his wife in Japan. And the reason was,
            she was pregnant. And at that time, you could not get a visa to the United States unless you had a smallpox
            vaccination. And because she was pregnant, she didn’t want to get a smallpox vaccination because there’s a
            small danger to the fetus if you get a smallpox vaccination while you’re pregnant. So she stayed back there.
        </p>
        <h2 id="unknown-430">未知</h2>
        <h2>Unknown</h2>
        <p>所以，让我们给他打电话。那是在他准备登机回家的前一天。我走进他的办公室，他的办公桌上摆满了他妻子的照片。顺便说一句，我应该告诉你，Yoshiaki
            是一个计算机视觉专家，对物体识别很感兴趣。所以你可能会怀疑他有问题。所以他正在看这些照片。我想，天哪，这是一个温柔的时刻。</p>
        <p>So Yoshiaki, let us call him. it was a day before he was to get on the airplane to go home. I walked into his
            office and his desk was covered with pictures of his wife. By the way, Yoshiaki, I should tell you, is a
            computer vision guy, interested in object recognition. So you might suspect he has some problem. So he’s
            looking at these pictures. I thought, oh my God, this is a tender moment.</p>
        <p>他期待着回到日本与妻子团聚。所以我嘟囔了些类似的话。然后他看着我，好像我是傻瓜之王。他说，这不是问题，温柔。我担心在东京机场我认不出她。所以我说，义昭。这怎么可能？你是研究计算机视觉的。你是研究物体识别的。这是你的妻子。
        </p>
        <p>He’s anticipating his return to Japan and reunion with his wife. So I muttered something to that effect. And
            then he looked at me like I was the king of the fools. And he said, it’s not a question tenderness. I’m
            afraid I won’t recognize her at the Tokyo airport. So I said, Yoshiaki. How can this be? You study computer
            vision. You study object recognition. This is your wife.</p>
        <p>你怎么会认为你在东京机场认不出她呢？然后他看着我，上帝作证。他说，他们看起来都一样。现在我们快要结束了，最大的问题是什么？有用吗？当然有用。现在，它是每个自称是计算机科学家的人的工具包的一部分。什么是强大的想法和这些东西？
        </p>
        <p>How can you think you wouldn’t recognize her at the Tokyo airport? And then he looks at me, and. God is my
            witness. he says, they all look alike. Well now as we come close to the end, what are the big questions? Is
            it useful? Of course it’s useful. It’s part of the toolkit, now, of everybody who claims to be a computer
            scientist. What are the powerful ideas and these things?</p>
        <h2 id="unknown-431">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，这是最强大的，最强大的想法就是最强大的想法。以下是我最喜欢的几个。这并不奇怪。这只是温斯顿的选择。但我想补充一点。那就是所有伟大的想法都很简单。很多时候，我们在麻省理工学院混淆了价值和复杂性。而这个主题中最简单的许多东西实际上是最强大的。
        </p>
        <p>Well, here’s the most powerful, powerful idea is the idea of powerful idea. And here are a few of my
            favorites. No surprises there. That’s just Winston’s picks. But there’s one more I would like to add. And
            that is all great ideas are simple. A lot of times we at MIT confuse value with complexity. And many of the
            things that were the simplest in this subject are actually the most powerful.</p>
        <p>所以要小心不要把简单和琐碎混为一谈，不要以为只有复杂且数学性很强的东西才重要。通常直觉才是最强大的，数学是其中的要素。有时人们会争论真正的智能是可能的。最常见的论点之一是，如果我们有一个房间会怎么样？你在这个房间里，被要求翻译一些中文文件。你有一堆书。
        </p>
        <p>So be careful about confusing simplicity with triviality and thinking that something can’t be important
            unless it’s complicated and deeply mathematical. It’s usually the intuition that’s powerful, and the
            mathematics is the element. Sometimes people argue that real intelligence is possible. One of the most
            common arguments is, well what if we had a room? And you’re in the room and you’re asked to translate some
            Chinese documents. You’ve got a bunch of books.</p>
        <p>最后你还是可以翻译的。但你不能说你懂中文。这是伯克利哲学家索雷尔的论点。问题是，这也是事实。好吧，这个论点是书本不是智能的。它们只是纸上的墨水。而人只是一台计算机，只是一个处理器。它实际上什么都不知道。所以既然它既不能存在于人身上，也不能存在于书本上，它就不可能存在。
        </p>
        <p>And in the end you could do the translation. But you cannot be said to understand Chinese. This is the
            argument of Berkeley philosopher named Sorel. So the trouble is, it’s also true. well, the argument is the
            books aren’t intelligent. They’re just ink on a page. And the person is just a computer, just a processor.
            It doesn’t actually know anything. So since it can’t be in either the person or the books, it can’t be.</p>
        <h2 id="unknown-432">未知</h2>
        <h2>Unknown</h2>
        <p>而这恰恰忘记了当一个正在运行的程序、一个进程在它不断贡献的知识之上及时执行时，就会产生一种魔力。因此，还原论论点是众多无效的论点之一，这些论点被用来论证人工智能是不可能的。但这值得更长时间的讨论。</p>
        <p>And that just forgets that there’s a magic that comes about when a running program, when a process, executes
            in time over knowledge that it continually contributes to. So the reductionist arguments are among the many
            that have been ineffectually posed to argue that artificial intelligence is impossible. But that bears
            longer discussion.</p>
        <p>让我来谈谈我心中最大的问题，那就是，问题不在于我们人类是否太聪明以至于无法让计算机复制或超越我们的智力。问题在于我们是否足够聪明来做到这一点。我曾经养过一只浣熊作为宠物。现在，养浣熊作为宠物是违法的。但这只浣熊是一只孤儿。它的妈妈被车撞了。
        </p>
        <p>Let me just bring up the biggest issue in my mind, which is, it’s not the question of whether we humans are
            too smart to have our intelligence duplicated or excelled in a computer. It’s a question whether we’re smart
            enough to pull it off. I once had a pet raccoon. Now, it’s illegal to have a pet raccoon. But this one was
            an orphan. Its mother had been hit by a car or something.</p>
        <p>我的一个朋友知道我喜欢动物，所以把浣熊带给我。我不得不说，我养了这只浣熊一年。那时，她想出去独自生活。所以我养了这只浣熊。这只浣熊比我养过的任何狗都要聪明。一天之内，她就学会了如何撬开冰箱门。</p>
        <p>A friend of mine brought the raccoon to me knowing I kind of like animals. And I have to say, I kept the
            raccoon for a year. At that point, she wanted to go out and be on her own. So I had this raccoon. And this
            raccoon is smarter than any dog I’ve ever had. Within a day, she learned how to pry the refrigerator door
            open.</p>
        <h2 id="unknown-433">未知</h2>
        <h2>Unknown</h2>
        <p>所以我花了整整一年的时间，每次都用胶带封住冰箱门。然后，我们会互相开玩笑。她不吃热狗。我非常希望她吃热狗，因为热狗便宜又容易吃。她只吃煮熟的鸡翅，不吃热狗。所以有一天我说，好吧，我要捉弄她。我拿了一根鸡骨头。</p>
        <p>So I spent that whole year taping the refrigerator door shut every time. And then, we’d play jokes on each
            other. She wouldn’t eat hot dogs. And I wanted her to eat hot dogs desperately because they’re cheap and
            easy to serve. All she would eat was cooked chicken wings, wouldn’t eat hot dogs. So one day I said, well,
            I’m going to play a trick on her. I took a chicken bone.</p>
        <p>我把它夹在热狗中间，然后扔进垃圾桶。她也接受了。她的基因占了上风，她也接受了。从此以后，她就喜欢上了热狗。她不让我读书。她会爬到书下面，干涉我，让我看书。她总是想吮吸我的拇指，最后我的拇指变成了蓝色。浣熊能吮吸多少东西，这真是令人惊讶。
        </p>
        <p>I stuck it in the middle of a hot dog and put it in a garbage can. And she went for it. Her genes took over
            and she went for it. And she was happy with hot dogs ever after. She wouldn’t let me read. She would crawl
            up underneath the book and interfere and make me. she always wanted to suck on my thumb, which turned blue
            eventually. You’d be amazed at how much a raccoon can suck.</p>
        <p>它非常强大。最棒的部分是她和我一起骑自行车的时候。我穿上了一件厚毛衣，因为它们抓地力很好。我穿上厚毛衣，她会骑在我背上，从我的肩膀上往外看，让方圆几英里的交通都停下来。所以她真的很聪明。</p>
        <p>It’s just extremely powerful. The best parts were when she would go bike riding with me. I put on a heavy
            sweater because they have a pretty good grip. I’d put on a heavy sweater and she’d kind of mount herself on
            my back and look out over my shoulder, stopped traffic for miles around. So she was really smart.</p>
        <h2 id="unknown-434">未知</h2>
        <h2>Unknown</h2>
        <p>但有趣的是，我从来没有假设浣熊足够聪明，能够制造出像浣熊一样聪明的机器。所以当我们认为我们能做到时，它就包含了一些可能合理也可能不合理的狂妄自大。好了，就是这样。还有几件事要做。</p>
        <p>But the interesting thing is that at no point did I ever presume that raccoon was smart enough to build a
            machine that was as smart as a raccoon. So when we think that we can, it involves a certain element of
            hubris that may or may not be justified. Well, there it is. Just a couple more things to do.</p>
        <p>其中之一是，你应该明白，肯德拉、肯尼、袁、马丁和格莱布正在做很多超出他们职责范围的事情。他们安排的所有这些测验复习交易都不在他们的职责范围内。我没有要求他们这样做。那只是普通的专业精神。所以和他们一起工作很愉快，我只想向他们致以热烈的掌声。
        </p>
        <p>One of which is, you should understand that Kendra and Kenny and Yuan and Martin and Gleb are doing a lot of
            stuff that’s outside their job description. All of these quiz review deals that they’ve arranged are not in
            their job description. I didn’t ask them to do it. That’s all just plain old professionalism. So they’ve
            been wonderful to work with and I’d just like to. offer them a round of applause.</p>
        <p>当然，鲍勃、兰迪和马克也做了出色的工作。我们这些工作人员，助教马克、鲍勃和兰迪，除了祝愿你们在最后的狩猎中取得好成绩，以及之后漫长的冬至冬眠期外，没有别的事要做。故事到此结束。我们希望你们从此以后过上幸福的生活。
        </p>
        <p>And of course, Bob and Randy and Mark have done fabulous stuff as well. And we of the staff, the TAs, Mark,
            Bob, and Randy, have nothing else to do except wish you good hunting on the final and a good long winter
            solstice hibernation period after that. And that is the end of the story. And we hope you live happily ever
            after.</p>
        <h1 id="mega-r1.-rule-based-systems">Mega-R1. 基于规则的系统</h1>
        <h1>Mega-R1. Rule-Based Systems</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 46:58 <a
                href="https://youtube.com/watch?v=iusTmgQyZ44">https://youtube.com/watch?v=iusTmgQyZ44</a></p>
        <p> 11 years ago (Jan 11, 2014) — 46:58 <a
                href="https://youtube.com/watch?v=iusTmgQyZ44">https://youtube.com/watch?v=iusTmgQyZ44</a></p>
        <h2 id="rulebased-systems">基于规则的系统</h2>
        <h2>RuleBased Systems</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.</p>
        <h2 id="introduction-7">介绍</h2>
        <h2>Introduction</h2>
        <p>马克·西夫特：大家好。</p>
        <p>MARK SEIFTER: Hi, everyone.</p>
        <p>你们中的一些人可能通过我的辅导课认识我，但如果你不认识我，我是 Mark Seifter，我是 6034 的高级助教，这些周五的 omega 朗诵会与课堂上的其他内容略有不同。好吧，那么，什么是 omega 朗诵。
        </p>
        <p>A few of you may know me from my tutorial, but if you don’t I’m Mark Seifter I’m the senior TA here at 6034
            and these Friday omega recitations are going to be a little bit different than any of the other things in
            the class. All right, so, what is omega recitation.</p>
        <p>Omega 朗诵会在这个演讲室里举行，但我希望使它更像是一场朗诵或辅导课，不幸的是，对于你们中的一些人来说，也许，我们会要求你们在 Omega 朗诵会期间参加，因为我们会尝试使用帕特里克在讲座中为我们概述的一些材料。
        </p>
        <p>Omega recitation is in this lecture style room, but I hope to make it more like a recitation or a tutorial
            by, unfortunately for some of you, perhaps, calling on you guys to participate during the omega recitation,
            as we attempt to work with some of the material that Patrick has given us the big picture for in the
            lectures.</p>
        <h2 id="what-is-megarecitation">什么是 MegaRecitation</h2>
        <h2>What is MegaRecitation</h2>
        <p>如果你已经参加过背诵，那么你可能对算法的工作原理有了一点了解。那么，在 omega 背诵中，我们将弄清楚如何使用算法。</p>
        <p>And if you’ve gone to recitation so far you may received a little bit of enrichment into how the algorithms
            work. Well, here in omega recitation we’re going to figure out how you can work the algorithms.</p>
        <p>这非常重要，因为在这门课上，能够亲手操作我在这里列出的一些例子中的算法，或者像帕特里克有时在课堂上教你算法那样，这是证明你理解算法工作原理的关键方法，我们会在测验中使用它。我们在测验中使用它的方式是。嗯，这是一个古老的测验问题。它是去年的。
        </p>
        <p>And that’s very important, because in this class being able to work the algorithms by hand in some of these
            examples I put up here, or like Patrick sometimes does in lectures to teach you the algorithms, is a crucial
            way of demonstrating that you understand how the algorithms work, and we use it on a quiz. The way we use it
            on the quiz is we. well this is an old quiz problem. It’s from last year.</p>
        <p>它让很多人犯了错误。所以今天我们要复习一下。我将告诉你们所有让很多人犯错的技巧，并强调我之前在三年的助教生涯中学到的技巧，这样你们就不会成为这些技巧的牺牲品。希望如此。</p>
        <p>It tripped up a lot of people. So we’re going to go over it today. I’m going to tell you all the tricks that
            tripped up a lot of people, in addition to emphasizing tricks that I’ve picked up over of the three years of
            TA ing this course before, so that you will not fall prey to any of these tricks. Hopefully.</p>
        <p>我说希望，因为去年我在 omega 朗诵会上讲过一些技巧，但人们还是被骗了。我想也许有些人没有来参加 omega 朗诵会，或者这些技巧非常棘手。所以一定要来参加 omega
            朗诵会。注意技巧。注意如何解决这些问题。如果你不确定，就问问题。</p>
        <p>I say hopefully, because some of these tricks I talked about omega recitation last year and people still got
            tricked by them. I guess maybe some people didn’t come to omega recitation or they were just very tricky
            tricks. So be sure to come to omega recitation. Pay attention to the tricks. Pay attention to how these
            things are solved. Ask questions if you’re not sure.</p>
        <p>当我们一起度过这一个小时后，我希望每个人都能在第一次测验的规则部分做得很好。所以，事不宜迟，如果没有人问这个问题（我认为可能不会有），让我们继续讨论手头的问题。你们中的一些人可能已经注意到，这是一个基于哈利波特的问题。我们去年的助教之一就是哈利波特的忠实粉丝。
        </p>
        <p>And when we get done with the hour together I’m hoping that everyone’s going to do really well on the rules
            part of the first quiz. So without further ado, if there are no questions on that, which I don’t think there
            probably will be, let’s move on to the problem at hand. As some of you may have noticed, it is a Harry
            Potter based problem. One of our TAs last year was a big Harry Potter fan.</p>
        <p>因此，我们这里有一系列规则和一系列断言。现在，如果你碰巧是一个真正的干劲十足的人，并且已经看过所有过去的在线测验，那么你可能已经看到了这个问题。如果你真的非常注重细节，你可能会注意到我写的方式与在线写的方式略有不同。
        </p>
        <p>So what we have here a series of rules and a series of assertions. Now if you happen to be real go getter and
            have already looked at all the past quizzes online, you may have already seen this problem. And if you
            really, really have attention to detail you may notice something a little bit different in the way I wrote
            it as compared to the way it was written online.</p>
        <p>事实上，如果你对细节很在意，但又不熟悉 Scheme 或 LISP，你可能会认为我写的方式更容易理解。如果你无法理解另一种方式，或者某位助教决定再次以类似 LISP 的方式编写，请让我解释一下。</p>
        <p>In fact, you may, if you have a lot of times a detail, but you’re not really in on Scheme or LISP, you may
            think the way I wrote it is much easier to understand. And in case you can’t understand the other way, and
            in case one of the TAs decides to write it in a LISP like way again, let me explain.</p>
        <h2 id="fix-notation">修复符号</h2>
        <h2>Fix Notation</h2>
        <p>如果你看一下我的一条规则，假设 x 是雄心勃勃的，而 x 是哑炮，那么 x 有一个坏词。这就是顶部的规则 0。它以我们所说的固定符号书写。这很容易，很简单。所有运算符都按您期望的顺序排列。2 加 3 等于 5。</p>
        <p>If you look at one of my rules, say if x is ambitious, and x is a squib, then x has a bad term. That’s rule 0
            up at the top. That’s written in sort of what we call in fixed notation. It’s easy, it’s simple. All
            operators are in the order you would expect. 2 plus 3 equals 5.</p>
        <h2 id="prefix-notation">前缀表示法</h2>
        <h2>Prefix Notation</h2>
        <p>还有另一种符号，在以前的测验中有时会使用。</p>
        <p>There’s another kind of notation that is sometimes used on previous quizzes.</p>
        <p>有时，实际上，很多次都是这样。我不确定为什么我们仍然这样使用它，因为这个类已经转移到 Python 上了，但有时我们确实会这样。这叫做前缀表示法。在前缀表示法中，运算符在前。例如，加 2 3 等于
            5。加号在前。然后函数在前。在这种情况下，函数是加号。</p>
        <p>And by sometimes, good number of times, actually. I’m not sure why we still use it that way, because of the
            fact that the class has move on to Python, but we do, sometimes. And that’s called prefix notation. In
            prefix notation, the operator comes first. So for instance, plus 2 3 equals 5. The plus comes first. The
            function, then, comes first. In this case, the function is plus.</p>
        <p>因此，如果我们用前缀表示法来写这个，事实上，我收到它的方式是，当它说“如果和 x 是雄心勃勃的，x 是哑炮，那么 x 有一个坏词。and 先出现，并且有一个括号，其中包括 and
            范围内的所有内容。如果有人对前缀表示法有任何疑问，请提出问题？</p>
        <p>So if we were writing this in prefix notation, in fact, the way I received it, when it said, "if and x is
            ambitious, x is a squib, then x has a bad term. The and went first and there’s a parentheses which included
            everything that was scoped under the and. Does anyone have any questions about prefix notation in case it
            comes up?</p>
        <p>请记住，如果事情开始以一种非常奇怪的方式书写，带有很多很多的括号，那么括号外面的内容，无论是“加号”还是“与”或“或”，都是作用于括号内内容的运算符。但是，我们现在不讨论这个问题，因为我认为最重要的是理解规则，而不太重要的是理解前缀表示法。
        </p>
        <p>Just remember, if things start being written in a really weird way with lots and lots and lots of
            parentheses, whatever’s outside the parentheses, whether it be “plus” or “and” or “or” is an operator that
            acts on the things that are inside. However, we’re not going to deal with that now, because I think the most
            important thing is to understand is the rules, and the less important thing is to understand prefix
            notation.</p>
        <p>你可以在家里做一道用前缀符号写成的题目来测试自己。那么我们这里有什么规则呢？我们有从 0 到 5 的等号。出于某种原因，它们被标记为 P。它们通常被标记为 P 或 R。</p>
        <p>And you can do a problem at home that’s written in prefix notation to test yourself on that. So what rules do
            we have here. We’ve got equals 0 through 5. For some reason they are labeled with P’s. They are often
            labeled with either P’s or R’s.</p>
        <h2 id="labels">标签</h2>
        <h2>Labels</h2>
        <p>它们被贴上“P”标签的秘密在于，有一年有人给它们贴上了“P”标签，表示“命题”或类似的东西。</p>
        <p>The secret on why they are labelled with P’s is that one year somebody labeled them with P’s for like
            “proposition” or something like that.</p>
        <p>然后其他助教看了那个测试，有时也用 P 标记它，然后它就继续用 P 标记，有时用 R 标记。所以这些 P 就是这六条规则。第一条规则是，如果 x 是雄心勃勃的，而 x 是哑炮，那么 x 的术语就不好。那么这些问号 x
            是怎么回事呢？</p>
        <p>And then other TAs looked at that test and sometimes also labeled it with P’s and it sort of down the line
            continued to be labeled with P’s, and sometimes with R’s. So these P’s are these six rules. The first rule
            is if x is ambitious, and x is a squib, then x has a bad term. So what’s the deal with these question mark
            x’s?</p>
        <h2 id="question-marks">问号</h2>
        <h2>Question Marks</h2>
        <p>x 前面的问号，或者下面的 ay，表示有一个变量等待绑定。我们并没有假设有一个名为 x 的哈利波特角色，只有当那个问号 x，即神秘角色，只有当神秘 x 角色雄心勃勃时，这个人才可能有一个坏词。</p>
        <p>The question mark before an x, or perhaps a y down here, indicate that there’s a variable waiting to be
            bound. We’re not assuming there’s a Harry Potter character named x, and only if that question mark x, the
            mystery character, only if mystery x character is ambitious can that person possibly have a bad term.</p>
        <p>我们看到的是，哈利波特世界中的任何角色，或者哈利波特世界之外的任何角色，也许是犀牛，都可以符合这个 x。例如，如果一头犀牛有野心，而一头犀牛是哑炮，那么犀牛就有坏词。这条规则说，对于任何 x，这都是正确的。</p>
        <p>What we’re seeing is any character in the Harry Potter universe, or not the Harry Potter universe, maybe a
            rhinoceros, can fit into that x. So for instance, if a rhinoceros is ambitious, and a rhinoceros is a squib,
            then a rhinoceros has bad term. That rule saying, for any x this is true.</p>
        <p>当我们进行反向链接和正向链接时，如何处理问号 x 以及如何绑定问号 x 非常重要。我会回到这个问题，因为去年有些人犯了一些非常小的错误，这确实搞乱了他们的许多正向和反向链接。这实际上非常棘手。</p>
        <p>And it’s very important how we treat the question mark x, and how we bind question mark x, when we do both
            back chaining and forward chaining. I’ll get back to that, because some people made some very, very small
            mistakes that really messed up a lot of their forward and backward chaining last year. It was actually very
            tricky.</p>
        <p>杰里米，写这篇文章的助教非常聪明，这对你们来说是一个很好的案例研究。</p>
        <p>Jeremy, the TA who wrote this was very clever, and it makes it really great case study for you guys.</p>
        <h2 id="rule-1-protagonist">规则 1 主角</h2>
        <h2>Rule 1 Protagonist</h2>
        <p>好吧，规则 1，如果 x 住在格兰芬多塔楼，那么 x 就是主角。顺便说一句，为了简洁起见，我稍后写这些内容时将使用 GT 表示格兰芬多塔楼，使用 SD 表示斯莱特林地牢。</p>
        <p>All right, so Rule 1, if x lives in Gryffindor tower then x is a protagonist. By the way, for conciseness I’m
            going to be using GT for Gryffindor tower when I write these in later on, and I’ll use SD for Slytherin
            dungeon.</p>
        <p>说到这里，规则 2，如果 x 住在斯莱特林地牢，那么 x 就是坏人，x
            有野心。为什么这里有两件事？好吧，在“如果”之后，我们有所谓的先行词，这是这条规则必须为真才能匹配的东西。在“那么”之后，我们有所谓的后果，在这种情况下有两个后果。</p>
        <p>Speaking of which, Rule 2, if x lives in Slytherin dungeon, then x is a villain, x is ambitious. Why there
            are two things here? Well, after the “if” we have what we call antecedent, that’s something that needs to be
            true in order for this rule to match. After the “then” we have what is called the consequent, and in this
            case there are two consequents.</p>
        <p>任何住在斯莱特林地牢里的人都会自动成为恶棍，而且野心勃勃。因此，你可以认为存在一种“因此”的条件，出于这两个断言的目的，这些条件都会被添加到知识库中。规则 3，如果 x 是主角或 x 是恶棍，并且 x 有野心，那么 x
            就会学习很多东西。顺便说一句，为了确保我们清楚，这个范围是这样的。</p>
        <p>Anyone who lives in Slytherin dungeon is automatically a villain and also ambitious. So you can think of
            there being sort of an “and therefore” for the purposes of both of those assertions would be added to the
            knowledge base. Rule 3, if x is the protagonist or x is a villain, and x is ambitious, then x studies a lot.
            By the way, the scope for this, just to be sure that we’re clear, is this.</p>
        <p>所以我们需要他们成为主角或反派，无论他们有什么野心。规则 4，如果 x 学习很多，并且 x 是主角，那么 x 就会成为赫敏的朋友。规则 5，如果 x 亲吻 y，并且 x 住在格兰芬多塔楼而 y 住在斯莱特林地牢，那么
            x 的条件不好。所以这些就是我们可以用来理解杰里米的哈利波特世界的六条规则。</p>
        <p>So we need them to be a protagonist or villain and no matter what they have to be ambitious. Rule 4, if x
            studies a lot, and x is a protagonist, x becomes Hermione’s friend. And Rule 5, if x snogs y, and x lives in
            Gryffindor tower and y lives in Slytherin dungeon, then x has a bad term. So those are our six rules that we
            can use to understand Jeremy’s world of the Harry Potter universe.</p>
        <p>我们也从四个断言开始。不要低估始终关注断言的价值。这是我在黑板上列出的白星想法之一。让我看看。这个。完美。在使用规则之前始终检查断言。去年这确实让人们感到困惑，你会明白为什么，因为我们正在做去年的问题。我们开始的四个断言。从断言
            0 开始，米利森特住在斯莱特林地牢里。</p>
        <p>And we also start off with four assertions. Let me not underestimate the value of always looking to the
            assertions. It’s one of my white star ideas that are up here on the board. Let me see. This. Perfect. Always
            check the assertions before using a rule. This really tripped people up last year, and you’ll see why,
            because we’re doing last year’s problem. Our four assertions that we start. With assertion 0, Millicent
            lives in Slytherin dungeon.</p>
        <p>断言 1，米莉森特雄心勃勃。断言 1 让人们大跌眼镜，所以请记住，米莉森特雄心勃勃。断言 2，西莫住在格兰芬多塔楼。断言 3，西莫亲吻米莉森特。所以，这些就是我们已经开始的四个断言。</p>
        <p>Assertion 1, Millicent is ambitious. Assertion 1 is what tripped people up, so remember that Millicent is
            ambitious. Assertion 2 Seamus lives in Gryffindor tower. And assertion 3, Seamus snogs Millicent. So those
            are our four assertions that we’ve already started with.</p>
        <h2 id="backward-chaining">反向链接</h2>
        <h2>Backward Chaining</h2>
        <p>现在我们要做的两件事是后向链接和前向链接。</p>
        <p>Now the two things we’re going to have to do are backward chaining and forward chaining.</p>
        <p>现在，当你们学习了这两个反向链接和正向链接时，如果认为正向链接比反向链接更难，请举手。我就知道！我可以证明我想要的任何观点，因为没有人愿意举手。我还认为反向链接比正向链接更高。如果你认为反向链接比正向链接更高，请举手？首先，我们有相当多的人。
        </p>
        <p>Now when you guys learned these two backward chaining and forward chaining, raise your hand if thought
            forward chaining was harder than backward chaining. I knew it! I can prove whatever point I want because
            noone wants the raise their hand. I also think backward chaining is higher than forward chaining. Raise your
            hand if you think backward chaining is higher than forward chaining? First of all, we have a good number of
            people.</p>
        <p>其次，既然没人愿意举手，我就可以反过来问。如果你和一大群人在一起，这是一个专业技巧。你可以通过问另一个方向来证明你想要的任何观点。没有人会举手。所以我同意后向链接比前向链接更高。</p>
        <p>Second of all, since no one wants to raise their hand, I could just ask it the other way. That’s a pro tip if
            you’re ever with a large group of people. You can prove any point you want by asking the other direction. No
            one will raise their hand. So I agree that backward chaining is higher than forward chaining.</p>
        <p>我不同意 Patrick
            的观点，我们不应该早点离开，所以我们先从反向链接开始。这样我们才能确保花大部分时间在上面。对于正向链接，你只需要非常有条不紊地进行，并添加新规则。对于反向链接，你必须画出这棵疯狂的树。在路中间有很多地方容易迷路。所以让我们做一些反向链接。
        </p>
        <p>And I disagree with Patrick that we’re going to get out early, so let’s start with backward chaining first.
            So that we make sure that we spend the bulk of our time with it. The forward chaining, well, you just go
            through pretty methodically and add new rules. Backward chaining, you have to draw this crazy tree. There’s
            a lot of places to get lost in the middle of the road. So let’s do some backward chaining.</p>
        <p>为此，我将在左侧进行操作。因此，当我们进行反向链接时，我们必须记住一些直接写在测验上的内容。所以您不必担心这一点。但它们仍然非常重要。所以。实际上，我会先在这里写。我要读出来。</p>
        <p>And to do that, I’ll do it over here on the left side. So when we’re doing backward chaining, we have to
            remember a few things that are written directly on the quiz. So you’re not going to have to worry about
            that. But they’re still pretty important. So. actually, I’ll write it on here first. I’m going to read them
            off.</p>
        <p>因此，在处理假设时，反向链接器首先尝试在断言列表中找到匹配的断言。如果未找到匹配的断言，反向链接器将尝试找到具有匹配结果的规则。规则中包含一些可以证明断言正在尝试解决的内容。例如，如果我对 Seamus
            Millicent 进行反向链接，会发生什么？</p>
        <p>So when working on a hypothesis, the backward chainer tries to find a matching assertion in the list of
            assertions first. If no matching assertion is found, the backward chainer will try to find a rule with a
            matching consequence. A rule that has something in the then that can prove the assertion is trying to figure
            out. So for instance, if I was doing backward chaining on Seamus Millicent, what happens?</p>
        <p>我马上就完成了，因为有一个断言，即第三个断言，说的是 Seamus
            Millicent。我证明了这一点。我完成了。我很高兴。我们可以走了，我们可以回家了。不幸的是，这不是测验要求我们做的。现在，让我们假设我们应该说，我不知道，Seamus 是主角。</p>
        <p>I’m immediately done because there’s an assertion that’s assertion three that says Seamus Millicent. I proved
            it. I’m done. I’m happy. We can leave, we can go home. Unfortunately, that’s not what the quiz asks us to
            do. Now, let’s say that instead, we were supposed to say, I don’t know, Seamus is a protagonist.</p>
        <p>那么，我们最终会浏览这里，看看规则一可以证明某人是主角。我们会诅咒并试图证明规则一的前提，即，好吧，他住在格兰芬达塔吗？断言二，他确实住在。所以这真的很快。如果太快了，别担心。我们将逐步解决实际问题。</p>
        <p>Well then, we’d wind up looking through here and we would look at the fact that rule one can prove that
            someone is a protagonist. And we’d curse and try to prove whatever is in the antecedent of rule one, which
            is, OK, well, does he live in Gryffinder Tower? Assertion two, he does. So that’s some really quick. And if
            that was too, fast don’t worry. We’re going to go step by step with the actual problem.</p>
        <p>但我只想快速给出两个非常简单的问题，说明它将如何解决。让我们一步一步地解决真正的问题。请记住，反向链接器永远不会将新断言添加到断言列表中。</p>
        <p>But I just wanted to give two really easy problems, really quickly, how it’s going to do it. Let’s go step by
            step with the real problem. Let’s keep in mind, backward chainer never adds new assertions to the list of
            assertions.</p>
        <p>如果你有平局，你总是首先根据规则顺序排序，从 P0 到
            P5。如果同一条规则与列表断言中的多个事物匹配，那么你根据断言的顺序进行平局。非常重要。歧义消除和平局打破是容易出错的地方。所以我们要试着证明米莉森特成为了赫敏的朋友。</p>
        <p>And if you have a tiebreak, you always order based on rule order first, P0 through P5. And if the same rule
            matches with more than one thing in your list assertions, then you tiebreak based on the order of the
            assertions. Very important. Disambiguation and tie breaking are a big place to get messed up. So we’re going
            to try to prove that Millicent becomes Hermione’s friend.</p>
        <h2 id="goal-trees-1">目标树</h2>
        <h2>Goal Trees</h2>
        <p>所以我要简化一些内容，但不是第一行。所以米莉森特成为赫敏的朋友。我们开始画一棵目标树。现在你们很快就会知道这些到底是什么意思，事实上，下一节课就会讲到。但现在，相信我，这些目标树是深度优先的。</p>
        <p>And so I’m going to abbreviate some of the things, but not for the very first line. So Millicent becomes
            Hermione’s friend. We start drawing a goal tree. Now you guys are going to learn exactly what these mean
            very soon, in fact, next lecture. But for now, trust me when I say these goal trees are depth first.</p>
        <p>我会解释这意味着什么，因为有些人把自己搞得一团糟，用不同的方式处理目标树，花费了比他们需要的更多的时间。现在，米莉森特成了赫敏的朋友。让我们假装我们是反向链接者。我们试图证明这是真的，或者反驳说，嗯，根据我们拥有的，这绝对不是真的。让我们看看。我现在要让你们帮忙。
        </p>
        <p>And I’ll explain what that means because some people have messed themselves up and spent more time than they
            needed to by treating the goal tree in a different way. Now, Millicent becomes Hermione’s friend. Let’s
            pretend that we are the backward chainer. We’re trying to prove that this is true or disprove and say, well,
            it’s definitely not true with what we have. So let’s see. I’m now going to make you guys help.</p>
        <p>而后面的人认为他们不会被叫到。所以我经常喜欢叫到后面的人。你认为我们应该做的第一件事是什么？第一件事？是的，寻找匹配的断言。很好。大家，我们有匹配的断言吗？没有。这将是世界上最简单的测验问题。我们没有匹配的断言。太好了。那么，既然我们没有匹配的断言，现在该怎么办？
        </p>
        <p>And people in the back think they won’t be called on. So I often like to call on people in the back. What do
            you think is the first thing we should do? Very first thing? Yes, look for a matching assertion. Excellent.
            Do we have a matching assertion, everyone? No, we don’t. That would be the world’s easiest quiz problem. We
            do not have a matching assertion. Great. So, since we don’t have a matching assertion, what now?</p>
        <p>观众：我们开始研究规则。马克·西弗特：没错。你看到任何规则可以证明米莉森特成为赫敏的朋友了吗？观众：P4。马克·西弗特：没错。你可以在 P4 中看到，x，可以是任何人。任何人都可以成为赫敏的朋友。太好了。所以 P4
            是我们今天的规则。</p>
        <p>AUDIENCE: We start to look at the rules. MARK SEIFTER: That’s right. And do you see any rule that could prove
            that Millicent becomes Hermione’s friend? AUDIENCE: P4. MARK SEIFTER: That’s right. You can see in P4, x,
            which can be anybody. Anyone is capable of being Hermione’s friend. Great. So P4 is our rule of the hour.
        </p>
        <p>因此我们将使用 P4。当我们使用 P4
            来证明米莉森特成为赫敏的朋友时，我们将不得不在目标树中添加一些东西。让我们看看。我们必须在目标树中添加什么？观众：我们必须看看米莉森特是否学习了很多。马克·塞夫特：没错。每个人都看到了吗？</p>
        <p>So we’re going to use P4. And when we use P4 to prove that Millicent becomes Hermione’s friend, we’re going
            to have to add something or another to the goal tree. So let’s see. What do we have to add to the goal tree?
            AUDIENCE: We have to see if Millicent studies a lot MARK SEIFTER: That’s right. Does everyone see that?</p>
        <p>根据规则四，为了让她成为赫敏的朋友，我们必须看看她是否学习很多并且是主角。问题？观众：马克·西夫特：好的，这是一个非常好的问题。如果你先看前件，你就会搞砸了。它部分是反向的，是有原因的。你需要看看结果。那为什么呢？
        </p>
        <p>In order for her to become Hermione’s friend, by rule four, we have to see if she studies a lot and is a
            protagonist. Question? AUDIENCE: MARK SEIFTER: OK, that’s a very good question. You’re going to get screwed
            up if you look at the antecedent in backward chaining first. It’s backwards partially for a reason. You need
            to look at the consequent. Now why is that?</p>
        <p>好吧，假设有一条规则六，规定如果 x 成为赫敏的朋友，那么赫敏就会给 x
            喂复方汤剂，或者类似的东西。这条规则是否有助于我们反向推理，以找出米莉森特是否成为赫敏的朋友？观众：马克·西弗特：有些人摇头。但想想看，这条规则能证明这一点吗？不能。&nbsp;</p>
        <p>Well, let’s say there is a rule six that said if x becomes Hermione’s friend, then Hermione feeds x Polyjuice
            Potion, or something like that. Will that rule help us in back chaining to figure out if Millicent becomes
            Hermione’s friend? AUDIENCE: MARK SEIFTER: Some people are shaking their head. But think about it, will that
            rule be able to prove it? No.&nbsp;</p>
        <p>现在，如果他们真的成为了赫敏的朋友，我们想做一些前向连锁，我们会发现他们会因为复方汤剂而变成其他东西。但这不会帮助我们做我们想要反向连锁的事情，也就是证明最上面的那个东西。所以我们实际上需要寻找一个在结果中具有我们当前目标的东西。
        </p>
        <p>Now, if they do become Hermione’s friend and we want to do some forward chaining, we’ll figure out that
            they’re going to transmute into some kind of other thing because of the Polyjuice Potion. But it’s not going
            to help us do the one thing we want to back chain, which is to prove that thing on the top. So we actually
            need to look for something that has our current goal in its consequent.</p>
        <p>然后我们添加先行词。到目前为止，我问过的人。抱歉，我不知道你们的名字。像帕特里克这样的人每次都正确地给了我答案，这非常好。所以还要注意，她并没有说 x
            研究很多需要添加到目标树中。她说米莉森特研究很多。哦，另一个问题。太好了。听众：显然，在把规则 4 放在那里之后，我们不应该先检查断言吗？</p>
        <p>Then we add on the antecedents. As people I’ve asked so far. sorry, I don’t know your names. like Patrick,
            have correctly given me every time, which is excellent. So notice also that she didn’t say x studies a lot
            needs to be added to the goal tree. She said Millicent studies a lot. Oh, another question. Great. AUDIENCE:
            So obviously after putting rule 4 there, shouldn’t we first check the assertions.</p>
        <p>检查断言而不是条件 MARK SEIFTER：所以问题是，一旦我们将规则 4 放进去，在检查其他规则之前，我们是否应该检查规则 4
            的前提是否已经在断言中？答案是？先生，您不仅正确，而且您还领先一步。所以我假设我要求您做下一件事。这正是我们在将其绘制到树上后要做的事情。</p>
        <p>Check on the assertions instead of our conditions MARK SEIFTER: So the question is, once we put rule four in
            there, should we check to see if those antecedents of rule four are already in the assertions before
            checking other rules? The answer? You’re not only correct, sir, you’re exactly one step ahead. So I’m going
            to assume I called on you for the very next thing. That’s exactly what we do once we draw it onto the tree.
        </p>
        <p>你正好在路上。还有问题吗？完美。好的，太好了。所以再说一次，我们经常提到米莉森特的研究。米莉森特是主角。我们不提 x。有些人这样做了。当然，正如我们所听到的，它是一个 n
            节点。我们需要它们都是真的，否则我们就无法继续下去。所以我们有。我将使用 m 来表示米莉森特。米莉森特经常研究。</p>
        <p>You’re exactly on the way. Further question? Perfect. All right, great. So again, we’re putting up Millicent
            studies a lot. Millicent is a protagonist. We’re not putting up x. Some people did that. And of course it’s
            an n node as we heard. We need them both to be true or we’re not going to be able to continue onward. So we
            have. I’m going to use m for Millicent. Millicent studies a lot.</p>
        <p>同样，米莉森特是主角。我们已经听说，在进入任何规则之前，我们需要在断言中搜索我们接下来要寻找的内容。问题是，我们接下来要寻找什么，按照什么顺序？这就是我告诉你的深度搜索的关键所在。我们将在这里查看左侧节点。</p>
        <p>Also over here, Millicent is a protagonist. And we’ve already heard that we need to search in the assertions
            before we go into any rules for what we’re looking for next. The question is, what are we looking for next
            and in what order? This is where that thing I told you about depth search is crucial. We are going to look
            here on the left node.</p>
        <p>如果它有任何子分支，如果我们必须继续往下看，我们暂时不会在这里查看。有几个原因。其中之一是我们很懒惰。如果您了解了惰性求值，那么它也是惰性的。如果我们可以反驳这项研究的很多分支，我们就不必对主角分支进行任何工作。我们就完成了。我们离开这里。
        </p>
        <p>And if it has any children, if we have to keep going down, we are not going to look here yet. There’s a few
            reasons for that. One of which is that we’re lazy. The evaluation, if you learned about it, is also. if you
            learn about lazy evaluation, is also lazy. And if we can disprove this study’s a lot branch, we don’t have
            to do any work with the protagonist branch. We’re done. We’re out of here.</p>
        <p>这是一种损失。所以我们将进入研究的很多分支。我们已经听到观众说，我们需要看看它是否在断言中。各位，米利森特在断言中研究了很多吗？不，不是。那么，现在我们要回答我之前关于前因或后果的问题。</p>
        <p>And it’s a loss. So we’re going to move down the study’s a lot branch. And we already heard from the
            audience, we need to look to see if it’s in the assertions. Everyone, is Millicent studies a lot in the
            assertions? No, it’s not. Well then, now we’re going to get to that question that I had before about the
            antecedent or the consequent.</p>
        <p>因为研究位于规则 4 的前件中，但我们无法在此处使用规则 4。事实上，这就是我们一开始得到的结果。所以问题是。让我们看看，问题是，我们在后件中是否有一条规则与 Millicent
            研究非常匹配。是的，没错，P3。没错。所以 P3 会给我们很多 Millicent 研究。</p>
        <p>Because study lies here in the antecedent of rule four, but we’re not going to be able to use rule four here.
            In fact, that’s how we got here in the first place. So the question is. let’s see, the question is, do we
            have a rule in the consequent that matches Millicent studies a lot. Yeah, that’s right, P3. That’s right. So
            P3 would give us Millicent studies a lot.</p>
        <p>那么，我们要在目标树中添加什么呢？观众：我们添加了两个，即米莉森特是主角还是反派，以及马克·西弗特：是的，她。原来是个女孩。所以这是完全正确的。我们听说我们需要添加。我们需要添加一个 AND 节点，它的底部还有一个
            OR 节点，因为这是一个有点复杂的规则。</p>
        <p>And so therefore what will we add to the goal tree? AUDIENCE: We add both that is Millicent either a
            protagonist or a villain and MARK SEIFTER: Yeah, she. Turns out to be a girl. So that’s exactly right. We
            heard that we need to add. we need to add an AND node, which also has an OR node at the bottom of it,
            because this is a little bit of a complicated rule.</p>
        <p>因此，我们有一个 AND 节点。因此，我们有一个 AND 节点。AND 节点上的第一件事是 Millicent 是主角或 Millicent 是反派。AND 节点上的第二件事是 Millicent 有野心。实际上，这是
            AND 节点上的第二件事。我希望这就是我所说的。好吧，AND 节点上的第二件事是 Millicent 有野心。因此，这是我们方便的小树。</p>
        <p>So we have an AND node. So we have an AND node. And the first thing on the AND node is Millicent is a
            protagonist or Millicent is a villain. And the second thing on the AND node is Millicent is ambitious.
            Actually, that is the second thing on the AND node. I hope that’s what I said. All right, the second thing
            on the AND node is Millicent is ambitious. So here’s our handy little tree.</p>
        <p>现在，正如我所说，这是很重要的一点，我们正在进行深度优先搜索。我们沿着左边的分支向下搜索。那么你认为我们接下来要去这棵树的哪里呢？观众：马克·西夫特：不完全是。这是一个很好的猜测。你没有说我们试图证明 m
            是右边分支上的主角，这是一个非常常见的错误。你会怎么做？</p>
        <p>Now this is where it’s important, as I said, we’re doing a depth first search. We’re going down along the
            left branch. So where do you think we’re going to go next on this tree? AUDIENCE: MARK SEIFTER: Not quite.
            That’s a good guess. You didn’t say we’re trying to prove m is a protagonist on the right branch, which is a
            very common mistake. What do you do?</p>
        <p>观众：你应该往下看，看看 m
            是否有后果。米莉森特是主角。马克·塞夫特：我们想看这两个米莉森特是主角中的哪一个？是的，最左边的那个。没错。所以你们将在周一学习搜索深度，到那时我所说的内容就会变得更加清晰。但是，是的，你总是尽可能沿着左边的分支往下走。
        </p>
        <p>AUDIENCE: You should go down to see if there’s a consequence for m is. Millicent is a protagonist. MARK
            SEIFTER: Which one of the two Millicent is a protagonist do we want to look at? Yeah, the farthest down
            left. That’s right. So you guys will learn depth for search on Monday, at which point it will become much
            clearer what I’m saying. But yeah, you always follow the left branch as far down as you can.</p>
        <p>然后，你从你遵循的同一分支中选择正确的分支。太好了，所以我们将尝试找出米莉森特是主角。我们首先做什么？大家？检查断言，是的。它在里面吗？不，不在。好的。因此，我们将尝试找到一条规则，就像我们听到的关于米莉森特是否是主角一样。那么，一直到后面？
        </p>
        <p>Then you take the right branch of that same branch that you followed. Great, so we’re going to try to find
            that Millicent is a protagonist. What do we do first? Everyone? Check assertion, yes. Is it in there? No,
            it’s not. OK. So, therefore we’re going to try to find a rule as we heard about whether Millicent is a
            protagonist. So, all the way in the back?</p>
        <p>后面是否有匹配的规则？观众：马克·西弗特：匹配的结果为米莉森特是主角。观众：嗯，有一条规则。马克·西弗特：好的。那我们来试试。哪一条？观众：一条。马克·西弗特：规则一，没错。那么，我们需要在目标树中添加什么？观众：另一条
            马克·西弗特：对。请问上面有什么？</p>
        <p>All the way in the back, is there a rule that matches? AUDIENCE: MARK SEIFTER: That matches in its consequent
            that Millicent is a protagonist. AUDIENCE: Um, there is a rule. MARK SEIFTER: OK. Well, let’s give it a try.
            Which one? AUDIENCE: One. MARK SEIFTER: Rule one, that’s right. So therefore, we would have to add what to
            our goal tree? AUDIENCE: Another MARK SEIFTER: Right. And what’s on that please?</p>
        <p>观众：马克·塞夫特：是的，米莉森特住在格兰芬多塔楼，太好了。所以我们要寻找的新东西是。假装这有联系。实际上，我猜它确实有联系。我们正在寻找住在格兰芬多塔楼的米莉森特。太好了。既然这是深度优先，那我们下一步就去那里。太好了，我们开始行动了。那么我们首先要做什么？我们在断言中有吗？有些人说有，但答案是否定的。
        </p>
        <p>AUDIENCE: MARK SEIFTER: Yeah, Millicent lives in Gryffindor Tower, perfect. So, our new thing that we’re
            searching for is. pretend this connects. Actually, I guess it does connect. We’re looking for m lives in
            Gryffindor Tower. Great. And since it’s depth first, that’s where we go next. Great, we’re on a roll. So
            what do we do first? Do we have that in assertions? Some people say yes, but the answer is no.</p>
        <p>事实上，西莫住在格兰芬多塔楼。你们大多数人都说“不”。你说得对。多数规则。我们没有任何断言。但是，我们有一条规则来处理后件吗？没有。那么我们该怎么办？人们说了不同的事情，但都是正确的。你说回溯。转到下一个。我们无法证明这一点。全部正确。放一个大
            x。这不是真的。现在我们向上查找。我们处于 OR 节点。</p>
        <p>We have, in fact, Seamus living in Gryffindor Tower. Most of you said “no.” You’re right. The majority rules.
            We don’t have any assertions. However, do we have a rule with that in the consequent? No.&nbsp;So what do we
            do? People are saying different things that are all correct. Backtrack, you say. Go to the next. We can’t
            prove it. All correct. Put a big x. This isn’t true. Now we’d look up. We’re on an OR node.</p>
        <p>所以我们还没完，因为其中任何一个都可能是真的。所以现在我们回过头来，回溯一下，米莉森特是个坏蛋。我们首先要做什么？检查断言。它在里面吗？不，不在。别担心，我们很快就会找到一个，大约 40%。或者说 30% 到 40%
            的班级学生会很快被扣分。所以，我们看到米莉森特是个坏蛋。但它不在断言中。</p>
        <p>So we’re not done yet because either of those can be true. So now we go back up, backtrack, Millicent is a
            villain. What do we do first? Check assertions. Is it in there? No, it’s not. Don’t worry, we’re getting to
            one where it will be, where about 40%. or 30% to 40% of the class lost points, very, very soon. So, we see
            that Millicent is a villain. And it’s not in the assertions.</p>
        <p>那么，是否有任何规则的后件包含这一点？观众：马克·西弗特：我们正在寻找米莉森特是反派。观众：马克·西弗特：哦，你看不到。好吧，我会稍微往下移一点。当我在“和”周围加上括号时，它有点不平衡。等等。就是这样。观众：是主角还是反派。马克·西弗特：不过，我们正试图证明她是个反派。
        </p>
        <p>So therefore, is there any rule that has that in its consequent? AUDIENCE: MARK SEIFTER: We’re looking for
            Millicent is a villain. AUDIENCE: MARK SEIFTER: Oh, you can’t see it. All right, I will move it down
            slightly. It got a little bit off kilter when I put in the parentheses around the “and.” Hold on. There you
            go. AUDIENCE: If is a protagonist or is a villain. MARK SEIFTER: We’re trying to prove that she’s a villain,
            though.</p>
        <p>那么，在结果中我们有什么可以证明她是一个恶棍的东西吗？如果我们触发这条规则，有什么可以证明她是一个恶棍的东西吗？想在这里提供一点帮助吗？观众：P2。如果她住在斯莱特林，那么她就是一个恶棍。马克·西夫特：啊，现在看到了吗？有两个，所以有点难。现在有人问，那个雄心勃勃的东西怎么样？我们可以把它添加到树上吗？
        </p>
        <p>So do we have any with that in the consequent? anything that will prove that she’s a villain if we fire off
            that rule. Want to give here a little bit of help? AUDIENCE: P2. If she lives in Slytherin, then she’s a
            villain MARK SEIFTER: Ah, see that now? It had two, so it was a little bit harder. Now some people ask, what
            about that ambitious thing? Can we like add that to the tree or something?</p>
        <p>不。反向链接器是一心一意、专注和愚蠢的。即使它后来需要证明她有野心，它也不在乎。它只是看到那里的恶棍。恶棍，哦，那太好了。只要我们有先行词，我们就会很高兴。这实际上是一个重要的点，不是在这个特定的问题中，而是在其他一些问题中。所以通过使用
            P2，我们显然会让先行词 x 生命在斯莱特林地牢中。</p>
        <p>No.&nbsp;The backward chainer is single minded, focused, and stupid. Even though it’s later going to need to
            prove that she’s ambitious, it doesn’t care. It just sees the villain there. Villain, oh, that’s great. As
            long as we’ve got the antecedent we are going to be happy. This is actually an important point, not in this
            particular problem, but in some other problems. So by using P2, we’re obviously going to have the antecedent
            x lives in Slytherin dungeon.</p>
        <p>这里的 x 是米莉森特。所以米莉森特住在斯莱特林地牢里。好的，那么当我们看到这个新断言时，我们首先要做什么呢？我们检查断言。它在里面吗？是的。这不是每个人都失分的地方，但是是的，它在里面。检查。这个 OR
            节点很满意。现在我们转到 AND 节点，其中米莉森特很有野心。所以米莉森特很有野心。我们现在做什么？</p>
        <p>And x here is Millicent. So Millicent lives in Slytherin dungeon. All right, so what’s the first thing we do
            when we see this new assertion? We check the assertions. Is it in there? Yes. That’s not where everyone lost
            points, but yes, it is in there. Check. This OR node is happy. Now we move up to the AND node with Millicent
            is ambitious. So Millicent is ambitious. What do we do now?</p>
        <p>听众：那么 P2 的结果不就是 x 是个恶棍，x 有野心吗？我不知道是不是这样，但在我们的断言中，我们有 Millicent 有野心。这有必要吗？MARK SEIFTER：让我们使用规则二。实际上，不是。</p>
        <p>AUDIENCE: So isn’t the consequent of P2 that x is a villain and x is ambitious. I don’t know if it’s or
            whatever, but in our assertions we have that Millicent is ambitious. Is that necessary to let us. MARK
            SEIFTER: To let us use rule two. Actually, no.</p>
        <p>问题是，我们是否需要将 x 雄心勃勃列入我们的断言列表中才能使用 P2。例如，如果我们没有 Millicent 雄心勃勃的断言，P2
            是否会有问题？所以，这是一个非常有趣的问题。这有点像我之前所说的，它是一心一意的，它很专注，它不关心另一个。</p>
        <p>The question is, do we need the x is ambitious to be in our list of assertions in order to use P2. For
            instance, if we didn’t have the assertion that Millicent is ambitious, would P2 have a problem firing? So,
            it’s a very interesting question. It’s sort of what actually I was trying to mention earlier when I said
            it’s single minded, it’s focused, it doesn’t care about the other one.</p>
        <p>因为你可能会说，好吧，等一下，如果没有它的两个结果，P2 就不可能真正被触发。但事实上，反向链接器并不关心。反向链接器想要做的就是证明米莉森特是否有可能是坏人。</p>
        <p>Because you might say, well, wait a minute, P2 could never have actually fired if it didn’t have both of its
            consequents. But in fact, the backward chainer doesn’t care. All that the backward chainer is looking to do
            is to prove whether or not there’s a possibility that Millicent might be a villain.</p>
        <p>它并不是想说，哦，某些规则的所有结果都使米利森特成为一个恶棍，而所有其他的东西，比如雄心勃勃，都在数据库中。</p>
        <p>It’s not trying to say, oh, all the results of some certain rules are making Millicent a villain and all the
            other things that are there, like ambitious, are in the database.</p>
        <h2 id="backward-chainer">后向链接器</h2>
        <h2>Backward Chainer</h2>
        <p>它不是前向链接器。它实际上并不关心。这有时会导致不必要的计算。但它非常简单，并且编码非常简单。因此从长远来看，它通常会加快您的速度。</p>
        <p>It’s not the forward chainer. It actually doesn’t care. This sometimes leads to unnecessary computation. But
            it’s very simple, and it’s very simple to code. So it often will probably speed you up in the long run.</p>
        <p>比如，如果 then 中有 100 件事，而你只关心其中一件，那么添加这 99 件几乎是一种浪费。这里有后续内容吗？听众：99 的顺序。所以 MARK
            SEIFTER：你可以试试。但是，到那时，每次检查断言时，你都会尝试检查这 99 件。此外，你可能没有使用这些规则。</p>
        <p>Like if there’s, for instance, 100 thing in the then and you only care about one of them, it would be a
            waste, almost, to add those 99. There’s a follow up over here somewhere? AUDIENCE: Order m of 99. And so
            MARK SEIFTER: You can try. However, then at that point, every time you check the assertions, you’d be
            attempting to check those 99. Also, you might not have used those rules.</p>
        <p>问题是，您能否制作一个哈希表，其中包含结果中的所有 100
            个事物？然后，在制作了该哈希表之后，由于您在检查时已经遍历了这些事物，因此您可以将它们添加到断言中。问题是您可能不一定想要。使用后向链接器。您可能不一定想认为结果中的所有事物都必然为真。</p>
        <p>The question is, could you make a hash table with all of the 100 things that were in the consequent? And
            then, after having made that hash table since you were already walking through those anyway when you were
            checking it, you could add those to the assertions. The problem is that you might not necessarily want. with
            the backward chainer. you might not necessarily want to think that all those things in the consequent are
            necessarily true.</p>
        <p>但是，你可能并不关心它们。然后，每次检查左上角的断言列表时，你都必须进行排序和计算。听众：马克·塞夫特：你不知道有哪些断言。每次到达目标树中的新节点时，你都必须检查左上角的整个断言列表。</p>
        <p>However, you might not care about them. And then you have to make an order and computation every time you
            check that list of assertions in the upper left. AUDIENCE: MARK SEIFTER: You don’t know what assertions
            there are. You have to check the entire list of assertions in the upper left every time you get to a new
            node in the goal tree.</p>
        <p>所以每个断言都是 1 阶的。但我并不是说找到一个特定的断言是 n 阶的。我是说每次你在目标树中获得一个新节点时，你都必须检查你添加的所有断言。听众：无论如何你都必须这样做。断言。马克·塞夫特：是的，你必须检查这四个。
        </p>
        <p>So each one is order 1. But I’m not saying to find a specific assertion is order n.&nbsp;I’m saying every
            time you get a new node in the goal tree, you have to check all the assertions you’ve added. AUDIENCE: You
            have to do that anyway. assertions. MARK SEIFTER: Yeah, you’d have to check for the four.</p>
        <p>但是假设 P2 有 10,000 个结果。</p>
        <p>But let’s say that there were 10,000 consequents of P2.</p>
        <h2 id="hash-tables">哈希表</h2>
        <h2>Hash Tables</h2>
        <p>如果在目标树上证明 P2 为真后将它们全部加起来，则必须检查 10,000 个而不是四个。</p>
        <p>You would have to check 10,000 instead of four if you added them all after proving P2 was true on the goal
            tree.</p>
        <p>此外，另一个重要注意事项是，您可能不会使用目标树的每个分支，如果存在一个更高的
            OR，并且您最终使用了不同的分支，在这种情况下，您可能必须为每个子分支创建一个单独的哈希表，然后记住哪些哈希表是根据哪个子分支更新的。然后子分支就死了。这可能得不偿失。这当然是一个有趣的问题。</p>
        <p>Also, another important note is that you might not use every branch of the goal tree, if there’s like an OR
            that’s up higher and you wind up using a different branch, in which case you’d probably have to make a
            separate hash table for every sub branch that you have and then remember which hash tables were updated
            based on which sub branch. And then sub branches die. It’s probably more effort than it’s worth. It’s
            certainly an interesting question.</p>
        <p>这是一个很棒的辩论问题。另外，我很乐意稍后再谈论它。我认为，一个非常聪明地制作哈希表并仔细考虑问题的人，正如帕特里克所说，更多的知识可以意味着更好的搜索。好吧，实际上，也许他没有说更多的知识可以意味着更好的搜索，因为我们还没有上过那堂课，但是。
        </p>
        <p>It’s a great question for a debate in recitation. Also, I’m happy to talk about it later. I think that
            someone who very intelligently made hash tables and thought the problem through, as Patrick said, more
            knowledge can mean a better. well, actually maybe he hasn’t said more knowledge can mean a better search
            because we haven’t done that lecture yet, but.</p>
        <p>听众：您说的是实施决策，使用哈希表可能是实现此目的的好方法。问题是，我们假设规则如何触发以及它们触发的内容取决于表中断言的顺序。因此，如果我们使用哈希表，我们就会失去顺序。马克·塞夫特：没错。没错。</p>
        <p>AUDIENCE: You’re talking about implementation decision and using hash table might well be a good way to do
            this. The problem is that we’re presuming some things about how the rules are firing and what they’re firing
            that depend on the order of assertions in the table. So if we used the hash table, we’d lose the order. MARK
            SEIFTER: That’s true. That’s true.</p>
        <p>但我想如果有人想实现这种实现，就像在基于规则的系统中做一些研究一样，那么就有可能提高运行速度。在这门课上，我们并没有完全专注于最快的算法。但我仍然认为这是一件很酷的事情，值得一试。人群中有人有问题吗？我们从你开始。观众：所以忽略哈希表？马克·塞夫特：忽略哈希表。
        </p>
        <p>But I’m thinking like if someone wanted to make that implementation as like do some research in rules based
            systems, it’s possible you can increase the running speed. In this class, we are not completely as focused
            on the fastest algorithms. But I still think that a cool thing to try. Questions from people in the crowd?
            We’ll start with you. AUDIENCE: So ignoring the hash table? MARK SEIFTER: Ignore the hash table.</p>
        <p>这是个好主意，因为它可以丰富知识。它不是你在测验中可能需要知道的任何内容。听众：所以如果它变成了一个你使用这个“then”语句是因为你想要反派。因此，除了使用这个之外，你还会得到 x
            是雄心勃勃的。之后，我们还使用另一个“then”语句，表示 x 不是雄心勃勃的。那么你会如何解决这个问题？</p>
        <p>That’s a good idea because it’s a little extra enrichment thing. It’s not anything you would possibly need to
            know for the quiz. AUDIENCE: So if it became an are you using this “then” statement because you want the
            villain. So along with using this you get x is ambitious. used after that, another “then” statement that we
            also use that says x is not ambitious. So how would you resolve that?</p>
        <p>马克·塞夫特：那么问题是，假设你使用规则二在树中的某处得出 x 是恶棍，然后这条规则又说 x 有野心。然后，你在其他规则中的某处得出 x
            不具野心，而你随后又需要这条规则。问题是，你如何解决这个问题？它有什么作用？嗯，首先。</p>
        <p>MARK SEIFTER: So the question is, let’s say you used rule two somewhere in the tree to get that x is a
            villain, which then says that x is ambitious. Then later, you have x is not ambitious somewhere in one of
            the other rules that you then later need. The question is, how do you resolve that? What does it do? Well,
            first of all.</p>
        <p>观众：马克·塞夫特：首先，有趣的是，如果它说 x 不雄心勃勃。字面意思就是这样。我在这里不是吹毛求疵。这是某人在之前的测验中玩的把戏之一。你可以在列表中同时拥有 x 和 x
            雄心勃勃，因为它是一个愚蠢的基于规则的系统，它不知道你不能在列表中同时拥有这两者。</p>
        <p>AUDIENCE: MARK SEIFTER: First of all, interestingly, if it says x is not ambitious. literally like that. And
            I’m not being pedantic here. This is one of the tricks someone played in a previous quiz. You can have that
            and x is ambitious both in the list because it’s a dumb rules based system it doesn’t know that you can’t
            have both of those on the list.</p>
        <p>因此，如果您在后件中有一个肯定断言，x 不是雄心勃勃的，在后件中，x 不是雄心勃勃的，它会很乐意添加 Millicent 不是雄心勃勃的，而 Millicent
            雄心勃勃的这个已经存在了，因为它很愚蠢。现在，如果它删除了后件中的 Millicent 是雄心勃勃的，并删除了之前存在的内容，那将是一个有趣的问题。它可能会导致错误。您的反向链接可能会犯允许这种情况的错误。</p>
        <p>So if you have a positive assertion in a consequent, x is not ambitious in a consequent, x is not ambitious,
            it’ll be happy to add Millicent is not ambitious while Millicent ambitious this is already there, because
            it’s stupid. Now if it had delete Millicent is ambitious in the consequent and deleted something that was
            previously there, that would be an interesting problem. It could cause mistakes. Your back chaining would
            probably make the mistake of allowing this.</p>
        <p>然而，这正是我们所不具备的，我相信这是一项政策，即在测验中不添加 DELETE 语句。至少以前的测验没有这些，除非假设我们在这里添加 DELETE
            语句会发生什么。我认为我们从未让人们通过删除来解决问题。删除可能会引起一些问题。后面的问题？</p>
        <p>However, this is exactly what we do not, I believe as a policy, do not have DELETE statements on the quizzes.
            At least previous quizzes did not have them, except for in a hypothetical of what happens if we add a DELETE
            statement here. I don’t think we’ve ever made people work things through with the delete. Delete would
            possibly cause some issues. Question in the back?</p>
        <p>听众：所以，只是为了检查一下，我们正在进行反向链接，我们实际上并没有将断言添加到您的 MARK
            SEIFTER：永远不要将反向链接中的断言添加到断言表。这很愚蠢。系统很愚蠢。我不是说这样做是个愚蠢的想法。这样做实际上相当快。但系统很愚蠢。问题是，您添加断言还是不添加断言？你不添加。</p>
        <p>AUDIENCE: So, just to check this, we’re doing backwards chaining, we don’t actually add assertions to your
            MARK SEIFTER: Never add assertions in backward chain to the assertion table. It’s dumb. The system is dumb.
            I’m not saying it’s a dumb idea to do this. It’s actually pretty fast to do it this way. But the system is
            dumb. The question is, do you add assertions or you don’t add assertions? You do not.</p>
        <p>你只需检查目标树上的所有内容，直到目标树被证明或被证伪，然后你就可以离开这里了。到目前为止，大家觉得还不错吧？每个人都问了好问题。这些问题都是经常让人犯难的问题。所以，我们的下一个问题是，米利森特雄心勃勃。我们有点……秘密已经暴露了，因为有一个问题提到有一个断言这么说。
        </p>
        <p>You simply go through, checking all the things on your goal tree until the goal tree is either proven or
            disproven, then you’re out of here. So, cool so far, everyone? Good questions from everyone. These are all
            questions that are things that often trip people up. So, our next thing, Millicent is ambitious. We sort of.
            the cat is out of the bag because there is a question mentioning that there is an assertion that says that.
        </p>
        <p>但是我要向大家介绍我们最喜欢的规则，即规则二。我们不应该使用规则二来证明米莉森特雄心勃勃吗？答案是，如果你想要丢掉四分或多分，那么答案是肯定的。但是如果你不想丢分，那么根据断言一，这已经是正确的了。所以我想我还应该写一下，米莉森特在这里研究了很多规则，这条规则是规则四。
        </p>
        <p>But I direct you guys to our favorite rule, rule two. Shouldn’t we use rule two to prove that Millicent is
            ambitious? The answer is if you want to lose four points or however many points, then yes. But if you don’t
            want to lose points, then this is already correct by virtue of assertion one. So I guess I should write,
            also, that this over here, Millicent studies a lot rule, this rule is rule four.</p>
        <p>是的，这是规则四和主角。下面是规则三，证明她学习很多。然后在底部我们有什么规则？我们有主角规则，即规则一，反派规则，即规则二。这是 A0 的正确美德。好的，太好了。所以我们已经证明了整个 AND 节点。我们知道
            Millicent 学习很多。</p>
        <p>Yeah, that’s rule four and protagonist. Down here to prove that she studies a lot is rule three. And then at
            the bottom here we have what rules here? We have the protagonist rule, which is one, and the villain rule
            which is two. This is correct virtue of A0. OK, great. So we’ve proved this whole AND node. We know that
            Millicent studies a lot.</p>
        <p>但现在我们必须根据时间加快一点速度，所以我将完成剩下的事情。任何没有被叫到的人都可以免责。但这并不意味着可以不注意，因为还有一些有趣的事情要发生。所以，米莉森特是主角。好吧，大家，好消息。</p>
        <p>But now we’re going to have to go a little bit more quickly based on time, and so I will do the remaining
            things. Anyone who has not been called on is off the hook. But that doesn’t mean not to pay attention
            because there’s some interesting stuff still to come. So, Millicent is a protagonist. Well, good news,
            everyone.</p>
        <p>我可以这样做还有一个原因，那就是你已经告诉我我需要做什么。大多数是主角，你使用规则一，规则一。并检查她是否住在格兰芬多塔。现在，它会再次这样做吗？是的，它会。它没有缓存它已经尝试过。</p>
        <p>There’s one other reason that I can do this and that’s that you’ve already told me what I need to do. Most is
            a protagonist, you use rule one, rule one. And check if she lives in Gryffindor tower. Now, will it do it
            again? Yes it will. It hasn’t cached that it’s already tried that.</p>
        <p>现在，尽管哈希表不考虑，但使用反向链接缓存已经尝试过的东西可能是一个好主意，但我们在这个类中不这样做。这是一个实现细节。这是你可以做的另一种想法。我们不会捕获已经尝试过的东西。我们会再次尝试它们，该死的。当我们再次尝试时，也许这次会成功。不，它永远不会成功。这次永远不会成功。这失败了。这个
            AND 节点失败了。</p>
        <p>Now, hash table not withstanding, caching something you’ve already tried may be a good idea with backward
            chaining, but we do not do that in this class. That’s an implementation detail. It’s another idea of
            something you can do. We don’t catch things that we’ve already tried. We try them again, dammit. And when we
            try it again, maybe it’ll work this time. No, it never works. It never works this time. This fails. This AND
            node fails.</p>
        <p>整个事情都失败了。他们根本不是朋友。他们是死对头。米莉森特没有成为赫敏的朋友。所以现在他们问了我们几个问题，根据我们刚刚经历的磨难，这些问题很容易回答。所以，确定我们需要添加的最少额外断言数量，以使米莉森特成为赫敏的朋友。但你不能添加与规则结果相匹配的断言。
        </p>
        <p>The whole thing fails. They’re not friends at all. They’re bitter enemies. Millicent does not become
            Hermione’s friend. So now they ask us a few questions which are pretty easy to answer based on the ordeal
            we’ve just been through. So, determine the minimum number of additional assertions that we would need to add
            for Millicent to become Hermione’s friend. But you’re not allowed to add an assertion that matches a
            consequent of a rule.</p>
        <p>您只能添加一个断言。换句话说，您不能添加一个可以证明其他规则的断言。您必须直接添加一个断言。是的，这基本上就是这里的规则。因为有很多选择。但他想要一个特定的答案。所以有人能想到一个断言，它与任何规则的任何结果都不匹配，而这些规则会让她成为赫敏的朋友。很多人说米莉森特住在格兰芬多。
        </p>
        <p>You can only add an assertion. in other words, you can’t add an assertion that will prove some other rules.
            You have to add an assertion that directly. yeah, that’s basically the rule here. Because there’s a lot of
            choices. But he wanted one particular answer. And so can anyone think of an assertion that doesn’t match any
            consequent of any rule that will make her be Hermione’s friend. A lot of people say Millicent lives in
            Gryffindor.</p>
        <p>没错。所以，第三部分，你对第二部分的解决方案会导致不常见的情况。不常见的情况是什么？你应该对断言列表做什么来解决这个问题？有人知道如果我们加上她住在格兰芬多，不常见的情况会是什么吗？有人举手。你的答案是什么？没错。她住在格兰芬多和斯莱特林。那么你会如何解决这个问题？
        </p>
        <p>That’s correct. So, part three, your solution to part two causes an uncommon situation. What is the uncommon
            situation? And what should you do to the list of assertions to solve this problem? Does anyone know what the
            uncommon situation would be if we added that she lives in Gryffindor? Hand is raised up here. What is your
            answer? That’s true. She lives in Gryffindor and Slytherin. So how would you fix that?</p>
        <p>是的，你可以从断言中去掉斯莱特林，说她只住在格兰芬多。很多人对这个问题给出了答案，这在规则基础系统中是完全合理的。他们说，好吧，我们可以添加一条规则，规定如果你住在 x 并且 y 与 x 不同，那么你就不能住在 y。
        </p>
        <p>Yes, you could take out Slytherin from the assertions and say that she only lived in Gryffindor. A lot of
            people gave the answer to this question, which is a perfectly reasonable thing to do in a rules basis
            system. They said, well, we can add a rule that says that if you live in x and y is different than x, then
            you can’t live in y.</p>
        <p>除其他事项外，问题在于我们要求一种改变断言而不是规则的方法。而您给了我们一种改变断言的方法。这是正确答案。还有另一个向后链接问题，这是 2009 年测验一，我暂时不讨论它。你们应该看看它，特别是变量绑定。</p>
        <p>The problem with that, among other things, is that we asked for a way to change the assertions and not the
            rules. And you gave us a way to change the assertions. That’s the right answer. There was another backward
            chaining problem, this is 2009 quiz one, and I will leave it off for the moment. You guys should take a look
            at it, in particular with variable binding.</p>
        <p>因为请记住，您始终必须绑定与您相关的变量。但这并不意味着您始终必须绑定所有变量。例如，假设我们想要证明 Millicent 有一个坏词，事实证明，这正是我们在另一个反向链接中想要证明的。</p>
        <p>Because remember, you always have to bind the variable that’s relevant to you. But that doesn’t mean that you
            always have to bind all of the variables. For instance, let’s say that we wanted to prove that Millicent has
            a bad term, which as it turns out, is what we wanted to prove in the other backward chaining.</p>
        <p>非常快，而且不用做题，任何认为自己非常擅长反向链接的人能告诉我究竟需要将哪些东西添加到目标树中才能证明 Millicent
            有一个坏词吗？请举手。我不会挑出受害者，因为我要求这件事发生得非常快。没有人？好吧。听众：你会将三个前提添加到规则中。MARK SEIFTER：添加到规则五。那么你具体要添加什么？</p>
        <p>Very quickly and without doing the problem, can anyone who thinks they’re very clever at backward chaining
            tell me exactly what things would be added to the goal tree to prove that Millicent has a bad term? Raise
            your hand. I won’t pick out a victim because I’m asking this to happen pretty quickly. No one? All right.
            AUDIENCE: You would add the three antecedents to rule. MARK SEIFTER: To rule five. So what would you add
            specifically?</p>
        <p>观众：对于任何人，就变量而言，你只需表明至少有一个人与之匹配。马克·西弗特：那么，你能读出，第一个吻是什么意思吗？观众：X 吻。或者米莉森特吻任何人。马克·西弗特：是的，米莉森特吻
            y。这很关键。有些人做了很多不同的事情。有些人已经把西莫斯放进去了。你不能那样做。</p>
        <p>AUDIENCE: For anyone, variable wise, you would just have to show that there is at least one person who
            matches. MARK SEIFTER: So can you just sort of read out, what would the first thing say with the snogs?
            AUDIENCE: X snogs. or Millicent snogs anyone. MARK SEIFTER: Yeah, Millicent snogs y. That’s crucial. Some
            people did a lot of different things. Some people put Seamus in already. You can’t do that.</p>
        <p>系统很蠢，它不知道这个人是 Seamus。另外，有些人说它有效，因为看，他们在接吻，但那里的接吻方向不对。因为我们有 Seamus 接吻 Millicent，而不是 Millicent 接吻
            Seamus。但没错，确实如此。应该是 Millicent 接吻 y。Millicent 住在格兰芬多塔楼。y 住在斯莱特林地牢。</p>
        <p>The system is stupid, it doesn’t know that the person is Seamus. Also, some people said it worked, because
            look, they’re snogging each other, but it’s the wrong snoggle direction up there. Because we have Seamus
            snogging Millicent, not Millicent snogging Seamus. But yes, exactly. It would be Millicent snogs y.
            Millicent lives in Gryffindor tower. y lives in Slytherin dungeon.</p>
        <p>所以，让我们花大约八分钟的时间来关注前向链接，这已经足够了。好的，我们仍然有所有这些规则。我们仍然有所有这些断言。而不是进行那种可怕的后向链接。好吧，它并没有那么可怕。但我们将进行前向链接。这真的很容易。我们只需在进行过程中添加新的断言即可。记住决胜顺序。
        </p>
        <p>So, let’s give forward chaining all the attention it deserves, about eight minutes, which will be more than
            enough. OK, we still got all these rules. We’ve still got all these assertions. Instead of doing that
            horrible backward chaining. well, it’s not that horrible. but we’ll do forward chaining. It’ll be really
            easy. We’ll just add new assertions as they go. Remember the tiebreak order.</p>
        <p>规则按从 0 到 5 的顺序排列。如果同一条规则可以触发多个不同的断言，我们将按从 0 到 3
            的顺序使用这些断言。让我们看看。我们没有太多时间，但有了这四个断言，让我们看看哪些规则可能与这四个断言相匹配？好吧，我会帮你弄清楚的。我们有一个关于雄心勃勃的人的断言吗？有。但它们不是哑炮。所以不是规则零。</p>
        <p>Rules in order from 0 to 5. And if the same rule could fire off with multiple different assertions, we’ll use
            the assertions in order from 0 to 3. So let’s see. We don’t have much time, but with our four assertions,
            let’s see, what rules possibly could match with our four assertions? Well, I’ll figure out for you. Do we
            have an assertion about an ambitious person? Yes. But they’re not a squib. So not rule zero.</p>
        <p>是否有人住在格兰芬多？当然有。规则一是匹配。是否有人住在斯莱特林？有。规则二是匹配。是否有主角或反派？没有主角或反派。规则三不匹配，因为它在“and”中。是否有人学习很刻苦？没有学习很刻苦的人。规则四不匹配。</p>
        <p>Do we have someone that lives in Gryffindor? We absolutely do. Rule one is matches. Do we have someone that
            lives in Slytherin? We do. Rule two matches. Do we have a protagonist or a villain? We don’t have any
            protagonist or villain. Rule three is not going to match because it’s in an “and.” do we have someone who
            studies a lot? We do not have someone who studies a lot. Rule four is not going to match.</p>
        <p>我们会接吻吗？我们会接吻。所以规则一、二和五匹配。所以匹配的 m 是一、二和五。各位，哪一个在一、二和五之间赢得决胜局？一，因为它在数字上排在第一位。如果 x 住在格兰芬多塔，哪一个是规则一？x
            的唯一约束是西莫。西莫住在格兰芬多塔。我们正在执行规则一。</p>
        <p>Do we have some snogging? We do have some snogging. So rule one, two, and five match. So m for matching, we
            have one, two and five. Everyone, which one wins the tiebreak between one, two, and five? One, because it
            comes first numerically. Which one is rule one if x lives in Gryffindor tower? The only binding for x is
            Seamus. Seamus lives in Gryffindor tower. We’re firing off rule one.</p>
        <p>那么新的断言，我们要添加的第一个断言是 Seamus
            是主角吗？太好了。现在，还有一件事，这也是我想告诉你的关于正向链接的主要内容。关于我们的老朋友，规则一，现在我们已经完成了这件事，规则一是否仍然与数据库中的断言相匹配？是的。它确实如此。</p>
        <p>So new assertions, the first assertion that we’ll add is Seamus is it a protagonist. Great. Now, there’s one
            other thing, and it’s the main thing I wanted to tell you about in forward chaining. About our old friend,
            rule one, now that we’ve done this, does rule one still match an assertion in the database? Yes. It does.
        </p>
        <p>但是，是什么阻止我们每次都执行规则一，因为它在数字上排在其他规则之前呢？阻止我们执行的是我们实施的一部分。这是人们有时会忘记的一个非常重要的部分。这就是我喜欢称之为无无效规则实施细节。也就是说，如果一条规则完全、100%
            无效，它就什么也不做。那么你就不会触发它。你转到下一个。</p>
        <p>But what’s going to stop us from constantly doing rule one every time because it comes numerically before the
            other rules? What stops us there is a part of our implementation. And it’s a very important part that people
            sometimes forget. It’s what I like to call the no impotent rules implementation detail. That is, if a rule
            is completely, 100% impotent, it would do absolutely nothing. Then you do not fire it. You go to the next
            one.</p>
        <p>这非常重要。假设你可能删除了一条内容，但在测验中你不会看到这条内容。这意味着如果要删除的内容缺失，那么这条内容就是无效的。如果要删除的内容存在，那么这条内容就不是无效的，因为你可以删除它。假设你有 500,000
            条内容要添加到断言中。数据库中已经有 499,999 条内容了。其中一条缺失了。这是无效规则吗？不是。</p>
        <p>That’s pretty important. Let’s say that you possibly had a delete, which you won’t have on the quiz. That
            means that if the thing to delete is missing, it’s impotent. If the thing to delete is there, it’s not
            impotent because you can delete. Let’s say you have 500,000 things that you’re going to add to your
            assertions. 499,999 are already in your database. One of them is missing. Is it an impotent rule? No, it’s
            not.</p>
        <p>无论如何你都必须取消它。你不能说，哦，这基本上是一条无效的规则。不。如果它能起作用，你就必须取消这条规则。但现在规则一不会起作用，因为我们已经添加了 Seamus 是主角。那么哪些规则可以匹配呢？好吧，Seamus
            成为主角是件好事，但他没有野心，所以我们仍然不符合规则三。我们仍然符合规则一、二和五。</p>
        <p>You have to fire it anyway. You can’t be like, oh, it’s mostly impotent rule. No.&nbsp;You have fire off the
            rule if it will do anything. But right now rule one won’t do anything because we’ve already added Seamus is
            a protagonist. So what rules match down? Well, Seamus becoming a protagonist is nice and all, but he’s not
            ambitious, so we still don’t match rule three. We still match rules one, two, and five.</p>
        <p>我刚刚告诉过你，我们现在要触发的是第二条规则。没错。所以，第二条规则是，如果 x
            住在斯莱特林地牢，那么唯一的例外就是米莉森特。这将表明她是一个恶棍，而且她野心勃勃。那么我们要添加什么断言呢？米莉森特是一个恶棍。人们停了下来。没错。大家干得好。</p>
        <p>As I basically just got finished telling you, the one that we’re going to fire off now is two. That’s right.
            So, rule two is if x lives in Slytherin dungeon our only for that is Millicent. And that will show that
            she’s a villain and she’s ambitious. So what assertions will we add? Millicent is a villain. And people
            stopped. That’s right. Good job, everyone.</p>
        <p>您还记得，从断言一开始，米莉森特就已经雄心勃勃了，所以我们不必同时添加它们。很多人都这么做了。他们错了。所以米莉森特是个恶棍。该规则的另一部分没有任何作用。但这并不是一条无效的规则，因为其中一条规则不在那里。太好了。那么，现在哪些规则匹配呢？
        </p>
        <p>You remembered that Millicent is already ambitious from assertion one, so we don’t have to add them both. A
            lot of people did. They were wrong. So Millicent is a villain. The other part of that rule doesn’t do
            anything. But it’s not an impotent rule because one of the things is not on there. Great. So, what rules
            match now?</p>
        <p>好吧，事实证明，既然米莉森特是个恶棍，而且野心勃勃，那么我们也符合规则三。所以我们符合一、二、三和五。大家，我们要触发哪条规则？三。因为一和二是无效规则。我们将触发规则三。好吧，米莉森特是个恶棍。她也很野心勃勃。她是我们唯一符合规则三的规则。因此，我们要添加什么断言？米莉森特学习很多。没错。所以，米莉森特学习很多。
        </p>
        <p>Well, as it turns out, now that Millicent is a villain and ambitious, we match rule three as well. So we
            match one, two, three, and five. What rule are we going to fire, everyone? Three. Because one and two are
            impotent rules. We’ll fire rule three. Well, Millicent is a villain. She’s also ambitious. She’s our only
            match for three. Therefore, what assertion do we add? Millicent studies a lot. That’s right. So, Millicent
            studies a lot.</p>
        <p>然而，她不是主角。我们仍然没有匹配规则四。哦，好吧，我们已经从反向链接中自己弄清楚了，我们永远不会匹配它。那么匹配什么呢？它仍然是一、二、三和五。由于一、二和三无效，唯一的匹配是五。所以我们关心的唯一匹配，我们唯一要触发的匹配是五。结果呢？让我们看看。x
            snogs y。</p>
        <p>However, she’s not a protagonist. We still don’t match rule four. Oh well, we figured that out already
            ourselves from backward chaining that we were never going to match that. So what matches? It’s still one,
            two, three, and five. Seeing as one, two and three are impotent, the only match is five. So the only match
            we care about, the only one we’re going to fire is five. And the result? Let’s see. x snogs y.</p>
        <p>那是 Seamus 亲吻 Millicent。x 住在格兰芬多，那是 Seamus。y 住在斯莱特林，那是米利森特。那么我们要添加什么呢？Seamus 有一个坏词。简单、无痛、有效。我们完成了。我们在这次测验中得了
            100 分。希望你们也能这样。问题？</p>
        <p>That’s Seamus snogs Millicent. x lives in Gryffindor, that’s Seamus. y lives in Slytherin, that’s Millicent.
            So what do we add? Seamus has a bad term. Simple, painless, it works out. We’re done. And we got 100 on this
            quiz. Hopefully you guys will, too. Question?</p>
        <p>问题是，如果你有一个新的断言与规则相匹配，但它的数字真的很高，它一直处于规则的底部，但它是新的，它有新的东西，我们应该先做这个，也许，因为它是新的？哦，你的意思是它的数字较低，更低的数字？观众：马克·西夫特：好的，问题是，如果规则。假设规则三让米利森特成为一个哑炮。
        </p>
        <p>The question is, if you have a new assertion which matches a rule but it’s really high number, it’s all the
            way down at the bottom of the rules, but it’s new and it has new stuff, should we do that first, maybe,
            because it’s new? Oh, you mean that it has a lower number, a lower number? AUDIENCE: MARK SEIFTER: OK, so
            the question is, if a rule. let’s say that rule three made Millicent be a squib.</p>
        <p>那么是的，您不仅会立即匹配零。零会立即触发，因为从数字上讲它排在第一位。您会在一些旧测验中看到这一点。您可能会在教程中看到这一点。如果你们想在教程中做更多的练习题，您的助教将在教程中提供一次机会。但是，是的，这是完全正确的。这次碰巧是按顺序进行的，但它肯定会跳到任何可以跳到的地方，以便到达编号最低的断言。
        </p>
        <p>Then yes, you’d immediately not only match zero. zero would immediately fire because numerically it comes
            first. You’ll see that on some of the old quizzes. And you may see that in tutorial. If you guys want to do
            some more practice problems in tutorials, your TAs will have that available as one opportunity in tutorial.
            But yeah, that’s exactly right. It happened that it went in order this time, but it will definitely hop
            anywhere it can in order to get to the lowest numbered assertion.</p>
        <h1 id="mega-r2.-basic-search-optimal-search">Mega-R2. 基础搜索，最优搜索</h1>
        <h1>Mega-R2. Basic Search, Optimal Search</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 51:56 <a
                href="https://youtube.com/watch?v=Tl_p5pgBsyM">https://youtube.com/watch?v=Tl_p5pgBsyM</a></p>
        <p> 11 years ago (Jan 11, 2014) — 51:56 <a
                href="https://youtube.com/watch?v=Tl_p5pgBsyM">https://youtube.com/watch?v=Tl_p5pgBsyM</a></p>
        <h2 id="unknown-435">未知</h2>
        <h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。教授：本周的问题来自 2008 年关于搜索的测验。它的动机是邪恶霸主 Mark Vader 的搜索，他正在寻找新的邪恶据点。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: This week’s
            problem is from the 2008 quiz one on search. It is motivated by the search of evil overlord, Mark Vader, who
            is shopping for a new evil stronghold.</p>
        <p>他从他目前的据点 S 开始，即深度优先搜索星。现在深度优先搜索星具有以下特质。它有一个小型的热排气管弱点。它具有 That's No Moon
            的特质，但它没有被奴役的奴才种族、秘密逃生路线或带有激光束的鲨鱼。现在，当我在 2008 年最初写下这个问题时，还有另一个非常重要的特质。</p>
        <p>He starts from his current stronghold, which is S, the Depth First Search Star. Now the Depth First Search
            Star has the following qualities. It has a small, thermal Exhaust Pipe Weakness. It has the quality of
            That’s No Moon, but it does not have a Race of Enslaved Minions, or a Secret Escape Route, or Sharks with
            Laser Beams. Now when I originally wrote this problem in 2008, there was another quality that very
            important.</p>
        <p>那是一束巨大的激光，它确实具有这种特性，邪恶博士的月球基地也具有这种特性，但为了更容易解决，它被移除了。所以我们要解决这个问题，不用巨大的激光，我们只需要用激光束来对付鲨鱼。</p>
        <p>It was a giant Laser, and it did have that property, as did Dr.&nbsp;Evil’s Moon Base, but that was removed
            to make it easier to solve. So we’re going to solve it without the giant laser, that we can just have the
            Sharks with Laser Beams.</p>
        <h2 id="unknown-436">未知</h2>
        <h2>Unknown</h2>
        <p>因此，马克·维德来到了阿克巴的邪恶新据点商场，这些据点列在左侧，他正在试图找出从当前据点到目标据点 603
            堡垒的最佳路线，该堡垒拥有令人钦佩的品质，热排气管弱点不小，而且还有那不是月亮。它有一群被奴役的奴才，一条秘密逃生路线，还有带着激光束的鲨鱼。</p>
        <p>So Mark Vader has gone to Ackbar’s Emporium of New Evil Strongholds, which are listed on the left, and he’s
            trying to figure out the best way to get from his current stronghold to his goal stronghold, the 603
            Fortress, which has the admirable qualities of no small thermal Exhaust Pipe Weakness, and it still has
            That’s No Moon. It has a Race of Enslaved Minions, and a Secret Escape Route, and Sharks with Laser Beams.
        </p>
        <p>所以它具备你想要的一切，没有弱点。然而，他只能在只有一个差异的堡垒之间移动。幸运的是，马克记得如何执行他在 6.034 中从他的导师帕尔帕廷皇帝那里学到的搜索技术。所以我们这里有几个堡垒。我们有深度优先搜索星。我们还有
            Shaoul Ghoul，它基本上具有 That's No Moon 的品质，但没有其他品质。</p>
        <p>So it’s got everything you would want and no weakness. However, he can only move between fortresses that have
            exactly one difference. Fortunately, Mark remembers how to perform the search techniques he learned in 6.034
            from his mentor, Emperor Palpatine. So we’ve got several fortresses here. We’ve got the Depth First Search
            Star. We’ve also got Shaoul Ghoul, which has the qualities, basically, of That’s No Moon, and none of the
            other qualities.</p>
        <p>多古尔都虽然是索伦的大本营，但不知何故却存在排气管弱点。它不是月球，而且有那些被奴役的奴仆。我猜兽人是被奴役的。这里的 Moonraker 只有排气管弱点，所以不太好。但詹姆斯邦德不需要处理这些东西。Zeal
            Underwater Palace 有 That's No Moon 和一条秘密逃生路线。Zeromus 的 Lunar Core 有排气管弱点和一群被奴役的奴仆。</p>
        <p>Dol Guldur here, has got the Exhaust Pipe Weakness somehow, despite being Sauron’s stronghold. It’s not a
            moon, and it has those enslaved minions. I guess the Orcs are enslaved. Moonraker, here, only has the
            Exhaust Pipe Weakness, so it’s not very good. But James Bond didn’t need to deal with stuff. Zeal Underwater
            Palace has That’s No Moon and a Secret Escape Route. Zeromus’ Lunar Core has an Exhaust Pipe Weakness and a
            Race of Enslaved Minions.</p>
        <h2 id="unknown-437">未知</h2>
        <h2>Unknown</h2>
        <p>月球捕鲸者之旅有排气管弱点、奴役奴才种族和激光鲨鱼。6.03
            堡垒我们已经看过了。亚特兰蒂斯拥有除秘密逃生路线之外的所有特质。威利旺卡工厂拥有一切。毕竟，它是威利旺卡的工厂。亮点是奴役奴才种族，即乌姆帕伦帕人。而邪恶博士的月球基地只缺少那不是月球，因为它是月球，因为它是月球基地。
        </p>
        <p>Whalers of the Moon Ride has the Exhaust Pipe Weakness, Race of Enslaved Minions, and Sharks with Laser
            Beams. 6.03 Fortress we’ve already been over. Atlantis has all qualities except for a Secret Escape Route.
            Willy Wonka’s Factory has some of everything. It is, after all, Willy Wonka’s Factory. Highlight, the Race
            of Enslaved Minions, the Oompa Loompas. And Dr.&nbsp;Evil’s Moon Base is only missing That’s No Moon,
            because it is a moon, because it’s a moon base.</p>
        <p>所以，如果你已经知道了建造这些堡垒时使用的所有参考资料，那么我很抱歉。我无能为力。现在，马克，一个聪明的邪恶霸主，意识到他可以制作一个探索选择图，其中的边连接着仅相差一个特征的堡垒。所以虽然这是一张图表，但正如帕特里克经常说的，你现在又可以听到了。搜索是关于选择，而不仅仅是地图。
        </p>
        <p>So if you got all the references that were used in making those fortresses, I’m sorry. There’s nothing I can
            do for you. Now, Mark, being a clever evil overlord, realizes that he can produce a graph of the exploration
            choices with edges joining strongholds that differ by just one feature. So although this is a graph here, as
            Patrick often says. and you get to hear again right now. search is about choice, not just about maps.</p>
        <p>因此，我们不会去现实世界的任何地方，但我们肯定会在这个图表上移动，以便在众多要塞选择之间进行选择。我们如何决定去哪里旅行？好吧，我们马上就会知道。所以我们这里有这个可爱的图表，现在我们要对图表进行深度优先搜索。</p>
        <p>So we’re not moving around anywhere in the real world, but we certainly are going to move around this graph
            to pick between these many stronghold choices. How can we decide where to travel? Well, we’re about to find
            out. So we’ve got this lovely graph right here, and now we’re going to get to do depth first search on the
            graph.</p>
        <h2 id="unknown-438">未知</h2>
        <h2>Unknown</h2>
        <p>现在有很多方法可以进行搜索，你们已经看到过帕特里克这样做，你们可能也看到过一些人在背诵中这样做。我将为你们提供一个独特的机会，让你们看看我用几种方法中的一种，甚至更多方法进行搜索。我有经过验证的方法。你有队列、议程，或者你想叫它什么。你可以在每个级别跟踪它。
        </p>
        <p>Now there’s a lot of ways to do search, and you’ve seen Patrick do it, you may have seen some people do it in
            recitation. And I’m going to offer you guys a unique opportunity to see me do search in one of several ways,
            or possibly more. I have the tried and true method. You have the queue, or agenda, or whatever you want to
            call it. You keep track of it at every level.</p>
        <p>你一定要确保你所做的一切都是正确的。这需要很长时间，但你得到的答案是正确的。或者你只使用目标树，你画得非常快，你可能会犯错误，但它会更快地解决问题。那么谁更愿意看到可靠但较慢的方法呢？谁更愿意看到更快但不太可靠的方法呢？好吧。
        </p>
        <p>You make damn sure that everything you’re doing is right. It takes a long time, and you get the answer right.
            Or there’s one where you only work with the goal tree, and you draw it really fast, and you might make a
            mistake, but it’s going to solve it more quickly. So who would rather see the reliable, but slower,
            approach? Who would rather see the faster, but less reliable, approach? All right.</p>
        <p>我是蒙特卡罗人，而不是拉斯维加斯人。如果你不知道那是什么，你可以在算法课上学到。好吧。还有第三种方法，除非有压倒性的优势，否则我可能不会向你展示。这种方法速度极快，可以立即解决深度优先搜索，但很容易出错。这是我在解决深度优先搜索时使用的方法。
        </p>
        <p>Monte Carlo people here, rather than Las Vegas. If you don’t know what that is, you’ll learn it in algorithms
            class. All right. Then there’s a third approach that I probably won’t show you unless there’s overwhelming
            favor for it. That is the approach that is manically fast and will solve depth first search in no time at
            all, but it is very likely to make mistakes. That is the approach that I use when solving depth first
            search.</p>
        <h2 id="unknown-439">未知</h2>
        <h2>Unknown</h2>
        <p>我已经用过很多次了，所以不会经常犯错，但即便如此，我还是会犯错。所以有人想看这个吗？好吧。没有那么多人。好吧，这并不需要太多时间，但我只是担心如果我给你看，你会觉得，哦，这是唯一的方法。所以如果我们有更多时间，我们会把它留到以后再看。
        </p>
        <p>I’ve used it enough times that I don’t make mistakes that often, but even I still do. So does anyone want to
            see that one? OK. Not that many people. Well, it doesn’t take much time, but I’m just afraid that if I show
            it you, that you’ll be like, ooh, that’s the only way to do it. So we’ll save that for later if we have more
            time.</p>
        <p>对于有兴趣的人来说，它本质上只需要用粉笔或手指快速描画树干，通过画出细小的线条来弄清楚树干在做什么。它非常快，但不太准确。你已经表明，如果你做错了，基本上就没有任何工作，所以你应该感到惭愧。好吧。</p>
        <p>It essentially, for people that are interested, involves just using your chalk or finger to trace through the
            tree really fast and figure out what it’s doing by drawing little tiny lines. It is very fast, but it’s not
            very accurate. And you have shown, basically, no work if you get it wrong, so shame on you. OK.</p>
        <p>好吧，让我们采用一种稍微快一点的方法，即不绘制整个议程，然后几乎没有人举手赞成这种方法，可以在教程中查看使用议程的方法。所以让我们使用目标树来做。这样会快一点。所以我们将从 S（起始节点）开始，然后转到
            G（目标节点）。</p>
        <p>Well, let’s do the somewhat faster way where we don’t draw out the entire agenda, and then people who were
            the almost nobody who raised their hand for that way can check it out in tutorial the way that you do it
            using the agenda. So let’s do it using the goal tree. So it’s a bit faster that way. So we’re going to
            start, starting at S, the start node, and going to G, the goal node.</p>
        <h2 id="unknown-440">未知</h2>
        <h2>Unknown</h2>
        <p>这是标准符号，但请确保在进行测试时检查起始节点和目标节点的位置。如果 G 节点不是目标节点，那么这几乎是一个愚蠢的举动，但有时不会有 S 节点。这是一个很好的线索，表明其他节点是起始节点。</p>
        <p>That’s a standard notation, but make sure that when you’re taking the test, you check to make sure where the
            start node is, where the goal node is. It’s pretty much an asshole move to have a G node that isn’t the goal
            node, but sometimes there won’t be an S node. That’s a pretty good clue that something else is the start
            node.</p>
        <p>现在我们有一些用于搜索的小白星想法或银星想法。一个是词典编纂。什么是词典编纂？谁在乎？嗯，我所知道的最著名的词典编纂者是塞缪尔·约翰逊，他写了一本非常著名的词典。</p>
        <p>Now there’s a few little white star ideas that we have, or silver star ideas, for search. One is
            lexicography. What it lexicography? Who cares? Well, the most famous lexicographer I know of is Samuel
            Johnson, who wrote a really famous dictionary.</p>
        <p>我们在本课中关注它的唯一原因是，你总是会看到“按字典顺序打破平局”的说明。然后你可能会觉得，哇，这真是一个冗长的词，或者一个长词，或者你想用什么很长的词来描述“字典顺序”这个词。它到底是什么意思？基本上，它意味着按字母顺序排列，就像你在字典中所做的那样。
        </p>
        <p>And the only reason we care about it in this class is because you’ll always see the instructions “break ties
            in lexicographic order.” And then you might be like, wow, that’s really rather a wordy word, or a
            sesquipedalion word, or whatever really long word you want to use to describe the word “lexicographic.” What
            the heck does it mean? Basically, it means alphabetical order, like you would do in a dictionary.</p>
        <h2 id="unknown-441">未知</h2>
        <h2>Unknown</h2>
        <p>例如，按字母顺序排列，A 排在 B 之前。所以你会先选择 A，然后再选择 B。不，甚至超过这个点，还有几种方法可以进行字典顺序决胜局。而且，不同算法之间进行字典顺序决胜局的方法并不总是一致的。</p>
        <p>So for instance, in alphabetical order, A comes before B. So you would go to A before B. No, even beyond that
            point, there’s several ways to do a lexicographic tiebreak. And it’s not always consistent between
            algorithms how you would do a lexicographic tiebreaker.</p>
        <p>最近 Patrick 和员工随意交谈时，他明确表示，他希望我们采用的方式，也就是我们希望您采用的方式，是根据最后最新的节点进行决胜。所以您可能会说，这实际上不是按字典顺序排列的。字典中 SAB 不是应该排在 SGA
            之前吗？我的回答是，是的，应该排在 SGA 之前。</p>
        <p>In the most recent time that Patrick was kind of randomly talking to the staff, he made it clear that the way
            he would like us to do it, and therefore, the way we’d like you to do it, is to tiebreak based on the very
            latest nodes at the end. So you might say, that’s not really lexicographic. Wouldn’t SAB come before SGA in
            the dictionary? And my answer to you is, yes, it would.</p>
        <p>是的，是的，肯定会。但是，根据帕特里克最后说的话，这显然就是我们正在做的事情，如果他决定回到字典顺序，我们会及时通知您。这就是词典学。您可能想知道这里的图片是什么。好吧，这是我们的朋友，衔尾蛇。长期以来，它一直是炼金术中无限、永恒的象征，或者可能是一种无限能量引擎，将铁变成金子。
        </p>
        <p>Yes, yes, it definitely would. However, that is apparently what we’re doing, according to the last thing that
            Patrick said, and we will keep you updated if he decides to go back to dictionary order. So that’s
            lexicography. You might be wondering what the picture here is. Well, this is our friend, the ouroboros. Long
            has it been a symbol in alchemy of infinity, eternity, or maybe a sort of infinite energy engine, turning
            iron into gold.</p>
        <h2 id="unknown-442">未知</h2>
        <h2>Unknown</h2>
        <p>但不幸的是，如今衔尾蛇已经濒临灭绝，因为在 6.034
            中不允许咬自己的尾巴。很多人都因此犯了错误。这是一个无意之失。我的意思是，我们收到了一些员工的电子邮件，他们无法解决其中一个问题，因为他们忘记了这一点。所以如果你忘了也没关系，但请尽量记住。不咬自己的尾巴是我们系统唯一聪明的做法。
        </p>
        <p>But today, unfortunately, the ouroboros is an endangered species because no biting your own tail in 6.034. A
            lot of people mess up from this. It’s an honest mistake. I mean, we’ve gotten some emails from staff members
            who couldn’t solve one of the problems because they forgot about this. So it’s OK if you forget, but try
            your best to remember. No biting your own tail is the only smart thing our system does.</p>
        <p>你会记得，上周我们讨论规则时，我们说规则系统并不聪明。它不知道“不是 Polly 死了”应该等同于“Polly
            没有死”，或者类似的东西。它太笨了，无法弄清楚这一点。好吧，这个系统也很笨，但它知道的一件事是，Patrick 真的很讨厌同一个节点在同一条路径中出现两次。</p>
        <p>You’ll remember last week when we were talking about rules, and we were saying that the rule system is not a
            smart creature. It doesn’t know that “not Polly is dead” should be the same as “Polly is not dead”, or
            something like that. It’s too dumb to figure that out. Well, this system is also dumb, but one thing it
            knows is that Patrick really hates if the same node appears twice within the same path.</p>
        <p>它会立即销毁它，甚至在将它添加到队列之前。它消失了。它不会被考虑。它从那里消失了。所以非常重要，悲伤的衔尾蛇，不要咬自己的尾巴。好吧。既然我已经解决了这个问题，让我们通过深度优先搜索来解决这个问题。我们将如何做到这一点？好吧，我们不会使用队列，因为大家要求，我们将使用目标树。所以我们将从节点
            S 开始。</p>
        <p>And it will destroy that immediately, before even adding it to the queue. It’s gone. It’s not considered.
            It’s out of there. So very important, sad ouroboros, no biting your own tail. All right. So now that I’ve
            gotten through that, let’s actually solve the problem with some depth first search. How will we do that?
            Well, we’re not going to use the queue, as popular demand, we’re going to use a goal tree. So we’ll start at
            node S.</p>
        <h2 id="unknown-443">未知</h2>
        <h2>Unknown</h2>
        <p>在节点 S，我们的选择是什么？好吧，我强迫你们帮忙。虽然不会很快，但会很有趣。那么在节点 S，我们的选择是什么？你。听众：我？教授：是的。听众：A 还是
            B？教授：不完全是，这是我喜欢这个问题的原因。你回答了大部分问题。你看到可能还有其他选择吗？每个人？是的，C。</p>
        <p>What are our choices at node S? Well, I’m going to force you guys to help. So it won’t be quite as fast, but
            it’ll be fun. So what are our choices at node S? You. AUDIENCE: Me? PROFESSOR: Yes. AUDIENCE: A or B?
            PROFESSOR: Not quite, and this is something I like about this problem. You got most of them. Do you see that
            there might be another choice? Everyone? Yeah, C.</p>
        <p>这是一个大问题，它发生在一些不同的测验问题上，其中有一种看起来像树的网格，或者看起来像树的图表，人们不愿意向上走，而更愿意向下走。你也可以在其他一些过去的测验问题上看到它。确保检查连通性，并注意，除非另有说明。</p>
        <p>This is a big problem that has happened on a few different quiz problems where there’s a sort of a grid that
            looks like a tree, or a graph that looks like a tree, where people aren’t as willing to go up as they are to
            go down. You can see it on some of the other past quiz problems too. Make sure that you check the
            connectivity, and also note that unless otherwise specified.</p>
        <p>我认为我们并没有真正做过这么多。我们图表中的小连接边是双向的。您会看到一个大箭头，并且可能还有比其他所有字体大至少八个字号的说明，并且如果我们要为这个测验做一些不同的事情，说明会加粗。他们有时会这样做，我只会在背诵中说，我们从不这样做。然后在测验中，他们这样做了。
        </p>
        <p>And I don’t think we’ve really done this much. the little connecting edges in our graphs are bidirectional.
            You’ll see a big arrow, and probably instructions written at least eight font sizes higher than all the rest
            of them, and bold, if we’re going to do something different for this quiz. It happens that they do it
            sometimes, where I’ll just say in recitation, we never do this. And then on the quiz, they do it.</p>
        <h2 id="unknown-444">未知</h2>
        <h2>Unknown</h2>
        <p>但会用巨大的粗体字写，这也让我想到了另一个银星的想法，我上周忘了告诉你们。所以我希望大家不要抛弃我，比如，哦，马克很无聊。我们下周不会来了。然后因为突然想到这个就错过了。也就是说，阅读说明。</p>
        <p>But it will be in giant bold letters, which also leads me to another silver star idea that I forgot to tell
            you guys last week. So I hope people didn’t ditch out on me, like, oh, Mark’s boring. We won’t come next
            week. And then miss this one because it just came to my mind. That is, read the instructions.</p>
        <p>这对于测验来说非常重要，Patrick 会在稍后的讲座中告诉你们，问五次为什么，他会解释为什么这是一个金星想法。我与之相对应的是阅读说明五次。也许不是五次，也许三四次，但至少三次。阅读说明。再读一遍。</p>
        <p>It is a very important thing for the quizzes, and Patrick will, in a later lecture, tell you guys, ask why
            five times, and he’ll explain why that’s a gold star idea. My parallel to this is read the instructions five
            times. Maybe not five, maybe three or four, but at least three. Read the instructions. Read them again.</p>
        <p>然后读第三遍，只注意那些粗体字和比其他字大 8 号的字，因为那里肯定有那个大小的东西。而且那将是每个人都会错过的东西。所以确保你不是每个人都一样，而且你读了很多遍说明。但无论如何，是的，我们做对了。S 对应 A、B 和
            C。好的。</p>
        <p>Then read them a third time only paying attention to whatever is bold and eight sizes larger than all the
            other ones, cause there’s going to be something there that’s that size. And it going to be the thing that
            everyone misses. So make sure you’re not everyone, and you read the instructions a lot of times. But anyway,
            so yeah, we’ve got it right. S goes to A, B, and C. OK.</p>
        <h2 id="unknown-445">未知</h2>
        <h2>Unknown</h2>
        <p>我们正在考虑 A、B 和 C 的可能性。大家帮我做一下字典顺序决胜局。我们去哪里？观众：A。教授：A。没错。实际上，我会叫你们做其他的，但第一步很简单。我们可以让大家一起做。大家，A
            只会导致？观众：D。教授：没错。为什么它不会导致 S？观众：教授：不要咬自己的尾巴。每个人都对。干得好，大家。</p>
        <p>We’re looking at A, B, and C as possibilities. Everyone, help me make the lexicographic tiebreak. Where do we
            go? AUDIENCE: A. PROFESSOR: A. That’s right. Actually, I’ll call on you guys for other ones, but that first
            step is pretty simple. We could have everyone do it together. Everyone, A only leads to? AUDIENCE: D.
            PROFESSOR: That’s right. Why doesn’t it lead to S as well? AUDIENCE: PROFESSOR: No biting your own tail.
            Everyone’s right. Good job, everyone.</p>
        <p>好的。S 通向这三个。你去 A，去 D。死路一条。回溯。好的。当我们回溯时，我们回溯到 A。没有其他孩子。我们回溯到 S。我们现在去哪里？观众：B。教授：B。它是按字母顺序排列的下一个。好的。B 不能去
            S，所以它只能去？观众：H。教授：是的。当我们在 H 时，我们可以去吗？观众：教授：F 或 I。</p>
        <p>All right. S leads to these three. You go to A, go to D. Dead end. Backtrack. OK. When we backtrack, we
            backtrack up to A. There’s no other children. We backtrack up to S. Where will we go now? AUDIENCE: B.
            PROFESSOR: B. It’s the next one alphabetically. All right. B can’t go to S, so it only goes to? AUDIENCE: H.
            PROFESSOR: Yes. When we’re at H, we can go to? AUDIENCE: PROFESSOR: F or I.</p>
        <p>没错。你们明白了。深度优先搜索很简单。好吧。但是我们选择？F。没错。当我们在 F 时，我们可以去？E 和 J。没错。我们会选择？E。当我们在 E 时，我们可以去？我们可以去 C。它不在这个特定的路径上。人们说 C
            是正确的。当我们去 C 时，我们哪儿也去不了。我们死定了。回溯。我们回溯。</p>
        <p>That’s right. You guys get this. Depth first search is easy. All right. But we choose? F. That’s right. When
            we’re at F, we can go to? E and J. That’s right. We’ll choose? E. When we’re at E, we can go to? We can go
            to C. It’s not on this particular path. People are correct when they say C. When we go to C, we can go to
            nowhere. We’re dead. Backtrack. We backtrack.</p>
        <h2 id="unknown-446">未知</h2>
        <h2>Unknown</h2>
        <p>在 E 点我们什么也做不了。我们去 F
            点。现在我忘了告诉你们，但这是很重要的一点。有人会搞错。你们当中可能有人会。环顾四周，环顾整个房间。你们当中至少有一个人会这样做。所以，现在我要告诉你们不要这样做，然后希望这仍然是正确的，但涉及的人会更少。</p>
        <p>We can’t do anything at E. We go to F. Now I forgot to tell you guys, but this is an important note.
            Someone’s going to get this wrong. It’s going to be one of you. Look around you, all through the room. At
            least one of you is going to do this. So, now I’m going to tell you not to do this, and then hopefully, that
            will still be true, but for fewer people.</p>
        <p>当我转到 D 时，我回溯，然后转到 S，我回溯了多少次？一次。我回溯了一次。如果我们采用非常缓慢无聊的议程方式或队列方式，这种情况会更加明显，因为当我们到达 SAD
            时，我们对其进行了扩展，并且没有剩余内容，我们会将其从队列顶部删除，然后转到队列中的下一个内容。</p>
        <p>When I went to D, and then I backtracked, and I went to S, how many times did I backtrack? Once. I
            backtracked once. This would have been more obvious if we were doing the really slow boring agenda way, or
            queue way, because when we got to SAD, and we expanded it, and there was nothing left, we throw it off the
            top of the queue, and go to the next thing on the queue.</p>
        <p>结果是队列中的下一个是 SB。该步骤仅执行一次。由于我们使用了更快的目标树，因此看起来我们执行了两次，但实际上并非如此。如果您使用队列，您会看到这一点，因为它将进入 S，然后我们将其展开，SASBSC。SBIC
            OK。我们展开 SA。SADSBSC。我们展开 SAD。它已死。SBSC。回溯一次。我们回到 SB。</p>
        <p>And it turns out the next thing on the queue is SB. That step was only taken once. Since we’re using the goal
            tree, which is faster, it looked like we did two, but we didn’t. And if you use the queue, you’ll see that
            because it’ll go S, and then we expand that, SASBSC. SBIC OK. We expand SA. SADSBSC. We expand SAD. It’s
            dead. SBSC. One backtrack. We’re back at SB.</p>
        <h2 id="unknown-447">未知</h2>
        <h2>Unknown</h2>
        <p>所以，到目前为止，我们已经完成了两次，而不是四次。很直观地说，你在那里做了两次。事实证明你没有，因为支持我们进行目标树搜索的算法。所以尽量确保你不是那个说两次的人。另一方面，如果你这样做，我会尽量确保我们不会扣掉太多分。问题？
        </p>
        <p>So, so far we’ve done two, not four. It’s pretty intuitive to say that you did two there. It turns out you
            didn’t because of the algorithm that’s backing what lets us do this goal tree search. So try to make sure
            you’re not the one who says two. And I, on the other hand, will try to make sure that we don’t take off too
            many points if you do. Question?</p>
        <p>听众：所以任何时候你回溯，无论它链接了多少次，任何回溯都只被视为一次。教授：它总是会被视为一步。有可能连续回溯两次。比如，如果 B 没有去任何地方，而是去 H，那么我们可能会在到达 SC
            之前回溯两次。一般来说，任何时候你画一个 swizzle，如果你像我一样，你画一个 swizzle。我建议画 swizzle。</p>
        <p>AUDIENCE: So any time you backtrack, regardless of how much it chains, any backtracking is just considered
            once. PROFESSOR: It is always going to be considered one step. It’s possible to backtrack two times in a
            row. Like, if B didn’t go to anything, instead of going to H, then we might backtrack twice before we got to
            SC. Generally, anytime you draw a swizzle, if you’re like me, and you draw a swizzle. I suggest the
            swizzles.</p>
        <p>这些都是很不错的小东西。但是，只要你在图表上画一个旋转线，就说明你已经回溯了一次。你甚至可以在旋转线旁边写上
            BT，然后回去数一数，或者甚至只是数一数最后的旋转线。你明白了。你得到了答案。因为他们经常问，你回溯了多少次？好吧。大家的问题都很好。所以，无论如何，我们从 C 回溯了。E 处没有回溯。我们回去。</p>
        <p>They’re very nice little things. But any time you draw a swizzle on your graph, you have backtracked once.
            You could even write BT next to the swizzle, and go back and count those, or even just count the swizzles at
            the end. You’ve got it. You got the answer. Because they often ask, how many times have you backtracked? All
            right. So good questions, everyone. So anyway, we backtracked from C. Nothing at E. We go back.</p>
        <h2 id="unknown-448">未知</h2>
        <h2>Unknown</h2>
        <p>我们当时在 F。所以我们在 SBHF。我们去 J。这是唯一的选择。在 J，我们只能去？听众：I。教授：而在 I，我们只能去？我们赢了。这不是最佳搜索，所以只要我们在队列中看到任何带有 G
            的东西，砰的一声。赢家。好的。我们完成了。我们做到了。那还不错。</p>
        <p>We were at F. So we were at SBHF. We go to J. It’s the only choice. At J, we can only go to? AUDIENCE: I.
            PROFESSOR: And at I, we can only go to? And we win. It’s not an optimal search, so as soon as we see
            anything with a G on the queue, boom. Winner. All right. And we’re done. We did it. That wasn’t too bad.</p>
        <p>对于那些对我如何以超快的方式完成这件事情有点兴趣的人来说，事情是这样的。好的。ABC。A 先出现。只有 D。回溯。好的。B 先出现。H。F 先出现。EC
            没有。回溯。我们是从那边来的。JIG。所以这是进行深度优先搜索的真正快速的方法。不要那样做，孩子们。我们不喜欢。好吧，也许你可以。</p>
        <p>For those of you who are vaguely interested in seeing how I would do it the really super fast way, it goes
            something like this. All right. ABC. A comes first. Only D. Backtrack. All right. B comes first. H. F comes
            first. E. C. Nothing. Backtrack. We came that way. JIG. So that’s the really fast way to do depth first
            search. Don’t do that, kids. We don’t like. well, maybe you can.</p>
        <p>如果你答对了，我们通常不会扣分。一般来说，当我们说画出下面的目标树时，只会分配部分学分。然而，我们对此非常严格。如果有任何错误，除非可能，比如，好吧，你把所有东西都写得完全正确，然后忘记最后的 G。</p>
        <p>If you get it right, we’re not going to take off points usually. Generally, when we say draw the goal tree
            below, it’s just assigned partial credit. However, we are pretty strict about that. If anything is wrong,
            except for maybe, like, OK, you write everything exactly right, and forget G at the end.</p>
        <h2 id="unknown-449">未知</h2>
        <h2>Unknown</h2>
        <p>如果任何不能完全理解的地方是错误的，你可能会失去所有的分数，如果你不画出目标树，就会失去很多分数。我要强调的是，画出目标树是个好主意。好的。现在让我们进行广度优先搜索。</p>
        <p>If anything that’s not completely understandable is wrong, you will probably lose all of the points, and
            it’ll be a lot of points if you don’t draw the goal tree. I will emphasize drawing the goal tree is a good
            idea. All right. So now let’s do a breadth first search.</p>
        <p>在我们对这棵树进行广度优先搜索之前，我会告诉你们，还有一种快速的广度优先搜索方法，这种方法风险较小，而且这实际上取决于他们对所要求内容的态度。在这种情况下，广度优先搜索问题是，马克使用广度优先搜索找到了什么路径？而不是说他按顺序扩展了哪些节点，或者诸如此类的事情。这很重要。
        </p>
        <p>Before we do the breadth first search on this tree, I will tell you guys that there is also a fast way to do
            the breadth first search which is less risky, and it really depends on how nice they are about what they ask
            for. In this case, the breadth first search question asks, what path does Mark find using breadth first
            search? Rather than saying, what nodes does he expand in order, or anything like that. That is important.
        </p>
        <p>如果有人问你这个问题，有一个技巧可以让你非常非常快地解决它。事实上，比深度优先搜索更快。你可以在大约 30
            秒内通过检查解决它。有人知道答案是什么吗？他找到了什么路径？观众：教授：没错。那么答案是？在这个图表上，答案是？观众：SBHIG。教授：SBHIG。这就是答案。你会得到你的五点。人们看到了吗？</p>
        <p>If that question is asked you, there is a trick that will let you solve it very, very quickly. In fact,
            faster than depth first search. You can solve it by inspection in about 30 seconds. Does anyone know what
            the answer is? What path did he find? AUDIENCE: PROFESSOR: That’s correct. So the answer is? On this graph,
            the answer is? AUDIENCE: SBHIG. PROFESSOR: SBHIG. That’s the answer. You would have your five points. Did
            people see that?</p>
        <h2 id="unknown-450">未知</h2>
        <h2>Unknown</h2>
        <p>它并不总是有效，所以我们要真正解决它。但大家有没有发现，有时你可以不这样做？因为广度优先搜索保证会给你一条路径，正如我们没听错，这条路径的跳跃次数最少，如果有多个路径的跳跃次数最少，你可以按字典顺序找出答案，在这种情况下，按实际字典顺序。
        </p>
        <p>It doesn’t always work, so we’re going to actually solve it. But did everyone see that sometimes you can get
            away with not doing it? Because breadth first search is guaranteed to give you the path, as we heard
            correctly, with the least number of jumps, and if there are more than one that tie with the least number of
            jumps, you can just lexicographically figure it out, in this case with actual dictionary order.</p>
        <p>但 SBHIG 是唯一的。让我们进行实际的广度优先搜索，这样我们就可以感觉更好了。好的。所以你得到了 S，并且 S 指向，正如我们之前看到的，ABC。也许它不必那么高。好的。正如我们之前看到的，S 指向
            ABC。你已经告诉我 A 指向 D，你告诉我 B 指向 H。但是 C 指向什么？E。</p>
        <p>But SBHIG is the only one. Let’s do an actual breadth first search though, so we can feel better about
            ourselves. OK. So you’ve got S, and S goes to, as we saw before, ABC. Maybe that doesn’t have to go quite so
            high. All right. As we saw before, S goes to ABC. And you already told me that A goes to D, and you told me
            that B goes to H. But what does C go to? E.</p>
        <p>没错。如您所见，我们正在从左到右逐级扩展它。好的。那么 SAD，SAD 会去哪里？SAD 无处可去。它死了。SBH。现在，等等。您可能会说，等一下。帕特里克说，我们使用了一种奇怪的字典顺序，其中 E 位于末尾，在 H
            之前。这是我们的决胜顺序，但事实证明，广度优先搜索和深度优先搜索根本不按任何方式排序。</p>
        <p>That’s right. As you can see, we’re expanding it level by level, left to right. All right. So SAD, were does
            SAD go to? SAD goes to nowhere. It’s dead. SBH. Now, wait. You might say, wait a minute. Patrick said that
            we’re using this weird dictionary order where E is at the end that comes before H. That’s our tiebreak
            order, but it turns out that breadth first search and depth first search don’t sort in any way.</p>
        <h2 id="unknown-451">未知</h2>
        <h2>Unknown</h2>
        <p>这非常重要。他们不对队列中的当前路径进行排序。因此，您只需从左到右、从左到右、从左到右。并且只有在每个节点上，您才会按字典顺序打破平局。好吗？所以 SBH。正如我们已经知道的，H 转到 F 和
            I。E，我们认为我们已经知道，但实际上我们并不知道，因为这是来自另一个方向的 E。</p>
        <p>It’s very important. They don’t sort the paths that are currently on the queue. So you’re going to just go
            left to right, left to right, left to right. And only at each node are you going to break ties in
            lexicographic order. All right? So SBH. H goes to, as we already know, F and I. E, we think we already know,
            but we don’t quite because this is E coming from the other direction.</p>
        <p>E，这次转到 F。没错。嗯，实际上，你们已经知道了。好的。太好了。现在我们来到这里。HF 转到，正如我们已经知道的，E 和 J。I 转到，正如我们还不知道的，HI 转到 G 和
            J。没错。事实证明，通过实施细节，我们已经完成了。有疑问吗？它根本没有扩展深度吗？现在这是一个实施细节。</p>
        <p>E, this time goes to F. That’s right. Well, actually, you guys do already know it. All right. Great. Now we
            come over here. HF goes to, as we already know, E and J. I goes to, as we don’t already know, HI goes to G
            and J. That’s right. And as it turns out, by an implementation detail, we’re done. Questions? Does it not
            expand depth at all? Now this is an implementation detail.</p>
        <p>进行广度优先搜索并希望完成其正在处理的整个级别是完全明智和合理的。但是，在我们的实现中，如果我们过于拘谨并绘制了整个队列，我们​​就会看到这种情况。这就是绘制整个队列（正如我所说）更可靠的另一个原因。</p>
        <p>It’s perfectly sane and reasonable to make a breadth first search that likes to finish its entire level that
            it’s working on. However, in our implementation, and we would have seen this if we had been pedantic and
            drawn out the entire queue. that’s another reason why drawing out the entire queue is, as I said, more
            reliable.</p>
        <h2 id="unknown-452">未知</h2>
        <h2>Unknown</h2>
        <p>在我们的实现中，由于它不是最佳搜索，所以只要在队列的任何地方添加末尾带有 G
            的内容，就完成了。而且由于广度优先搜索的魔力，它会将所有内容添加到队列末尾。这就是它逐级执行的方式，对吧？它将内容添加到队列末尾，而不是前面。</p>
        <p>In our implementation, since it’s not an optimal search, the moment anywhere on the queue, you add something
            with a G at the end, you finish. And because of the fact that the way breadth first search does its mojo, is
            that it adds everything to the end of the queue. That’s how it does it level by level, right? It adds it to
            the end of the queue, instead of the front.</p>
        <p>那么，您将把它添加到队列末尾，然后队列中就会出现 G。所以您不必执行 SCEF。另一个问题？听众：我只是想知道，广度优先搜索没有回溯，也没有其他方法。教授：啊。这是个好问题。所以问题是，对于广度优先搜索，没有回溯。D
            死了。为什么我们没有回溯或做别的？</p>
        <p>Well, then you’ll add it to the end of the queue, and then you will have it on the queue with a G. So you
            won’t have to do SCEF. Another question? AUDIENCE: I’m just wondering, so the breadth first search, there
            was no backtracking had no other. PROFESSOR: Ah. That’s a good question. So the question was, so for the
            breadth first search, there was no backtracking. D died. Why didn’t we backtrack or something?</p>
        <p>答案是，对于广度优先搜索，回溯实际上并不存在，不像深度优先搜索那样。为什么？因为广度优先搜索会把无限的猴子送往每条路径。在深度优先搜索中，我们真正关注的是现在。我们想，我们想到达那里。哦，这样，这样，这样，这样。
        </p>
        <p>The answer to that one is, for breadth first search, backtracking doesn’t really, it isn’t really a thing,
            like it is for depth first search. Why? Well, because breadth first search, we’re sending our infinite
            monkeys down every path. In depth first search, we’re really focused in now. We’re like, we want to get
            there. We want to get there. Ooh, this way, this way, this way, this way.</p>
        <h2 id="unknown-453">未知</h2>
        <h2>Unknown</h2>
        <p>可能会走错路，然后我们走进死胡同，我们会想，哦，糟糕。然后我们倒退。但对于广度优先搜索，我们真的像一个邪恶的霸主。我们就像马克·维德说的，冲锋队，朝每个方向走。然后从那里，朝其他方向走。</p>
        <p>And might have gone the wrong way, and then we hit a dead end, we’re like, oh, crap. And we go backwards. But
            for breadth first search, we really are like an evil overlord. And we’re like Mark Vader saying, storm
            troopers, go every direction. And then from there, go every other direction.</p>
        <p>所以，尽管，是的，当我们到达 D 时，一些冲锋队员撞上了死胡同，可能是因为树反光，他们开枪射击，然后打中了自己或类似情况。与此同时，我们派往 B 和 C
            的冲锋队员仍然安然无恙，所以我们不需要回溯。观众：因为还有其他部队正在从树上下来。教授：是的，四面八方都有其他部队从树上下来。</p>
        <p>And so, even though, yes, when we got to D, some of the storm troopers hit a dead end, and probably it was
            reflective, and they shot it, and hit themselves or something like that. Meanwhile, the storm troopers we
            sent to B and C are still fine, so we don’t need to backtrack. AUDIENCE: Cause there are, like, other troops
            going down the tree. PROFESSOR: Yeah, there are other troops going down the tree in every direction.</p>
        <p>而在深度优先搜索中，我们只发送了……我们觉得，这个逃生舱里肯定没有机器人。把每个人都送到这个直线方向，我们只把他们送到了 A 和
            D。然后，我们不得不回溯，因为我们走到了死胡同。这有意义吗？另一个问题？听众：教授：那么问题是，在这个广度优先搜索中，字典顺序有没有发挥作用？</p>
        <p>Whereas in the depth first search, we only sent. we were like, there are definitely no droids on this escape
            pod. Send everyone this straight direction, and we only sent them to A and D. And then, so we had to
            backtrack because we hit a dead end. Does that make sense? Another question? AUDIENCE: PROFESSOR: So the
            question is, in this breadth first search, did lexicographic order ever come into play?</p>
        <h2 id="unknown-454">未知</h2>
        <h2>Unknown</h2>
        <p>答案是肯定的，而且方式非常微妙和狡猾。也就是说，我将 E 写在 J 之前，将 F 写在 I 之前，将 G 写在 J 之前。如果我没有使用字典顺序，那么当我扩展 I 时，将 J 写在 G 之前可能是合理的，因为 J
            在树上的位置更高或类似情况。</p>
        <p>The answer is yes, in a very subtle and sneaky way. Which is, I wrote E before J, F before I, and G before J.
            If I was not using lexicographic order, it might have been reasonable to write, for instance, when I was
            expanding I, to write J before G because J was higher up on the tree or something like that.</p>
        <p>但它发挥作用的唯一方式是我每次都按字母顺序从左到右写，我写的是 ABC。另一个问题？听众：我们看到 F 节点两次。假设没有到达目标节点，那么你将再次访问 F。教授：我们将再次访问 F。没错。问题是，F 被列出了两次。
        </p>
        <p>But the only way it came into play is that I wrote them left to right in alphabetical order every time, and I
            wrote ABC. Another question? AUDIENCE: We see the F node twice. Suppose that the goal node wasn’t reached
            then you would have visited F again. PROFESSOR: And we would have visited F again. That’s correct. The
            question is, F is listed twice.</p>
        <p>那么如果我们没有到达目标节点，假设在那里，I 之后是 Z，Z 之后是目标节点，那么我们会再次访问 F 吗？答案是肯定的。正如我所说，这种方法会让你的部队四面八方。所以有冲锋队从 E 到 F，有冲锋队从 H 到
            F，他们到处都是。</p>
        <p>So if we hadn’t reached the goal node, let’s say that down there, after I is Z, and after Z is the goal node,
            then would we have visited F again? The answer is yes. As I said, this is the approach that throws your
            troops every possible way. So there’s storm troopers going from E to F, and there’s storm troopers going
            from H to F, and they’re going everywhere.</p>
        <h2 id="unknown-455">未知</h2>
        <h2>Unknown</h2>
        <p>现在有一种方法可以减少这种情况。您可以使用扩展列表进行广度优先搜索。如果您确实使用扩展列表进行广度优先搜索，这将相当于您扩展一个节点后，您立即将冲锋队从 I 派出去查看 G 和 J，其中一个留在 I，如果其他冲锋队来到
            I，他们会说，我们想看看 I 后面的情况，他会说，不，不，不。我们已经派部队越过 I。我们搞定了。</p>
        <p>Now there is a way to cut down on this. You could do breadth first search with an extended list. If you did
            do breadth first search with an extended list, that would be sort of equivalent to as soon as you expand one
            node, as soon you send storm troopers out of I to look at G and J, one of them stays at I, and if any other
            storm troopers come to I, and they’re like, we want to see what’s past I, he’s like, no, no, no. We’ve
            already sent troops past I. We’ve got this.</p>
        <p>回家吧。没关系。这里没有叛军，但可能是汉索罗装扮成冲锋队员，我们将在下一个问题中看到。我们的扩展列表让我们陷入困境。但对于这个问题，你明白我的意思吗？有了扩展列表，我们可以避免这种情况，因为扩展列表基本上说，一旦我扩展并搜索过这里，就不要再这样做了。
        </p>
        <p>Go back home. It’s OK. There are no rebels here, but it might be Han Solo dressed up as a storm trooper, as
            we’ll see in the next problem. Where our extended list screws us over. But for this problem, do you see what
            I mean? With an extended list, we could avoid this because the extended list basically says, once I’ve
            expanded and searched past here, don’t do it again.</p>
        <p>但是如果你没有，是的，你会做两次 F。事实上，我们在这里不是做了两次吗？哦，事实证明我们没有。但我们很容易做到。我们差点做了两次 E。我们差点做了两次
            F。事实证明我们没有，但我们本可以做到。另一个问题？听众：对于队列的实现，当你展开 I 时，G 和 J 都会同时按字典顺序添加？</p>
        <p>But if you don’t have one, yeah, you’ll do F twice. In fact, didn’t we do something twice here? Oh, it turns
            out we didn’t. But we easily could have. We almost did E twice. We almost did F twice. It turns out we
            didn’t, but we could have. Another question? AUDIENCE: for the implementation of the queue that when you
            expand I, that both G and J were added simultaneously and in lexicographic order?</p>
        <h2 id="unknown-456">未知</h2>
        <h2>Unknown</h2>
        <p>或者是否有一个部分是按顺序排列的，G 是在 J 之前添加的，因此它停止了 教授：哈。这是一个非常好的问题，引发了一点小小的改变。问题是，当你扩展 SBHI 时，SBHIG 是否因为字典顺序而先于 SBHIJ
            添加，还是它们同时添加？答案是同时添加。</p>
        <p>Or is there a piece where it would be order, G got added before J, so hence, it stopped PROFESSOR: Ha. That
            is a very good question that provokes a minor change. The question is, when you expand SBHI, is SBHIG added
            before SBHIJ because of lexicographic ordering, or are they both added at the same time? The answer is,
            simultaneous.</p>
        <p>当你进行扩展时，你会立即收到一个列表，或者任何你喜欢的数据结构，其中包含所有子节点，它们会按顺序一次性附加到队列中。所以你会收到所有的子节点。我想你可以创建某种搜索问题，比如你有一个对手节点，比如
            wumpus，或者你正在猎捕 wumpus。</p>
        <p>When you do the expansion, you instantly receive say, a list maybe, or whatever data structure you like,
            containing all of the children, and they get appended to the queue at once in order. So you will receive all
            of them. I suppose you could create some kind of search problem where you have, like, an adversary node,
            like a wumpus, or you’re hunting a wumpus.</p>
        <p>如果你不想将确切的 wumpus 添加到你的搜索树中，否则你就输了。你只想添加 wumpus
            旁边的那个。我不知道。我编的。不幸的是，你必须一次添加所有子项。我们当前的算法不会一次添加一个。好的。所以我们完成了广度优先搜索。太好了。现在我们要做一些最佳搜索。</p>
        <p>And if you don’t want to add the exact wumpus to your search tree, or you lose. You only want to add the one
            next to the wumpus. I don’t know. I’m making this up. And unfortunately, you would have to add all the
            children at once. Our current algorithm does not add one at a time. All right. So we’ve done a breadth first
            search. Great. Now we’re going to do some optimal search.</p>
        <h2 id="unknown-457">未知</h2>
        <h2>Unknown</h2>
        <p>这需要我画一个略有不同的图。我会把它画在底部的左边。然后我们会在底部的左边解决这个问题。好了。马克有了新的据点，他想入侵平行宇宙。现在他已经对他的邪恶超级计算机进行了编程，以找到从他的起始宇宙到目标宇宙 G
            的最短跳跃路径。</p>
        <p>That is going to require me to draw a slightly different graph. I will draw it on what is left of the bottom
            here. And then we’ll solve it on what is left of the bottom here. All right. Mark has his new stronghold,
            and he wants to invade parallel universes. Now he’s programmed his evil supercomputer to find the shortest
            path of jumps from his starting universe to the goal universe, G.</p>
        <p>根据不同宇宙之间的不同因素，从一个宇宙移动到另一个宇宙需要一定量的能量。例如，在一个宇宙中，蝴蝶在中国不会扇动翅膀，而在另一个宇宙中，有意识的爬行动物统治着地球。</p>
        <p>It takes a certain amount of energy to move from universe to universe based on the differing factors between
            those universes. Like maybe in one universe, a butterfly didn’t flap its wings in China, whereas in the
            other universe, sentient reptiles rule the earth.</p>
        <p>因此，马克的超级计算机试图创建一个启发式值，该值决定了宇宙之间的差异，以便猜测从他的起始宇宙 S 到达他的目标宇宙 G 需要多少能量，前提是他的军队目前位于某个特定宇宙。换句话说，好吧，你在搜索中输入的不是
            A，但该死的，我们将有一个到目标的启发式距离。</p>
        <p>And so, Mark’s supercomputer has tried to create a heuristic value that determines how different the
            universes are to guess, sort of, how much energy it’s going to take to get from his start universe, S, to
            his goal universe, G, given that currently his armies are at a particular universe. In other words, OK, it’s
            not A you put in search, but dammit, we’re going to have a heuristic distance to the goal.</p>
        <h2 id="unknown-458">未知</h2>
        <h2>Unknown</h2>
        <p>我们将得到一个包含距离的图表。让我们看看它是什么样子。好的。下面是 G。这个原本不起眼的心形隐藏着一支邪恶的入侵力量。让我们看看。这里的距离是 100。这里的距离是 3,4。这里的距离是 4。这里的距离是
            50,50.14,4.16,16。哦，抱歉，这些是相连的。</p>
        <p>And we’re going to have a graph with distances. So let’s see what it looks like. All right. And down here is
            G. So this otherwise unassuming heart shape hides an evil invasion force. So let’s see. The distance here is
            100. The distance here is 3,4. The distance here is 4. The distance here is 50,50.14,4.16,16. Oh sorry,
            these are connected.</p>
        <p>30 和 10。启发式值在起始节点为 0，在 B 为 50，在 A 为 60，在 C 为 55，在 D 为 50，在 E 为 56，在 F 为 50，在 G 为 0，因为它是目标节点，在 H 为 39。我会把小 H
            画小一点。在 I 为 0。毕竟，I 直接到 G。它可以是 0。这样就太好了。好。</p>
        <p>30 and 10. The heuristic values are 0 at the start node, 50 at B, 60 at A, 55 at C, 50 at D, 56 at E, 50 at
            F, 0 at G because it’s the goal node, 39 at H. I’ll draw the little H smaller. And 0 at I. Well, after all,
            I goes right to G. It can be 0. It’ll be great. OK.</p>
        <p>现在，马克这次试图节省他的平行宇宙跳跃的能量。可以理解的是，这需要大量能量。因此，他需要编写最短的宇宙跳跃次数来达到目标​​。他的意思不是跳跃次数最少。他指的是这些边缘上的能量最少。</p>
        <p>Now, this time Mark is trying to conserve the energy of his parallel universe jump. That, understandably,
            takes a lot of energy. So he needs to program in the shortest number of universe jumps that will get him to
            the goal. He doesn’t mean the least number of jumps. He means the least amount of energy on these edges.</p>
        <h2 id="unknown-459">未知</h2>
        <h2>Unknown</h2>
        <p>他对进入这个新世界不感兴趣，而且没有足够的精力去摧毁它。所以我们需要找到最短的路径。所以马克首先编写了一个简单的分支定界搜索程序。他为分支定界程序添加了一个扩展列表，只是为了让它快一点。像往常一样，他按字典顺序打破了长度相等的平局。
        </p>
        <p>He’s not interested in getting to this new world, and not having enough energy left to blast the crap out of
            it. So we need to find the shortest path. So first Mark programs a simple branch and bound search. He adds
            in an extended list to his branch and bound just to make it a little bit faster. As usual, he breaks ties of
            equal length in lexicographic order.</p>
        <p>那么让我们按顺序列出 Mark
            的计算机添加到扩展列表中的节点。距离显示在边缘旁边。暂时忽略括号中的数字。它们是启发式方法，我们没有使用它们。所以这让我想到了另一点，在我们进入最后冲刺阶段并解决这些问题之前，我想强调这一点。分支定界和 A*
            之间到底有什么区别？好的。</p>
        <p>So let’s list the nodes that Mark’s computer adds to the extended list in order. Distances are shown next to
            the edges. Ignore the number in parentheses for now. They are heuristics, and we’re not using them. So that
            brings me to another point that I’d like to drive home before we go for the home stretch and solve these
            problems. What the heck is the difference between branch and bound and A*? OK.</p>
        <p>我喜欢把它比作以下内容。我可能应该这样做，我会把它变成一个银星创意。好的。银星创意是披萨。它看起来不像披萨，但这就是银星创意。Branch and bound
            是一种奶酪披萨。很简单。如果你为大学生的大型团体活动订购它，他们会吃它，事情就没问题了。</p>
        <p>I like to liken it to the following. I probably should have, I’ll make this a silver star idea. OK. The
            silver star idea is pizza. It doesn’t really look like pizza, but that’s the silver star idea. Branch and
            bound is a cheese pizza. It’s simple. If you order it for a large group event of college students, they will
            eat it, and things will be OK.</p>
        <h2 id="unknown-460">未知</h2>
        <h2>Unknown</h2>
        <p>现在 A* 搜索是某种肉食爱好者或极品披萨。也许是肉食爱好者的披萨。它有所有这些额外的配料。很多人真的会喜欢它。它可能会更好。但是如果你有一个素食主义者，一切都搞砸了。所以基本上，A*
            只是添加了一些额外配料的分支和边界。</p>
        <p>Now A* search is some kind of meat lover’s or supreme pizza. Maybe a meat lover’s pizza. It’s got all these
            extra toppings. A lot of people are really going to like it. It might be better. But then you’ve got a
            vegetarian, and everything’s screwed up. So basically, A* is just branch and bound with some extra toppings
            added on.</p>
        <p>在这种情况下，其中一个顶部是扩展列表，我们将看到扩展列表跟踪我们经过和已经扩展的位置，并且它永远不会返回。我们要添加的另一个顶部是启发式。启发式告诉我们我们认为我们还剩下多远才能到达终点，这样我们就不会走完全错误方向的非常短的路。好吗？
        </p>
        <p>In this case, one of the toppings is an extended list, an extended list which we’ll see keeps track of where
            we’ve passed through and already expanded out, and it never goes back. The other topping that we’re going to
            add is a heuristic. A heuristic tells us about how far we think we have left until we’re at the end, so we
            don’t go through really short paths that go completely in the wrong direction. All right?</p>
        <p>有了这两种配料，我们就可以制作出极品披萨，但有时，正如我们所见，它们会互相混淆。所以其中一种配料与另一种配料不相配。也许有人不喜欢夏威夷披萨。他们认为火腿和菠萝不相配。所以让我们做一个分支定界，只加一个扩展列表。也许我们在披萨上放了一些青椒。这样就安全了。
        </p>
        <p>Between the two of those, we get our supreme pizza, but sometimes, as we’ll see, they sort of mess each other
            up. So one of the toppings just doesn’t go well with the other one. Maybe someone doesn’t Hawaiian. They
            think that the ham doesn’t go well with the pineapple. So let’s do a branch and bound that just has an
            extended list. Maybe we’ve got some green peppers on this pizza. This is going to be safe.</p>
        <h2 id="unknown-461">未知</h2>
        <h2>Unknown</h2>
        <p>好的。所以我们将列出扩展列表中的节点，而我们要这样做的方式是，嗯，你们说你们更喜欢目标树而不是队列，所以让我们用目标树来做。所以我们得到了 S，而且我们已经知道，S 是唯一的路径。我们在 S 的当前长度是
            0。这是树上所有长度中最小的，因为它是唯一的长度。</p>
        <p>All right. So we’re going to list the nodes as to the extended list, and the way we’re going to do that is,
            well, you guys said you like goal tree more than queue, so let’s do it with a goal tree. So we’ve got S, and
            as we already know, S is the only path. Our current length at S is 0. That’s the lowest of all of our
            lengths on the tree cause it’s the only length.</p>
        <p>所以我们去 A、B 和 C。可能很难看出来，所以 SA 的长度是 100。SB 的长度是。听众：你的意思是 ABEF？听众：是的。教授：哦。你说得对。这是一棵不同的树。我从另一棵树上做 A、B 和
            C。谢谢朋友们纠正我的愚蠢。SB 的长度是 3。这是一棵不同的树。</p>
        <p>So we go to A, B, and C. It may be hard to see, so the length of SA is 100. The length of SB is. AUDIENCE:
            You mean ABEF? AUDIENCE: Yeah. PROFESSOR: Oh. You’re right. It’s a different tree. I was doing A, B, and C
            from the other tree. Thank you, friends, for correcting my foolishness. The length of SB is 3. It is a
            different tree.</p>
        <p>我试图保存最终导致产生更多问题的工作，因为有人会对此感到困惑。因此，A、B、E 和 F 的长度分别为 100、3.14 和 14。我们选择哪一个？按字典顺序排列，是 A。我们选择那个，对吗？观众：教授：F 将是
            14。那是 1。抱歉。观众：4.4。教授：哦，是 4？我写了这个问题。我应该知道。你说得对。</p>
        <p>I was trying to save work that wound up creating more because someone’s going to be confused by this. So the
            length of A, B, E, and F are 100,3.14, and 14. Which one do we choose? Lexicographically, it’s A. We choose
            that one, right? AUDIENCE: PROFESSOR: F is going to be 14. That’ a 1. Sorry. AUDIENCE: 4.4. PROFESSOR: Oh,
            it’s 4? I wrote this problem. I should know it. You’re right.</p>
        <h2 id="unknown-462">未知</h2>
        <h2>Unknown</h2>
        <p>F 是 4。再次抱歉。这就是我得到的。好的。从现在起，当我在那边写字时，我会站在这里。好的。那么我们要选择哪一个呢？即使有 4，我们也要选择
            B。没错。不管字典顺序如何。它只用于决胜局。我们想要最短路径。分支定界法的特别之处在于，我们选择当前最短的路径，无论它是什么。太好了。</p>
        <p>F is 4. My apologies once more. That’s what I get. All right. I’m going to stand over here from now on when I
            write over there. OK. So which one of these are we going to choose? Even with the 4, we’re going to choose
            B. That’s right. Lexicographic be damned. It’s only for tiebreaks. We want the shortest path. Our special
            thing with branch and bound is we take the currently shortest path, whatever it is. Great.</p>
        <p>所以我们展开 B。幸运的是，我很确定这次我答对了。哦，等等，我有一个更好的主意。我只需要带上这张小手抄，然后我就不用看它了。好的。所以一旦我们展开 B，B 就会到达 D。从 B 到 D 的路径长度是
            4，那么我们要在这里的小 G 旁边写什么呢？7。没错。</p>
        <p>So we expand B. Fortunately, and I’m pretty sure I’ve got this correct this time. oh wait, I’ve got an even
            better idea. I’ll just take this little hand sheet with me, and then I don’t have to look at that at all.
            OK. So once we expand B, B goes to D. Our path length from B to D is 4, so what will we write next to little
            G here? 7. That’s right.</p>
        <p>我们把它们全部加在一起。下次我们用启发式方法做这件事时，我会再问你们，这将是问题的下一部分，下次有人会给我错误的答案。所以请继续关注。好的。所以我们得到了
            SBD。我们得到了所有这些。我们下一步去哪里？F。没错，因为目前 4 是最短的，因为你们正确地纠正了我。</p>
        <p>We add them all together. I’m going to ask you guys again the next time we do this with the heuristics, which
            is going to be the next part of the problem, and someone’s going to give me the wrong answer next time. So
            stay tuned for that. All right. So we’ve got SBD. We’ve got all of these. Where do we go next? F. That’s
            right, because currently 4 is the shortest because you guys corrected me correctly.</p>
        <h2 id="unknown-463">未知</h2>
        <h2>Unknown</h2>
        <p>所以 SF. 4 是最短的。F 只到 H。我们在 H 旁边写什么？20。没错。16 加 4,20。好的。我们目前最短的在哪里？是 D，SBD。好吗？所以 SBD. D 只到 I，我们得到了
            57。等一下。我想把这个问题解决好，所以我们最好写出扩展列表，因为这是他们唯一要求我们做的事情。</p>
        <p>So SF. 4 is the shortest. F only goes to H. And what do we write next to the H? 20. That’s right. 16 plus
            4,20. All right. Where’s our current shortest? It’s the D, SBD. All right? So SBD. D only goes to I, and
            we’ve got a 57. Wait a minute. I want to get this problem right, so we better actually write the extended
            list, because that’s the only thing they’re asking us for.</p>
        <p>因此，我们首先扩展 S。然后扩展 B。然后扩展 F。然后扩展 D。好的。到目前为止，一切顺利。我有答案，所以我知道我们做对了。好的。接下来我们做什么？E。没错。因此，我们扩展 E。E 转到 H。当 E 转到 H
            时，我们得到的长度为 30。好的。现在谁是我们的赢家？哪个 H？SFH。没错。</p>
        <p>So first we extended S. Then we extended B. Then we extended F. Then we extended D. All right. So far, so
            good. And I have the answer key, so I know we’re doing it right. OK. What are we doing next? E. That’s
            right. So we extend E. E goes to H. And when E goes to H, we’ve got a length of 30. All right. Who’s our
            winner now? Which H? SFH. That’s right.</p>
        <p>SFH 的长度为 20。哦，对了，我当然应该在这里写一个 E。我总是忘记写。SFH 的长度为 20，所以我们将扩展 H，我将在这里预先写上它。当我们在这里扩展 H 时，H 只会到达长度为 50 的
            I。太好了。下一个最短的是什么？下一个最短的是另一个 H。但是，我们会扩展它吗？</p>
        <p>SFH is length 20. Oh yeah, of course I should write an E in here. I always forget to do that. SFH is length
            20, so we will extend H, and I’m going to write it here preemptively. When we extend H over here, H only
            goes to I, with length 50. Great. What’s the next shortest? The next shortest is the other H. However, will
            we expand it?</p>
        <h2 id="unknown-464">未知</h2>
        <h2>Unknown</h2>
        <p>你猜对了，因为我就是这么问的。你知道答案是否定的。我们为什么不扩展它呢？它已经在扩展列表中了。没错。既然它已经在扩展列表中了，这个就惨遭淘汰了。我喜欢在它上面写一个 X，而不是在底部写一个
            swizzle。你想做什么都可以。好吧，它没了。这不是一个选择。下一个最好的是什么？SFHI。</p>
        <p>You guessed that because I asked that question in that way. You knew the answer was no. Why don’t we expand
            it? It’s already on the extended list. That’s right. So since it’s already on the extended list, this one
            dies a horrible death. I like writing an X through it instead of writing a swizzle at the bottom. You can do
            whatever you want. All right, it’s gone. It’s not a choice. What’s the next best one? SFHI.</p>
        <p>没错。它到达 G，长度为 60。我们扩展了 I。好的。问题是，我们结束了吗？那些说“不”的人，我喜欢你。你很聪明。你意识到，仅仅因为 G 在那里，我们就不能结束。</p>
        <p>That’s right. That goes to G, and the length is 60. And we’ve extended I. All right. Question is, are we
            done? The people who say no, I like you. You’re smart. You realize that just because the G is on there, that
            we can’t end.</p>
        <p>但是，那些说“是”的人，你们要么是无知，要么是真的非常聪明，我选择假设你们都非常非常聪明，因为那些非常非常聪明的人说，好的。是的，我们并没有因为添加了一个 G
            而完全完成。我们仍然需要检查以确保没有更短路径的长度，但是看。唯一一个具有更短路径的长度是 I，它已经在扩展列表中了。</p>
        <p>However, the people who said yes, you are either oblivious or really, really smart, and I’m going to choose
            to assume you’re all really, really smart because the really, really smart people said, OK. Yes, we’re not
            quite done just because we added a G. We still have to check to make sure there are no lengths with shorter
            path, but look. The only one with a shorter path is I, which is already on the extended list.</p>
        <h2 id="unknown-465">未知</h2>
        <h2>Unknown</h2>
        <p>所以实际上，我们已经完成了。再检查一下。我们搞定了。所以这些是我们按顺序延伸的路径，我们的最终路径是？各位。听众：教授：没错。SFHIG。我声称这是正确的路径。然而，马克对分支定界法的速度感到沮丧。我不知道。我没有那么沮丧。看起来还不错。但马克对分支定界法的速度感到沮丧，所以他重新编程他的计算机以使用
            A*。</p>
        <p>So actually, we are done. Double check. We’ve got it. So these are the paths we extended in order, and our
            final path is? Everyone. AUDIENCE: PROFESSOR: That’s right. SFHIG. I claim that is the correct path.
            However, Mark is frustrated by branch and bound’s speed. I don’t know. I wasn’t that frustrated. Seemed
            pretty good. But Mark is frustrated by branch and bound’s speed, so he reprograms his computer to use A*.
        </p>
        <p>马克计算每个宇宙与目标之间的子空间异常数量，并将此计数用作 A* 的启发式方法。这些数字在括号中。希望您能读懂。是吗？哦，我们有一个问题。就在这里？听众：那么，考虑到实现，您说您扩展了所有可能的节点。那么为什么我不去
            C 和 D，以及为什么它不扩展到教授：啊。这是一个非常好的问题。</p>
        <p>Mark counts the number of subspace anomalies between each universe and the goal, and uses this count as the
            heuristic for A*. These numbers are in parentheses. Hopefully, you can read them. Yes? Oh, we’ve got a
            question. Right here? AUDIENCE: So, given the implementation, you said that you expand all possible nodes.
            So why doesn’t I go to C and D, as well as Like, why doesn’t it expand to PROFESSOR: Ah. That is a very good
            question.</p>
        <p>答案很简单。你们骗了我。不，但我应该能想出来。它确实指向 C 和 D。正确的树，我们不会因为这里有错误的树而丢分，因为我们确实得到了正确的答案，是的。原因也是如此，我第一次问别人一个问题时，他不记得 C。</p>
        <p>And the answer is, a very simple answer. You guys tricked me. No, but I should have been able to figure it
            out. It does go to C and D. The correct tree, which we wouldn’t have lost points for having the incorrect
            tree here cause we did get the correct answer, yes. The reason why it is, that same reason that very first
            time I asked someone something, he didn’t remember the C.</p>
        <h2 id="unknown-466">未知</h2>
        <h2>Unknown</h2>
        <p>很容易忘记向上走树。它确实有 C 和 D。然而，它们是可怕的路径。它们的路径长度分别为 100 和 100，所以这并不重要。但你是对的。我们上面的官方答案是错误的。听众：D
            会被添加到已经在扩展列表中的子列表中吗？教授：好问题。</p>
        <p>It’s easy to forget to go up the tree. It does actually have a C and D. However, they are horrendous paths.
            They are 100 and 100 on their path length, and so it doesn’t matter. But you were correct. The official
            answer we had up there is wrong. AUDIENCE: Would the D get added to this list of children that’s already in
            the extended list? PROFESSOR: Good question.</p>
        <p>问题是，D
            会被添加到子节点中吗？毕竟，它已经在扩展列表中了。答案是，当我们尝试扩展它时，我们会搜索、删除并终止所有扩展已在扩展列表中的内容的尝试。我们尝试扩展它的时间只有当它位于队列的最前面时，因为它是当前最短的路径。所以这意味着它们会被添加。
        </p>
        <p>The question is, would the D get added to the children? After all, it’s already in the extended list. The
            answer is, we search for, and remove, and kill all of the attempts to extend something that’s already on the
            expanded list at the time we try to expand it. The time we try to expand it is only when it’s on the front
            of the queue because it’s the currently shortest path. So that means they get added.</p>
        <p>只是到了扩展的时候，不管怎样它都会被划掉。结果发现，它之所以没有执行是因为我们从来没有扩展过它。听众：所以 H 也应该去 E？教授：所以 H 也应该去 E，这是个问题。答案是肯定的。H 也应该去 E。这次我错过了很多。
        </p>
        <p>It’s just that when it comes time to expand it, it will get crossed off no matter what. Turns out, it escaped
            execution because of the fact that we never expanded it. AUDIENCE: So H should go to E as well? PROFESSOR:
            So H should go to E as well, is the question. The answer is yes. H should go to E as well. A lot slipped
            past me this time.</p>
        <h2 id="unknown-467">未知</h2>
        <h2>Unknown</h2>
        <p>H 也应该转到 E，长度为
            36，然后它就死在那里了。这个节点实际上会被检查，所以它确实会产生影响。如果我们问一个节点由于已经在扩展列表中而被执行了多少次？很好。很好的通知。它应该在那里。下次我们会把它弄好的。好的，各位。所以我和你们一起努力。我也犯了错误。
        </p>
        <p>H should go to E as well, with a length of 36, and it dies there. This one will actually be checked, so it
            actually does make a difference. If we ask how many times was a node executed due to already being on the
            extended list? Very good. Very good notice. It should be on there. We’ll get it right next time. All right,
            everyone. So I’m working together with you. I made the mistake too.</p>
        <p>这个问题很简单。它可能会把你搞得一团糟。这次没有。我们会搞定的。问题？听众：如果最终目标是 G，而你的结果中得到了 G，并且你知道比其他所有数字都短的数字，那么你真的必须在 G
            处进行扩展吗？教授：那么，问题是，你真的会扩展 G 吗？我们是否应该将 G 放入扩展列表中？</p>
        <p>Easy one to make. It can mess you up. It didn’t this time. We’re going to get it. question? AUDIENCE: If G’s
            the goal at the end, and you get G in your outcomes, and you know the number that’s shorter than everything
            else, do you have to actually extend at G? PROFESSOR: So, the question is, do you actually extend G? Should
            we even put G in the extended list?</p>
        <p>答案是，这个问题的答案是，这是一个品味问题。在我看来，在我们相当愚蠢地询问扩展了多少个节点而不是要求您写出它们的问题中，我们通常会接受您没有扩展或扩展了 G 的答案。这是一个实现细节。</p>
        <p>The answer is, the answer to that question is, it is a matter of taste. And in the questions where we, in my
            opinion, rather foolishly, asked how many nodes were extended, rather than ask you to write them out, we
            generally accept the answer where you didn’t extend or where you did extend G. It’s an implementation
            detail.</p>
        <h2 id="unknown-468">未知</h2>
        <h2>Unknown</h2>
        <p>你可以设置一个故障保险，当它在列表开头看到 G 时，它就会说，我们出局了。我们不会再延长了。我们完成了。我们赢了。或者当你准备延长某项时，你进入延长过程，它看到它以 G 结尾，它就赢了。所以这是一个品味问题。</p>
        <p>You can either have a fail safe that, as soon as it sees G at the beginning of the list, says, we’re out.
            We’re not going to extend. We’re done. We win. Or a fail safe that, when you’re about to extend something,
            and you go into the extension process, and it sees that it ends in G, that it wins. So it is a matter of
            taste.</p>
        <p>如果您喜欢看您的小家伙在搜索中获胜，并且有 S 和 G，您可以将其放上去。如果您没有，您就不能放上去。我们不会因为末尾是否有 G 而扣分，因为很明显，如果您做对了所有其他疯狂的事情，那么，如果您愿意，您可以写上 G。
        </p>
        <p>If you kind of like to watch your little guy doing the search win, and have an S and a G, you can put it on.
            If you don’t, you can not put it on. We won’t take off points for whether or not it has a G at the end
            because clearly, if you did all the other crazy stuff correct, well, you could have written in a G if you
            wanted to.</p>
        <p>我认为，除非有人在最后错误地解决了问题，否则，谁会在乎 G？哦不，它卡住了。这是死胡同。我们输了。但这种情况可能很少见。所以让我们解决 A*。这样我就不会犯同样的错误两次。我们从 S 到 ABEF。顺便说一下，S
            的值为 0。好吧。A 的值是多少？</p>
        <p>I think, unless there’s someone who mis solves the problem right at the end, is like, oh, who cares about G?
            Oh no, it’s stuck. It’s a dead end. We lose. But that would be probably pretty rare. So let’s solve the A*.
            So I’m not going to make the same mistake twice. We go from S to ABEF. And S, by the way, had a value of 0.
            All right. What is the value at A?</p>
        <h2 id="unknown-469">未知</h2>
        <h2>Unknown</h2>
        <p>嗯，我认为是 160。问题是，我们如何计算这个值？嗯，这是我们到目前为止走过的路径加上最终节点的启发式值。这就是为什么有人会在 BD 给我错误的答案，但让我们看看。所以这里是 160。好的。那么
            SB。这里的启发式值是多少？我们得到路径 3，启发式 50，所以是 53。SE。</p>
        <p>Well, I think it’s 160. The question is, how do we calculate this? Well, it’s the path that we’ve traveled so
            far plus the heuristic value at the final node. This is why someone’s going to give me the wrong answer at
            BD, but let’s see. So we have 160 here. OK. So SB. What is the heuristic value here? We’ve got 3 for the
            path, and 50 for the heuristic, so it’s 53. SE.</p>
        <p>这里我们得到了什么？路径为 14，启发式为 56，所以是 70。好的。SF。路径为 4，启发式为 50，54。好的。谁是赢家？又是 B。勉强，但它是赢家。扩展列表在这里。SB。好的。正如我们所见，B 只适用于
            D。我应该在这里写什么值？听众：教授：好的。</p>
        <p>What have we got here? We’ve got 14 for the path, 56 for the heuristic, so it’s 70. All right. SF. We’ve got
            4 for the path, 50 for the heuristic, 54. OK. Who’s our winner? It’s B again. Barely, but it is the winner.
            Extended list up here. SB. All right. B, as we saw, goes only to D. What is the value that I should write
            here? AUDIENCE: PROFESSOR: OK.</p>
        <p>我很高兴听到了所有我期望听到的内容。我听到了正确答案，即 57。我还听到有人说 107。那么为什么是 57，而不是 107？每次都有人这样做。不要一路上把所有的启发式方法加起来。我会试着向你解释为什么你永远不想这样做。
        </p>
        <p>I am happy I heard all the things that I expected to hear. I heard the correct answer, which is 57. I also
            heard someone say 107. So why is it 57, and not 107? Someone does it this way every time. Do not add up all
            the heuristics along the way. I will try to explain to you why you would never want to do that.</p>
        <h2 id="unknown-470">未知</h2>
        <h2>Unknown</h2>
        <p>任何给定节点的启发式值表示，假设我在这里，我认为我还剩下多少工作才能到达终点？好吗？这有点像，让我们猜测一下我们还没有完成的路径中的最后几个节点。所以你可以明白为什么为列表中的每个节点添加这个值是不好的，因为这样你就重复计算了所有的最后节点。
        </p>
        <p>The heuristic value at any given node says, given that I’m here, how much work do I think I have left to get
            to the end? All right? It’s sort of like, let’s guess the last few nodes in the path that we haven’t done
            out yet. So you can see why it would be bad to add that for every node in your list because then you’re
            double counting all of the last nodes.</p>
        <p>因此，将迄今为止的路径添加到最后的启发式中。即 3 加 4 加 D 的 50 等于 57。因此，我们当前的获胜者是 54，F。与上次相同。F 归 H。我们在 H 的总价值是多少？20 加，即
            39,59。那么现在谁赢了？D，57。但是我们扩展了 F，是的。好的。所以 D 是 57。好的。</p>
        <p>So add the path so far to the very final heuristic. That’s 3 plus 4 plus the 50 that’s with D is 57. So our
            current winner, then, is 54, with F. Just the same as last time. F goes to H. And what’s our total value at
            H? 20 plus, that’s a 39,59. So who wins now? D, with 57. But we extended F, yes. All right. So D was 57. All
            right.</p>
        <p>所以 D 不是我用来欺骗自己的那个。D 只到 I。我们在 I 处的值是多少？我认为我们的值是 57，因为 I 的启发式值为 0。是的。I 的启发式值为 0。我们在 I 处的值是 57。好吗？然后我扩展了
            D。现在谁是赢家？I？好。所以 I 是赢家。以 I 作为赢家，我们扩展了 I。I 到 C、D。</p>
        <p>So D was not one of the ones that I tricked myself with. D only goes to I. What’s our value at I? I think our
            value is 57 because I has a heuristic of 0. Yes. I has a heuristic of 0. Our value at I is 57. OK? And I
            extended D. Who’s the winner now? I? OK. So I is the winner. With I as the winner, we extend I. I goes to C,
            D.</p>
        <h2 id="unknown-471">未知</h2>
        <h2>Unknown</h2>
        <p>哈哈。说得好，前面那位谁发现了我的秘密错误。C、D 和 G。听众：教授：C、H 和 G？哦，你说得对。我们已经到了 D。啊哈。C、H 和 G。你说得对。大家都看到了吗？C、H 和 G。完全正确。我到了 C、H 和
            G。所以到 C、H 和 G 的路径是一条艰难的路径。到 C，我们得到了什么？</p>
        <p>Ha ha. Good call, whoever back there figured out my secret error before. C, D, and G. AUDIENCE: PROFESSOR: C,
            H, and G? Oh, you’re right. We already went to D. Aha. C, H, and G. You’re correct. Everyone see that? C, H,
            and G. Absolutely right. I goes to C, H, and G. So the path to C, H, and G is a hard path. To C, we got,
            what did we have?</p>
        <p>57 加 50 等于 127。哦，顺便说一下，C、G 和 H。我们必须按字典顺序进行。所以 C 是 127。G 是 67。H 是 87。那么现在谁赢了？H。H 是 59。没错。H 是
            59。我们已经知道，因为上次是赢家，H 是 59，归 I，我认为 I 是 50。好吧，它仍然被加起来了。还记得吗？</p>
        <p>57 plus 50 is 127. Oh, and by the way, C, G, and H. We’ve got to do it in lexicographic order. So 127 to C.
            To G, we have 67. And to H, we have 87. So who wins now? H. H, with 59. That’s right. H, with 59. And as we
            already know, cause it was the winner last time, H, with 59, goes to I, which has, I believe, 50. Well, it’s
            still gets added. Remember?</p>
        <p>只有当它延长时，我们才会将其杀死。那么谁最短？所以无论谁问我这个问题，你都领先了一步。谁最短？H 也通向 E。完全正确。H 也通向 E。我在这里有点操之过急。H 也通向 E，长度多 16。所以我们得到 20 加 16
            等于 36 加 56 等于 92。好的。很好。那么谁最短？</p>
        <p>We’re only going to kill it when it gets extended. So who’s the shortest? So you were a step ahead, whoever
            asked me that question. Who’s the shortest? H also goes to E. Absolutely right. H also goes to E. I was
            getting ahead of myself here. H also goes to E with a length of extra 16. So we’ve got 20 plus 16 is 36 plus
            56 is 92. All right. That’s good. So who’s the shortest?</p>
        <h2 id="unknown-472">未知</h2>
        <h2>Unknown</h2>
        <p>I 是最短的。我们要扩展它吗？不。它在扩展列表中。好的。现在谁是最短的？很难读懂，但最短的是 G 上的这个 67。所以我们完成了。我们赢了。我们的路径是 SBDIG。是的，对于 A*
            来说。它得到了正确的答案，对吧？不。不幸的是没有。那么这里发生了什么？为什么我们没有得到正确的答案？尽可能具体。人们对启发式方法有何看法？</p>
        <p>I is the shortest. Do we extend it? No.&nbsp;It’s on the extended list. All right. Who’s the shortest now?
            It’s hard to read, but the shortest is this 67 on the G. So we’re done. We win. Our path is SBDIG. Yeah, for
            A*. It gets the right answer, right? No.&nbsp;Unfortunately not. So what happened here? Why did we not get
            the right answer? Be as specific as possible. What are people saying about heuristics?</p>
        <p>听众：教授：好的。人们说启发式方法必须是一致的。这既正确又具体，所以我为你鼓掌。说启发式方法不可接受太容易了。实际上，它在任何地方都是完全可接受的。所以，这引出了我们今天的最后一点。这些启发式一致性和可接受性到底是什么，我为什么要关心？
        </p>
        <p>AUDIENCE: PROFESSOR: All right. People are saying the heuristic must be consistent. That is both correct and
            specific, so I applaud you. Too easy to say the heuristic was inadmissible. It actually was completely
            admissible everywhere. So that leads us into our very last point for the day. What the hell are these
            heuristic consistency and admissibility things, and why do I care?</p>
        <p>嗯，你关心的原因有很多，其中之一就是它几乎肯定会出现在测验中。但是它们是什么呢？可采性是在每个点进行的检查，以确保你当时的启发式方法（应该是对你还有多少工作要做的估计）总是低估或准确估计。它永远不会被高估。为什么不呢？
        </p>
        <p>Well, the reason why you care is many fold, one of which is that it’s almost guaranteed to be on the quiz.
            But what are they? Admissibility is a check at every point to make sure your heuristic at that point, which
            is supposed to be an estimate of how much work you have left to do, is always an underestimate or an
            accurate estimate. It can never be an overestimate. Why not?</p>
        <h2 id="unknown-473">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，正如帕特里克在讲座中用他最正确的例子向您展示的那样，如果估计过高，它就永远不会扩展该节点，因为它会认为，你知道，如果你把一百万写成启发式方法，它会认为在这样做完之后还需要做一百万。它不会想这样做。好吗？所以它总是被低估或被平等估计。
        </p>
        <p>Well, as Patrick showed you with his mostly correct example in lecture, if it’s an overestimate, it’s never
            going to expand that node cause it’s going to think, you know, if you write one million as a heuristic, it’s
            going to think it needs to do one million after it’s done going that way. It’s not going to want to go that
            way. All right? So it’s always got to be an underestimate or an equal estimate.</p>
        <p>但是，假设你在测验时忘记了马克告诉我的这件事。你没有带任何笔记。你只是大脑一片空白。你会想，哦不。这应该是高估还是低估？我不记得是哪一个了。我该怎么弄清楚？我该怎么弄清楚？我建议你遵循以下平静、舒缓的咒语/经文，这将有助于保护你。想想以下问题。
        </p>
        <p>But let’s say that you, at the quiz, and you forget Mark told me this. You didn’t bring any notes. You’re
            just having a brain freeze. You’re like, oh no. Is it supposed to be an overestimate or an underestimate? I
            can’t remember which one. How can I figure it out? How can I figure it out? I propose you the following
            calm, soothing mantra slash sutra that will help protect you. Think to yourself of the following question.
        </p>
        <p>我们知道 A* 有时会搞乱启发式算法。分支定界法总能正确吗？是的。分支定界法在任何地方都会增加哪些启发式值？0。对吗？它没有启发式算法。它本质上增加了 0。0
            是高估还是低估？低估。因此，由于分支定界法总是有效，所以你要找的是一个低估值。</p>
        <p>We know that A* sometimes messes up with the heuristic. Does branch and bound always get it right? Yes. What
            heuristic values does branch and bound add everywhere? 0. Right? It has no heuristic. It essentially adds 0.
            Is 0 an overestimate or an underestimate? An underestimate. Therefore, since branch and bound always works,
            the one you’re looking for is an underestimate.</p>
        <h2 id="unknown-474">未知</h2>
        <h2>Unknown</h2>
        <p>我知道，当我上课的时候，我总是需要花大约两分钟的时间说服自己，好吧，我要低估，而不是高估。这会有所帮助。做到这一点只需不到两分钟的时间。那么一致性是什么？一致性是一个更强有力的要求。</p>
        <p>And I know from when I was taking the class, I always had a moment where I had to spend, like, two minutes
            convincing myself, OK, I’m going for underestimate, not overestimate. This will help. It will take fewer
            than two minutes to do that. So what is consistency? Consistency is a little bit stronger of a claim.</p>
        <p>当你声称一个图是一致的，你所说的是，在任意两个节点之间，或者更简单地说，在任意两个相邻节点之间，启发式之间的距离小于节点之间的距离。换句话说，可接受性是 G
            中每个节点之间的一致性。而一致性是每个节点与其他每个节点之间的一致性。好吗？一致性是一个更强有力的说法。</p>
        <p>When you claim that a graph is consistent, what you’re saying is, between any two nodes, or to do it more
            simply, between any two adjacent nodes, the distance between the heuristics is less than the distance
            between the nodes. In other words, admissibility is a sort of, like, consistency between every node in G.
            Whereas consistency is consistency between every node and every other node. All right? Consistency is a
            stronger claim.</p>
        <p>任何一致的图都是可接受的。任何不可接受的图都是不一致的。这就是逆否命题。问题？听众：小于还是小于或等于？教授：等于还是可以的。等于总是可以的。很好。这是一个完美的估计。但永远不会更大。现在，如果一个图不一致，你会输吗？你会输吗？那么，如果它有时不可接受，为什么它被称为可接受的？为什么我们会输？
        </p>
        <p>Any graph that’s consistent is always admissible. Any graph that is inadmissible is always inconsistent.
            That’s the contrapositive. Question? AUDIENCE: Less than or less than or equal to? PROFESSOR: Equal to is
            still OK. Equal to is always OK. That’s great. It’s a perfect estimate. But never greater. Now, if a graph
            is inconsistent, will you lose? Will you lose? Why is it called admissible, then, if it’s sometimes not
            admissible? Why do we lose?</p>
        <h2 id="unknown-475">未知</h2>
        <h2>Unknown</h2>
        <p>答案是扩展列表，您可以在这里看到。如果图表是可接受的，除非您使用扩展列表，否则您将始终得到正确的答案，因为您要从每个节点检查到目标节点。并且您确信您的估计是正确的。</p>
        <p>The answer is the extended list, and you see that here. If a graph is admissible, you will always get the
            right answer unless you use an extended list because you’re checking from every node to the goal node. And
            you’re sure that your estimates are right.</p>
        <p>但是如果节点内的估计不正确，您可能会无序地浏览它们，这违背了您在决定使用扩展列表时所做的假设，即您将始终按顺序浏览子图。此图非常巧妙地为您设计了这一点，因为 I 可能有点像瓶颈、目标节点，并且 I 存在不一致。</p>
        <p>But if estimates within nodes aren’t correct, you might go through them out of order, and that violates your
            assumption that you made when you decided to use the extended list that you would always go through the sub
            graphs in order. This graph is very expertly crafted to do that to you cause I might as well be sort of a
            bottleneck, goal node, and I has an inconsistency.</p>
        <p>此图中还有其他一些不一致之处，但它们不会对您造成任何影响。事实上，即使是不可接受性有时也不会让您陷入困境。因此，您不能盲目地说它是不可接受的，所以它永远不会起作用。它可能会起作用。事实证明，这里唯一重要的不一致之处不是
            S 和 F 之间的不一致。事实证明这并不重要。这是其中一些节点之间的不一致，具体来说包括 I 和 H。</p>
        <p>There’s a few other inconsistencies in this graph, but they don’t do anything to you. In fact, even
            inadmissibilities sometimes don’t mess you up. So you can’t just say, blindly, it’s inadmissible, so it will
            never work. It might work. It turns out the only inconsistency that matters here is not the inconsistency
            between, say, S and F. That doesn’t turn out to matter. It’s the inconsistency between some of these nodes,
            including I and H, specifically.</p>
        <h2 id="unknown-476">未知</h2>
        <h2>Unknown</h2>
        <p>所以祝大家周末愉快。周一和周二来上辅导课。询问队列方法或其他方法。询问任何困扰你的事情。但希望你们已经明白了。你们会做得很好。</p>
        <p>So have a good weekend.come in on Monday and Tuesday to tutorial. Ask about the queue method, or other
            methods. Ask about anything that’s niggling in your brain. But hopefully, you guys have got this. You’re
            going to do fine.</p>
        <h1 id="mega-r3.-games-minimax-alpha-beta">Mega-R3。游戏，Minimax，Alpha-Beta</h1>
        <h1>Mega-R3. Games, Minimax, Alpha-Beta</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 50:56 <a
                href="https://youtube.com/watch?v=hM2EAvMkhtk">https://youtube.com/watch?v=hM2EAvMkhtk</a></p>
        <p> 11 years ago (Jan 11, 2014) — 50:56 <a
                href="https://youtube.com/watch?v=hM2EAvMkhtk">https://youtube.com/watch?v=hM2EAvMkhtk</a></p>
        <h2 id="intro-1">简介</h2>
        <h2>Intro</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。教授：今天我们将讨论游戏。我很了解你们，我希望我了解你们。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Today we’re
            going to be talking about games. And I know you guys as well as I hope I do.</p>
        <p>你们想讨论的主要内容是游戏如何实现 alpha beta 算法。因为它非常令人困惑。而且很容易迷失在某个角落或其他地方。而根据我的经验，大多数 6034 学生都可以实现常规的 minimax
            算法。而且他们几乎总是能正确完成。但是，我们将重点关注游戏的所有不同组成部分。</p>
        <p>The main thing that you guys want to talk about with games is how to do that alpha beta thing. Because it’s
            pretty confusing. And it’s easy to get lost in a corner or something. Whereas doing the regular minimax, in
            my experience, most 6034 students can do that. And they do it right pretty much all the time. However, we’re
            going to focus on all the different components of games.</p>
        <p>我在黑板上贴了两个很有启发性的银星想法，它们将在这里发挥作用。白雪公主原则是一个新名字。直到今天才公布。因为我最近编了这个名字。所以你们将是第一批听到它的人，并决定它是否比“祖父条款”这个词更适合我试图描述的事情。因为大多数祖父不会吃掉他们的孩子。
        </p>
        <p>And I put up two provocative silver star ideas up on the board, which will come into play here. The Snow
            White principle is a new name. And it has never been revealed until today. Because I made up name recently.
            So you will be the first people to hear it and decide if it works better than the term “grandfather clause”
            for the thing that I’m trying to describe. Because most grandfathers don’t eat their children.</p>
        <p>所以这里我们得到了一棵漂亮的博弈树。它有从 A 到 R 的节点。这是我们从 6034
            开始的标准博弈树。我们在顶部有一个最大化者，她试图获得尽可能高的分数。最小化者是她的对手。最小化者试图获得尽可能低的分数。而且在每一点上谁赢谁输真的不清楚。</p>
        <p>So here we’ve got a beautiful game tree. It has nodes from A through R. This is our standard game tree from
            6034. We’ve got a maximizer up at the top who’s trying to get the highest score possible. The minimizer is
            her opponent. And the minimizer is trying to get to the lowest score possible. And it’s really unclear who
            wins or loses at each point.</p>
        <p>他们只是想得到最高分或最低分。好吧，让我们复习一下。希望这次测验没有让人们陷入恐慌，以至于他们忘记了周一的讲座。所以让我们确保我们可以在这棵树上执行常规的极小极大算法，并找出 A
            处的极小极大值。让我们看看它是如何工作的。</p>
        <p>They’re just trying to get it to the highest or the lowest score. All right, so let’s do a refresher.
            Hopefully the quiz didn’t put people into such panic modes that they forgot Monday’s lecture. So let’s make
            sure that we can do regular minimax algorithm on this tree and figure out the minimax value at A. So let’s
            see how that works.</p>
        <p>好吧，正如你们所记得的，使用常规极小极大值时的游戏搜索本质上是深度优先搜索。在每个级别，它都会在所有子级中选择父级想要的值。因此，在此之后，它会选择 K 和 L
            中的最大值。但这有点操之过急。因为这是深度优先搜索。所以我们最好从顶部开始。我会帮助你们一段时间。</p>
        <p>All right, as you guys remember, the game search when using regular minimax is essentially a depth first
            search. And at each level, it chooses between all of the children whichever value that the parent wants. So
            here after it would choose the maximum of K and L, for instance. But that’s getting ahead of ourselves.
            Because it’s a depth first search. So we best start at the top. I’ll help you guys up for a while.</p>
        <p>所以我们要做 A。我们需要 B、C、D 的最大值，深度优先搜索。我们转到 B。我们寻找 E 和 F 的最小值。所以看过 E 之后，我们目前的 E 和 F 的最小值暂时只有 2。所以这将小于或等于 2。好的，然后我们转到
            F，这是一个最大化器。它的子元素是 K 和 L。</p>
        <p>So we’re doing A. We need the maximum of B, C, D, depth first search. We go to B. We’re looking for the
            minimum of E and F. So having looked at E, our current minimum of E and F is just 2 for the moment. So this
            is going to be less than or equal to 2. All right, then we go down to F, which is a maximizer. And its
            children are K and L.</p>
        <p>现在我要开始让你们做一些事情了。你们觉得怎么样？F 处的极小极大值会是多少？F 处的极小极大值会是多少？听众：教授：所以那个级别是最大化器。最大、最小、最大。F 是最大化器。K 和 L
            本身是最小化器。但它们是相当无能的最小化器。因为它们没有选择权。它们只能做 K 或 L。</p>
        <p>So now I’m going to start making you guys do stuff. So what do you think? What is going to be the minimax
            value at F? The minimax value at F, what will that be? AUDIENCE: PROFESSOR: So that level is a maximizer.
            max, min, max. F is a maximizer. K and L themselves are minimizers. But they’re pretty impotent minimizers.
            Because they don’t get to choose. They just have to do K or L.</p>
        <p>所以极小极大值是 3。是的，它想要走的路径是 K。所以我们会说这里的极小极大值是 3。它实际上正好等于 3。所以如果这是 3，这是 2，那么大家，我们知道 B 的值是多少？听众：教授：我听到的是 3 和
            2。是哪一个？听众：2。教授：2，没错。</p>
        <p>So the minimax value is three. And yeah, the path it would like to go is K. So we’ll say that the minimax
            value here is 3. It’s in fact exactly equal to 3. So if this is 3 and this is 2, then everyone, we know that
            the value of B is? AUDIENCE: PROFESSOR: I hear 3 and 2. Which one is it? AUDIENCE: 2. PROFESSOR: 2, that’s
            right.</p>
        <p>所以这里的值是 2。太好了，让我们进入这个分支。所以 C 将是 G 和 6 中的最小值。但我们还没有看到。因为我们在进行深度优先搜索。它将是 G 的最小值。现在我们需要 M 和 N 的最大值。我们需要最小值。M 是 Q
            和 R 中的最小值。所以让我们换边。</p>
        <p>So the value here is 2. Great, let’s go down into this branch. So C is going to be the minimum of G and 6.
            But we don’t see that yet. Because we’re doing a depth first search. It’s going to be the minimum of G. Now
            we need the maximum of M and N. We’re going to need the minimum. M is the minimum of Q and R. So let’s
            switch sides.</p>
        <p>Q 和 R 的最小值是多少？ 听众：1。 教授：让我们看看。没错，是 1。所以 M 的值为 1。但我要在这里停留一会儿。因为 M 的值为 1。知道了这一点，我们就知道 G 的值为？ 听众：7 教授：没错。7 大于
            1。而且由于 G 是 7，我们现在知道上升到 C，C 的值为？</p>
        <p>The minimum of Q and R is? AUDIENCE: 1. PROFESSOR: Let’s see. That’s right, it’s 1. So M has a value of 1.
            But I’m going to stay over here. Because M has a value of 1. Knowing that, then we know that G has a value
            of? AUDIENCE: 7 PROFESSOR: That’s right. 7 is higher than 1. And since G is a 7, we now know going up to C
            that C has a value of?</p>
        <p>听众：6。教授：是的，C 的值为 6。这是 6 和 7 的最小值。所以现在我要往下走。因为我们已经完成了另一棵子树。这是 6。好的，太好了。现在我们要往下走到 D。希望情况不会太糟。这些事情通常不会太糟。因为它们在
            alpha beta 中被大量修剪。所以让我们看看，在 D 中，我们往下走到 I。</p>
        <p>AUDIENCE: 6. PROFESSOR: Yes, C has a value of 6. That’s the minimum 6 and 7. So now I’m going to go back
            down. Because we’ve done one of the other sub trees. This is a 6. All right, great. Now we’re going to go
            down to D. Hopefully it won’t be too bad. These things usually aren’t terrible. Because they’re made to be
            pruned a lot in alpha beta. So let’s see, in D, we go down to I.</p>
        <p>这只是 1。我们去到 J。我们看看，J 的极小极大值是多少？ 听众：是 20。 教授：没错。20 是 20 和 2 中的最大值。太好了，那么 D 的极小极大值是多少？每个人都说了。1。好吧，那么 A
            处的极小极大值是多少？ 听众：6。 教授：6 是正确的。6 大于 2。它大于 1。我们的值是 6。我们的路径是。每个人。</p>
        <p>And that’s just a 1. We go down to J. And let’s see, what’s the minimax value of J? AUDIENCE: It’s 20.
            PROFESSOR: That’s right. 20 is the maximum of 20 and 2. Great, so what’s the minimax value of D? Everyone
            said it. 1. All right, so what’s the minimax value at A? AUDIENCE: 6. PROFESSOR: 6 is right. 6 is higher
            than 2. It’s higher than 1. Our value is 6. And our path is. everyone.</p>
        <p>A、C、H。就是这样。太好了，大家对极小极大理论都了解吗？我知道通常很多人都了解。但也有少数人不了解。所以，如果你是那些想要澄清极小极大理论的人之一，请举手。可能还有其他一些人也想了解。好的。</p>
        <p>A, C, H. That’s it. Great, is everyone good with minimax? I know that usually a lot of people are. There’s
            usually a few people who aren’t. So if you’re one of the people who would like some clarifications on
            minimax, raise your hand. There’s probably a few other people who would like some too. OK.</p>
        <p>听众：当你进行这个极小极大值计算时，无论哪个值没有显示出来，你都会继续沿着树向下走，然后看看你是否在试图找到极小极大值。无论你得到什么值，你都会回到上一个位置？教授：是的。问题是，当你去进行极小极大值计算时。</p>
        <p>AUDIENCE: When you’re doing this minimax, whatever values are not showing, you keep going down the tree and
            then just look at whether you’re trying to find the minimax. And just whatever values you get you go back up
            one? PROFESSOR: Yes. The question was, when you go to do the minimax.</p>
        <p>假设您得到 E 为 2，并且您知道 B 将小于或等于 2，但您还不知道 F。问题是，您是否会沿着树向下查找 F 处的值，然后再向上查找？答案是肯定的。默认情况下，我们使用深度优先搜索。然而，在非 alpha beta
            版本中，只是常规的极小极大值，结果发现您做什么可能并不重要。</p>
        <p>And let’s say you got E was 2, and you know that B is going to be less than or equal to 2, but you don’t know
            F yet. The question is, do you go down the tree, find the value at F, and then go back up? The answer is
            yes. By default, we use a depth first search. However, in non alpha beta version, just regular minimax, it
            turns out it probably doesn’t matter what you do.</p>
        <p>我建议进行深度优先搜索，让自己进入 alpha beta 的思维模式。因为顺序在 alpha beta
            中非常非常重要。但在这里，我不知道，你可以做一些奇怪的自下而上的搜索。无论你想要什么，它都会给你正确的答案，除非它询问它们评估的顺序。但这里有一个提示。它们评估的顺序是深度优先搜索顺序。</p>
        <p>I suggested doing a depth first search to get yourself in the mindset of alpha beta. Because order is very,
            very important in alpha beta. But here, I don’t know, you could do some weird bottom up search. Whatever you
            want, it’s going to give you the right answer unless it asks what order they evaluated. But here’s a hint.
            The order they’re evaluated in is depth first search order.</p>
        <p>因此，无需执行任何操作，E、K、L、Q、R、N、H、I、O、P 就是此树中开始求值的顺序。听众：教授：所以问题是，像 M 和 G
            这样的节点我们不必在其旁边放置值。从技术上讲，如果我们非常正式地执行此操作，并且我们记不住，而且我也不在上面，我们会在那里放置 1。</p>
        <p>So without even doing anything, E, K, L, Q, R, N, H, I, O, P are the order of starting evaluation in this
            tree. AUDIENCE: PROFESSOR: So the question is, nodes like M and G we don’t have to put values next to.
            Technically, if we were doing this very formally, and we couldn’t remember, and I wasn’t up there among the
            people, we would put 1 there.</p>
        <p>所以在 M 处，我们会放 1。但人们记住了。所以我们没有这么做。但在 G 处，我们会放 7。所以如果我们非常正式地写出来，我们会有一个 1 和一个 7。在 D 处，我们会有一个 1。然后在 A 处，我们会放
            6。这就是答案。</p>
        <p>So at M, we would put a 1. But people remembered that. So we didn’t do it. But then at G, we would put a 7.
            So if we were writing it out very formally, we would have a 1 and a 7. And at this D, we would have a 1. And
            then at the A, we would put a 6. And then that’s the answer.</p>
        <p>此外，我们甚至在过程中加入了小于或大于等元素。但是，我相信我们的 alpha beta 搜索肯定会满足每个人的配额，即在游戏树的节点旁边严谨地放置大量数字。因此，一旦你完成了 alpha
            beta，如果你能正确地完成它，你就会想，哦，极小化，哦，那些日子真是太美好了。它会很容易。因为 alpha beta 稍微复杂一些。</p>
        <p>Also, we’ve even put things like less than or greater than part way along the way. However, I believe that
            our alpha beta search is going to definitely fulfill everyone’s quota of pedantically putting lots of
            numbers next to nodes on a game tree. And so once you’ve done alpha beta, if you can do it correctly, you’ll
            think, oh, minimax, oh, those were the days. It’s going to be easy. Because alpha beta is a little bit more
            complicated.</p>
        <p>这里有很多东西会让人困惑。不过，对于 alpha beta，我暂时会删除一些数字。它们仍然正确。但我们的做法略有不同。那么 alpha beta 和 beta 会给这个公式增加什么呢？</p>
        <p>There’s a lot of things that trip people up here. For alpha beta, however, I will erase some of these numbers
            for the moment. They’re still right. But we do it a little differently. So what do alpha beta and beta add
            to this formula?</p>
        <h2 id="alphabeta">阿尔法贝塔</h2>
        <h2>AlphaBeta</h2>
        <p>好吧，这完全是一个成功的公式，但事实并非如此。因为它耗时太长。但这是一个非常好的公式。U 是最大化器。</p>
        <p>Well, this is all sort of a winning formula, except for it’s not. Because it takes too long. But it’s a very
            nice formula. U is the maximizer, say.</p>
        <p>我会试着思考，如果我这样做，他会怎么做？如果他那样做，我又会怎么做？如果我那样做，他会怎么做，等等，等等，一直到最后。有了 alpha 和 beta，我们添加了我喜欢称之为核选项的东西。我喜欢在这场最大化最小化游戏中。
        </p>
        <p>I would try to think, if I do this, what’s he going to do? And then if he does that, what am I going to do?
            And then what is he going to do if I do that, et cetera, et cetera, all the way to the bottom. With alpha
            and beta, we add in what I like to call nuclear options. I’d like in this game of maximizer minimizer.</p>
        <p>你可以把它想象成冷战或伯罗奔尼撒战争，只不过伯罗奔尼撒战争没有核武器，所以很可能是冷战。而在冷战中，或者在任何情况下，你面对的对手……实际上，这对冷战来说并不适用。</p>
        <p>You can think of it as like the Cold War or the Peloponnesian War, except the Peloponnesian War didn’t have
            nukes, so probably the Cold War. And in the Cold War, or any situation where you’re up against an adversary
            who. actually, this doesn’t really work as well for the Cold War.</p>
        <p>但在任何情况下，如果你面对的对手的唯一目标是摧毁你，你总是想知道，如果他们按下按钮，从古巴发射核弹，或者派出战斗机飞行员，或者发生其他事情，你能做的最好的事情是什么。所以阿尔法和贝塔的概念是，它们代表着故障保险，也就是最坏的情况。
        </p>
        <p>But in any situation where you’re up against an adversary whose only goal in life is to destroy you, you
            always want to find out what the best thing you can possibly do is if they hit that button and send nukes in
            from Cuba, or if they send fighter pilots, or whatever is going on. So the idea of alpha and beta is that
            they are numbers that represent the fail safe, the worst case.</p>
        <p>因为在冷战中，发射核弹显然不是一个好计划。但据推测，发射核弹总比被攻击和杀死要好。所以阿尔法和贝塔代表了你愿意接受的最坏结果。因为现在，你知道你一定能够将冲突推向那个地步或更好。所以阿尔法是最大化者的核选项，是故障保护。核选项。
        </p>
        <p>Because obviously in the Cold War, sending nukes was not a good plan. But presumably, us sending nukes would
            be better than just being attacked and killed. So the alpha and beta represent the worst possible outcome
            you’d be willing to accept for your side. Because right now, you know you’re guaranteed to be able to force
            the conflict to that point or better. So the alpha is the nuclear option, the fail safe, of the maximizer.
            Nuclear options.</p>
        <p>Alpha 是最大化者的核心选项。而 beta
            是最小化者的核心选项。所以我们问自己。那些在课堂上专心听讲或写下东西的人已经知道答案了。我们可以设置什么来开始？在我们探索树并找到任何东西之前，我们将设置什么作为我们的核心选项，作为我们的某种故障保护？我们总是可以依靠这个数字。
        </p>
        <p>Alpha is maximizer’s nuclear option. And beta is the minimizer’s nuclear option. So we ask ourselves. and
            people who were paying attention at lecture or wrote stuff down know the answer already. what could we
            possibly set to start off? Before we explore the tree and find anything, what will we set as our nuclear
            option, as our sort of fail safe? We could always fall back on this number.</p>
        <p>所以你可以设置
            0。你可以尝试为最大化器设置一些较低的数字。因为如果你为最大化器设置一个较高的数字作为它的故障保护，它会非常傲慢，只是说，哦，我不会走这条路。我已经有一个比所有这些路径都好的故障保护。如果你设置，比如，100，你就没有树了。
        </p>
        <p>So you could set 0. You could try to set some low number for the maximizer. Because if you set a high number
            for the maximizer as its fail safe, it’s going to be really snooty and just say, oh, I won’t take this path.
            I already have a fail safe that’s better than all these paths. If you set, like, 100, you have no tree.</p>
        <p>在 6034 中，我们的默认设置通常是将 alpha 设置为负无穷大，或者如果您在实验室中这样做的话，则将 alpha 设置为某个非常大的负数。因此，如果我们将负无穷大设置为 alpha
            的默认值，那么该负无穷大基本上就是最大化者输了。因此最大化者会想，天哪，如果我不看这棵博弈树，我就会自动输掉。他愿意选择可能出现的第一条路径。</p>
        <p>Our default usually, in 6034, is to set negative infinity for alpha or negative some very large number if
            you’re doing it in your lab. So if we set negative infinity as a default for alpha, that negative infinity
            is basically maximizer loses. So the maximizer goes in thinking, oh my god, if I don’t look at this game
            tree, I automatically lose. He’s willing to take the first path possibly presented.</p>
        <p>这就是为什么负无穷是 alpha 的良好默认值。有人知道 beta
            的良好默认值是什么吗，或者只是记得吗？正无穷，没错。因为最小化器进来了，她会说，哦，该死，如果我不看这个节点，最大化器就会自动获胜。这确保了最大化器和最小化器都愿意每次都查看它们看到的第一条路径。因为看看这棵树。
        </p>
        <p>And that’s why that negative infinity is a good default for alpha. Anyone have a good idea what a good
            default for beta is, or just remember? Positive infinity, that’s right. Because the minimizer comes in, and
            she’s like, oh crap, the maximizer automatically wins if I don’t look at this node here. That makes sure the
            maximizer and the minimizer both are willing to look at the first path they see every time. Because look on
            this tree.</p>
        <p>如果 10 是 alpha，最大化者会直接拒绝除 P 之外的所有东西。然后我们就没有树了。最大化者会输，因为他会觉得，嗯，这个测试游戏很有趣。但是，我还有另一个选择。pft，然后你把桌子扔过去。对我来说这是
            10，因为你必须收拾残局。我不拥有这套。我不知道。</p>
        <p>If 10 was alpha, the maximizer would just reject out of hand everything except for P. And then we wouldn’t
            have a tree. The maximizer would lose, because he would be like, hmm, this test game is very interesting.
            However, I have another option. pft, and then you throw over the table. That’s 10 for me, because you have
            to pick up the pieces. I don’t own this set. I don’t know.</p>
        <p>这就是为什么我们将负无穷和正无穷设置为 alpha 和 beta 的默认值。那么 alpha 和 beta 如何传​​播？它们有什么作用？alpha 和 beta
            的主要目的是，正如我们所说，alpha。假设我们有一些值图表。从负无穷开始的 Alpha 是最大化者愿意接受的最坏结果。因为他们知道他们可以得到那么多甚至更好的结果。</p>
        <p>So that is why we set negative infinity and positive infinity as the default for alpha and beta. So how do
            alpha and beta propagate? And what do they do? The main purpose of alpha and beta is that, as we said,
            alpha. let’s say we have some chart of values. Alpha, which starts at negative infinity, is the worst that
            the maximizer is willing to accept. Because they know they can get that much or better.</p>
        <p>一开始，这是你能得到的最糟糕的结果。所以这不是问题。无穷大是最小化器愿意接受的最大值。这就是 beta。然而，随着你的前进，最小化器看到，哦，看看那个。我可以保证最大化器最多得到 100。哈哈，beta 现在是
            100。最大化器说，哦，是吗？</p>
        <p>It starts out, that’s the worst thing you can have. So it’s not a problem. Infinity is the highest that the
            minimizer is willing to accept. That’s beta. As you go along, though, the minimizer sees, oh, look at that.
            I can guarantee that at best the maximizer gets 100. Haha, beta is now 100. The maximizer says, oh yeah?</p>
        <p>好吧，我可以保证你能让我达到的最低值是 0。所以它将会是 0。并且这个数字会一直持续到 6。注意，不是按比例绘制的。也许在 6 的时候，最大化者会说，哈哈，你不能让我低于 6。最小化者会说，啊哈，你不能让我高于
            6。然后 6 就是答案。</p>
        <p>Well I can guarantee that the lowest you can get me to go to is 0. So it’s going to be 0. And this keeps
            going on until maybe at 6. note, not drawn to scale. Maybe at 6, the maximizer said, haha, you can’t make me
            go lower than 6. And the minimizer says, aha, you can’t make me go higher than 6. And then 6 is the answer.
        </p>
        <p>如果你遇到过 Beta 低于 Alpha 或 Alpha 低于 Beta 的情况，那么你就说，算了吧。我甚至不会再看剩下的东西了。我现在要修剪一下，去别的地方，那里没有这么没意义。因为如果 Alpha 高于
            Beta，这就意味着最大化者会说，哦，天哪，看看这个，最小化者。</p>
        <p>If you ever get to a point where beta gets lower than alpha, or alpha gets lower than beta, then you just
            say, screw this. I’m not even going to look at the remaining stuff. I’m going to just prune now and go
            somewhere else that’s less pointless than this. Because if the alpha gets higher than the beta, what that’s
            saying is the maximizer says, oh man, look at this, minimizer.</p>
        <p>你能让我达到的最低值是 50。最小化器说，这很奇怪。因为你能让我达到的最高值是
            40。所以那里通常有些不对劲。这通常意味着他们两个中的一个根本不想探索那个分支。所以你在那个时候修剪。好吧，既然这就是我们要找的，我们如何在整个树中移动 alpha 和 beta？</p>
        <p>The lowest you can make me go is, say, 50. And the minimizer says, that’s strange. Because the highest that
            you can make me go is 40. So something’s generally amiss there. It usually means that one of the two of them
            doesn’t even want to be exploring that branch at all. So you prune at that point. All right, so given that’s
            what we’re looking for, how do we move the alphas and betas throughout the tree?</p>
        <p>有几种不同的绘制方法。其中一些我认为非常复杂。可能在复习和教程中你会看到一种更复杂且数字更多的方法。</p>
        <p>There’s a few different ways to draw them. And some of them I consider to be very busy. Probably in
            recitation and tutorial you will see a way that’s busier and has more numbers.</p>
        <h2 id="snowwhite-principle">白雪公主原则</h2>
        <h2>SnowWhite Principle</h2>
        <p>从技术上讲，每个节点都有 alpha 和 beta。但是，如果节点是最大化者，则关注 alpha；如果节点是最小化者，则关注 beta。</p>
        <p>Technically, every node has both an alpha and a beta. However, the one that node is paying attention to is
            the alpha, if it’s a maximizer, and the beta if it’s a minimizer.</p>
        <p>因此，出于我的目的，我通常只为最大化器绘制 alpha，为最小化器绘制 beta。这种情况很少见，但确实会发生，他们有时会问你，这个最大化器节点的 beta
            是多少？所以知道它是如何得出的很好。但我认为把它写出来会浪费你的时间。这是我的观点。我们拭目以待吧。</p>
        <p>So I generally, for my purposes, only draw the alpha out for the maximizer and only draw the beta out for the
            minimizer. Very rarely, but it happens, they’ll sometimes ask you, well, what’s the beta of this node, which
            is a maximizer node? So it’s good to know how it’s derived. But I think that it wastes your time to write it
            out. That’s my opinion. We’ll see how it goes.</p>
        <p>所以，阿尔法和贝塔的运作方式就是白雪公主的原则。那么，每个人都知道白雪公主的故事吗？有一位美丽的公主。有一位邪恶的继母。镜子镜子，谁是她们中最美丽的，发现那是继女。就像在现实世界中一样，在《白雪公主》中，继女白雪公主拥有父母的美貌。她继承了这些。
        </p>
        <p>So the way that it works, the way that alpha and beta works, is the Snow White principal. So does everyone
            know the story of Snow White? So there’s a beautiful princess. There’s an evil queen stepmother. Mirror
            mirror on the wall, who’s the fairest of them all, finds out that it’s the stepdaughter. So much like in the
            real world, in Snow White, the stepdaughter, Snow White, had the beauty of her parents. She inherited those.
        </p>
        <p>然而，就像在现实世界中一样，继母可能有更好的计划。她雇佣了一个猎人来猎杀白雪公主，掏出白雪公主的心脏，喂给她，这样她就能获得白雪公主的美貌。有多少人知道这个故事的这个版本？有几个人。这是故事的原始版本。迪士尼没有把它放进去。
        </p>
        <p>However, much like in the real world, maybe or perhaps not, the stepmother had an even better plan. She hired
            a hunter to sort of hunt Snow White, pull out Snow White’s heart, and feed it to her so that she could gain
            Snow White’s beauty for herself. How many people knew that version of the story? A few people. That’s the
            original version of the story. Disney didn’t put that in.</p>
        <p>然后猎人拿来一颗鹿的心脏，我认为在迪士尼中猎人确实任意杀死了一只鹿，但没有解释他这样做的原因。所以在 alpha beta 中就是这样。我的意思是你从继承父母的 alpha 和 beta
            开始。但是如果你在孩子中看到你喜欢的东西，你就把它占为己有。白雪公主原则。</p>
        <p>The hunter then brought the heart of a deer, which I think in Disney the hunter did kill a deer arbitrarily,
            but it was not explained that’s why he was doing it. So in alpha beta, it’s just like that. By which I mean
            you start by inheriting the alpha and beta of your parents. But if you see something that you like amongst
            your children, you take it for yourself. the Snow White principle.</p>
        <p>让我们看看情况如何。好吧，我告诉过你们默认的 alpha 是。听众：负无穷。教授：负无穷。所以这里的 alpha 是负无穷。我告诉过你们默认的 beta 是正无穷。我们在这里进行深度优先搜索。好的，beta
            是无穷大。好的，所以我们来到了 E。现在，我们可以放一个 alpha。</p>
        <p>So let’s see how that goes. Well, I told you guys that the default alpha was. AUDIENCE: Negative infinity.
            PROFESSOR: Negative infinity. So here alpha is negative infinity. And I told you that the default beta was
            positive infinity. We’re doing a depth first search here. All right, beta is infinity. All right, so we come
            here to E. Now, we could put an alpha.</p>
        <p>但我从未将 alpha 或 beta 放在终端节点之一。因为它实际上什么也做不了。它只有 2。因此，当我们向下移动时，我们会从父节点获取 alpha 和
            beta。但是当我们向上移动到父节点时，如果父节点喜欢它在子节点中看到的东西，它就会选择它。</p>
        <p>But I never put an alpha or a beta for one of the terminal nodes. Because it can’t really do anything. It’s
            just 2. So as we go down, we take the alpha and beta from our parents. But as we go up to a parent, if the
            parent likes what it sees in the child, it takes it instead.</p>
        <p>所以我问大家一个问题，最小化器是更喜欢从子节点看到的这个 2 还是它自己的无穷大作为 beta？ 听众：2 教授：它喜欢 2。完全正确。所以是 2。好的，太好了，现在我们来看看 F。F 的 alpha
            是多少？谁说是负无穷大？谁说是 2？没有人。哦，你们真棒。它是负无穷大。</p>
        <p>So I ask you all the question, would the minimizer prefer this 2 that it sees from its child or its own
            infinity for a beta? AUDIENCE: 2 PROFESSOR: It likes the 2. That’s absolutely right. So 2. All right, great,
            so now we go down to F. What is F’s alpha? Who says negative infinity? Who says 2? No one. oh, you guys are
            good. It’s negative infinity.</p>
        <p>从技术上讲，它的 beta 值也是 2。但我们忽略了 beta。而 alpha
            值则是从父代向下演化而来的。负无穷。这就是我之前称之为祖父子句的原因。因为你经常会查看你的祖父母来了解你的默认数字是多少。所以我们得到了一个负无穷的 alpha。然后我们继续向下到 K。这是一个静态评估。</p>
        <p>Technically, it also will have a beta of 2. But we’re ignoring the beta. And the alphas that have been
            progressing downward from the parents. negative infinity. That’s why I called it the grandfather clause
            before. Because you would often look up to your grandparent to see what your default number is. So we get an
            alpha of negative infinity. We then go down to the K. It’s a static evaluation.</p>
        <p>现在我要开始逐一点名。希望大家注意群众，他们总是正确的。好吧，我们转到 K。我们看到 3。F 是最大化节点。那么 F 现在做什么？听众：将其 alpha 切换为 3。教授：是的，将其 alpha 切换为
            3，太棒了。好吧，这已经相当不错了。它将 alpha 切换为 3。它非常高兴。</p>
        <p>And now I’m going to start calling on people individually. So hopefully people paid attention to the mob, who
            were always correct. All right, so we go down to K. And we see a 3. F is a maximizer node. So what does F do
            now? AUDIENCE: Switches its alpha to 3. PROFESSOR: Yes, switches its alpha to 3, great. All right, so that’s
            already quite good. It switches alpha to 3. It’s very happy.</p>
        <p>这里是 3。这是一个不错的值。那么它在下一个节点 L 处做什么呢？它转到 K，然后回到 F。深度优先搜索，下一个节点是 L，对吗？听众：教授：好吧，从技术上讲，如果 F 更喜欢 0 而不是 3，它可以取 L 的值
            0。但它是一个最大化器。那么它想取这个值吗？听众：教授：好的，从技术上讲这是正确的。但是我很抱歉。</p>
        <p>It’s got a 3 here. That’s a nice value. So what does it do at L, the next node? It’s gone to K, went back up
            to F. Depth first search, the next one would be L, right? AUDIENCE: PROFESSOR: Well, technically F could
            take L’s value of 0 if it liked it better than 3. But it’s a maximizer. So does it want to take that?
            AUDIENCE: PROFESSOR: OK, that technically would be correct. But I’m sorry.</p>
        <p>我给你出了一个难题。事实上，我们根本不看 L。大家看得出来吗？我来解释一下。F 处的 alpha 值已达到 3。但 B 处的 beta 值为 2。因此 B 向下看并说，等一下。如果我下降到
            F，即我敌人的核选项，那么对我的敌人来说，情况已经是最糟糕的了。对我来说，最好的情况是 3。F 正在大肆宣扬这一点。</p>
        <p>I burdened you with a trick question. In fact, we don’t look at L at all. Does everyone see that? I’ll
            explain. The alpha at F has reached 3. But the beta at B is 2. So B looks down and says, wait a minute. If I
            go down to F, my enemy’s nuclear option, my enemy is the worst it can be for. the best it can be for me is
            3. F is trumpeting it around.</p>
        <p>我本来想吃掉他的心脏，或者其他什么，但我不想这么做。但结果会是 3。结果会是 3 或更高，在 F 那里。我绝对不想那样。我已经有自己的默认逃跑计划了。那就是 2。这比那个可怕的 F 的结果要好。所以管他呢。我们从不看
            L。每个人都明白吗？</p>
        <p>I was thinking of eating his heart, or whatever, but I didn’t want to. But it’s going to be 3. It’s going to
            be 3 or higher down there at F. There’s no way I want that. I already have my own default escape plan. And
            that’s 2. That’s going to be better than whatever comes out of that horrible F. So screw it. And we never
            look at L. Does everyone get that?</p>
        <p>这是 alpha beta 修剪的主要原则。如果您看到 alpha 高于其上方的 beta。正如我所说，如果 alpha 高于 beta。或者如果您看到 beta，比如这里有一个 beta，并且它低于其上方的
            alpha，则修剪它。停止这样做。问题是，谁修剪？谁决定您不看 L？</p>
        <p>That is the main principle of alpha beta pruning. If you see an alpha that’s higher than the beta above it.
            as I said, if alpha goes up above the beta. or if you see a beta, like if there’s a beta down here, and it’s
            lower than the alpha above it, prune it. Stop doing that. And the question is, who prunes? Who decides that
            you don’t look at L?</p>
        <p>不想看 L 的人总是比 L 高出至少两级。所以在这里，B 说，嗯，我不想看 L。因为 F 对我来说已经太糟糕了，简直难以置信。如果这是 100，那可能是 100。即使它更低，我仍然会得到 3。</p>
        <p>The person who is thinking not to look at L is always up higher by at least two levels. So up here, B is
            saying, hmm, I don’t want to look at L. Because F is already so terrible for me that it’s just beyond
            belief. If this is 100, it might be 100. Even if it’s lower, I’m still going to get a three.</p>
        <p>我写了一个健全性检查，以防万一你不确定是否可以跳过它。因为在很多这样的测试中，我们会问你，你要评估哪些，你要跳过哪些，对吗？或者我们只是说，你要评估哪些，而你不会写下你跳过的那些。这是我的健全性测试，看看你是否可以跳过它。
        </p>
        <p>There’s a sanity check that I’ve written that I sort of came up with just in case you’re not sure that you
            can skip it. Because on a lot of these tests, we ask you, which one’s do you evaluate, which ones do you
            skip, right? Or we just say, which ones do you evaluate, and you don’t write the ones that you skip. Here’s
            my sanity test to see if you can skip it.</p>
        <p>问问自己，如果我要跳过的那个节点包含负无穷或某个任意小的数字，负无穷是最小化器获胜，它会改变什么吗？现在我已经回答了这个问题，如果它包含正无穷，它会改变什么吗？如果两次答案都是否，那么你修剪它肯定是正确的。所以看看那个
            0。如果它是负无穷，最小化器获胜，会发生什么？</p>
        <p>Ask yourself, if that node that I’m about to skip contained a negative infinity or some arbitrarily small
            number, negative infinity being the minimizer wins, would it change anything? Now that I’ve answered that,
            if it contained a positive infinity, would it change anything? If the answer is no both times, then you’re
            definitely correct in pruning it. So look at that 0. If it was a negative infinity, minimizer wins, what
            would happen?</p>
        <p>最大化者会说，我不会碰 10 英尺长的杆子，选择 3。最小化者会说，算了吧，我要 E。假设它是正无穷大。最大化者会说，我赢了。最小化者会说，是的，如果我是个白痴，那就去 F，然后去 E 取
            2。所以无论那里是什么，最小化者都会去 E。</p>
        <p>The maximizer would say, I’m not touching that was a 10 foot pole, choosing 3. The minimizer would say, screw
            that, I’ll take E. Let’s say it was a positive infinity. The maximizer would say, eureka, holy grain, I win.
            The minimizer would say, yeah, if I’m a moron, and go down to F, and then would go to E and take 2. So no
            matter what was there, the minimizer would go to E.</p>
        <p>你可能会说，好吧，如果它正好是 2，那会怎么样？但最大化者仍然会选择 K。最小化者会选择 E。所以没有理由往下走。我们现在可以把它剪掉。大家同意吗？大家明白我在说什么吗？太好了，我们现在完成了这个分支。因为 beta
            是 2。所以现在我们到了老爷爷 A。</p>
        <p>And you could say, well, what if it was exactly 2? But still the maximizer would choose K. The minimizer
            would go to E. So there’s no reason to go down there. We can just prune it off right now. Does everyone
            agree, everyone see what I’m talking about here? Great, so we’re now done with this branch. Because beta is
            2. So now we’re up at old grandpappy A.</p>
        <h2 id="old-grandpappy-a">老爷爷A</h2>
        <h2>Old Grandpappy A</h2>
        <p>并且他的 alpha 值为负无穷。</p>
        <p>And he has an alpha of negative infinity.</p>
        <p>各位，他会怎么做？他会选择 2。对他来说，这比负无穷要好。虽然不是很好。但肯定任何东西都比自动损失要好。好的，现在我们的最高节点是 2。所以让我们记住这一点作为我们的 alpha。好的，让我们从这里开始。让我们看看，C
            的值是多少？beta 值是多少？听众：教授：你回到哪一个？回到 G。</p>
        <p>Everyone, what will he do? He’ll take the 2. It’s better than negative infinity for him. It’s not wonderful.
            But certainly anything is better than an automatic loss. All right, now our highest node is a 2. So let’s
            keep that in mind for our alpha. OK, so let’s go over here. Let’s see, so what will be the value at C? What
            will be the beta value? AUDIENCE: PROFESSOR: You go back to which one? To G.</p>
        <p>我还没到 G 点。实际上我才刚刚开始中间分支。所以我要到 C 点。在往下走之前，它的起始 beta 值是多少？听众：无穷大。教授：无穷大，没错。默认值。这比看起来要简单。好吧，是的，beta
            等于无穷大。最好把它删掉。我觉得这会让人感到困惑。很好，所以在 C 点 beta 等于无穷大。</p>
        <p>I’m not at G yet. I’m actually just starting the middle branch. So I’m going to C. And what’s going to be its
            starting beta before I go down? AUDIENCE: Infinity. PROFESSOR: Infinity, that’s right. default value. It’s
            easier than it seemed. All right, so yes, beta is equal to infinity. This should be better erased. I think
            it’s confusing people. Great, OK, so beta is equal to infinity at C.</p>
        <p>现在我们深入到 G 的深度优先搜索。G 处的 alpha 是多少？听众：负无穷。教授：啊，看起来是这样。但是，看看曾祖父 A。它似乎已经变成了 2。所以这次是 2。为什么是 2 而不是负无穷？为什么我们可以让 A
            如此有害，而不是一开始就说，哦，我自动输了？</p>
        <p>Now we go down depth first search to G. What’s going to be our alpha at G? AUDIENCE: Minus infinity.
            PROFESSOR: Ahh, it would seem so. However, take a look up at the great grandpappy A. It seems to have
            changed to 2. So this time it’s 2. Why is it 2 instead of negative infinity? Why can we let A be so noxious
            and not start with saying, oh, I automatically lose?</p>
        <p>嗯，A 知道，无论中间分支的情况有多糟糕，他都可以说，去他妈的中间分支。我要去
            B。这是最小化者做不到的。对于最小化者来说，我们必须从无穷大开始。但最大化者可以。因为他在顶部有选择。每个人都看到了吗？他可以说，哦，我甚至不会去 C。是的，向你展示。</p>
        <p>Well, A knows that no matter how awful things get in that middle branch, he can just say, screw the whole
            middle branch. I’m going to B. That’s something that the minimizer can’t do. And we have to start at
            infinity for the minimizer. But the maximizer can. Because he has the choice at the top. Does everyone see
            that? He can just say, oh, I’m not even going to C. Yeah, shows you.</p>
        <p>我要去 A 并取 2。因此，alpha 在 G 处实际上是 2。好的，太好了，所以我们在 G 处得到的 alpha 是 2。我们要去 M。这是一个最小化器。好的，M 处的 beta 值是多少？听众：教授：或者 beta
            默认值是负无穷还是正无穷？最小化器是什么？听众：正无穷。教授：正无穷，没错。</p>
        <p>I’m going to A and taking the 2. So therefore alpha is actually 2 at G. All right, great, so we’ve got an
            alpha that’s 2 at G. We’re going to go down to M. It’s a minimizer. All right, what’s going to be our beta
            value at M? AUDIENCE: PROFESSOR: Or which is the beta default, minus or positive infinity? What would be the
            minimizer? AUDIENCE: Positive. PROFESSOR: Positive infinity, that’s right.</p>
        <p>M 将是 beta 的正无穷大。同样，它从 C 中获取它。太好了，现在我们得到了一些实际值。所以我们得到了一些实际值。我们在 Q 处。那么当 M 看到 Q 为 1 时，M 处会发生什么？听众：教授：beta
            是什么？它说无穷大。对不起，很难读懂。M 处的 Beta 是无穷大。听众：好的，所以它会最小化，对吗？</p>
        <p>M is going to be a positive infinity for beta. Again, it picks it up from C. Great, now we get to some actual
            values. So we’re at some actual values. We are at Q. So what’s going to happen at M when M sees that Q is 1?
            AUDIENCE: PROFESSOR: What is beta? It says infinity. I’m sorry, it’s hard to read. Beta is infinity at M.
            AUDIENCE: OK, so it’s going to minimize, right?</p>
        <p>所以它会像，好的，教授：没错。所以他们会把 beta 设为 1。因为它看到了 Q。太好了，所以我的下一个问题是，R 会发生什么？听众：教授：非常聪明。你发现了我的陷阱。问题是，它会看 R 吗？答案是，不。它不看
            R。它为什么不看 R？每个人都看到了吗？是的，alpha 现在大于它下面的 beta。</p>
        <p>So it’s going to be like, OK, PROFESSOR: That’s right. So they’re going to put beta to 1. Because it sees Q.
            Great, so my next question is, what’s going to happen at R? AUDIENCE: PROFESSOR: Very smart. You’ve detected
            my trap. The question is, does it look at R? The answer is, no. It doesn’t look at R. Why doesn’t it look at
            R? Does everyone see? Yeah, alpha is now greater than the beta below it.</p>
        <p>Beta 已经低于 alpha。这和我之前讨论的一样，当时我们发现这里的 alpha 是 2。最大化器说，等一下。最大化器 G 说，如果我选择 M，我能得到的最好结果是 1。因为如果这是负无穷大，最小化器就会选择它。
        </p>
        <p>Beta has gotten lower than alpha. This is the same thing I was talking about before, when we figured out that
            the alpha here is 2. The maximizer says, wait a minute. The maximizer G says, if I go to M, the best I’m
            getting out of this is 1. Because if this is negative infinity, the minimizer will choose it.</p>
        <p>如果这是正无穷大，他会选择 1。我能得到的最好结果就是 1。如果是这样，我还不如直接去 B，甚至不去 C。所以我不会去 M。我可能会去 N。也许 N 更好。每个人都明白了吗？太好了，那么假设最大化器确实去了
            N。那么这个 alpha 会发生什么？</p>
        <p>If this is positive infinity, he’ll choose 1. The best I’m going to get out of here is 1. If that’s the case,
            I might as well have just gone to B and not even gone to C. So I’m not going to go to M. I’ll go to N,
            maybe. Maybe N is better. Does everyone see that? Great, so let’s say that the maximizer does go to N. So
            what’s going to happen with this alpha?</p>
        <p>听众：教授：没错，7.7 比 2 好。而且最大化器可以控制到 7，至少如果它达到 G 的话。好了，现在最小化器在 C。这次我们将处理所有人。最小化器在 C，看到 7，最小化器会做什么？有人吗？所以它看到了 7。它对它的
            beta 做了什么？它取 7。无论如何，比无穷大要好。</p>
        <p>AUDIENCE: PROFESSOR: That’s right, it’s going to be 7.7 is better than 2. And the maximizer has control to
            get to that seven, at least if it gets to G. All right, now the minimizer at C. we’ll do everyone this time.
            The minimizer at C, seeing that 7, what does the minimizer do? Anyone? So it sees the 7. What does it do to
            its beta? It takes the 7. better than infinity, anyway.</p>
        <p>是的，然后它检查 H。大家再问一遍，H 处会发生什么？它取 6。它小于 7。好的，现在我们回到让人们自己做这件事。好吧，一直回到顶部，当 A 看到 C 出来的是 6 时，它会做什么？观众：改为 6。教授：改为
            6，没错。Alpha 等于 6。太好了。冲刺阶段，大家，冲刺阶段。</p>
        <p>And yeah, then it checks H. And everybody, again, what happens at H? It takes the 6. It’s lower than 7. All
            right, now we’ll go back to having people do it on their own. Well, all the way back to the top, what does A
            do when it sees the 6 coming out of C? AUDIENCE: Changes to 6. PROFESSOR: Changes to 6, that’s right. Alpha
            equals 6. Great. homestretch, people, homestretch.</p>
        <p>因此，最小化器，各位，具有无穷大的 beta。如果我不是静态节点，它的 alpha 将是 6。但它是静态节点。因此它的值只是 1。因此，由于它的值为 1，各位，beta 变为
            1。各位，接下来呢？听众：修剪。教授：修剪，没错。为什么要修剪？嗯，这次是 A 自己可以修剪。</p>
        <p>So the minimizer, everyone, has a beta of infinity. And if I wasn’t a static node, it would have an alpha of
            6. But it is a static node. So it just has a value of 1. So since it has a value of 1, everyone, the beta
            becomes 1. And what next, everyone? AUDIENCE: Prune. PROFESSOR: Prune, that’s right. Why prune? Well, this
            time it’s A himself who can prune.</p>
        <p>A 说，该死，如果我转到 D，我会得到 1 或比 1 更差的结果。我最好趁还有 6
            的时候把它取出来，然后把其余的都修剪掉。每个人都看到了吗？每个人都觉得这样不错吗？如果你一步一步来，结果还不错。我们做到了。我们的问题是，哪些节点是按顺序评估的？我们的答案是，每个人。</p>
        <p>A says, well darn, if I go to D, I’m going to get 1 or something even worse than 1. I might as well take my 6
            while I have it, prune all the rest all the way down. Everyone see that? Everyone cool with that? It’s not
            too bad if you take it one step at a time. We did it. Our question is, which nodes are evaluated in order?
            Our answer is, everyone.</p>
        <p>E、K、Q、N、H、I。好吧，我想不是那么明显。有几个人跟着我。但事实是 E、K、Q、N、H、I。这只是深度优先顺序。我们修剪了其中一些。太好了，这就是 alpha
            beta。在我提出一些关于渐进深化的问题之前，大家对此有什么疑问吗？好吧，我们有很多问题。第一个问题。听众：像 F、B、C 和 D 这样的节点？</p>
        <p>E, K, Q, N, H, I. OK, not so obvious, I guess. A few people followed me. But it is E, K, Q, N, H, I. It’s
            just depth first order. And we pruned some of them away. Great, so that is alpha beta. Any questions about
            that before I give some questions about progressive deepening? All right, we’ve got a bunch. So first
            question. AUDIENCE: nodes like F, B, C, and D?</p>
        <h2 id="static-evaluation">静态评估</h2>
        <h2>Static Evaluation</h2>
        <p>教授：。</p>
        <p>PROFESSOR:.</p>
        <p>问题是，当被问及求值的顺序时，我们是否排除了 F、B、C 和
            D？答案是，我们在这里讨论的是静态求值。静态求值器是一个非常重要和有趣的函数。稍后我将回答一些学生问我的关于静态求值器的问题，并尝试解释它是什么。它基本上就是弹出叶子底部数字的东西。</p>
        <p>The question is, when asked for the order of evaluation, are we excluding F, B, C, and D? The answer is we’re
            talking about here static evaluation. The static evaluator is a very important and interesting function. And
            I’ll get back to something a few students have asked me about the static evaluator later and try to explain
            what it is. It’s basically the thing that pops out those numbers at the bottom of the leaves.</p>
        <p>因此，当我们问静态评估的节点顺序是什么时，我们指的是仅叶子节点。这是个好问题。还有其他问题吗？让我们看看，之前这里有一个。但它不见了。它可能是同一个。问题？听众：所以这是一个类似的问题。当您说静态节点时，这仅指叶子节点？教授：指叶子节点，没错。问题是，静态节点是否意味着叶子节点。
        </p>
        <p>So when we ask, what is the order of nodes that were statically evaluated, we mean leaves only. That’s a good
            question. Any other questions? Let’s see, there was one up here before. But it’s gone. It might have been
            the same one. Question? AUDIENCE: So a similar question. When you say, static nodes, that just means the
            leaf nodes? PROFESSOR: Means the leaf nodes, that’s right. The question is, does static nodes mean the leaf
            nodes.</p>
        <p>答案是肯定的。听众：那么静态评估就是将静态节点的值与某个东西进行比较？教授：静态评估就是获取该数字，即静态节点。让我解释一下。除非其他人对 alpha beta
            有其他问题，否则让我解释一下静态值。因为我正要这样做。有一个关于 alpha beta 的问题。回答完这个问题后，我会再回答你们两个的问题。</p>
        <p>The answer is yes. AUDIENCE: And so static evaluation is when you compare the value of a static node to
            something? PROFESSOR: Static evaluation is when you get that number, the static node. Let me explain. Unless
            someone else has another question about alpha beta, let me explain static values. Because I was about to do
            that. There is a question about alpha beta. I’ll come back to both of yours after I answer this.</p>
        <p>听众：你提到了，我有点困惑。如果你正在查看一个节点，你会看到要么从祖父节点获取值，要么从...获取值。</p>
        <p>AUDIENCE: You were mentioning And I’m a little bit confused. If you’re looking at one node, and you’re seeing
            either grab the value from the grandparent or grab it from the.</p>
        <h2 id="snow-white-principle">白雪公主原则</h2>
        <h2>Snow White Principle</h2>
        <p>教授：所以它总是从头开始。问题是，白雪公主原则是什么？它是如何工作的？每个节点总是从其祖父节点开始获取相同类型的值（alpha 或 beta）。它总是这样开始。现在，你说，为什么是祖父节点？</p>
        <p>PROFESSOR: So it always starts. the question is, what is the Snow White principle? How does it work? Every
            node always starts off with taking the value of the same type, alpha or beta, from its grandparent. It
            always starts that way. Now, you say, why the grandparent?</p>
        <p>它不会从父节点那里获取吗？它确实会。但我不会在所有最小化器级别上绘制
            alpha。因为它们什么都不做。它们只是在那里传递它们。所以所有的值都向下传递，向下，向下，向下，向下。事实上，每个节点都是从它的祖父母开始的，带有父母的值，好吗？但是当节点看到一个子节点时，它就完全完成了评估。它结束了。
        </p>
        <p>Wouldn’t it take it from the parent? It actually does. But I’m not drawing out the alphas at all the
            minimizer levels. Because they don’t do anything. They’re only even there to pass them down. So all of the
            values pass down, down, down, down, down to begin. Every node, in fact, starts off with its grandparents
            with its parents’ values, OK? But then when the node sees a child, it’s completely done evaluating. It’s
            finished.</p>
        <p>它不能处于过程中。假设 C。当 C 看到 G 已完全完成其所有子分支并准备返回值时，或者如果它只是静态评估，那么它会自动完全完成。因为它没有子节点。像 K 为 3 这样的静态值会自动完全完成。</p>
        <p>It can’t be in the process. Let’s say C. When C sees that G is completely done with all of its sub branches
            and is ready to return a value, or if it’s just a static evaluation, then it’s automatically completely
            done. Because it has no children. A static value like K of 3 is automatically completely done.</p>
        <p>它有 3。同样，当我们从 N 回到 G 时，我们知道它的值为 7，那就完全搞定了。它的值肯定是 7。没有其他可能性。听众：那是在查看子节点之后，对吧？教授：是的。所以一旦你处理完了 G 的所有子节点，G
            就会出现并说，猜猜怎么了？猜猜怎么了，伙计们？</p>
        <p>It’s got a 3. Similarly, when we came back to G after going to N, and we knew that the value was 7, that was
            completely done. The value was definitely 7. There was no other possibilities. AUDIENCE: That’s after
            looking at the children, right? PROFESSOR: Yes. So once you’re done with all the children of G, then G comes
            up and says, guess what? Guess what, guys?</p>
        <p>因此从技术上讲，在此之前，当我们查看 Q 时，您会说 G 的 alpha 大于或等于 1。然后我们查看 M。我们会说，它正好等于
            7。我们在这里完成了。然后到了那个时候，当它新鲜成熟并且具有其最高值或最佳值时，父母就可以吃掉它的心脏并获得该值。</p>
        <p>So technically before that, you would have said that G’s alpha is greater than or equal to 1 when we looked
            at Q. And then we looked at M. We’d say, it’s equal exactly to 7. We’re done here. And then at that point,
            when it’s fresh and ripe and has all of its highest value or its best value, that’s when the parent can eat
            its heart and gain that value itself.</p>
        <p>所以这时候 C 会说，例如，哦天哪，我有一个无穷大。我真的更喜欢那个 7。然后它就取了 7。但是后来它看到了 H。它说，哦天哪，那是 6。这比 7 还要好。所以它取了 6。观众：那么 alpha 不应该取 7 吗？
        </p>
        <p>So that’s when C says, for instance, oh man, I have an infinity. I really like that 7 better. And it takes
            the 7. But then it saw H. And it said, oh man, that’s a 6. That’s even better than 7. So it took the 6.
            AUDIENCE: So shouldn’t the alpha take 7 then?</p>
        <h2 id="static-evaluations">静态评估</h2>
        <h2>Static Evaluations</h2>
        <p>教授：所以 alpha 取 6。因为 C 是一个最小化器。</p>
        <p>PROFESSOR: So alpha takes 6. Because C is a minimizer.</p>
        <p>C 从 G 中取出 7，但紧接着 C 看到 H 并取出 6。因为 6 比 7 还要低。然后 alpha 取出 6。因为 6 比 2 高。听众：所以它不会查看分支下方？教授：是的，问题是最大化器无法控制那里。最小化器在
            C 处有控制权。最小化器会确保它尽可能低。</p>
        <p>C took the 7 from G, but then right after that C saw H and took the 6. Because 6 is even lower than 7. And
            then alpha took the 6. Because 6 was higher than 2. AUDIENCE: So it’s not going to look below the branch?
            PROFESSOR: Yeah, the problem is that the maximizer doesn’t have control there. The minimizer has got control
            at C. And the minimizer is going to make sure it’s as low as possible.</p>
        <p>在 A 处的最大化器，他或她唯一的控制权，是能够将任一方向发送到 B 或 C 或 D。然后在那个点，在 C 处，最小化器可以选择我们是否转到 G 或 H。它永远不会选择 G。因为 G 高于
            H。好的，太棒了，还有其他问题吗？好吧，让我们回到静态评估。</p>
        <p>The maximizer at A, his only control, or her only control, is the ability to send either way to B or C or D.
            And then at that point, at C, the minimizer gets to choose if we go to G or H. And it’s never going to
            choose G. Because G is higher than H. All right, awesome, was there another question? All right, let’s go
            back to static evaluations.</p>
        <p>我第一次上这门课的时候，对静态演化有一些奇怪的想法。我听到一些学生问我这个问题。我差点在一次考试中就被问到这个问题，但在最后一刻，它被编辑成了另一个奇怪的问题，即 m 到 b 到 d 减 1 或类似的问题。</p>
        <p>When I first took this class, I had some weird thoughts about static evolutions. I heard some students ask me
            this. I almost got a question about it onto one of the tests, but it was edited to some other weird question
            that was m to the b to the d minus 1 or something like that at the last minute.</p>
        <p>因此，我将向大家提出一个实际问题，该问题原本应该出现在较旧的测试中，如下所示。我的一个学生来找我，说，你知道，当我们进行这种 alpha beta
            剪枝以及所有其他操作时，我们试图假设通过摆脱一些静态评估，我们确实可以节省那么多时间。</p>
        <p>So I’m going to pose you guys the actual question that would have been on one of the older test, which is the
            following. I had a student who came to me and said, you know, when we do this alpha beta pruning, and all
            this other stuff, we’re trying to assume that we’re really saving that much time by getting rid of a few
            static evaluations.</p>
        <p>事实上，当我们进行渐进式深化时，我们总是在计算，我们需要进行多少次静态评估？他说，我看看这些静态评估。那里只有一个 3。进行静态评估不需要花费时间。它在黑板上。进行 alpha beta 需要更长的时间。不进行
            alpha beta 会快得多。所以我试着向那位学生解释。</p>
        <p>In fact, when we do progressive deepening, we’re always just counting, how many static evaluations do we have
            to do? And he said, I look at these static evaluations. And there’s just a 3 there. It takes no time to do
            the static evaluation. It’s on the board. It takes much longer to do the alpha beta. It’s faster by far to
            not do alpha beta. So I then tried to explain to that student.</p>
        <p>我说，好吧，我们需要明确静态评估是什么。你们很容易理解。我们将这些数字放在棋盘上。静态评估。假设您正在玩国际象棋之类的游戏。静态评估需要很长时间。当我在 6170 时，Java 曾经存在过，我们有一个名为 Anti
            Chess 的程序，我使用我的 6034 技能编写了 AI。静态评估器花了很长时间。</p>
        <p>I said, OK, we need to be clear about what static evaluations are. You guys get it easy. We put these numbers
            on the board. A static evaluation. let’s say you’re playing a game like chess. Static evaluation takes a
            long time. When I was in 6170, Java the class that used to exist, we had a program called Anti Chess where I
            used my 6034 skills to write the AI. And the static evaluator took a long time.</p>
        <p>而且我们有时间限制。因此，更快地获得静态评估器是最重要的事情。为什么需要很长时间？好吧，静态评估器是在某个时间点对棋盘位置、游戏状态进行评估。这并不像说“哦，这是答案”那么简单。</p>
        <p>And we were timed. So getting the static evaluator faster, that was the most important thing. Why does it
            take a long time? Well, the static evaluator is an evaluation of the board position, the state of the game,
            at a snapshot of time. And that’s not as easy as just saying, oh, here’s the answer.</p>
        <p>因为在国际象棋中，首先，我不仅要看我有多少棋子，我控制了哪些区域。而且，这是反国际象棋。但这不重要。让我们假设这是常规国际象棋。我还必须看，如果它是在常规国际象棋中。而且我仍然必须在反国际象棋中这样做。如果我的国王被将军了。
        </p>
        <p>Because in chess, first of all, not only did I have to look at how many pieces I had, what areas that I
            controlled. Also. well, it was anti chess. But that’s not withstanding. Let’s pretend it’s regular chess. I
            also had to look, if it was in regular chess. and I still had to do this in anti chess. if my king was in
            check.</p>
        <p>这意味着我必须查看对手的所有动作，可能的举动，看看是否有人可以拿下我的王。因为在常规国际象棋中，将王置于将军位置是违法的。所以你最好不要允许那一步。无论如何，将王置于将军位置对你来说都是负无穷。所以通常需要很长时间才能进行静态评估，至少是好的评估。你想避免它们。
        </p>
        <p>And what that meant is I had to look at all of my opponent’s moves, possible moves, to see if anyone of them
            could take my king. Because in regular chess, it’s illegal to put your king into check. So you better not
            even allow that move. And regardless, getting into checkmate is negative infinity for you. So it takes a
            really long time to do static evaluations, at least good ones, usually. You want to avoid them.</p>
        <p>因为它们不仅仅是页面上的数字。它们是你编写的函数，可以非常仔细地分析游戏状态，并说，我可以启发式地猜测我的值是 pi 或其他数字，然后将其与其他状态进行比较。这对每个人来说都有意义吗？</p>
        <p>Because they’re not just some number on the page. They are some function you wrote that does a very careful
            analysis of the state of the game and says, I’m good to heuristically guess that my value is pi, or some
            other number, and then rates that compared to other states. Does that make sense to everyone?</p>
        <p>因此，对于旧测试中可能出现的假设问题，当有人说“我有一个好主意，你可以进行大量静态评估，而不必进行这么长的 alpha beta
            测试”时，答案是“不要这样做”。静态评估实际上需要很长时间。这是否能解释之前问我什么是静态评估的人，为什么叶节点被称为静态的？</p>
        <p>So the answer to the hypothetical question that might have been on the old test, when the person said, I’ve
            got this great idea where you do tons of static evaluation, and you don’t have to do this long alpha beta,
            is, don’t do that. The static evaluations actually take a long time. Does that clear it up for people who
            asked me before about what is a static evaluation, why are the leaf nodes called static?</p>
        <p>你可能会问，为什么有些是静态的，只是随意的？答案是，当你没有时间进一步扩展时，你只需要停止游戏的那个阶段。也许它变得太复杂了，也许它扩展得太广了，你有一些启发式方法说，这是我现在停止的地方。这是对值的启发式猜测。它有点像搜索树中的启发式值。
        </p>
        <p>And you might ask, why are some of these static just arbitrarily? The answer is, when you’re running out of
            time to expand deeper, and you just need to stop that stage of the game. maybe it’s just getting too hairy,
            maybe it’s spreading out too much, you have some heuristic that says, this is where I stop for now. it’s a
            heuristic guess of the value. It’s kind of like those heuristic values in the search tree.</p>
        <p>这是对你还剩下多少工作才能达到目标的猜测。在这里，你说，好吧，我希望我能更深入。但我没有时间。所以我认为我在这个层面上做得不错。它并不总是正确的。这将引导我们找到关于渐进深化的问题之一的答案。所以我会很快提出渐进深化问题。问题是这样的。
        </p>
        <p>It’s a guess of how much work you have left to get to the goal. Here, you say, well, I wish I could go
            deeper. But I just don’t have the time. So here’s how I think I’m doing at this level. It’s not always
            right. And that’s going to lead us into the answer to one of the questions about progressive deepening. So
            I’ll put up the progressive deepening question really quickly. So the question is this.</p>
        <h2 id="progressive-deepening">逐步深化</h2>
        <h2>Progressive Deepening</h2>
        <p>让我看看，这是一个最大化器。是的。假设我们对只有两层深度的树进行渐进式深化。如果你不记得讲座上的内容，那么渐进式深化简而言之是什么？思路是这样的。在这棵树上，它不起作用。</p>
        <p>Let me see, this is a maximizer. yes. Suppose that we do progressive deepening on the tree that is only two
            levels deep. What is progressive deepening in a nutshell if you don’t remember from the lecture? The idea is
            this. In this tree, it doesn’t work.</p>
        <p>但在实际上分支为 2 到 n 的树中，先处理一些顶层然后再处理底层并不需要花费太多时间。每次处理一个就行。因此，假设我们只处理了 J 层。我们只处理了树的顶部两层。</p>
        <p>But in trees that actually branch like 2 to the n, it doesn’t take that much time to do some of the top
            levels first and then move on to the bottom levels. Just do them one at a time. So let’s say we only did it
            up through J. We only did the top two levels of the tree.</p>
        <p>我们希望重新排序树，以便 alpha beta 可以尽可能地修剪，至少我们希望如此。所以让我们假装我们有一位通灵的天才朋友，他告诉我们当我们上升到两个级别时，静态值。记住，当我们上升到两个级别时，F、G 和 J
            必须获得一个静态值，对吧？因为我们不会下降。我们进行静态评估。</p>
        <p>We’d like to reorder the tree so that alpha beta can prune as much as it possibly can, at least we hope. So
            let’s pretend that we had a psychic awesome genius friend who told us that the static values when we went up
            to two levels. remember, when we go to two levels, F, G, and J have to get a static value, right? Because
            we’re not going down. We do a static evaluation.</p>
        <p>他们得到了完全正确的数字。3、7 和 20。天才，太聪明了。好吧，如果发生这种情况，我们可以用什么最佳方式重新排列这棵树？哦，对了，所以它是 A、B、C、D，值为 2、3.7、6.1、20。</p>
        <p>They get the exact correct numbers. 3,7, and 20. Genius, brilliant. All right, so if that happens, what is
            the best way that we could reorder that tree? Oh yeah, so it’s A, B, C, D with values of 2,3.7,6.1,20.</p>
        <h2 id="reordering">重新排序</h2>
        <h2>Reordering</h2>
        <p>我来画一下。这是未重新排序的树。让我们看看，所以它是 2,3.7,6.1,20。那么重新排序的最佳方法是什么？好吧，首先，有人记得帕特里克在谈到渐进深化时说过什么吗？</p>
        <p>I’ll draw that. This is the non reordered tree. Let’s see, so it’s 2,3.7,6.1,20. So what’s the best way to
            reorder? Well, first of all, does anyone remember what Patrick said when he talked about progressive
            deepening?</p>
        <p>通常没人会这样做，所以不用担心。因为那时你们不会想，哦，我必须在测验中这样做。你只是在想，哦，天哪，我们已经听过 alpha beta
            和所有其他东西了。这只是一个小事实。但它是一个非常重要的事实。现在你知道你必须在测验中这样做。所以你可能会记住它。</p>
        <p>Usually no one does, so don’t worry about it. Because at that time you guys didn’t think, oh, I have to do
            this for the quiz. You were just thinking, oh man, we’ve already heard alpha beta and all this other stuff.
            And this is just a small fact. But it’s a very important fact. And now you know you have to do it for the
            quiz. So you’re probably going to remember it.</p>
        <p>你要做的就是试着猜测，然后说，哪一个会成为赢家？无论我认为哪个会成为赢家，我都会把它放在第一位。为什么会这样呢？好吧，你可能已经注意到了这里一些有趣的事情。</p>
        <p>The way you do it is you try to guess, and you say, which one of these is going to be a winner? Whichever one
            I think is going to be a winner at that level, I put first. Why is that the case? Well, something
            interesting you may have noticed here.</p>
        <p>每当你有获胜者，比如中间节点，或者每当你有当前最适合你的 alpha 的节点时，你就必须探索该区域的很多地方。例如，左节点是我们当前的最佳节点，当时是 2。中间分支是我们当前的最佳节点，当时是
            6。这是总体最佳。我们必须探索大量节点。</p>
        <p>Whenever you have a winner, like the middle node, or whenever you have whatever is the current best for your
            alpha, you sort of have to explore out a lot of that area. Like for instance, the left node was our current
            best at 2. The middle branch was our current best, at that time was 6. It was the total best. We had to
            explore a good number of nodes.</p>
        <p>但在右边，我们只看到，哦，有 1。我们完成了。我们把一切都切断了。换句话说，对于你选择的那个分支，你必须进行大量的探索才能证明它是正确的。而如果它是错误的，你有时只需一个节点就可以说，这是错误的，完成了。</p>
        <p>But on the right, we just saw, oh, there’s 1. We’re done. We cut everything off. In other words, the branch
            that turns out to be the one that you take, you have to do a pretty good amount of exploration to prove that
            it’s the right one. Whereas if it’s the wrong one, you can sometimes with just one node say, this is wrong,
            done.</p>
        <p>因此，如果最终获胜的那个是第一个，那么拒绝所有其他分支就很容易了。人们是否从概念上理解了这一点，即如果你立即获得最佳节点，你就可以很快拒绝所有错误的节点？这就是我们的目标。那么我们如何才能立即“获得正确的”，即最好的节点？好吧，我们是这样做的。
        </p>
        <p>So therefore, if the one that turns out to be the eventual winner is first of all, then it’s really easy to
            reject all the other branches. Do people see that sort of conceptually a little bit, that if you get the
            best node right away, you can just reject all the wrong ones pretty quickly? That’s our goal. So how can we,
            quote, “get the right one,” the best one right away? Well, here’s how we do it.</p>
        <p>假设我们在 B 点。假设我们的启发式方法正确，并且这些猜测非常接近事实，那么最小化器可能会选择哪一个？事实证明它们是完美的，所以这会起作用。那么，如果最小化器必须在 E 和 F
            之间做出选择，我们认为它会选择哪一个？观众：E。教授：E，完美。它会在 G 和 H 之间选择哪一个？观众：H。教授：H。</p>
        <p>Let’s say we’re at B. Which one is the minimizer likely to pick assuming that our heuristic is good and that
            these guesses are pretty much close to the truth? It turns out they’re perfect, so this is going to work. So
            which one will the minimizer pick if it has to choose between E and F, do we think? AUDIENCE: E. PROFESSOR:
            E, perfect. Which one will it pick between G and H? AUDIENCE: H. PROFESSOR: H.</p>
        <p>它会在 I 和 J 之间选择哪一个？听众：I。教授：好的，所以我们说我们认为它会选择 E。我们认为它会选择 H。我们认为它会选择 I。所以首先，我们应该把 E 放在 F 之前，把 H 放在 G 之前，把 I 放在 J
            之前。因为我们认为它会先选择这些。这些可能是我们使错误分支无效的最佳选择。</p>
        <p>Which one will it pick between I and J? AUDIENCE: I. PROFESSOR: OK, so what we’re saying is we think it’s
            going to pick E. We think it’s going to pick H. We think it’s going to pick I. So first of all, we should
            put E before F, H before G, and I before J. Because we think it’s going to pick those first. Those are
            probably our best ones to invalidate a poor branch.</p>
        <p>那么现在，在我们认为会得到的 2、6 和 1 之间，我们认为最大化器会选择哪一个呢？听众：6。教授：6。那么如果它不能选择 6，它的下一个最佳选择是什么？2，然后是
            1。这就是我们的顺序。就这么简单。没有什么比这更简单的了，它可以演化出非常复杂的树，大量的数字，并重新排序这些树。所以 C。</p>
        <p>So now between 2,6, and 1, which is what we think we’re going to get, which one do we think the maximizer is
            going to take? AUDIENCE: 6. PROFESSOR: 6. Then if it couldn’t take 6, what would be its next best choice? 2,
            then 1. That’s just our order. simple as that. It couldn’t be anything easier that evolves really complex
            trees, a huge number of numbers, and reordering those trees. So C.</p>
        <p>你们告诉我 C、B、D。我想你们告诉我的是 C、B、D？是的，这些是最大化器喜欢的。然后你们告诉我最小化器喜欢的是 H，然后是 G。因为 H 比 G 小。你们告诉我 E 先于 F。你们告诉我 I 先于
            J。你们在所有方面都是正确的。我们有 6、7.2、3.1、20。所有最小化器都从最小到最大进行选择。</p>
        <p>You guys told me C, B, D. You told me C, B, D, I think? Yeah, those are the ones the maximizer likes. And
            then the ones the minimizer likes you told me was H, and before G. Because H is smaller than G. You guys
            told me E before F. And you guys told me I before J. And you guys would be correct in all regards. We have
            6,7.2,3.1,20. All the minimizers choose from smallest to highest.</p>
        <p>最大化器从最小化器将采用的那些中从最高到最低选择。如果我们这样做，你可以看到我们可能会节省一些时间。让我们看看有多少时间。假设我们先看 H。好吧，如果我们先看 H，我们实际上仍然必须查看 Q 和 N。但是，我们不必查看
            K。大家明白为什么了吗？</p>
        <p>The maximizer chooses from highest to lowest of the ones that the minimizers will take. And if we did that,
            you can see we would probably save some time. Let’s see how much time. Let’s say we looked at H first. Well,
            if we looked at H first, we would still have actually had to look at Q and N. However, we would not have had
            to look at K. Do people see why?</p>
        <p>如果我们已经知道这个分支是 6，那么只要我们在这里看到 beta 为 2。2 小于 6。我们就可以进行修剪。我们仍然必须在这里查看 I。因为您必须在新子分支中查看至少一件事。而且它实际上只会为我们节省一个节点。哎呀。
        </p>
        <p>If we already knew this branch was 6, as soon as we saw 2 for the beta here. 2 is less than 6. we could have
            pruned. We still would have had to look at I over here. Because you have to look at least one thing in the
            new sub branch. And it actually only would have saved us one node. oops.</p>
        <p>所以总的来说，如果我们采用这种重新排序的小方案，我们需要评估多少个节点？通常情况下，我们需要评估六个节点。E、K、Q、N、H、I。如果我们采用这种渐进式深化方案，我们需要评估多少个节点？我们需要运行静态评估器多少次？当然，静态评估器需要很长时间。有人能猜一下吗？
        </p>
        <p>So it winds up that in total, how many nodes would we have evaluated if we did that little scheme of
            reordering? Well, we normally had to do six. E, K, Q, N, H, I. How many do we evaluate if we do this
            progressive deepening scheme? How many times do we run the static evaluator, which of course you know the
            static evaluator takes a long time? Anyone have a guess?</p>
        <p>我告诉过你我们唯一不评估的是 K。举手。我不会让任何人给出这个。所以我说我们唯一可以节省的是 K。所以我们仍然在这里做 E、Q、N、H 和 I。有两个可能的答案我会接受。所以你有更高的机会猜出来。无论如何？</p>
        <p>I told you the only one we don’t evaluate is K. Raise your hand. I won’t make anyone give this one. So I said
            the only one we save on is K. So we still do E, Q, N, H, and I over here. There’s two possible answers that
            I will accept. So you have a higher chance of guessing it. Anyway?</p>
        <h2 id="possible-answers">可能的答案</h2>
        <h2>Possible Answers</h2>
        <p>大家同意我们之前做过六次吗？</p>
        <p>Does everyone agree that we did six before?</p>
        <p>如果我们没有进行任何渐进深化，我们只会进行 E、K、Q、N、H、I。现在我们不做
            K。好吧，人们说五。好吧，很好。这不是正确答案。但它至少表明你可以减去一个。我们在这里至少做了五个。不过，有两个可能的答案。因为看那边。为了进行渐进深化，我们必须进行那些静态评估，对吧？</p>
        <p>If we didn’t do any progressive deepening, we just did E, K, Q, N, H, I. And now we’re not doing K. OK,
            people are saying five. All right, good. That’s not the right answer. But it at least shows that you can do
            taking away the one. We did at least five over here. There’s two possible answers, though. Because look over
            there. In order to do the progressive deepening, we had to do those static evaluations, right?</p>
        <p>因此，我们要么进行所有这些静态评估，要么进行这五个。E、K、Q、N、H、I。静态评估。因为我们没有进行
            K。或者我们可能已经拯救了自己。因为也许我们很聪明，决定在沿着树向下移动时缓存静态值。这是一个实现细节，在这个测试中，当我们问这个问题时，我们没有说。</p>
        <p>So we either did all those static evaluations and these five. E, K, Q, N, H, I. static evaluations. Because
            we didn’t do the K. Or we might have saved ourselves. Because maybe we were smart and decided to cache the
            static values when we were going down the tree. It’s an implementation detail that on this test when we
            asked that question we didn’t say.</p>
        <p>我所说的缓存是指当我们在这里执行此操作并看到 E 是 2，然后在这里。哦，我们必须在 E 处执行静态值。如果我们聪明的话，我们可能会制作一个小哈希表或类似的东西并放下 2，这样我们就不必在 E
            处进行静态评估。如果发生这种情况，那么我们保存 E、H 和 I，我们少做三个。</p>
        <p>What I mean by cache is when we did it here and saw that E was a 2, and then here. oh, we have to do the
            static value at E. If we were smart, we might have made a little hash table or something and put down 2 so
            we didn’t have to do a static evaluation at E. And if that happened, well, we save E, H, and I, and we do
            three fewer.</p>
        <p>大家看到了吗？但是，这仍然超过六个。</p>
        <p>Does everyone see that? However, that’s still more than six.</p>
        <h2 id="conceptual-riddle">概念谜语</h2>
        <h2>Conceptual Riddle</h2>
        <p>所以它并没有为我们节省时间。所以你可能会说，哦，渐进深化是浪费时间。但事实并非如此。因为这是一棵非常非常小、分支不多的树，它是为了你们可以轻松地进行 alpha beta 和参加测验而制作的，而且它不会很糟糕。</p>
        <p>So it didn’t save us time. So you might say, oh, progressive deepening is a waste of time. But it’s not.
            Because this is a very, very small, not very branchy tree that was made so that you guys could easily do
            alpha beta and take the quiz, and it wouldn’t be bad.</p>
        <p>如果这实际上是在每个级别上分支两倍，那么在底部会有 16 个节点。那么你会想要进行这种渐进式深化。现在我问你一个概念谜题。这其实并不是一个谜题。但我们会看看是否有人愿意回答。再次重申，我不会要求你回答这个问题。</p>
        <p>If this was actually branching even double at each level, it would have, what, 16 nodes down here at the
            bottom. Then you would want to be doing that progressive deepening. So now I ask you a conceptual riddle
            question. It’s not really that much of a riddle. But we’ll see if anyone wants to answer. Again, I won’t
            call on you for this.</p>
        <p>根据这个测试，一个名叫史蒂夫的学生说，好吧，我知道我必须付出代价才能在这里进行渐进式深化。但我们先忽略这一点。因为它在一棵大树中很小，对吧？它不会花那么多钱。让我们忽略渐进式深化的成本，只看我们在这里做了多少。</p>
        <p>According to this test, a student named Steve says, OK, I know I have to pay to do the progressive deepening
            here. But let’s ignore that. Because it’s small in a large tree, right? It’s not going to take that much.
            Let’s ignore the costs of the progressive deepening and only look at how much we do here.</p>
        <p>他说，在最后一层执行 alpha beta
            时，如果我根据渐进深化的最佳结果重新排列节点，我保证修剪效果至少会一样好甚至更好。你同意吗？听众：教授：我可以重复一遍吗？好的，问题是，忽略我们在这里为渐进深化付出的成本。忘掉它吧。</p>
        <p>He says, when it comes to performing the alpha beta on the final level, I’m guaranteed to always prune at
            least as well or better if I rearrange the nodes based on the best result from progressive deepening. Do you
            agree? AUDIENCE: PROFESSOR: Can I repeat it? OK, the question is, ignoring the cost that we pay
            progressively deepening here. just forget about it.</p>
        <p>在最后一步，在最后一次迭代中，问题是，当我根据渐进深化的最佳顺序重新排序时，我是否保证在 alpha beta
            剪枝中至少做得一样好或更好？在这里我们当然做到了。但问题是，史蒂夫能保证吗？答案？听众：教授：你说什么？听众：教授：这就是答案和原因，我们要求解释。我们得到的答案是，这不取决于启发式方法吗？</p>
        <p>At the final step, at the final iteration, the question is, am I guaranteed to do at least as well or better
            in my alpha beta pruning when I reorder based on the best order for progressive deepening? Here certainly we
            did. But the question is, is Steve guaranteed? Answer? AUDIENCE: PROFESSOR: What did you say? AUDIENCE:
            PROFESSOR: That’s the answer and the why, which we asked to explain. The answer we got is, doesn’t that
            depend on the heuristic?</p>
        <p>完全正确。答案是，不，我们不能保证，这取决于启发式方法。所以如果我们能保证，那就是我们的启发式方法就像这个启发式方法一样，是神一样的。如果你的启发式方法无论如何都已经告诉你正确答案，那就不要进行游戏搜索。只需去空棋盘，把所有棋子放在前排，然后在上面运行静态评估器。
        </p>
        <p>Perfectly correct. The answer is, no, we’re not guaranteed, and it depends on the heuristic. So if we were
            guaranteed, that would be our heuristic was godlike, like this heuristic. If your heuristic already tells
            you the correct answer no matter what, don’t do game search. Just go to the empty chess board, put all the
            pieces in the front rows, and run static evaluator on that.</p>
        <p>它会说，哦，看起来这局游戏还没开始，白棋就很蠢，所以黑棋会在 15 轮后获胜。然后你就完了。你不用再搜索了。我们知道我们的启发式方法在某种程度上是有缺陷的。它可能非常有缺陷。</p>
        <p>And it’ll say, oh, it looks like with this game not started that white is stupid, so black will win in 15
            turns. And then you’re done. And you don’t do a search. We know that our heuristic is flawed in some way. It
            could be very flawed.</p>
        <p>如果算法存在严重缺陷，以至于它告诉我们实际结果非常糟糕，即使我们认为最小化器会走向 H，但算法也可能错得很多，走向
            G。这可能会让我们走上一条更糟糕的道路，让我们花费更长的时间。问题？听众：如果算法是启发式算法，您如何缓存这些值，以便以后不必重新计算它们？教授：。</p>
        <p>If it’s flawed so badly that it tells us a very bad result of what’s actually going to happen, even though we
            think the minimizer is going to go to H, maybe it’s wrong by a lot and it goes to G. It could take us up an
            even worse path and make us take longer. Question? AUDIENCE: If it’s the heuristic, how could you cache the
            values so you didn’t have to recalculate them later? PROFESSOR:.</p>
        <p>问题是，如果是启发式方法，您如何缓存这些值，以便以后不必重新计算它们？答案是，如果没有这些奇怪的多级事物，即使它下降到五个级别，我们也会因为某种原因在 E
            处停止，这不会有帮助。您可以缓存它的方式是启发式方法。但它是一致的。我不是说搜索中的一致性。我的意思是它是一种一致的启发式方法。</p>
        <p>The question is, how can you cache the values if it’s a heuristic so you don’t have to recalculate them
            later? The answer is, it wouldn’t help if there weren’t these weird multi level things where we stop at E
            for some reason, even though it goes down to five levels. The way you could cache it is it is a heuristic.
            But it’s consistent. And I don’t mean consistent from search. I mean it’s a consistent heuristic in.</p>
        <p>游戏状态 E 是，假设这是我作为最大化者移出骑士的状态，而最小化者说，你实际上是在进行骑士开局，然后进行了反击。无论我们如何到达 E，或者我们去哪里到达 E，那永远都是状态 E。它总是具有相同的启发式价值。</p>
        <p>The game state E is, let’s say that’s the state where I moved out my knight as the maximizer, and the
            minimizer said, you’re doing the knight opening, really, and then did a counterattack. No matter how we get
            to E, or where we go to get to E, that’s always going to be state E. It’s always going to have the same
            heuristic value.</p>
        <p>这不像某个人到处乱摸，然后从帽子里随机抽出一个数字。我们会根据状态 E 得到一些值，这些值会给我们带来积分。而且每次我们进入状态 E 时，这些值都会相同。这说得通吗？这是一种启发式方法。但无论你如何到达 E，它总是会在
            E 处给出相同的值。但这可能真的很糟糕。</p>
        <p>It’s not like some guy who goes around and just randomly pulls a number out of a hat. We’re going to have
            some value that gives us points based on state E. And it’s going to be the same any time we go to state E.
            Does that make sense? It is a heuristic. But it’s always going to give the same value at E no matter how you
            got to E. But it could be really bad.</p>
        <p>事实上，你可能会考虑一种与正确相反的启发式方法，它总是告诉我们最糟糕的举动，并声称这是最好的。也许这就是最小化程序对我们的计算机所做的启发式方法。在这种情况下，当我们进行渐进式深化和重新排序时，我们可能会得到最糟糕的修剪。我们可能不会。但我们可能会。所以在这种情况下，你没有保证。我希望这能给出一些线索。
        </p>
        <p>In fact, you might consider a heuristic that’s the opposite of correct and always tells us the worst move and
            claims it’s the best. That’s the heuristic that the minimizer program did to our computer, perhaps. In that
            case, when we do progressive deepening and we reorder, we’ll probably get the worst pruning possible. We
            might not. But we may. So in that case, you’re not guaranteed. I hope that’s given a few clues.</p>
        <p>在教程中，你们将会看到一些更有趣的问题，这些问题涉及一些其他细节。我至少计划做去年的有趣游戏问题，它问了很多不同的事情，与这些有点不同。所以下次测验应该很有趣，或者至少很有用。所以周末愉快。不要对测验太紧张。</p>
        <p>In tutorial, you guys are going to see some more interesting problems that go into a few other details. I at
            least plan on doing interesting game problem from last year, which asked a bunch of varied things that are a
            little bit different from these. So it should be a lot of fun, hopefully, or at least useful, to do the next
            quiz. So have a great weekend. Don’t stress out too much about the quiz.</p>
        <h1 id="mega-r4.-neural-nets">Mega-R4. 神经网络</h1>
        <h1>Mega-R4. Neural Nets</h1>
        <p><img
                src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2MBERISGBUYLxoaL2NCOEJjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAABBQEBAAAAAAAAAAAAAAAAAgMEBQYBB//EAEsQAAIBAwMBBQQFCAkDAgUFAAECAwAEEQUSITETIkFRYQZxgZEUMqGx0RYjQlJiksHSFRczU1Ryk+HxJILwQ7IHJTREomNkc3TC/8QAGAEBAQEBAQAAAAAAAAAAAAAAAAECAwT/xAAiEQEBAQACAgMBAAMBAAAAAAAAARECIRIxA0FREyIyYXH/2gAMAwEAAhEDEQA/APP6KKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKKKAooooCiiigKKlrp8rAHcnzP4V1dNmaVY8oCxwM5/Cpoh0Vfx+yGoSkhZrbjzZv5ai6hoF3p4BmeFs/qEn+FTyi4qqK1UHsBqtxBHMlxZBXUMMu2cH/tpz+rvV/8RZfvv/LTz4/pjI0Vrv6u9X/xFj++/wDLR/V1q/8AibH99/5aefH9MZGitd/V1q/+Jsf33/lo/q61f/E2P77/AMtPPj+mMjRWu/q61f8AxNj++/8ALTFz7C6nbPCrz2ZMzbVw7dcE8930p58TGYorQn2O1AHHbWv7zfy10exuon/17X95v5avlDKztFaMexeon/17X99v5aB7F6iWI7e0/fb+WnlDKzlFaX8iNS/v7T99v5a7+Q+p/wB/afvt/LU8oZWZorTfkNqf9/afvt/LQvsNqbDIntP32/lp5QyszRWo/IPVP8RZ/vt/LXfyC1T/ABFn++38tPKGVlqK1P5BapkD6RZ/vt/LXfyA1X/EWX77fy08oZWVorV/kBqv+Isv32/lpu49hdUggklMtq4RS21GYk+7u08oZWYoqc+k3ifXhZfep/CmjZSjrtHzrSI1FTbbTJrm5igRow0jBQSTjJPurQf1e6t/iLL99/5al5SLlZKitX/V9q/99Z/vt/LR/V9q2cfSLP8Aff8AlqefH9MrKUVrP6vdW/xFl++/8tH9Xurf4iy/ff8Alp58TKydFaC59kL61lKXF1ZRgfpGQ4PuGM/ZUIaHMXK/SLcAfpHdg/8A45q7EVlFW7ez8wQsl5aSH9VS+ftWkDQrojO+H5n8KbBV0VZnQ7hRlpYAPMsfwrkeizyttSeAny3H8KbBW0Va/wBAz5ANxbAnw3Hj7K4dCuFZlaa3G04+sfwpsFXRVqNAuC6L9Itu/nB3Hw+FLb2duBHI4urU7BuI3NyPTu08oYp6Kth7P3Bxm4thnzLcf/jR+T9xvkX6Rbdw4zubnx44p5QxU0VbPoFwmz8/bsXOMBm49TxQugXDNj6Rbj1JbH3U8oYqaKs10O4aNXEsPeGcZOfuq6T/AOHurOiuLiywwB5d/wCWpeUgyVFa7+rvV/8AEWP77/y0y3sJqikgz2fH7bfy1PPj+rlZeitRH7CapK+1Z7TpnJdsf+2iT2E1SM4M9n++38tP6cf0ysvRWjk9i9RjXJntPg7fy03+SV//AH1t+834VfKGVQUVaw6BdTR71lgAzjlj+FMXGlvbTCKS4g3HyLHHv4q+UMQaKkxWbyzCNXTk4DHOPuqY2g3CsQ09uMDPLN+FNiLnU72aSMWz7CkZGD2YDdPMVXRuTPGSSdrDFKmumuF3OoDHGSKZU4YH1rnJkdOdl5W8fTe2gw591U/tXH+YDVc2XIQ+a1Xe1CZss+Rrhx/2K0+iNv0ezPnCn3VNzVZ7Ntu0GyP/AOkBVpV5exzNGa7XKiOiiiigKrNZ/ttP/wD5z/7Gq0qq1r+0sD/+4/8A8NVFc+e0bkYz5V0Z9K45/ON76Aa0pwFvIfOlLnec46CkA1S67eX1tPF9DD4Ze9hN1WdmtDmlBqxP9NauvUH4xUoe0WqL1VfjHV8aeTbZoibCdD1Ph61ix7UagOscR/7TWr0udriwhmcAM67iBUssJdTQ48j8jSg4pGeaUDUVx544ypeRV/zHFKW4ib6siH3MKrNe0x9VtY4ElEZV92SM+B/GqA+xt2Pq3UR+BFWSX7Tttg4PQg10txWG/JXVU/s7iP4OR/CnINM1bTZ47q6nJtoWDybZSe6OvHjTx/6bW5BGa40cTjvRo3vAqjX2p0k//cEe9DU221iyu/7CbcB1O04HxrGVdiS1paZDi2i3qchtgyDUoGqa81zT44pES+iMuONhDYP3VUS+1Nwm6OFFJHHaP+Aq+NqbI2Gahajei3iDRyxhg36QJHT0rKj2luW/tRHJ8xUfUtbTULXsHSOMKwYsDnpVnHDVxL7XbVxFAHbxZu6Plz99IHtSG/tYm/7XrMQYuXbs8KP0VpTIy/WUj3iunjGdqZqd0t3ctKgKoxyFPhxiom0eFNS9ogyoIGOTjpSoGmYNkFlK8ZFXEOlxGu4k49KVBdJISpBUgEg5qITMG2MORztK1386u+RY9qnAyBwOKodmu1kURlcLnOR1puOVI5Swy20nB6cUrs5pOzHYgkc52gGhWlBcdnuxkFSooOGSPcuN/OSQTSllhCuSrhsEg7s80kRzRRhihAHU48aWRLNIGEPeCgZ2jn1xUUkTJ2gOw8DGd1KM8Z3gKWXOBk44rjPKA0Lx58wVwR+FOQwzEgIAgHjirJvpPRRuVKd+GSItyjYOGHj1pEdwsYP5vIJzyTmrK5lN3DDGCITEuCwWqueSWPMMrsyuOM+PqKt4cpNsNm5CnmQzF0Xd5bj0FEtykkZjaNVORypPTxqOJDjhiPQGot1M0sv1uQuM+dZwWUdywcbVTHQDFeoJ3UUeQxXkWnqTcxg8gsBivXjXL5PawE1WyHvH31Ymqp25rm2ds9v0knd3gn1fj1oufrVETUbSyuH+kyCMlBgkdeaize0GnNIQk+9uuFUk1Mumw7qMixxplgPfWf1bUDBaHs3wzcA+VUtxcS31w8rksWPy9KS1rO6HbGWUctjngV6OPDGfIm2RZHBmnKIeuMk03dRBZHlZ9wyFGzn7acUFuQOK72ErHKjIUbjgjwrbJdhYWcnfu7x4vJAmT867qMcSRxmKcygDDEjGDSEXeMjp68UtbKWeUJHsJ8F3gZp6EZT3B7q7406iW+xSRJ08x+FLSGGRX2doGVSeceAzQbTSzuggPmg+6mfaRM6c5peiHdY2x/ZqbqFmLu2aInGfHFeW9cm6c9km3+z1pz0Uj7TVzisEPZu4jz2V06e6l/0PqifU1KYD/Ma3ct3U7bqisMLLXY/qanJ8WNLC+0idNQz7+amT9Tttq7isUJ/ahOlyje9R+FLGo+06dRA/wFMGyqs1sd6x/wD7A/8AY1UY1n2kXrZ27fD/AHqRb3mqajJELy2hhSJw+QTk8EcfOmCkvPaJYbyaI2xOxyud/XB91Nj2oj8bZv3qs7rSbGS5kd7dSzMSTk03/QunH/7YfvGuk8cXtDHtTD428nzFWGmanFqXaNHGy7MDvfGmxoWnf4f/API1JtLK2sWYQJ2e8ZPJNL4/R2mjFdwD1FIBX9YUoMPMVhStieKj5UuDhMBTjJ++kBx5ilwHufE/fTQ7k/qmuhv2TQAT05oZ44zh3APl1PyFRXd3eHB+VLBqu1HU1skSQQsQSQC52jpVH+UmoSBpBEnZKcHs+Mfbmr42mtcxCDLsFHqcVC1WSB9NuEcSNGyEMVGMD41n09pWTn6PEW89xzXLn2ikvLeSD6Oo3jGQ1XxrPkq3ktreRltrWPI47ST84ft4+yo91NJcD8/cyMo/RzhR8OlEysjkONp6802ZIVdRIQyMcH0rsybjhTcGVzwakg9eR86Te3tublEtYkWNVAbA6t4mn/oE0lqbldvYJ9Zgenwqb+mdmWZVG5nX4HNdmu7aa1jjKFHVuWVeSPnU2CbTVsuxMQeUNndInOfn0omd444Gu7KDss7k7N8bsenNT39NZJ9oMbdjMojU4IyGcjpUuy0+41GWG0hZCeSW3DCimr5xdXUlxDEqR4H5tCO6PHj51M0u+tbFw6pcCQ4y25RjzGKu9Ic1TRDpcXZgyyzyIRmNe7VMYJ4X7GdGSUdQ1bi49oIBaiSMOGxwMg/HmspcLLe3LTtMssz4yAAvhjz9KzL+qjiKZ893dxgYYHgfGkgSbML9V8Z5A4FKiVyN6FVx0ywFKaB0h7TaCgIXIIPPhW2XHjkUoSvL/V5zmliGWV2PdYk5+uv40LDM5VcLhc476jrj19KdtkEOowxzpyWB2nxHwqWqjgyYZFBxkBvDoaW0T7DJtJA6kHOKttcigOoKLYDtHXe8a+Bql+lxqxUMMHr99TjdmlmXEkwSyY3bOgABkFWipPp1hHPO7rEwONjZA8ulUwmjK7g4xSr7V5oLKC3hfCsG3L58/wDNLv0L680+7lsReCRVRo9zM+SQvnj3VldUnWSZYkkDpGDggEcn/itZrt9PBoOmxxsFe5VVcnpgrz99UM/s7M06JbuJJXBO3cMAD1qcfkuZyq+P3FZbRN2itjgHNOf0VfzXLlLZyeuOnB5B+VWTaFqdrCZJYUCAct2gwPfTltHLHAIx33/SMZ3j5itb+M4RpunXjX1upt2x2i7sEHAzz416S7qgyzBR6nFefI/0W6gafdEu8EkjBwDzipGpaiuo3ryRsTH0Qelc+UvKtTpt+0VkLIwYDyOaqJn2jPQ+GazumajHY3M7SMQexZVHm3GM1Hu4rlCrzrtMg3DewGQfeaz4drprWWme83XAZBjCbhgEUzfLaWioiSoZoQGDx4IY+8UzqMM4Vd1xbvEoLBY5g23/AMxVU5byNdpGdWEJQqqKcnpipJtLsf2cEu7oMKc1VWztG4fxFWYkzGWzVqE9nsk7BlZZF7uwg5+VKezueiwvnp0rkTmW4eeRiXwFBPXgYpyTCqTUDKoFnEEp7FhwQ6kY+yri2tLW1iuGa5EszRFUVY2GCfUiqdCZ52mbvMAEBPoKdkGFJqWas6NJp90Y1Ii4x+sKetrK4jZi8eF2nJ3DyqtT6o5PSu8+Zqo2vs82dOgPkSPtq9xWc9mm/wDlqejmtJXm5+3QnFcxS65isBGKMelKrlEJxRilUUCQo8hSgAOgooqitn/t399JFKuB/wBQ/J60gD1NdJ6CwaSx/OD3UY9TTcjrHIu5wMg4yaoeBpa5PAqM9zbwttklXf8AqAjPx8q4b6Ig5uI0X9VG+80wTGZIziRgG/VHJ+VVs2vJbZRLduGI3OR/7Qf404L60QYWaID/ADCsvqMgkvJmVsqXJGK1xmpavvylgb+1Wc+gwB8hS09prJBhbeRR6AVkxz6fGu7DgnqB1wc1vxibV7rOsw6hbpHEjqVbPe91U8F6Yre4TfjcAAmPrV20SGSYLPN2SeLYzSrrTo2vCllMsiFQ2Sf9qTjPRP1HR1fjcM+VXOn6RfPaDUIIwyxvwme8cdaTForSabua3ETqe7MxADfCrqx1+17D+j0Y2siKIyZMbRjqcjz5pyvS9aoNdgmS3hvHiZUkO3kdD1qqa1MsStvUZ/RyM1o/arUre6soba2k7VIWBZ1HjyDVLNYzw2MV3IEEcpwoDZPyqcb12WdokdpJEyO/QEHGPCre41OE6XJapvDyNwDjH1s+dVpcs6CTcqbcA46/+Zpz6IlxbsRIyuDlQYm5+Nasl9pueiEKmQpKu1y2Mr0B91OTiVG/OqcA7Ac5APlSILEGFpGnRZUOdjcZ+dLllabcwQ7S4bgZxwa1ssZy6UiTIT2eAxGPrgfxpISQyGPYQ46g0jcOD51ISCeaTcsMhDYCnbwRjzqK52E5P1NwA6KwNSNMtPpsrqJxC6jKE4xn1py3068+kr2YQFTnJkGB8qkaVoEEszxTXb9rHyFi44+NY5XI1xm1XS2slsZEdGxEdpbHHWkKXaPaqOQWB4U44zXdTllhv7q2klMoY7dxbccZGMn3CkqwPjWp6ZOMjRsgkUx7zgFxtFLfS7uG4k7S+toZPDMnJGcjGBTtiEvLmOKU7ooG7Rg3QnwH/nlUK9u/peu5+sM7Rmp2JF5cSC6hnnNu0iIql4nBJ2jqeepqlUDknOT0qXfop1BkVQAMA44qZqcNnJFbmyCCU4Uov6XzNNzIub2roVHaL55qe2hapcy7XiyYxgKXHA6/xptNNvo2DPayKo6kjgVax3ctuqRwSGNF4AFW38QvWLbVJdLt21A29vBa4VfNjjGABnPSkQX0OooIjcxwyrwFk7gb49Kha3dXV2Xe4n7SKM7YgD4edQo1jtnDTBWRnBABz3c56/ZTJkbzktLmSa0MsTZWQAoVJ9MURyGOFIkbCoMcGlajLbXl6b9o2Ct1UHIz6/8ANMQ3idoQYUUp4FBSyfTHYuppLoR2xclNwY5PSpIlKgKp2qvQDwqHCsf0u4m2dzeVSMnIHnUpLiFQe0t0K+hI+41FNzTG6aESYZYix68+HHup5ryZuWlc4/aPFQbeNS8jlmaPeRGCcEjPiamK0A7skJKnyc5oIrSi81FZpAGMSYJI+scnGfOphuph3jK3HrUK2iiSa4LFnjWQqhBwXx4mpIaAoVdJMeasM/dQIhvJHv5JgI/7HY3dHeOevypQcg57o+FQ7dB9IlAYrEh5bxbyqWOwZdrGT35FMwR5St1eLIyqAqd4IMbjk+XpT3aHyUDyxUaaIx33Y20paNlDbyAMD1FTBHDt2tNIx8wgH8aoaeN9RubSCGNRKS24ooG4cYz9vNSryxmsl2sqFQOqndipei2kMby3EUzOyrtIKbSv2mmWnZJpWaXudAo5pEZxT3R7qVS4be4kjVkiZlPiATSpLW6QkpA7KPEriqNL7LPmydf1XrUjpWP9k3LwTZ8wa18fKKfSvN8nt0np3FFdrlchw1yu1yqgrldNJNAUGiuVRX3P9u1N5qn1i/vE1eWCA5AxgBc+AqM2rT24w0qzS/qqBtX3nxrtONxNaLKqMscfxqq1q6aHsmCBc5Gf0vn4VTnVr3cW7Y7j44FM3F5PdAdtIXx0yK3OOJaY3nOfnXdxNJAzTjIFGQwNdMZcGT0+6uOwX65C++mt7eBPzoL74wrHJzkZ8KiuFgxOMke6upMUVgOrDH20pZHVgdx+dcD953XjJPSqHbe1lu32xDLfGpUbR2ZXtJlEiMQ23J48qiLdzQxt2czoxwAVYjxpUU8kbqQ/1T49KkuC0l1eElRuwEUDDAjPj99UslyssskjcF2LECrXV7kXMEHahHcdGA5x5GqiSVgrBcZIxwKvlpmVdyLFeez5kt4wssK4Zj3cr4j1qhBfaA3h65p+SZxYrAQQq5IHqadhH0qwEJEYZGBVyMEDnIz41jj/AIt8jSyfmBGfFt1ceQL1YipAKRnb2MfHHKCkzLA9whVFChQWUdN1a1ha6RpNpfRGae4ldkxleBj08ajTafd2UjTJDKtv2hCt1IGeM+VctZ1EgQwptY4OOKS93PFPJ+fMmTxtchce4Gsf5a1sxHDkzSyKSC7k56eNOLcTAhRK2Dx1pzto7kOZo++EJDBjxgZ86qGnctwcelbjKyhleIs0bFS3UjrUr+kZltp+1kZx2bAEnkHoD86po7llzuGf4UTSuYgM8MOaWaQyMsCec+JzS1kdVIDcUhEbBbwHjUi3gMzcsFUdSaqOt2yMez3gnGcZpVhBIb5GC5ZTk5OB860lpqFuAqSygBVCqViAOB5+ZqRKbm4MkNkwmjQA52d4/bj7KfJePH/Wunx8LzqktI4LnV5E1AvH2gIUKuRu6A+4VAvorW3kRbWRpDyWLDGOePsrSQQr2vaXUQ+kLjljyvOeKgXMGhwziK4ttRjkYZG1kO73Vz485emvk+Hlwm0iy1GQ2hFzLLMM8KXwAPh1q1s9ctLi4/6oOdoLbQRg4HTpVFJabxItrHOsJPd7Re8B64rtglhaTb7389tP1CcYq8sc4srlLC5kkkdJFjkbckSkAKPlUTUrOyltWltmlSWMZ2tghhnoMYxTcuo28lxJ2URjiJ7gB6Co0spmOyMkgkbseVJFvK+j62stxBE1xcdngd1AufnzSru12RPdLcK7KBuTbtJHTIpRc+PHoeKjySiQ7Ce6Tg1WUu1tWeFZHmjj3chXByfXiuX1s8VsZUkilRcb9pOR8CPupPa7ucimZJe0zGTw2BRT9nDJJArrsCDgbmwT8KcuLadIzKirIiDcxRwcD3ZzSC27HgB0FNtOYw4B/QYceox/GoFQozwqwK7fPNLnt7nsgVgkZTzlFJH2VJsbCOaOEyFztXaSCMA003aWE3ZCU70PDLxiszltxq8cmoEMijEeDuJyQozUh45VjyI35/ZNdgYCWW6QBZJnZgRxtBNPC6mU57Vh6g1tlBjcKzs2QWwOR4CnmkAQuOQBSLnF9qKPLyVQdof1j6/DFTfp8ygIp2qOgHSgVZ3tvHprvDI4llba6v0HHhSIeylZjK+2MKSSOamWV7ZvMsWpxo6MRt7mCD55HNT7lfZqSMqWkVcfo7hmsb9JsY+C9u4YVjjl2oBwMCh9QvCpBnb5CmV+qK43jXQX/sg3M6+gP21tYeYl91YT2SbF3KPNP4itzbnMQrz/AC+256O1yiuZrirtJooqo4a5Qa5QGaKK5VGL9pJXGqTRrhVIBOOrceNVcEZlkVB1JwK0ftJpFxJK98igxYAOOtZxSUbIyCK9fH0wvNX9mZdNsUuDKH8GGOlUGMHmreS71HU7dIdxkVOigjNVtxHcWozPAFH7QqjsUKupbtFAHUkHAqLKV7Rhw2D1BOKftZVlEkYCRq/BZmIAqPPF2UrKHWRM9116NTSxw7OyO1drDHjQoQHlSfjSezlPROnqKG3KcMpB64oFuEMxAztwDgHmn44YyQGjdA3AbPSmSgSMStIu5uieNWF1fxXVhDCnelBwMtnH/nFZtWRWKqkbnLe4HFJdwW7ucDz5qRPZXVuAskJyV3Dad3HnxTlwi3AtUtwrO67dqryTV2GGDcOyojY2r0UDirQa1eJ2UebVlK9WgG5cVTyxPbylJ43jdequpBpafnZVCcnGBVsiJtw9vfXHbXs8mQMMyRjOPnSLCEzZEUbmPdjPGamQ+zt3cRyHfGCBlNrhgx8uDxStPini1BtJN4kcZyHbwAxk4z41nZ6bkqLcQrLdTi2mEgTJXd1bjwqHbpJIS52qG/WNOTdlBdzrbymSJcqj+dNjoB5VYzT0sU8QLoA6qMllbOPXFRo51PBOMedPrJ2cMp/YI+fH8arwKqJbT4Rtv6S461D5D1b6bp0c8InuM9gO6ezkUOCfQ9RVhH7P2TgslzOMjjdGD/Gm4M2Ku9KjglURrbyS3I690MmzHljrmnm9m4c92/X/ALoiKjzWo0ubCXKTu69QpG351Nl6FvdQ2FrATc21sjlchCuCx9KzkbOVL9lsRjngcCplr2t3dpGFR3Y+K0oQ3NzqcNmNyR9pnvrjGOvyqelSbTRrozI11bzJARksEzmpllFeQiSVNpMh7qpkhfhU72ohhhhtL1bYLv7jbWKZ8R0PvqFaatYQ6fKhsk7TAJZu8W5Axk++s2eTr8fyfzvrXLLTrqW8L3eQCCEBYZZvdUKfT9SExuJLOdEjTAYqeBkk/fTiyaRA8TiO4kmUd5mfgE9ceWKW19LJKRY380UTDb2UzZHzqcePjdX5Plvye0GK2updjRwyESNtVgpwT76YvdHvoLh+1tJnLANlVOBW40TVLPT9DhW4mCOhZShOTkGlXPtJZSnbGnbKR+sF++r5VxmPOBbyvOqRxPk8YwatL6OSO3jsroglGUg+KjByAaf1/U7j6apt+0tUKZKb8gnzBqtP0y4IlmjmYkdSh5rXd9r19JcV20KBIY4lQdAUB+00PLDNbyiSKNH2Eq0a7Tu8OlRcODhlKn1FNCbc3dBIHXg1cZTYGht4wrxCaTxZiR8OKcjayfIa12Z/SVzkfPNQQ4PjQ0m0imB23RUkk7aVuzRyqlerYqfYWthcznes8gHJV2AH2Cq+3ilmAWKN5COu1Sasezms7QA2k4lbnJjOKnKdLKuBaC7lC2MKptPBBxhf41Pv7axvbY20o/6hRhXX6yGq7TLu4iXt4o9x7MAhuMVy5vUtA9w47SY/Wx0DVxy615dKYabdA7YpIJsfohsH7agiR7g7II2kbxC+FOQXzLc9u3BDF8e7mmonMcW1TgnliPGu/bKVDY3W9i0bYbqVXOPSutGquFMmD+q6kVGWV1PDH51InunnshAxLHtBtJ/R65oi8S/tZ7m3WW1ikht4xgk94tjHQDpVjNqVn9HZXsFfKnCog5+eKysdzJagCKMKPNlyTTjazIeJxG3vUVm8YsuKUfVHurjUpR3R7q4a2ys/ZdsaljzUit5a/wBlj1rz72efZq8fqcfZW/tT3D764fL7dJ6P0VygmuI7XDR4Vw0HDXKM01NcRwJvlkVF82OKsDlRb3UrSwx9JmCFugxk0j+kbaQ7Yp43byDCsZr0z3mrzlQWCEIMDpj/AHzW+PHb2lq61T2hNzC1tZOGhYZbqD68Vn1MZP5w49dwpu0S5jm3xo4ZQTnHTio8jtLIzscsxya9MmMrRra17LdHec56Mn8c1Buo7l8JuDJ1x2g/Gu21y1tGzpI6SZG3bUYsSetA5PaXNrHG8sW1JfqHgg0po27OMIj9MtkeNIkkLCNT0XJHx/4rgkIPJoa6quzbVVi3kBWgs7WzW0QX0OZsdSD08qpZb1pECoxA6nnqakW8N3GI55Ip0t2P19pwR76u4e0TVFiF230ZcIOODmo1uSJQ20EA85qfqG0SLJEQDx9Xj7PCk20kDSFr4StHjAKHkGpuxqTvFnPNIlnG9myxOwZGC/WYYqqti0MsDRBxIjbsjrn0pV3JbKY/ojyEY53gcGpVjcBHbdGjF0YFiORxWJ1HXlxnt3ULhrhZBcydtIoXY7dRnHGfdmoSMiAYhjOPEjmhGiQYePtOepYipEa2twdgDxOehzkZ9a16cDM6xSQh0Ts5N4GVJwRg00I4s8qxH+ak9oGxjpXc1cHbq3ijtxNC7DvbSrHPyqMkzAY607dPlI0B6ZJpgGrBKjhmucpFG78AnaCaQ9lcIe9DIPetaDS54Y5F+iCYThFDlcbSMDw881dx3V+EDM4z5Mg/CunP4+XHudpLrH6c/ZThpIN6quMMSOfhWgg1lsbIrNMDyY/xqyOtXKja8MDY6gpWW1nUnvb52SNI1QBdsYwCfOuPbSxutRnPKWDtnrtbOPso0rRn1y4nkll+jqvAU4L5/wAuc4qgimcHO4gipEl27yQuSd8QOGzz14piNJPFb+zlzi1ZJp2Xa6NJtYDzGae0vT4pLeSW6a5gUOZQzYwdw6g9Kyx1C6ZizTMxPXPOatLDW70Wxs0CyLN3QhXxNZsrU76K9rHvZLtIoi5s4olManx46n1rPxs/AfIOa1mvSTwaVZi4hCXHabSoAGVAPl8KoZba17pcvG0mSShDAH3Gpx5dNXh+Iwbzo7XYQR4Vx7eRbowo6uoAIk6DB86d/o+V3ytzbMP1d+P4V01zJLlvrHJ6mubsc12e1uLVd88TCPON6kMv2VHaQdmSve5wcUEkztI0e7kRAkZ8z/xSxdTFs9q2ffUFXAHXk+tOhselMFjHfS5VZSJEJ5VxkVAjSFQWMfaMxJG48AVxXYuSvJUZ455pCSsZWVlKkeBGMVMEtHjVdrQIV95GPtpq+itxEJ4AUkBAIzkGubqbL7mCeGdxqi20bULvTocfT2hVju7NIw3xNXJ9oL+VAIdSgDD+8t8Z+RP3VlN2Tk13eV6Vm8TWjl1e5tFlN2kMtzMBhohxj0qha2uDu2pdKH5I4b+IpLXLNMWz9RQi+nnSN7k53HPvpOOLanw6lcWFvsI2qpyC9n/HNVAuWmldigAJJ7o4FWEN7LD0kbHvpq5SK41EygdmuxTJs4y2Ks6DQbOAOp6c1oNNv4dJCtBBFLIR3nkGWB8QMeFVSTWqYAtFIH7TfjUlLm1ZWVRLCH+tsepe1mfbRSe0jXkTLFpazIFy4J34+GKzF3awmFj9CmSdl3AluB9tcfVYLC27HTVxP1N0Rh/dUGfUbkxbmJPackkdTUy/TUsJGNo91INCnuj3V2tuR/SDt1WE/tivQ7XowrzizOy+iP7Q++vR7bhj61x+VvikGuUVyuCu1w1zNMXdzHbQmSVwoHmcZoFSSBATWK9qrxptQEIPdiUcep/8FGoa9f3En/T7YkzwAQT86pZ5J7i5LT5Mr/s4zXo4cMu1m1JsreSd8rJ2eOhIJpieN4JGjfOVPz9amxmbSmRplGGzjxwfKrk6ZY6zBHKkzw3TIMnBKk+X/FavLLv0uTGZhlaNwcmkBjjg4pd/bTWF09tcLtdfkfUUypyMitz9ZSYZmDAOdy+INNpKYx3cD3imi2CRXQcjNUS4p1kbbMqFSME7RkfGmEZV4KB8+dJjwz7SwXPU+VW9nayI2+wsri7cDIl7M7R7hWb0FWFjGHR7u17PJGyPkF/9q1TXJ1Cd4Ly3VbRVATPiaydsNRbVo5b6KdTyRvQj761CkQ7WupVjc8BMEkHyOB1rF/6qbBaabK/0NbOFkZTnKeFYzWrG3sb9rSM9mkfOWbcW+XSrzS7i4PtIwjcCJIsyEjOBWZ1q9W+1SedQAGbGQeuPGnH3i7Z2ctbC1uDh7sL+yq8n505qd9Z4ijto23xAodwxn34qthP51ecc1ElkaSV5CeXYk1rx7W87Yux7PauUDixlKkZBGDUSe3uLLeLiGSJgpwHUimINTvrdNkN5PGn6qyECuXV3cXG7tpZH5/Tct99a7YRunFOozFuuQFPWmueuKdgYKzMQDgdPOqi3sNAmv4Fnjlj736J6inz7IajnuIrf9wpj6VEbjt+wW2RsFYo/Dj1qxttUi4CXM0R9WOKzd1roi39nNesw7JE6luvZyAfxqvfVL0Hb9Ik465bNXFx7QXsDvbx3cruF48c5HFZw21x17Fz7hmru+0TI9Xu1PekDj9oZpy2jtbi7k7rKXj3HHO05OT91RYdOupZhEUEZPOZWCDHvNXen6c2nXRluPHuNgjBBHh51z58pF4+1JYWy3UkrSbgvJBU4z86U9nG0bSW0xkIx3CuG8uPOpN6sNg5jjkV8gkFP41WRzOinYxUnxHh41eNt7Z27ZT6WU7MVdOzIOCHIU/I1ohp0YubWaxZOyQBWXPfznr61l5JHlcu7FmPJJNTbCOc/nElVADxluavKfet8av8AUZU1Kw/6iXDoCUO0kk54HFZJpHyFIIA8DVtPd3FpdC5jJR9xK4bhc+nxpMuuXF0At4kc6jpvUcfGpxmejfpWF2bJ86mxaYtxbb1MvbbSwBXu8etX2l+zNtqlot5J2lpEc91TnePMZ6U3qNp/RV9Fb20jyJIoKhhk05cvw4yb2zqSyraywyEjeAuD7wf4UqO5lgQRwsUUeXjV7qfspqOTcwASq3fKZwyny9aphpt+RkWVwQPHsmrU7iUpL6Q92ZUlXydQah3CRi/fsi0UIwcA9MgZA+NdYOpwVYY68UiK3lu7kQxKXZuePCqRLS67OZ2srZo0xlgX3ZPnnio4331zLMXUSsMqhX6/p760U72en6dsa3cMBt3OuQTWajuEt75biJSyo2VDeNSfrTkAmugUhjO/PUH78089jcwqXe2k4HLBg/3U1C5ji2qcZ5NKYu6lTNtXxB8arBHaAY5+YxXHl/OhMc+8Uy6OjlSCfwqckNoozOGkY/qtgCgZAKDaR7yOaUGp9bWwk+pJNC3h0Yfwpu7tprUxP2iTRvkBx9x9aaYbLCuq52k/rHNNJb3DtvWCWRD9XAJFPCF4mzcq6r5DGao6DTMkpYFV4qyi/oh7Fwbif6W3CKVAUc9T8KrLoCJNqkMM8MOCammFW9zEqdnPZxyAeO4q3zp5bnT4WL/R5CR9RTLkKflSLG3s7lF7aYxMud2fHypvUrWC3lAt7hZ42GQw6j302bi5c0tfqD3UGhB3R7q6RUZIQ7bhD616TatnB81rzU8SL769G0077eBvNB91c/l9N8U3HFJY4rruEXJOBVNqOrSQBuyhBUfplv4Vwk1Vo0oWsX7W3bS6gsIY7Y16ep/8FLm9opCMrcRqR4bCaoLu7e8uXmkI3P1xXfhwsu1m0nJqVanfc26k9CSDULOOCakWsxhnSbb3VGM11qLO9nin00qY8tGwJA4GemflVtazO+gq8KfVYMCvl0NVEs1v/RMrmQ75SQsYA59fSrL2Ouu1gmsm4ZQSPMqevy61y5f6tfZz2heG+02K87jSxsEbK9VPhWSZEE0mB3QxwKttUnWFDZq4fDcsQcjHh/xVQTkkmt8JkZpY7Jj3o/kTSLhFjK9mx2sM4PgaM0mckuo8gBWw7a9p9L7RFYlW3AjPBrQRe0+tRDZvkIbgZUE/DIrObz50/bTusyYc43DxqWDVy6jdQo0U920l8RzuwRBnw/zfdVXe3UqWnZ9vKzBgw34zUG9MJkkZMiXf318PfUZmJk25JBTPNSSYLiO/WL2YuCJD9LuWy7Y5xnGPlWfDDHlTQJxtyceVdx76s44tupGdgPqOOfSpdppsMVoL3VHeKB/7KJMdpL6jPQetSbG2t7eD6dcJ9IfdtigwdoP6zny9PGomoySXt288zu5PGcfYPIUQXr2D26Czs2hJb67yliR7ulT9O0uwvz2RunW4LNlNoCqAfM1WokBhRZDIrLzkdD8Kk2tozuZIy7D1jIoLuT2PDKDFMGHm1RLj2Qu4onkEsIRVLMSx6DnyobUdStYOwszsZee6Rk/A81Al1fV0SVLiWbbMpRhIDjBrOcv1ekHPGPKgNiuxRNM4Cc5pU0PZc71b0Fa1HGlLOzZ8h8hihZHU5DEU2PlXd1UX+mXdvNbTJqIWWNACm76wPpUZdaeJWiijxCeiMxOPd5VW9sRAIto5O7PjTdY8Z9mrAfRLs4bfC7fpZyKh9g/0hoVwWQkE5449aSrYYHwFcUnBJPLHJrUmCUbC5/RVX/yuDU7TYGyElOwgnKMOcenNU5kaPlSQaUb644BkZhjFSy2NcbJUzU4lt5giyvIDzz5U3p8Aur6GFs7WYbiOoHj9lI+nJI4Ese9AABvPI+VT7G4tY5d8e+J9pA5yBU7kLlrcSalaQSokT4t44sbVH1fADFGnakmoGSZYtkcZ2h3Azx5V59qt/erctG86gEA5hXaHGODUzR7q7A7O9F01tjK4zwflWM61etyPTIyWjBI25GcV5xL7TajuuUSc9gznshjkLnjBqTd+2F5Fd9nbEGILhhIucn76zO/ccmtcZ12zeuliusXYbJkJz1zzVnYa/IQ0TrGpcHvbQOfWouiada3ME9xqEjRwKpVWU87uKpmmKE9mTjJw3Q4q+M5Eth8LJfzM7yxqnm2cD0Apw6N2neiu4GPkcrUBJyHwSdh8BxT2/DZQnFaymnZdPurfG+FiCcBl5B+IpppWgJjLbSeCKW1xOwEaM2M84NMOgbLKMAeJp/6h76QJUYNjJGAcc0k5zzSFAhUZHfPPPgKki4jltmjK4K42E9fWpmLezOcUppS4jjPKglyPupvax4UZJ8BRsdMlkZc+YxVqH2uZCeGIA6AU5FfXIIUSMc8Y65qHmkyOVXIODTA/qsgaRQqp2g6lE2n7KgyLIcbgattBieS5a5aFZlh5wxHJ8OPGua4w+kdosPZb+cbcVmXvG82ah2TCNtpCZPGWHSn7rTJ5BvhWOTz7Nv4VBQ5OfGpMMrr9UnmtdsFqO4PdRSl+ouB4UtYJn+pE7e5agivwVPrXoGiNu062b9nFYaa0nVCzRkAdc1svZyTdpMX7JIrHydxrif1MvICinFZnUVkhiYMQQavNR1Ozt2bdKGf9VOTWW1LUzc7ljj2r5nrWOEqVUMe8a6xTswAO940k8sOldI4Br0suwNGs6mWPtEzyu7GfjW30eb2XnRIpbZYJB0E7EjPv6Vh9mCDnxp6TBAFSzYu49TvooLbSp7m1tbftI4yyYUEHArG2l5HcyNL2YjkP1yigZqlt9TvLVWihncRsMFCcrj3GpOkXdpHK4u2lRSOsahv41iccXdXbR2LjL2iufMgfwqq1O0tVt3kgi2MOeGJ++pcmtaPHnZBeTH9plQfZmo0ntFBgiHSrfn++ZpP9qTUUo24BJxThCyIWCAkDryM01cOJG37FTcSdqDge6nYpyeCitj1xWwzmlKGJyoJx5CnnvFA/N28YPm3e/wBqb7V5Mbz1Pw+VOwqImSaXxp5Ld2d2A4RBn4mrC0gt4btJI5UbPBB4qzvbBGk+khsYwMIRgjNZ8pDGM2nyroHNa4+z4HMJ2H3ZpuP2XuHbIMXxFPOGK1b6SG3Fssh7NmJZM8ZB4JFP6eIXiIk+sCfGrmz9iyoMtzNvOCcKMCo9npFq3s9JqUkro6q5wD1wTgVqWX2Kea6jtrx2iRX8OasbL2jSPuy2kR9R1rOk5OaAaGp+sg3FxJeRMCkzDAB5XAqE00htdjSORnoWOK5M57JFz4k0g7mjGcmpBP0j/wBYBgCQBg+VRrqIxzlc93P1sU1G8kWHjJHOD60893LIVVxhQchR0qZ3rU9YZbenDKceoxQ0mHIUkLgcVJW9mVvrkjyPNOP9HuYZC0axyhSwZeM49KrOIQffjcenSlY43E4XzwaILOScZBRR4FjjNOS2N1DHns9y5zlDmrobPHTJz5ipWnWovb2OBnEase8x8BUe2uIot3bQ9rkYzuxitFae0Oh26LGdEBGe8zMHb7alCtR9ne31a3isoCkDgAyA5X/momr6bFpEWxtrztluGzsGcAY8DU689rLMKF0y0eHplmcg+7ANRtQ1Wz1q3SO6Btpl6SjkH3/Kuc8uPtu5fTMElmzT8TGre10+0t4pXe6guMjugA5qn+pk4x44rpOWs2YdZy1yZDzsAVc+lPJeXCNlZXHxqKpwoFKzVxFrDfi4dYryNZUY4yRyPcarIYhLM6KwVVON7dK4r7csOoHHvri91AvlUwXhSOTTYLOG5jIRy7ZJXdmqrVLW5jmaVrfZCTwU5UfKmQSDwas9K1N7W7jdmJjU5deuQOtTLBR4IAyCM81LsDEbhfpBYR+OAD99atPa62vI5fp2mwyIGPY5UdPX/auRa3o0qGOXSoFDcZQYI9xpbfwUc9kgXEBJJODlhnFQpZEi4j529D5mrK60jtHY6dN2kB/RdsMB/Goq6cspUtdW8UYHQsSaks+2rLfSuBYvuY5J5OaUDiIEdS3WnLuO3t2ZUm7YnowGMVHc/VAPAFanbPpLSbst3ZYUk5JHFPR6hcIf7QkeRqEtODOM1RYrNbXJCzwqM8b0GCPxqbZaVp1tHLPrZk7PLLCi5BcjqftGKrNMtJNQv4bWH6zt1PgPE1q/abThcjTtJssNNnknwUDljWPtYzQvLWKRl0+GWOLOe8+7P4U/Z6fcaxOqKVjzwC+Sa2Fn7N2VpGsaZ3gclgMk1W+0GoHQ7bZCw+ly5CMD9RfE4rPu9N70yI0tob97W6Ywsn1srz8Keu4bax2pbyPNMw5YgBfgKTasuozbbyZxI3CTEklT6+Yrs2jz9k08F5a3KLzlZMN8jit/bPWLOLCxJgAd0VIguY4wwd1GfM0iHTYTEhcsxKjqakxWNsD/AGYrGwVV5cRGJwHzkeAqBD2rxBU3FfIdK0z20QyAg+VNJHgEKvHpSVMUq6fK/Ld0etdOmL+sT8KvVt2bqMe+um3A8M08jGbfTV3Z2kmm5LEKOlaCSEn6pqFPAcck5rU5JileGNSNxOAecCpeqLp7SodMEwQL3hL5+lIksriWbuJ3f2qkwad2Z3TOpPvq6KgRsZCCOaULeTdkI3wq2VLdL45ZQnZ9fXNSIhHPJshLMevANPIZ1o2BIIIOfEVwqVNX9/YvG8BYcM+ORXJNNiPLDHup5GM+31VoViOh5qyvbOKOIlA5fwqNBEduGQLx1bxrUqIpq1tNMFxaGbt4Igq7iZXxnrwB49KiG1k6hFYfstTTFxwc8cAeVdOHKSXUq0Fu6ZLqTn6pFcfegxtZeabgmuB3SzeeGqfHfPJEplUE+gxWcsUmDVbyNQBM+B581Z2/tFcQrmVEkA544NV30m2Y4dCPfzSZ1tzbyNDIOFJxmud4z8XT+re1t/NNi0l7KF0BC7RuXwIzVXHqjrpNxYtuYyspBLYCgHPSoMylGTPigNG381vz41r2Oc56U5HDJK4RI2Zm4CgcmmCxzwaVbyPHcRyIzLIrAgg8g0Rsn9k57qxtAIRFsTLsFBds+HUVWa17NXumAtHC80BIVZF5PTxA6VLu/a++7NYYZWXH6fBb54qGvtHqIck3lw7eCsBisdtqFdyDryD0oBMjjjn0q203Rb3WpJjbNG7L3n7Q7TzU2H2Q1lZe7HGjYJzv+H21vU6Z0nDYPWlq3B9eKtL7R2gsfpUkka9mdjKvJJz1qpwQinwPQ+dSWX0m6WXPninobmSI91yKjZroPIFUP3225uVcYUsgLkef/mKBbWbjHayKfMgEUyWySfOioHZNOdI2kjYSoOcr1HvFMk4jzUm2uDC+7qADkeYxTcDQqo7aMvnwzimiL2h7PyOeo44rodT4Y+FWQt9OuBhS8DehyPtqJe6fJZqH3LJExwHXz8jSUwjGT3efdThjVVyZo/cGyaiIzKwKsQc8EU5LKTcO/Uk9TVC1dRwOc+OK6DTayKTzn76cjQu3dzgckgE4HwojvhRu/Nt5t3RQ/ZAdyXef2VNNg8jyFFOZ4wOgroNIBruaCZZ3TwzoVYjBzUVY1kmeSVmWMkkAUjcQCR1PApWeAPKpgmBdMlwrxSRn9ZWrr6OknesrhZPJH7p/CoNSLQTSTLHArM56AUppyHR795VRrWdQxxuMbYH2VKmW2EYjeArJGuOD1rRw+0Nvp+nG2uL1ZZFiK7ETI3f5vj5eFZD6Vjuhs58az7jXGw5oV7PbavE9qjPI2UCD9LPhXpOkaa9mjz3DCS9m/tG8FHgo9BWS9lvolnN9OmGWA2xjHCnxPyrVXBl1C3EthciKQebVm3aSGNY1F7c7XVE/VLEg/AisBr1wbq+7VpjK2ME07qpvkvJPp8shlY/+quOnkemPdVS5JBIUk+OK6cZIlpUTlAWHUDj30pHMce1T16+tMeAHnzS81UbiM/mI8D9EfdS1Zh0FOwL/ANPHgfoj7qVsz4V52zB3Hk05bL3jxTnZkinIYyH6VNAwGKYbk42mrDZx60louOBWdVUTRFTmmWQsuGFXBtQTk5NdNuMYxWvJMZ76NIHGSdmeT1wK1Vj7O6Xt7R1F0zAd5zx8BVPfpLbW7yxxh9nJB8vGjSNbjVt9tGZJGGGjxzXTjUsamPStPi5jsrdT59mKcngtxCS8Csq87VTP2VDi1qLA+lxNa54y5BHzHSp0dwkuey74HiK69VnGR1m/tUu1EEDqBxLE6BR6EeRqO8cc8fa2z708Rjla1Wo6edRjKSQQA4wJGJLD5Y++qeT2Xls4HmtLlnuFGQu3Ab0rneJKpWtd446039AbPKD5VYW9ykz9lcJ2M/kwxmrAW/lWdxpnxoyOcsCvqDUqz9nLTOXDSH9o1ciBfMVOsoVLFc44rp8XLbiVV/0bYnG+LcQMA5ximJdIsZEwiFPUGrKSIRytxnqKZVTnmr/TsxQXPs+2cwy59GqpvrC4tYXaRO6P0hW0JAOPAeVR7+OC5tJIJSdrjqOorU5amMBv34WQbgOAfEU03ddlBJAOKmXtqLS4Mat2oHIIFOW1ukyEyBQx6YHWoK09acthmTPlUi5tHhVj2fdHjRFF2cOWGC3NNB2Uk0uyJdzEZxS4ZIBGFnZkkU+K5ptbiSCQmI7SRjNR55Xkfc5JPmakF5Z69PZS9paSKDt29Oo91Sj7basAcyxk+A2Cssq7s0oIcjFXBIuL+W5nklkxukJJ8s00G3cePlTZO2XnpXSQX46UxDpjCEiRtjD9Eqc0hiAeN2fUUTOxdQTnaoAzSQ3nxRSgaVmhIy4LHKoOrYJArr9kB3WZ/cuKIAe4fXiuZpJYEALXAaKdDY5pyScm0aLruYYFMZ4px0KhSfLigct1tY8dsjSN6NjFP/0da3AJgnKN+rJyPnUEUpWKnIqYC6sbi0I7VO6ejLyDTURaOVW5BU5zV/HZI2mdvNcl5x3hECCoHrVM0oYsPA+FSXVsxHRsZyAc04GB8fn+NXWkez9pqlt3NTSK6PSGRMD555qJqegajpRP0m3bs/CRO8p+NXYiCg3NgED3nFLkRY+GlQ/5TmkwzSxq8Yc9mw7y+B+FIgBJ2hWLNwAvWqO8cenNdzU+HRNUm+pYTkesZX76RfaRfadGsl3AY1c4XJBOfcDU2Bu0tnu5uzjKqByzscKo8zU25eHTlaG0uDOZBiSQDAPoKl6JBbXGlzwHAuZGzubwAxgD388+6qu9spYGJC52nDJjvD3ip7ZqNcA7A7nLP0rsEZLDPT76aLtLMu7zxVnFaTz3CwW8bMx/VFW9JWqtooNU9nJFIjjniG1ceBGDmsxb6lc2cpUuQVODV3fzRaZYR2cewSMBuI67c5yT6n7AKyU8va3Lsv6TGs8Z12s6X+qajDfKhmMe6VByzEFDyMjwqlktlHEV3FtI5y4FQ7p98uM8KNopmrOOTpu3Ulo2VucH3Gk5xTKsyng4p4Thhh1HvFbZelW6f9NHx+gPupYU+Ap22jzaxZH6A+6l9ma8euxjGKUn1xkmnxH18PfXVjGcgVNClUN4UsR+fFLjXjmndvHWoGex3dCK4YF86buo7kgfR2x5g1y3t7hX3OwI9TmgcNuDlSAQa851aFtK1maOFivZtlSD4Hn+NenBQPSsJ7YWTtqNzdIMqoTf6ZBGfsrp8V7Z5J2m+0NpdWZXUHRHUYbd0cUnT/aC1sNSEVrK0lo/6wPc9B5isW1dV8dK7zjJdjOvaDeWwi7UzxhP1twxTQ1SzY4SUyHyRS33VgvZvWXS4SCdRLCeobpXokDxtGDFtC+QHStdpYqNb046vAhhhdJkPddxt48QfGqeyubuzkNveQTEjjO0nH41tKj3lqt1btGcA/otjODU5cdhLioVlf6vXy8acjZkqmaebSL8wXgRhwQ6Ljg+NXcMkcyB0YFSOCK4d8brfsycu2eaSQSeKl9mOtHZYGfGs6K2WNiar57diTWgMQ8qant1/RFJyGVntBIe8gyKjPp68kd0+ladrcdf4U2YEGe7W/NMZS4tLj6O6oxYHwJqDskHEo2kfbWwmt0b/aq+W32nha3OSYq0tbUKrOGJxzUDUURpx2KgIBirqa3Zsjb8qrJ7ZlPIPyqyor404POa6zlSMYoOVkI6Ulyd2K2hLZLEmhOtGSD1pxggVSpJJ60HZ4yjluqN0YdKGZTEo8RRHIyHu9D1B6Gm3wJWCjAzwKQOIxRHIJGVI+dIU4HSlrG0kbhBkgdPE1yHAcBsY9aKOD604kAKlmmjT0Y8/Km3wHIHSuzMZOz3tkhep99EKVUeVY0fJJA6YFLuxsnaMHO04pqNgoA5B8xXWQsC/XzNBwUodRim84NK3kNwao0SwwtiR0EbYxtXIGKp7y1jhUFSTI54ApdvcNIvekYkdQaTeah2idkgGM8muc42V0vKVx+0t4lSRCpOOowastL9rNQsB2Uj/SYOhjl5499RbXUQ6CG5USR+TeFWA9nrSALe3s7wWZG5Yj/aP6D09atue2VlFYaH7TQSy2kb6fcKuXwv5se/w+6mGtfyf08XGnWslzKwIN8yZC/5R4D1qDfaqZrAx2kQtrJPqxJ+l6sfE057O6tqkZ7O0k3Qr9aOc5Qe49RU7FU2r6hcSBZLuUgnnLE0/bWk1/cFUTtEC8yFyu0+h86vNcGkTW0k/wBEFvqAxjszlG5GenpnqBUWxn7GwUAYUsTWp2zaqZrLUNNm7RSz7fFTmrfTP/nKgl0SWHghuTIfInrig3Be5RY8M7HG0EUq69n5gDcQzrHL1AXj8KvLimu3Wi287CSWGO3YdTCcgnywOnxxT39I6XplnIkTtcStn82R8gWzjHp41mbye9V2S6llJ8csSDUQOWPd5rPiJU07SszyNyeaiIdu5/Ice+lFlVcO/wAuTSXxsAQcHmtLEc+tFdbrXKoK74VyjOKD2O2TNnEfHYPupW0+IpNt/wDSRf5B91KbjHOa8LuGGfSuZC/GleHXiuEccdaBxCAB40pn2jmkIDgGiba0ZUmgQbpR1Ipt7xfA1Wy5RyGplpOeK14s6snvT+iMCq6KcPqV5vVWUqilW6EYNNtMc4xUS3lb6ddE8cr91akZ07quh6ZMlqbePsTLOqPsPQHNRdY0Sw0xbKSGMnNwquXbdkVDvNWuRepHFGv5qUFWZsgkVJmGqa1AEllg2qwYBBjBrclmdnS/sYoIrrUIDGix7lOMADBUfhTltcf0ZcoGk3WjnarZ+qfAGqm20W9mmke4vZFDKAWU4J60/eadFY6dtDtKDPGzFzknvAfcazLl6rX0ur3X47QjNvIwPRsjBqtl9rJf/St0X/MSfwqugvEF3dafcKzW8T4DnnYPDJqPf2D2j5B3RN9Vq6znftjBqeoS6lKskyoCowNoxXLC+mspO4dyHqh6VGApWwVRsLK9iu4g0TDPiD1FSSSfhWNtpHt5RIjlSPLx9Kt11wdn+cKxv+qfGuPLh+NS6ujx4GggMpOeax2pe0M3aAI7BRkd04zRb+0tx2iqxXGMHPjU8KrTuMcYpl045wKrrbW4nY9oCMnn0q3TbIu5CCDyDmpmGILxA9abNqDnAqxdTjikEHypqKx7Ty61Fns1JIIzV0ybseFNNBnPGa1qM7NpcMpy0fPpxVdPo+N2wknwFa14Nvhnyppoj121qcjGTbSCPU0zLZNGmCCceVat7bJ6UzJaq2citeSYx6hlbhTkedJCFn5IGTWkn05Tk4B9ahPY7c4PwrU5IqeQeKWz9op3gFgOG8f96VcQ9k3XNNhSRjHXxrQSOaJVPaYPGAKeliEWCDkGgShlCyruA4BHDD40HIIu03dRgUjJUnFLz2TfmnyrDyqfo9rb3d1HbzRStJI2AQcAClvWiC1wTb7Nkag+IXk0wOTxVprukf0de9nC5ljbleORVWuegFJZZsLMKBKqRnGetN+NKJ6AjpSPGqiXpxZb2Fk+srgj4VcsNU1A3Urwzzb02hyhx18D0qn0sf8AWKfIE/ZWgtIri6thI0r7ASqqxJGBWOTUUc6yLEITkEYG3yNPwSdgoVDgAePiacuotjHxyait91ajNPvdGYFSijx4piFZ727W2gHJPXyHnT8SKVAxzjJPnT+nO1juZMCSQZY+Q8qexo7GKz0a2KxsZZj9aUr191RbvUzL3Qaor3UGOO1LEnoQ5qDJfS4KqxHr41ZMTEvUJAz5LjPketVZwWJA4pDsWOSxJ9aEJFKobGQPGlKxU0Iu6QYHJNSzGfFcVBBJy2ceNPqgdcqm0+RNOCHnypxYZD9UFselNEJk2nypBFTiQ3DCmHgLZKkUV67bkfRYv8i/dSjyfGk24H0WL/Iv3UsivE7EFfOg+hFd6+NcIBHFApWBGOah3LMr4GT76kjHTFcdQ31hjFCqmTc3JHSosjBOTVy1sCM5HypmWyRoyM848q3KzimMhzwPnUUCRZ5m/vMH5VJkjeGRkccimJWO8V0jKJdRL2sDHktIM/KrKJzb96Pg+IFQZoy8kLdAr5Pyp9gQDzVo0Ntch0VlwykVzUkM9hLHHGTIcbRx1zmqSyuXtnU5JQnkVoYpFkjDjJU9K5WY17Uunqo9rdRikAKSRAlT0PC07Lc2+n3/APR1y263kXcpbnZnPH2U3xD7aAsQBJb55P8A55VH125tovaKxmkKtEFIcdeOfxrfuxC9Q082h7RMvC3Rh4VGQA44q1tbuJIS8ZM2mMxTJBzEf4imb/TewXtYe/bt0I5xWpy+qlisu3EUBJHd9fGs68sksud5+dWmsO6wrGW3KfnVMvANbiFsxDc80lXKnnu0hmJ61zvEc1UT7aVmcKScGvQNGVVsVCklQK880yA3d7HAW27zgE16jZWq2lokSlm2j6x8a5fI3PQMeeaQyEeFSguPCuGMnPhXIQyPjSdvocmphiFHYDrQQ2j3eFI7IeIqY8YU4FIJ5HGaCCbZfU0hrdOlWBwRkfKmtnjV0Vkluq54qFNAGPK1cvCec/bTLwZ8BWpUZy4sVcHK5x4iq6WxK5MfPvrWS2+c4qJNaKevB9BW5yTGTlt5CQFQ58gac+gmIAzsFz4A81O1ZGtVRlI7x8KppJnc94k11nbKWDHuCxJlug8TWw9mfZ68huBeXKhcqQqnqM+dZDRtRuNNvRcW2zeBg71yMV6JontZZ6kOznIt5x1BPdPuqcpsxZ7cuvZtpZHuFmUz9U3AgD0yKzWr28cB7PVbLsmJ4lTjPx8a38t7bxKSZFPuNZnXfaF3ieCGG37NuCZiGz8OlY8ZG5y5MU1nYOTsuXA9cUlrKzVf/qGLeHFMywwoSTMmc/VQZpvdEOiO/vOK1lTYVHazfSEjUcu21SPGt5cxQ6bp3ZqwDxx4AzyfWsJDd9lcLKq7SnIAJ4qRDfSzTsWYky8PnxqcuNvtNxMvGGxemffXbXTy8PaycbvqrUi2s1nnimd1KIeVxyatJUUqSDS8voxSxQiLtTjpTI7zksfjUiZ/zczAYBOP4VECmSI4PNa4pVbM3aXBPXnoKca2l27wuR6UmS3eM9KftbxoTg9PI0tEE9cEYNHSr2T6JfREvhJMcGk6f7PSTxfS7+QWdkP03Hef/KKeRiv0y3kur6OKGNpHPQKMmtWLOx0s/wDVgXd54W6Hur/mNV1xrcNjbm00WH6PE3DSnmST3mqiO5mSTeHOT19almr6aY6neqD2UVlB6JADioD+0OrGfsRdlecd1FX+FNWuoo3Eo2npkUzYr2+pneTtGSeagQ9o7lmc5Y8knxptrCUqdmPia0n0ZQMrTbRLtbOOBTyMa+2wbaH/ACD7qS5w3TxpNuwNtEecBB91cfkcV5XV0tShyPCkKTny+NcAwKBYIAPIzXeCevhTIOWpYxgjmgX0pBJ9/lRnvdDRnx8KCDf2nbJvVe8vp1qkngJG5eo8K1IYc4xmqu/tmVi4+q3Wt8azYqCh204FBU7uPLFIl/NsCTwaT2wHOeK6Mo91cyW0OIkzJnjjNRYptXmG3t3iVuoHdqfI+45Xr6eNKicAbicetalRItPZhp5FlvLxyxHBHX50xrOiWun6hpqICUmk2uxOc8j8asLTXbKAKlxcptXoQc/dVb7Ra7a6hNZtaCR+wk3klcZrM8tauNnBawW9v2McaiI/o44qtmSTRWLKrTae/VcZMX4ioR9qbibiz02Rz+0fwqVotzrVzek38CRW+093HJPhWPGztfbJ+0JEzCaC0lii/WZTzVDyTXqvtBBb3Nj2U5xk5B8sV5hexdjcOinIB4PpXThy1iww3BrqHnzPhSKk2MTS3CAL410SLb2cs3l1a33KcBwelemgcVW6LpMOnxB/rzOO85/hVpXn5Xa6euicUBfSu0ZxWRzbRwKVXKBqVM8jNMCPeTgZqWwzSNmDxUDAhKnmksijrgZ9aksuT0+R60js+OmTQRjHnrTJQeQqfs86QYuOBV1FeYwe6DyaiTRZyCMVZtDyR4elRrjES5xnHpVgyWtXFu8Eluh3svO4dAazRqVcEG4fnAJJ+2oxr1cZjFOxPsXJxz60NLyCpwR0IpigGqh83lwRgzyY/wAxplmLHJOaWsRYZ4HvrkkLoASO6fHwoEgluvhXTk9a7HG7IxVcheSfKnFWMRNvJD+GOlb48NTSoImK78cU9GqiTcOKnWkG6yjxgHGftrjWjojykDAIHHmf+K5e605b3bQy9Tg+RqcdTtB3ZZJBkdducn4VSsWBbbyRTDAyknJ48Kt4w1cJMSikLkMeRjqKDZvlhDnHUc8iuaXIrSLG0gj944NaFLYgYKj4Vz5csWRnG0+Y8uM/Gm2005zj7K1Qtzj+zzSJrfbC77QOKz5tYrE/ozRI+0UDULwDPA/Nxn+Jqsu7671KVri7csoHA8B6AeFXGjtarDJJPdQxs8h4d+ce6mfaK8t3tkgtpllBOSVBxWpe/SIel6W19FJIU/NjgMT4+6mLvSprc5QFh5Vp/ZyAppERPG/Lc++rRrRJI9si5PrWP6ZV8ennMHEw3jockVoNDtbO5nJScozDgnqD7vGnPaDTo7S2MwRQSdoPjWfgiuEj+kRA7c9RXT/aM+mrnSWzbFwpEZOBKvKn3+VDRI6kjxHnxULTPaI7RBqAMkZ43ePx86un0aO4tfpWjXClSOYicqfd5Gs5ntdWlrj6NFyclB4eldOAaZte9axHyQfdTuOo49+a87Y/7s+6uHG08UtV8DQF65B+VFJCHy5pZHHT5UsDvClMvABx8aBjA8qNuMHGRTmweg9KUFxjrQNFD4+NNz2hniKcipYAAzjpR4ZB5oMfcRbHMb+B8ahuoBOBgVotZs/qzqMA8MP41TywnG4c1243pzsQSZ1k7gTZjxJzTd2rzIYim3d5GpgBxycD0rvZK4wQT8a1qK2LSlhGZkOSeCxG0e+r7T/6NthGtxsaRmxlOQB5nFQGtUU4YE5PUnNPLCkYEkQEcikEEUvazpsLRYgcwEmMjxHHwzTepanDYRnew7TGQtc06Y3FupZyGA5A/wCK8/1a+e8v7mTcSpchcnwHSuc47WrcPanrUty7d7iqSRi7E11zSK7yYxa4Ap8MVPsmI4Wbb5DZmoajJqVGQMeHupUehez2oTXcTJNJHIyD6ycfMVc1gNI1aSynByMHgk+Pvra2N7HeIWTgjqM1x5THSJVFFFYUUZoooDrRiijmoDHNcxSgOaWwAq51qaZ24HFcPFKJxSSDUDTc5xVPqct8swW2tY54yOWZsYNXJBPnSHVRz4+lalweZ6hp13HkyxKoBJ4NVTgg16ZqdosyHABHrWF1eGKG47NVwRya78eWsWKs0KpY8UoqM9akwqijIznxrbJMCJnvGpi2m8boJACeqgffUN8Z4GKn2cgW2357wbGM+FRUePZFb3kbkCQlML7jk1byapoUVrst9I7SQrgvM54PnVBM2+eRvNjTZyTiu957JPxnFnZXasiQy90AYDCpU8QDgNISCM561UbGRtrDBqXA+0BpCSoNc5OtUSoqZPGfSo3CkkeNXRht7mPtEJweo8qjT2kajuis+Sq9GO7irnTtWktVCSfnI/Inke6q5rcoM7c48RXB4HwrNko3VrcRXkQkgbIxjHiKY1UCLTpWxg1lrOeW2lDwyFWHlWt0zVbe9QRTYSU+fQ1x5cfG7G5VLp/sxFdW0VxLcOGkG7ao6VB9oNPgsJo4oWduMncRW87vQAHyArF+0BNxrixDk91cU4crb2WZGm02DsrC3TptjFTMY8vWnEjAVcAcDHFdKhASSAo65rm0yvtnL+at4vUk1P0fT0OiW4ZBuK7jkeZrPe0Gqw3mpExKGjjG1Seh9aLHXp4CAsrADjBOV+VdvG4z1qy1DQVbJiGxvMeNVMM9/osxeJioPBI5VvfWqsNctbvCXIEbHjd4f7VMvdOhniYqqkFfeDV48vql4nLVR9Eh/wAgz8qe7PHjj315vH7b6tGioFtsKABlD+NK/LnVv1bb9w/jWP5cjyj0deF5xRnLZH215x+XWr/q237h/Guj271cfoWv+mfxp/HkeUej8g5z0pROfA15t+XerfqWv+mfxrn5dav+pbfuH8afx5HlHpBGKVhiuRXmv5c6t+pbfuH8a7+Xmr/qWv8Apn8afx5L5x6SeQR1rgXBJ4rzf8u9X/Utv3D+NH5d6v8AqWv+mfxp/HkecejSxLMjRue6wxWfnhNtK0TeH21mfy71f9S1/wBM/jTNx7Y6lclTJHbbh4hDn76s+LlEvKVeSLtc7ehoRsnn3Vmn9pb5xysP7p/GkD2gvB+jD+7/AL1vwrOtWy7hg/A02DjhqzP5RXvlF+6fxob2hvGIO2HPop/Gr4U1u7C4KRqYz3lH2ViJVIeQN1DEGkL7R3yZwIuf2T+NQnv5nZmITLHJ4qzjU1JNcxUT6VJ+z8qPpUn7Pyq5RNHFLVuar/pcn7Pyo+lyfs/KmUXEbZNXehap9BuVaUns/qtz0FY4X0w6bflS01KdCSNhyMHIqXjpK9qUh1DKQVPII8a7Xldn7barZ2sdvGtuyRjALoScfOn/AMv9Y/u7T/TP41z/AJVvyj06jFeZD/4g6x/d2n+m381c/rA1j+7tP9M/jU/lyPKPThXa8x/rB1j+7tP9M/jXP6wNY/u7T/TP40/lU8o9PrnxrzL+sDWP7q0/02/mrn9YGsf3Vp/pn+ar/PkeUemkZo215l/WBrH93af6Z/Gj+sDWP7u0/wBM/wA1T+XJfKPTNoPvFIaIE8nNebfl/rH93af6Z/Gj+sDV8f2Vp/pt/NT+XJPKPQZ4u6axXtBpjTXHaoo6c4qBJ7d6tJwyWvwjP41El9qr+X6yQfun8a1x4coWw7HpxxlhtX1rjW4HdQGoj+0F44wyxfu/703/AE1dfqxfu/710yspRtznkVK0yAJqEDPGJEDjKnoaq/6Zuf1Ij/2/71063dldoEaj0H+9awbj2qhtbnS4+wswLgyDYUXGPPOPCszBod0zKxVVwc941Dj9pdRQYLJIP2wT/Glj2ovh+hB+6fxrczO0rR3thbuoLYXOMnwBqiu4+wLRDkrxmkflZfbdvY2uP8h/GocmtXDkns4RnwCnA+2lv1BZ2rtDEGQ+8VNgkS49H8qzQ1S4AxhMeWKP6TuM5GwH0FZslGqe3IT6uR7+ajSWxAOzOPEGqge0d8FCnsjjzX/ekHX7s/oxD/tP41z8aqzA2k5Xn7qcHODVI2sXLHJWL92kjVbgEkBPlVw1uNL1h4MQ3RLx9A3itVsBF57VoRyva7h7hzWbXWLpc/U58wfxrtnrV1Z3f0mNYjJz9ZSRz8az/P3i69fU+gGazHtnrHYW62MLfnJBmQjwXy+NZz8utW/Utf8ATP41SXepXF7cyXE5Bkc5PFY4fFZdq3kcLHNAYrUXt39KO3f0rtjC0t7pkI5rS6Tr8luhRiXixyp8PdWGFw48qcS+mT6pHyqXjrU5YjUUUVtkUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQf/9k=">11
            年前 (2014 年 1 月 11 日) — 52:38 <a
                href="https://youtube.com/watch?v=JMrFgnqSS0w">https://youtube.com/watch?v=JMrFgnqSS0w</a></p>
        <p> 11 years ago (Jan 11, 2014) — 52:38 <a
                href="https://youtube.com/watch?v=JMrFgnqSS0w">https://youtube.com/watch?v=JMrFgnqSS0w</a></p>
        <h2 id="unknown-477">未知</h2>
        <h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu.</p>
        <p>教授：今天，我们将在 6034 中引入一个令人兴奋的新承诺。任何已经看过神经网络问题的人都很容易发现，尽管帕特里克现在只掌握了 2006
            年以前的神经网络，但仍然有……在四项测试中，神经网络的绘制方式可能只有两三种。我们令人兴奋的新承诺是，今年我们将以一种特定的方式绘制它们。</p>
        <p>PROFESSOR: Today we are introducing an exciting new pledge in 6034. Anyone who has already looked at any of
            the neural net problems will have easily been able to see that even though Patrick only has them back up to
            2006 now, there’s still. well out of four tests, perhaps two or three different ways that the neural nets
            were drawn. Our exciting new pledge is we’re going to draw them in a particular way this year.</p>
        <p>我会告诉你怎么做，假设这个方法可行。是的。我们将像右边那样画它们。左边的和右边的是一样的。一开始，由于不需要解释它们之间的区别，你可能会认为你想要左边的。但你真正想要的是右边的，我会解释为什么。</p>
        <p>And I will show you which way, assuming that this works. Yes. We are going to draw them like the one on the
            right. The one on the left is the same as the one on the right. At first, not having had to explain the
            difference between the two of them, you might think you want the one on the left. But you really want the
            one on the right, and I’ll explain why.</p>
        <h2 id="unknown-478">未知</h2>
        <h2>Unknown</h2>
        <p>2007 年的测验大致与此类似。不过，如果你在辅导课或其他地方做了一些旧测验，你会发现很多测验都是这样画的。</p>
        <p>The 2007 quiz was drawn, roughly similarly, to this. Although if you somehow wind up in tutorial or somewhere
            else doing one of the older quizzes, a lot of them were drawn exactly like this.</p>
        <p>在这种表示中，我非常不喜欢的一点是，输入被称为 x，输出被称为 y，但有两个 x，因此输入不是 x 和 y，然后它们通常对应于图中的 x，然后人们会感到困惑。许多人遇到的其他问题是，加法和与​​权重的乘法是隐含的。
        </p>
        <p>In this representation, one thing I really don’t like, is that the inputs are called x’s, and the outputs are
            called y’s, but there’s two x’s, so the inputs are not x and y, and then they often correspond to x’s of a
            graph, and then people get confused. Additional issues that many people have are the fact that the summation
            and the multiplication with the weight is implied.</p>
        <p>权重写在输出和输入所在的边缘上，也暗示了两个输入到节点的总和。但看看这里。这是同一个网络。这里的 w 就是写在这些线上的 w。实际上，更好的绘制方式应该是这样的，因为每个 w 都可以有自己的 w，而 w 是不同的。
        </p>
        <p>The weights are written on the edges, where outputs and inputs go, and the summation of the two inputs into
            the node are also implied. But take a look here. This is the same net. These w’s here would be the w’s that
            are written onto these lines are here. Actually the better way to draw it would be like so, since each of
            these can have their own w, which is different.</p>
        <h2 id="unknown-479">未知</h2>
        <h2>Unknown</h2>
        <p>因此，这里的每个 w 都被明确设置为乘数。而在这里，你只需要记住将权重乘以即将传入的输入即可。</p>
        <p>So each of the w’s that are down here, are being explicitly set to a multiplier. Where as here, you just had
            to remember to multiply the weight by the input that was coming by.</p>
        <p>在这里，您可以看到一个输入，它来自一个乘数，您将其乘以权重，然后将所有输入乘以权重，然后将它们发送到一个总和，因此 sigma 只是一个总和，您将它们相加，将它们全部加在一起，将结果发送到 S
            型函数，我们的老朋友，1/1 加上 e 的负数，无论我们的输入是什么，都有一个权重作为偏移量，然后我们将这个结果发送到更多的乘法器中，使用更多的权重、更多的总和、更多的 S 型函数。</p>
        <p>Here you see an input, comes to a multiplier, you multiply by the weight, then once you multiplied all the
            inputs by the weight, then you send them to a sum, so the sigma is just a sum, you sum them, add them all
            together, send the result of that into the sigmoid function, our old buddy, 1 over 1 plus e to the negative
            whatever our input was, with a weight for an offset, and then we send the result of that into more
            multipliers with more weights, more sums, more sigmoids.</p>
        <p>所以这就是它在测验中的样子。这是从 0.9 版数据到 1.0
            版的转换指南。所以如果你在你正在做的一个旧测验中看到类似这样的内容，看看你是否可以转换它，然后解决问题。如果你可以转换它，你很可能会做得很好。我们不仅会从这个转换指南开始，还会。</p>
        <p>So this is how it’s going to look like on the quiz. And this is a conversion guide from version 0.9 data into
            version 1.0. So if you see something that looks like this, on one of the old quizzes that you’re doing, see
            if you can convert it, and then solve the problem. Chances are if you can convert it, you’re probably going
            to do fine. We’ll start off not only with this conversion guide, but also.</p>
        <h2 id="unknown-480">未知</h2>
        <h2>Unknown</h2>
        <p>我会把它留在这里。另外，我会再一次为你们计算公式。这些都是你们在测验中需要的公式。</p>
        <p>I’ll leave that up here. also I’m going to work out the formulas for you guys one more time. These are all
            the formulae that you’re going to need on the quiz.</p>
        <p>然后，我们将决定公式中哪些部分会发生变化，如果（这是非常有可能的，这种情况似乎经常发生）这些神经元中的 S
            型函数曾经被改变为其他类型的函数。提示。在我们即将解决的问题中，它已经变成了一个加号。人们总是把它变成一些奇怪的函数。</p>
        <p>And then we’re going to decide what will change in the formulae, if, and this is a very likely if, there
            seems to be good amount of times that this happens, is that the sigmoid function in those neurons out there
            was ever changed into some other kind of function. Hint. It’s changed into a plus already in the problem
            we’re about to do. People change it all the time into some bizarro function.</p>
        <p>我想我见过反正切。那么我们开始吧。让我们看看它的前面。首先是 S 形函数。我们的老朋友 S 形函数，我刚才说过，S 形函数等于 1/1 加上 e 减去 x。此外，关于 S 形函数还有一个有趣的事实，S
            形函数的导数等于它本身。S 形函数的导数是。假设 S 形函数。我们将 S 形函数变成像字母 y 那样的函数。</p>
        <p>I’ve seen arc tangent, I think. So here we go. Let’s look at the front of it. First of all, sigmoid. Well our
            old buddy, sigmoid, I just said it a moment ago, sigmoid is 1 over 1 plus e to the minus x. Also, fun fact
            about sigmoid, the derivative of sigmoid, is itself. the derivative of sigmoid is. let’s say that the
            sigmoid. we’ll just turn sigmoid into like the letter say y.</p>
        <h2 id="unknown-481">未知</h2>
        <h2>Unknown</h2>
        <p>Y 是结果，对吧？所以如果你说 y 等于 1/1 加上 e 的负 x，那么 sigmoid 的导数就是 y 乘以 1 减去 y。你也可以把整个难懂的东西写出来，它是 1/1 加上 e 的负 x 乘以 1 减去 1/1
            加上 e 的负 x。</p>
        <p>Y is the result, right? So if you say y equals 1 over 1 plus e to the negative x, then the derivative of
            sigmoid is y times 1 minus y. You can also write out the whole nasty thing, it’s 1 over 1 plus e to the
            negative x times 1 minus 1 over 1 plus e to negative x.</p>
        <p>因此，S 型函数的优良特性在不久的将来对我们很重要，而这个未来现在就开始了。现在介绍性能函数。这是我们用来告诉神经网络何时不可避免地出现故障并给出非常糟糕的结果的函数。首先，我们通过性能函数告诉它们它们的长度。</p>
        <p>So the nice property of sigmoid it’s going to be important for us in the very near future, and that future
            begins now. So now the performance function. This is a function we used to tell neural nets when they
            inevitably act up and give us really crappy results. At first we tell them just how long they are, with our
            performance function.</p>
        <p>第一个函数可以是任何合理的函数，只要它能给你一个更好的分数，如果你愿意，可以将“更好”定义为更低或更高，如果你的答案更接近你想要的答案，那么它会给你一个更好的分数。然而，在这种情况下，出于一个非常隐秘的原因，我们选择了性能函数为
            1/2 d，即期望输出减去 o，即实际输出的平方。</p>
        <p>The first function can be any sane function that gives you a better score, where better can be decided as
            lower or higher, if you feel like, that gives you a better score, if your answers are closer to the answer
            you’re looking for. However, in this case, we have, for a very sneaky reason, chosen the performance
            function to be 1/2 d, which is the desired output, minus o, the actual output squared.</p>
        <h2 id="unknown-482">未知</h2>
        <h2>Unknown</h2>
        <p>所以我们想要一个很小的，嗯，它是负数，所以我们想要一个很小的负数或 0。这意味着我们表现良好。那么为什么呢？嗯，主要原因是性能的 ddx 是，2 下降，o 是我们实际的变量，所以也许我应该说
            ddo，那个负数出来了，我们得到一个简单的 d 减去 o。是的，我们在这里使用导数。所以这些都很好。这是两个假设。</p>
        <p>So we want a small, well it’s negative, So we want a small negative or 0. That would mean we performed well.
            So why this? Well the main reason is ddx of performance is, the 2 comes down, the o is the variable that
            we’re actually, so maybe I should say ddo, that negative comes out, we get a simple d minus o. And yeah,
            we’re using derivatives here. So those are fine. These are two assumptions.</p>
        <p>它们可以在测试中更改。我们将弄清楚如果更改它们会发生什么，如果我们更改性能，如果我们更改 S 型函数，也就是说如果我们将 S
            型函数更改为其他函数，接下来的三个函数会发生什么，这基本上是进行反向传播时需要知道的唯一事情。让我们看看。首先，w 素数。这是新权重的公式。</p>
        <p>They could be changed on your test. We’re going to figure out what happens, if we change them, if we change
            the performance, if we change the sigmoid, that is if we change the sigmoid to some other function, what’s
            going to happen to the next three functions, which are basically the only things that you need to know to do
            backpropagation. So let’s look at that. First, w prime. This is the formula for a new weight.</p>
        <p>经过一步反向传播后，在这个漂亮的神经网络中，任何位置都会有一个新的权重。这个权重
            w，每个权重都必须逐步改变。事实上，这就是爬山神经网络的工作原理。你逐步改变权重。你朝着你想要的结果的方向一点点前进，直到最终，你希望拥有一个智能神经网络。</p>
        <p>After one step of backpropagation. A new weight in any of these positions that you can see up here on this
            beautiful neural net. That w. each of the w’s will have to change step by step. That’s, in fact, how you do
            the hill climbing neural nets. You change the weights incrementally. You step a little bit in the direction
            towards giving you your desired results until eventually, you hope, you have an intelligent neural net.</p>
        <h2 id="unknown-483">未知</h2>
        <h2>Unknown</h2>
        <p>也许你有许多不同的训练样例，你循环运行它，希望你不会在计算机上过度拟合你的一个样本。但在测试中，我们可能不会这样做。所以让我们看看如何计算下一级别的权重。然后你有当前级别的权重。所以首先要做的事情。新权重，权重素数相等。从旧权重开始。
        </p>
        <p>And maybe you have many different training examples that you run it on, in a cycle, hoping that you don’t
            over fit to your one sample, on a computer. But on the test, we will probably will not do that. So let’s
            take a look at how you calculate the weights for the next level. And then you have the weights for the
            current level. So first things first. New weight, weight prime equals. starts with the old weight.</p>
        <p>必须把它放在那里，否则我们就会随机跳到某个地方。我们想朝某个方向迈出一小步，所以我们想从我们现在的位置开始，也就是权重。然后我们要添加三个东西。所以如果我们谈论的是 I 和 j
            之间的权重。这里有一些权重名称的例子。所以这是 w 1 i，这是 1 和之间的权重。</p>
        <p>That has to go there because otherwise we’re just going to jump off somewhere at random. We want to make a
            little step in some direction, so we want to start where we are, with the weight. And then we’re going to
            add three things. So if we’re talking about the weight between some I and some j. there’s some examples of
            the names of weights. So this is w 1 i, that’s the weight between 1 and.</p>
        <p>所以这是 w 1 a，它是 1 和 a 之间的权重。这是 w 2 b，它是 2 和 b 之间的权重。说得通吗？到目前为止说得通，但如果它只是称为 wb，那么它就是之间的权重。这些只有一个字母的
            w，我们稍后会讲到。它们是偏差。它们是偏移。</p>
        <p>So this is w 1 a, it’s the weight between 1 and a. This is w 2 b, which is the weight between 2 and b Makes
            sense? Well makes sense so far, but what if it’s just called w b, then it’s the weight between. these w’s
            that only have one letter, we’ll get to later. They’re the bias. They’re the offset.</p>
        <h2 id="unknown-484">未知</h2>
        <h2>Unknown</h2>
        <p>它们总是与负 1 相关联。因此，如果您愿意，您可以将它们视为负 1，然后将其与 wb 一起输入到乘数中。这意味着就是这样。所有偏移量都意味着是这样。所以 w 加上 alpha
            的总和。为什么是这个希腊字母？它来自哪里？我们如何计算它？</p>
        <p>They are always attached to a negative 1. So you can pretty much treat them as being a negative 1 here, that
            is then fed into a multiplier with this w b, if you like. This is implied to be that. All of the offsets are
            implied to be that. So w plus sum of alpha. why is this Greek letter? Where does it come from? How do we
            calculate it?</p>
        <p>嗯，alpha 只是测验中告诉你的一些值。你会在某个地方找到它。你没办法计算 alpha。你可能会被要求尝试给出一个 alpha，但可能不会。alpha 应该给出我们在爬山时迈出的小步的大小。alpha
            非常大，迈出一大步。alpha 非常小，迈出试探性的步伐。</p>
        <p>Well alpha is just some value told to you on the quiz. You’ll find it somewhere. There’s no way you’re going
            to have to calculate alpha. You might be asked to try to give us an alpha, but probably not. Alpha is
            supposed to give the size of our little steps that we take when we’re doing hill climbing. Very large alpha,
            take a huge step. Very small alpha, take tentative steps.</p>
        <p>因此，alpha 基本上可以改变这个答案，使新值非常接近 w 或远离 w，这取决于我们的喜好。因此，alpha 乘以 i，因此 I 是进入节点的值。我们在这里改变权重。因此，I 是值，例如，我在这里减 1，I 将是
            WAC 的值，I 将是节点 a 的输出值。</p>
        <p>So alpha is there, basically, to change this answer and to make the new value either very close to w, or far
            from w, depending on our taste. So plus alpha times i, so I is the value coming in into the node. We’re
            changing the weight here. So I is the value, for instance, I sub 1 here, I would be the value of WAC, I
            would be the value coming output of node a.</p>
        <h2 id="unknown-485">未知</h2>
        <h2>Unknown</h2>
        <p>WBC，I 将是节点 b 的输出。I 有时和 I 一样小，I 是输入，用于在乘数处满足该权重。然后它乘以 delta j。您的 delta 是属于这些神经网络节点的 delta。您说 delta
            是什么？您可能会问这很有趣。这是一个奇怪的希腊字母。</p>
        <p>WBC, I would be the output of node b. I is sometimes as little as I is the input coming in to meet that
            weight at the multiplier. And then it’s multiplied by delta j. Your delta is the delta that belongs to these
            neural net nodes. What is a delta, you said? Funny you may ask. It is a strange Greek letter.</p>
        <p>这在某种程度上是因为我们正在做一些偏导数之类的事情，但你要弄清楚增量的主要方法是这两个我还没有写出来的公式。所以先不要急着弄清楚增量是什么。现在，我要告诉你增量是什么。</p>
        <p>It sort of comes from the fact that we’re doing some partial derivatives and stuff, but the main way you’re
            going to figure out what the deltas are these two formulae that I’ve not written in yet. So hold off on
            trying to figure out what the delta is until. well right now, I’m about to tell you the delta is.</p>
        <p>因此，delta 基本上就是，当你在爬山时，delta 就像使用偏导数来确定你要往哪个方向迈步一样。因为你知道当你在爬山时，你会环顾四周，弄清楚，好吧，这是最高增幅的方向，然后你朝那个方向迈步。所以 delta
            会告诉你要往哪个方向迈步，以及权重。</p>
        <p>So the delta is basically, think of the delta as using partial derivatives to figure out which way you’re
            going to step, when you’re doing hill climbing. Because you know when you’re doing hill climbing, you look
            around, you figure out, OK, this is the direction of the highest increase, and then you step off in that
            direction. So the deltas are telling you which way to step, with the weights.</p>
        <h2 id="unknown-486">未知</h2>
        <h2>Unknown</h2>
        <p>他们这样做的方法是取偏导数。基本上，您要尝试弄清楚您当前正在查看的权重对网络性能的影响。无论是对网络的良好性能还是对网络的不良性能都有影响。因此，当您处理权重（如 WBC、WAC）时，它们几乎直接影响网络的末端。
        </p>
        <p>And the way they do that is by taking the partial derivative of. basically you try to figure out how the
            weight that you’re currently looking at is contributing to the performance of the net. Contributing to,
            either the good performance of the net, or the bad performance of the net. So when you’re dealing with the
            weights, like WBC, WAC, that pretty much directly feed into the end of the net.</p>
        <p>它们被输入到最后一个节点，然后输出。这就是输出。这很简单。你可以准确地知道这些权重以及来自它们的值对最终结果的贡献有多大。我们这样做本质上是通过记住偏导数，所以这里的偏导数实际上就是最终权重对性能的贡献方式，只是性能函数。偏导数。
        </p>
        <p>They feed into the last node, and it then comes out. It’s the output. That’s pretty easy. You can tell
            exactly how much those weights, and the values coming from them, are contributing to the end. And we do that
            by essentially, remember what the partial derivative, so partial derivative here is, in fact, the way that
            the final weights are contributing to the performance, is just the performance function. Partial derivative.
        </p>
        <p>我已经算出了这里的导数，它就是 d 减 o。这相当于最终权重，即最后一级的权重。D 减 o，但我们还没有完成，因为当我们求导数时，请记住链式法则。</p>
        <p>I’ve already figured out the derivative here, it’s just d minus o. This is for sort of final weights, the
            weights in the last level. D minus o, except we’re not done yet, because when we do derivatives, remember
            the chain rule.</p>
        <h2 id="unknown-487">未知</h2>
        <h2>Unknown</h2>
        <p>为了从末端得到这些权重，我们经过，嗯，它应该是一个 S 形函数，但这里不是，我们暂时假设它是，我们经过一个 S 形函数，既然我们经过了 S 形函数，我们最好取 S 形函数的导数。即 y 乘以 1 减 y。那么 y
            是多少？S 形函数的输出是多少？它是向上的。</p>
        <p>To get from the end to these weights, we pass through, well it should be a sigmoid, here it’s not, we’re
            going to pretend it is for the moment, we pass through a sigmoid, and since we passed through the sigmoid,
            we had better take the derivative of the sigmoid function. That is, y times 1 minus y. Well what is y? What
            is the output of the sigmoid? It’s up.</p>
        <p>所以这也乘以 o 乘以 1 减
            o。但是，有一个。让我看看，让我看看，是的。抱歉，我正在仔细研究这张表，以确保我的命名法完全符合我们的新命名法，这个命名法如此新颖和大胆，以至于我们正在做这件事，我们只知道我们肯定会在周三做这件事。所以我们有 d 减
            o 乘以 o 乘以 1 减 o。</p>
        <p>So that’s also multiplied by o times 1 minus o. However, there is a. let me see, let me see, yes. sorry, I’m
            carefully studying this sheet to make sure my nomenclature is exactly right for our new nomenclature, which
            so new and brave, that we’re doing it, that we only knew for sure we’re going to do it on Wednesday. So we
            have d minus o times o times 1 minus o.</p>
        <p>所以你会说，这很好，这可以让我们得到这里的权重，甚至是这个 wc，我们如何获得这里的新权重的增量？哦，我明白了。是的，我明白了。所以增量。顺便说一下，这是一个增量 c，神经元 c
            对输出有何贡献？嗯，它直接对输出有贡献，它里面有一个 S 形函数。其实不是，但我们现在假装它有。</p>
        <p>So you say, that’s fine, that can get us these weights here, even this w c, how are we going to get the
            deltas for the new weights here? Oh, I realize. yeah, I got it. So the delta. by the way, this is a delta c,
            how is neuron c contributing to the output? Well it’s directly contributing to the output,and it’s got a
            sigmoid in it. It doesn’t really, but we’re pretending it does for now.</p>
        <h2 id="unknown-488">未知</h2>
        <h2>Unknown</h2>
        <p>D 减 o 乘以 1 减 o。内部节点呢？节点 d、节点 a，我们要做什么？它们对输出的贡献方式是它们对节点 c 的贡献。所以我们可以递归地解决这个问题。所以让我们递归地做这件事。</p>
        <p>D minus o times 1 minus o. What about inner node? Node d, node a, what are we going to have to do? Well the
            way they contribute to the output is that they contribute to node c.&nbsp;So we can do this problem
            recursively. So let’s do this recursively.</p>
        <p>首先，正如您可能已经猜到的那样，根据链式法则，它们都将有一个 o 乘以 1 减去 o 的因式分解，因为它们都是 S 型函数，假设它们都是 S 型函数。目前网络上真正 S 型函数的好问题也很少。只有 2007 年。但这是
            o 乘以 1 减去 o，我们该如何处理其余的问题？</p>
        <p>First of all, as you have probably figured out, all of them are going to have an o times 1 minus o factoring
            from the chain rule, because they’re all sigmoid, pretending that they’re all sigmoids. We also have a
            dearth of good problems that are actually sigmoid on the web right now. There’s only 2007. But here’s o
            times 1 minus o, what are we going to do for the rest of it?</p>
        <p>它对我们的最终结果有何贡献？嗯，它以递归方式对我们的最终结果做出贡献。所以我们讨论的是 delta i。I 是一个内部节点。它不是最终节点。它位于路径的某个位置。因此，对 w 的 j 求和，从 I 到 j，乘以
            delta j。现在对所有 j 求和，j 使得 I 指向 j。I 需要有一条直接通向 j 的路径。</p>
        <p>How does it contribute to our final result? Well it contributes to our final result recursively. So we’re
            talking about delta i. I is an inner node. It’s not a final node. It’s somewhere along the way. So sum over
            j of w, going from I to j, times delta j. Now sum over all j, j such that I leads to j. I needs to have a
            direct path into j.</p>
        <h2 id="unknown-489">未知</h2>
        <h2>Unknown</h2>
        <p>因此，如果本例中的 i 是 j，那么所有人，唯一可能的 j 就是 c。没错。我们不会将 b 作为 j 之一进行求和。i 不会导致 b，或者 a 不会导致 b，a 只会导致 c。还要注意，c
            不必在这里。这是倒退。所以你只需。</p>
        <p>So if i, in this instance, was j, everyone, the only possible j in this would be c.&nbsp;That’s right. We
            would not sum over b as one of the j. I does not lead to b, or a does not lead to b, a only leads to
            c.&nbsp;Also note that c does not need to be here. That’s going backwards. So you just.</p>
        <p>要弄清楚您正在查看哪个 j，请直接向前查看下一个 j。因此，如果这里有另一个 d，或者 a 不转到 d，则 a 转到
            c。您只需查看下一级子级，然后将所有这些子级相加，计算它们之间的权重，再乘以子级的增量。这很合理，对吧？</p>
        <p>To figure out which j you’re looking at, look directly forwards at the next one. So if there was another d
            here, or that a does not go to d, a goes to c.&nbsp;You only look at the next level children, and you sum
            over all those children, the weight between them, multiplied by the child’s delta. That makes sense, right?
        </p>
        <p>因为我们影响的方式，如果子节点的增量是子节点影响输出的方式，暂时称这些子节点为，然后如果这个直接影响输出，那么这个影响它的方式就是。它影响它，因为它影响这个，但它也乘以它的权重。所以事实上，例如，如果 a 和 c
            之间的权重为 0，那么 a 根本不影响输出，对吗？</p>
        <p>Because the way we affect, if the child’s delta is the way the child affects the output, calling these
            children for a moment, and then if this one directly affects the output, then the way this one affects it
            is. it affects it because it affects this, but it’s also multiplied by it’s weight. So in fact, for
            instance, if the weight between a and c were 0, then a doesn’t affect the output at all, right?</p>
        <h2 id="unknown-490">未知</h2>
        <h2>Unknown</h2>
        <p>因为它的权重是 0，当我们做这个问题时，我们将其乘以 0，然后尝试将其添加到那里，不会影响任何东西。它的权重非常高，它将真正主导 c，这里会考虑到这一点，然后乘以右侧节点的增量。</p>
        <p>Because its weight is 0, and when we do this problem, we go this times 0, and then we try to add it in there,
            doesn’t affect anything. It’s weight is very high, it’s going to really dominate c, and that is taken into
            account here, and then multiply by the delta for the right node.</p>
        <p>因此，对于以下问题，由于我花了很多时间研究公式，而没有花太多时间着手解决问题，因此我不会随机找人回答，而是找一个志愿者。如果没有人自愿回答，我最终会告诉你，也就是说，我们在最后三个公式上有一些不错的公式。如果我们改变
            S 型函数，需要改变什么？</p>
        <p>So the following question, and since I spent a lot of time with formulae and not that much time starting on
            the problem, I will not call on someone at random, but rather take a volunteer. If no one volunteers, I’ll
            eventually tell you, which is, we’ve got some nice formulae on the bottom three. If we change the sigmoid
            function, what has to change?</p>
        <p>这是这个疯狂问题中唯一改变的地方，顺便说一下，将 S 型函数改为加法器，就是我们将 o 乘以 1 减去 o 的 delta f 和 delta i
            全部取整，然后将其改为新的导数。然后我们做和我们之前做的一模一样的事情。正确。</p>
        <p>That’s the only thing that changes in this crazy assed problem right here, which by the way, changes the
            sigmoid functions into adders, is that we take all of the o times 1 minus o in delta f and the delta i, and
            we change it to a new derivative. We then do the exact same thing that we would’ve done. Correct.</p>
        <h2 id="unknown-491">未知</h2>
        <h2>Unknown</h2>
        <p>同样，如果你改变性能函数，下面三个方程中有多少个需要改变？是的。没错，只有一个，只是 delta f。取 d 减 o，使其成为新性能函数的新导数。事实上，delta I 根本没有变化。大家看到了吗？</p>
        <p>And on a similar note, if you change the performance function, how many of these equations at all have to
            change out of the bottom three. Yeah. That’s right, just one, just delta f.&nbsp;Take the d minus o, make it
            the new derivative of the new performance function. And in fact, delta I doesn’t change at all. Does
            everyone see that?</p>
        <p>因为更换某些东西是很常见的，我认为我们现有的四份测验中有三份在某些方面被更换了。在某些方面改变了一些东西。好的。我们开始吧。我们将进行 2008
            年测验，因为它的结尾部分让每个人都很困惑，所以让我们确保我们能够完成这一部分。这可能是您目前最关心的部分。</p>
        <p>Because it is very common for something to be replaced, I think three of the four the quizzes that we have,
            replaced in some. changed something in some way. All right. Let’s go. We’re going to do 2008 quiz, because
            it has a part of the end that screwed up everyone, and so let’s make sure we get to that part. That’s going
            to be the part that you probably care about the most at this point.</p>
        <p>所以这些都是加法器，而不是 S 型函数。这意味着它们只是像平常一样将所有东西加起来，对于正常的神经网络来说，没有 S
            型函数阈值。它们只是给出某种值。问题？学生：所以我们讨论了那些乘数，节点中没有这些？教授：它们不是神经网络节点。这就是为什么你在那边看到的另一种形式很优雅的原因之一。</p>
        <p>So these are all adders instead of sigmoids. That means that they simply add up everything as normal, for a
            normal neural net, and then there’s no sigmoid threshold. They just give some kind of value. Question?
            STUDENT: So we talked about those multiplier things, we don’t have those in nodes? PROFESSOR: They’re not
            neural net nodes. That is one of the reasons why that other form that you can see over there is elegant.</p>
        <h2 id="unknown-492">未知</h2>
        <h2>Unknown</h2>
        <p>它上面只有实际节点。它非常紧凑。这是我们在之前的测试中使用的前端之一。问题是，这些乘数算作节点吗？然而，如果不放入乘数，我们觉得有时会让人感到困惑。</p>
        <p>It only has the actual nodes on it. It is very compact. It’s one of the front we’ve used in the previous
            tests. The question is, do those multipliers count as nodes? However by not putting in the multipliers, we
            feel it sometimes confuses people of explicitness.</p>
        <p>节点总是带有标签，例如，或者这里，您会看到一个 S 形函数和一个 L1。乘数是为了方便您使用，提醒您进行乘法运算，此外，如果您查看那里的 S
            形函数，它们是为了方便您使用，提醒您进行加法运算。事实上，在神经网络中，只有节点才算一个。这是一个非常好的问题。</p>
        <p>The ones that are nodes will always have a label, like a or here, you see there’s a sigmoid and an L1. The
            multipliers are there for your convenience, to remind you to multiply, and also those, if you look those
            sigmoids that are over there, are there for your convenience to remind you to add. In fact, the only thing
            that counts as a node in the neural net. and that’s a very good question.</p>
        <p>通常是 S 形函数，这里是加法器。我们基本上去掉了 S 形函数。这些加法器是。哦，这是区分的方法。如果它有一个与之相关的阈值权重，那么它就是实际节点之一。阈值权重。我猜乘法器看起来像是有权重，但这只是被乘以的权重。
        </p>
        <p>Is usually the sigmoids, here it’s the adders. We’ve essentially taken out the sigmoids. These adders are
            the. oh, here’s the way to tell. If it’s got a threshold weight associated with it, then it’s one of the
            actual nodes. A threshold weight. I guess the multipliers look like they have a weight, but this is just the
            weight that is being multiplied in.</p>
        <h2 id="unknown-493">未知</h2>
        <h2>Unknown</h2>
        <p>这是我们的见证，与输入相乘，但如果它有一个阈值权重，如 wa、wb。哦，我答应过我会告诉你们这两个权重之间的区别。让我们很快做完。权重类型，比如 w2b 或 w1a，我们的权重介于输入 1 和 a 之间或介于 a 和
            c 之间，然后在心里将输入乘以这个权重，然后最终将它们相加。</p>
        <p>This is our witness be multiplied in with the input, but if it has a threshold weight, like wa, wb. oh, I
            promised I would tell you guys the difference between the two weights. So let’s do that very quickly. The
            kinds of weights that, say w2b or w1a, our weight the comes between input 1 and a or between a and c, then
            mentally multiplying the input by this weight, and then eventually that’s added together.</p>
        <p>阈值权重，它们只有像 wb、wa、wc 这样的值。它们本质上是决定成功或失败的阈值，是 1 还是
            0，或者介于两者之间的任何值，在任何给定节点上。因此，这个想法是，也许你在某个节点想要有一个非常高的截止值，你必须输入非常高的值，否则它就是 0。所以你设置了一个高阈值。</p>
        <p>The threshold weights, they just have like wb, wa, wc. They are essentially to decide the threshold for a
            success or failure, for a 1 or a 0, or anything in between, at any of the given nodes. So the idea is maybe
            you at some node want to have a really high cut off, you have to very high value coming in, or else it’s a
            0. So you put a high threshold.</p>
        <p>权重乘以-1。事实上，阈值权重不会。如果您愿意，可以考虑将阈值权重乘以-1。也添加到该总和中，而不是放在与节点相同的位置。如果这对您来说更有效，那么在转换时，您也可以这样想。</p>
        <p>The weight is multiplied by negative 1. And in fact, the threshold weight won’t. one could consider if you
            wanted to that the threshold weight times negative 1. was also added in it that sum, instead of putting at
            the same location as the node. If that works better for you, when you’re converting it, you can also think
            of it that way.</p>
        <h2 id="unknown-494">未知</h2>
        <h2>Unknown</h2>
        <p>因为阈值权重本质上是乘以负 1，然后以相同的和加在那里。所以这是另一种方法。有很多方法可以可视化这些神经网络。</p>
        <p>Because the threshold weight is essentially multiplied by negative 1 and added in at that same sum over
            there. So that’s another way to do it. There’s a lot of ways to visualize these neural nets.</p>
        <p>只要确保你有一种对你来说有意义的方法，并且你能分辨出我们写的任何内容（只要它看起来有点像），如何将它融入你的脑海中，融入适合你的表示中。因为一旦你有了适合你的表示，你就已经解决了这些家伙的一半问题。它们没那么糟糕。它们只是看起来很讨厌。它们不会咬人。好吧。这些只是加法器。
        </p>
        <p>Just make sure you have a way that makes sense to you, and that you can tell pretty much whatever we write,
            as long as it looks vaguely like that, how to get it in your mind, into the representation that works for
            you. Because once you have the representation right for you, you’re more than halfway to solving these guys.
            They aren’t that bad. They just look nasty. They don’t bite. OK. These are just adders.</p>
        <p>因此，如果它只是一个加法器，那么这意味着，如果我们将所有 x 输入都输入进来。我们暂时做 x 和 y，这样我们就可以计算出导数。那么，我们将 x 相加后得到的结果是，y 等于
            x，对吗？我们只是将其相加。将所有输入相加，我们不会对它做任何事情。Y 等于 x 是此节点所做的。</p>
        <p>So if it’s just an adder, then that means that, if we take all the x inputs coming in. let’s do x and y for
            the moment, so we can figure out the derivative. then what comes out after we just add up the x, what comes
            out, y equals x, right? We’re just adding it up. Adding up all the input, we’re not doing anything to it. Y
            equals x is what this node does.</p>
        <h2 id="unknown-495">未知</h2>
        <h2>Unknown</h2>
        <p>你们看到了吗？所以导数就是 1。所以这很简单，因为第一个问题说的是，新的公式是什么，delta f。所以我就告诉你们吧。你们可能已经算出来了。o 乘以 1 减去 o。因为我们用 1 替换了 d 减去
            o。好吗？到目前为止，明白了吗？请一路提问，因为我不会问你们。我会自己做。问题？</p>
        <p>You people see that? So the derivative is just one. So that’s pretty easy, because the first problem says,
            what is the new formula, delta f.&nbsp;So I’ll just tell you. You guys probably figured it out. o times 1
            minus o. Because we replaced d minus o with 1. OK? Makes sense so far? Please ask questions along the way,
            because I’m not going to be asking you guys. I’ll do it myself. Question?</p>
        <p>学生：为什么我们要用 d 减 o 等于 1？教授：这个问题问得好。原因是我做错了。所以，你们问问题很好。实际上应该用 o 乘以 1 减去 o 等于 1 来代替。答案是 delta f 等于 d 减
            o。所以是的，也许我这样做是为了欺骗你。不，我实际上搞砸了。但是是的，请一路提问。</p>
        <p>STUDENT: Why to we use d minus o of 1? PROFESSOR: That’s a good question. The reason is because I did the
            wrong thing. So see, it’s good that you guys are asking questions. It actually should be replaced with o
            times 1 minus o with 1. The answer is delta f equals d minus o. So yes, perhaps I did it to trick you. No, I
            actually messed up. But yes, please ask questions along the way.</p>
        <p>再次强调，我没有时间随机点名你们，看看你们是否跟上。所以我自己来做。我们将 o 乘以 1 减去 o 放在 1 的位置，因为 S 形函数已经消失，我们得到的只是 delta f 等于 d 减去
            o。太好了。我们现在想知道节点 a 处 delta i 的方程式是什么。所以 delta a。</p>
        <p>Again, I don’t have time to call on you guys at random to figure out if you guys are following along. So I’ll
            do it myself. We’re placing the o times 1 minus o with 1 because of the fact that the sigmoid is gone, and
            we get just delta f equals d minus o. So great. We now want to know what the equation is for delta i, at the
            node a. So delta a.</p>
        <h2 id="unknown-496">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，我们来看看。o 乘以 1 减去 o 消失了。现在我们只剩下 j 的和，你们已经告诉我了，也就是 WAC 的 c 乘以 delta c。我们知道 delta c 等于 d 减去 o。答案是 delta a 等于
            WAC 乘以 d 减去 o。那一次，我答对了。我在这里看到了答案。虽然它的格式与旧测验大不相同。</p>
        <p>Well let’s take a look. The o times 1 minus o is gone. Now we just have the sum over j, which you guys
            already told me is, only c of WAC times delta c.&nbsp;We know that delta c is d minus o. The answer is delta
            a is just WAC times d minus o. That time, I got it right. I see the answer here. Though it’s written in a
            very different format from the old quiz.</p>
        <p>对此有什么问题吗？好吧，这是我们在 c 中完成的 a 部分。我们来看 b 部分。b 部分是进行一步反向传播。这里几乎总是会有其中之一。</p>
        <p>Any questions on that? Well that’s part a that we finished out of c.&nbsp;Let’s go to part b. Part b is doing
            one step backpropagation. There’s almost always going to be one of these in here.</p>
        <p>因此，它要求做的第一件事是找出这个神经网络的输出 o 是什么，如果所有权重最初都是 1，除了这个是 -0.5。所有其他的都从 1 开始。让我们来做一步。哦，让我们看看输入是什么。</p>
        <p>So the first thing it asks is to figure out what the output o is for this neural net if all weights are
            initially 1 except that this guy right here is negative 0.5. All the other ones start off as 1. Let’s do a
            step. oh, let’s see what are the inputs.</p>
        <h2 id="unknown-497">未知</h2>
        <h2>Unknown</h2>
        <p>输入也全部为 1。期望输出也为 1。事实上，速率常数 alpha 也为 1。这是唯一不为 1 的数。让我们看看会发生什么。1 乘以 1 等于 1，那么这就是负 1 乘以 1 等于负 1。也就是
            0。这里也发生了完全相同的事情，因为它是对称的。</p>
        <p>The inputs are also all 1. Desired output is also 1. And in fact, the rate constant alpha is also 1. This is
            the only thing that isn’t 1, folks. So let’s see what happens. 1 times 1 is 1, then this is a negative 1
            times 1 is negative 1. That’s 0. The exact same thing happens here because it’s symmetrical.</p>
        <p>所以这些都是 0.0 乘以 1 等于 0，0 乘以 1 等于 0。那么这是负 1 乘以负 0.5 等于正 0.5，所以 0 加 0 加正 0.5，输出就是正 0.5。大家看到了吗？如果没有，你可以说服自己它是正
            0.5。这对你来说是个很好的练习，进行一次向前运行。输出肯定是正 0.5。第一次。好吗？</p>
        <p>So these are both 0.0 times 1 is 0,0 times 1 is 0. Then this is negative 1 times negative 0.5 is positive
            0.5, so 0 plus 0 plus a positive 0.5, the output is positive 0.5. Does everyone see that? If not, you can
            convince yourself that it is positive 0.5. That would be a good exercise for you, run through one forward
            run. The output is definitely positive 0.5. First time around. OK?</p>
        <p>现在我们必须进行一步反向传播。为此，让我们计算所有的增量，以便计算所有新的权重，新的权重素数。所以增量 c。这很简单。你们可以告诉我增量 c 是什么。我们算出了新的增量 c 是多少。那么简单的加法或减法问题？各位，增量
            c 是多少？学生：0.5。教授：0.5，一半，是的。好的。</p>
        <p>Now we have to do one step of backpropagation. To do that, let’s calculate all the delta so that we can
            calculate all the new weights, the new weight primes. So delta c.&nbsp;That’s easy. You guys can tell me
            what delta c is. We figured out what the new delta c is going to be. So simple addition or subtraction
            problem? Everyone, delta c is? STUDENT: 0.5. PROFESSOR: 0.5, one half, yes. All right.</p>
        <h2 id="unknown-498">未知</h2>
        <h2>Unknown</h2>
        <p>我们知道 delta a 和 delta b 就是 WAC 乘以 delta c，WBC 乘以 delta c。那么它们是多少？学生：一半。教授：也是一半，因为所有权重都是 1。很简单。好的。我们得到所有 delta
            都是一半。除了少数权重外，其他权重都是 1。那么让我们算出新的权重是多少。新的 WAC，好的。是的，让我们看看。</p>
        <p>We know that delta a and delta b are just WAC times delta c, and WBC times delta c.&nbsp;So they are?
            STUDENT: One half. PROFESSOR: Also one half, because all the weights were 1. Easy street. OK. We’ve got all
            of the deltas are one half. And all but a few of the weights are 1. So let’s figure out what the new weights
            are. New WAC, OK. Yeah, so let’s see.</p>
        <p>新的 WAC 会是多少？所以新的 WAC 将是旧的 WAC，也就是 1，因为除了 wc 之外，其他都是 1，再加上速率常数 1，乘以这里的输入，但请记住那是 0，所以实际上它与旧的 WAC 相同。这是 b 和 a
            之间的度量问题，目前，这将是相同的。</p>
        <p>What’s going to be the new WAC? So the new WAC is going to be old WAC, which is 1, because all of them are 1
            except for wc, plus the rate constant which is 1, times the input coming in here, but remember that was 0,
            so actually it’s just going to be the same as the old WAC. This is a metrical problem between b and a, at
            the moment, this is going to be the same.</p>
        <p>好的。不过有些事情会改变。wc 呢？它实际上不是 1？好的。所以新的 wc，记住，wc 的 I，我们在这个等式中使用的 I 始终为 -1，因为它是一个阈值。所以我们有旧的 wc，它是 -0.5，加上 1 乘以 -1
            乘以 delta c，它是一半。</p>
        <p>All right. Somethings are going to change though. What about wc, that was the one that was actually not 1?
            OK. So new wc, remember, the I for wc, the I that we use in this equation is always negative 1 because it’s
            a threshold. So we have the old wc, which is negative 0.5, plus 1 times negative 1 times delta c, which is
            one half.</p>
        <h2 id="unknown-499">未知</h2>
        <h2>Unknown</h2>
        <p>因此，-0.5 加 -0.5 等于 -1。w 1 a，w 1 a 一开始为 1。然后我们还知道 w 1 a 等于 1 加 1 乘以输入，也就是 1 乘以 a 的增量，也就是二分之一，也就是 1.5。由于 a 和 b
            之间是对称的，所以 w 2 b 也是 1.5。最后，wa 和 wb，这里的偏移量，一开始为 1 加 1 乘以 -1 乘以 0.5。所以它们都是，各位？</p>
        <p>So we have negative 0.5 plus negative 0.5 equals negative 1. w 1 a, well we’ve got w 1 a starts out as 1.
            Then we also know that w 1 a is going to be equal to 1 plus 1 times the input, which is 1, times delta of a,
            which is one half, so 1.5. And since it’s symmetrical between a and b, then w 2 b is also 1.5. And then
            finally, wa and wb, the offsets here, well they start at 1 plus 1 times negative 1 times 0.5. So they’re
            both, everyone?</p>
        <p>学生：一半。 教授：一半。没错。没错。因为负 1 是它们的 i。负 1 乘以一半加上正 1
            等于一半。这就是完整的一步。可能比你习惯看到的要容易一些，但这是一个完整的步骤。它问的是经过一步反向传播后输出会是什么？我们可以看看。</p>
        <p>STUDENT: One half. PROFESSOR: One half. That’s right. That’s right. Because negative 1 is their i. Negative 1
            times one half plus positive 1 is just one half. That’s one full step. Maybe a mite easier than you might be
            used to seeing, but there’s a full step. And it asks what’s going to be the output after one step of
            backpropagation? We can take a look.</p>
        <p>因此，我们有 1 乘以新的 wa，也就是 1.5，你有 1.5，那么新的 wa 就是 0.5，现在是 0.5，这是一个进入加法器的 1。我们在这里得到了另一个 1，因为它是对称的。所以 1 和 1，1 乘以 WAC
            等于 1.1 乘以 WBC 等于 1。所以我们这里有两个 1，它们相加，就是 2。那么实际上，此时它已经变成了负 1。</p>
        <p>So we have 1 times the new wa, which is 1.5, you’ve got 1.5, then the new wa is just 0.5, now is 0.5, that’s
            a 1 coming into an adder. We’ve got another 1 coming in here because it’s symmetrical. So 1 and a 1,1 times
            WAC is 1.1 times WBC is 1. So we have two 1s coming in here, they’re added, that’s 2. Then this has become
            negative 1, in fact, at this point.</p>
        <h2 id="unknown-500">未知</h2>
        <h2>Unknown</h2>
        <p>所以 -1 乘以 -1，等于 3，输出结果为 3。好的。太棒了。现在我们已经完成了 b 部分，也就是所有内容的一半以上。哦不，我们还没有完成。还有一件事。这些是加法器。它们不是 S 型函数。</p>
        <p>So negative 1 times negative 1, that’s 3, and the output is 3. All right. Cool. We’ve now finished part b,
            which is over half of everything. Oh no, we’ve not. One more thing. These are adders. They’re not sigmoids.
        </p>
        <p>如果我们训练整个神经网络来尝试学习这些数据，以便它可以在图形上画一条线，或者画一些线，或者进行某种学习，将缺点与优点区分开来。</p>
        <p>What if we train this entire neural net to try to learn this data, so that it can draw a line on a graph, or
            draw some lines, or do some kind of learning, to separate off the minuses from all the pluses.</p>
        <p>你可能已经看到了，如果没有，你马上就会看到了，因为它要求你详细地做这件事，神经网络通常可以在图上为网络中的每个节点画一条线，因为每个节点都有某种阈值。你可以在它们之间做一些逻辑运算，比如“与”或“或”。你们觉得这个网络会画什么？
        </p>
        <p>You’ve seen, maybe, and if not, you are about to in a second, because it asks you to do this in detail, than
            neural nets can usually draw one line on the graph for each of these, sort of, nodes in the net, because
            each of the nodes has some kind of threshold. And you can do some logic between them like ands or ors. What
            do you guys think this net is going to draw?</p>
        <h2 id="unknown-501">未知</h2>
        <h2>Unknown</h2>
        <p>任何人都可以自愿回答，我不会要求任何人给出这个答案。这有点棘手，因为通常如果你有这么多节点，你可以轻松地画一个框，并将减号和加号分开。然而，它画出了这个。它问错误是什么？错误是。哦，是的，它甚至告诉你错误是
            1/8，因为为什么？这些都是加法器。你实际上不能做任何合乎逻辑的事情。</p>
        <p>Anyone could volunteer, I’m not going to ask anyone to give this answer. That’s a little bit tricky, because
            usually if you had this many nodes, you could easily draw a box and box off the minuses from the pluses.
            However, it draws this. And it asks what is the error? The error is. oh yeah, it even tells you the error is
            1/8, because why? These are all adders. You can’t actually do anything logical.</p>
        <p>整个网络归结为一个节点，因为它每次都加起来。它从来不在任何时候取阈值。所以你不能变成逻辑上的 1 和
            0，因为它基本上不是数字的，而是模拟的。它给了我们一个非常高的数字。所以一切都归结为一个截止点。这是最好的一个。我在这里画的那个。好的。这对你来说没有意义吗？</p>
        <p>This entire net boils down to just one node, because it just adds up every time. It never takes a threshold
            at any point. So you can’t turn into logical ones and zeroes, because it’s basically not digital at all, its
            analog. It’s giving us some very high number. So it all boils down to one cut off. And that’s the best one.
            The one that I drew right here. OK. Did that not make sense to you?</p>
        <p>没关系。这个问题更难。把它们放在同一个测验中，有点残酷，但当你完成这个后，你就会明白神经网络能做什么，不能做什么。我把它们简化了，因为我们不关心它们的值或类似的东西。但这些小圆圈里面是一个 S
            形函数，乘数和求和函数是隐含的。</p>
        <p>That’s OK. This problem is much harder. And putting them both on the same quiz, was a bit brutal, but by the
            time you’re done with this, you’ll understand what a neural net can do or not. I put these in simplified
            form because of the fact that we don’t care about their values or anything like that. But inside of these
            little circles is a sigmoid, the multipliers and the summers are implied.</p>
        <h2 id="unknown-502">未知</h2>
        <h2>Unknown</h2>
        <p>我认为，当我们实际上不进行反向传播时，简化形式更容易查看它，并查看有多少个节点。出于同样的原因，你问了有多少个节点的问题。所以所有这些大圆圈都是节点。这些节点中现在是一个 S
            形函数，而不是那些疯狂的加法器。我们有以下问题。</p>
        <p>I think in the simplified form when we’re not actually doing backpropagation is easier to view it, and see
            how many nodes there are. For the same reason you asked your question about how many there are. So all of
            those big circles are node. And in those nodes is a sigmoid now, not those crazy adders. We have the
            following problem.</p>
        <p>我们必须尝试将 a、b、c、d、e、f 中的每一个与 1、2、3、4、5、6
            进行匹配，并且每个只使用一次。这很重要，因为这里的一些更强大的网络可以完成很多任务。所以，就像是的，强大的网络可以解决这里的一些较简单的问题，但我们希望将每个网络与它可以解决的问题进行匹配，并且只有一个映射可以进行映射。
        </p>
        <p>We have to try to match each of a, b, c, d, e, f to 1,2.3,4.5,6, using each of them only once. That’s
            important, because some of the more powerful networks in here can do a lot of these. So it’s like yes, the
            powerful networks could do some of the easier problems here, but we want to match each net to a problem it
            can do, and there is exactly one mapping that will map.</p>
        <p>这是一对一的，并且完全映射，使用所有六个网络来解决这里的所有六个问题。所以你们中的一些人可能会想，什么？我要如何解决这些问题？我之前给了一个提示，即神经网络中的每个节点，每个 S
            形节点通常可以在图片上画一条线。它可以在图片中画一条线。</p>
        <p>That is one to one, and maps exactly, uses all six of the nets to solve all six of these problems here. So
            some of you may be going like, what? How am I going to solve these problems? I gave away a hint before,
            which is that each node in the neural net, each sigmoid node can usually draw one line on the. it can draw
            one line into the picture.</p>
        <h2 id="unknown-503">未知</h2>
        <h2>Unknown</h2>
        <p>如果节点接收两个输入，即这里的 I 1 和 I 2，则线可以是对角线。请注意，有一个 I 1 轴和一个 I 2 轴。就像 x 轴和 y
            轴一样。节点必须是水平的，或者垂直的。抱歉，如果节点只接收其中一个输入，则线必须是水平的或垂直的。</p>
        <p>The line can be diagonal if that nodes receives both of the inputs, which is here, I 1 and I 2. See there is
            an I 1 and an I 2 axis. Like x and a y axis. The node has to be horizontal, or vertical, if. sorry, the line
            has to be horizontal or vertical if the node only receives one of the inputs.</p>
        <p>然后，如果你有更深层次的东西，这些次级节点可以做一些合乎逻辑的事情，可以做一些很棒的事情，比如前两个节点的和或，这可以帮助你。好的。让我们试着弄清楚。</p>
        <p>And then, if you have a deeper level, these secondary level nodes can sort of do a logical, can do some kind
            of brilliant thing like and or of the first two, which can help you out. All right. And so let’s try to
            figure it out.</p>
        <p>所以一开始，我希望大家能帮忙并指出这一点，因为我知道我们没有足够的时间强迫你们都明白。但一开始，哪一个看起来是最简单的？学生：六。教授：六。太好了。六绝对是最简单的。它是一条线。</p>
        <p>So right off the bat, and I hope that people will help and call this out, because I know we don’t have enough
            time that I can force you guys to all get it. But right off the bat, which one of these looks like it’s the
            easiest one? STUDENT: Six. PROFESSOR: Six. That’s great. Six is definitely the easiest one. It’s a single
            line.</p>
        <h2 id="unknown-504">未知</h2>
        <h2>Unknown</h2>
        <p>所以这就是我解决这个问题的方法，就是找出最简单的一个。哪一个是最糟糕的网络？学生：A。教授：A 是最糟糕的网络。但是 A 绝对不可能得到除了六之外的任何一个。所以让我们马上说六是 A。好的。六是 A。那就是
            A。我们不必担心 A。好的。很酷。</p>
        <p>So this is just how I would have solved this problem, is find the easiest one. Which of these is the
            crappiest net? STUDENT: A. PROFESSOR: A is the crappiest net. But there’s no way in hell that A is going to
            be able to get any of these except for six. So let’s, right off the bat, say that six is A. All right. Six
            is A. That’s A. We don’t have to worry about A. OK. Cool.</p>
        <p>现在让我们看看其他一些非常有趣的。其余的都画了两条线，这三个画了两条线。这三个画了三条线。它们画了一个三角形。因此，尽管这个 c 是一个非常强大的节点，而且确实，这里有三个完整的 S
            型层，但看起来我们的小网络中只有两个能够处理数字 1 和 2。</p>
        <p>Now let’s look at some other ones that are very interesting. All the rest of these draw two lines, well these
            three draw two lines. These three draw three lines. They draw a triangle. So despite the fact that this c is
            a very powerful node, that indeed, with three whole levels here of sigmoids, it looks like there’s only two
            that’s in our little stable of nets that are equipped to handle number one and two.</p>
        <p>那些是？E 和 F，因为 E 和 F 在第一层有三个节点。它们可以画三条线。然后它们可以对这些线做一些合乎逻辑的事情，比如，如果它在所有这些线内。有一种方法可以做到这一点。你只需。</p>
        <p>And those are? E and F, because E and F have three nodes at the first level. They can draw three lines. And
            then they can do something logical about those lines, like for instance, maybe, if it’s inside all of those
            lines. There’s a way to do that. You just.</p>
        <h2 id="unknown-505">未知</h2>
        <h2>Unknown</h2>
        <p>基本上，您可以根据需要赋予负权重和正权重，以确保它低于某些权重，高于其他权重，然后设置阈值，使其必须遵循所有三个规则。因此，在 E 和 F
            之间，哪一个应该是二，哪一个应该是一。有人明白了吗？让我们看看二和一。哪一个更容易做到？在二和一之间。二。</p>
        <p>Basically you can give negative and positive weights as you so choose to make sure that it’s under certain
            ones, above other ones, and then make the threshold such that it has to follow all three of your rules. So
            between E and F, which one should be two and which one should be one. Anyone see? Well let’s look at two and
            one. Which one is easier to do? Between two and one. Two.</p>
        <p>它有一条水平线和一条垂直线。一条有三条对角线。在 E 和 FF 之间，哪一个是较弱的网络？F 有一个节点只能做水平线，一个节点只能做垂直线。那么 F 必须做哪一个呢？两个。E
            做什么？干得好，伙计们。干得好，你明白了。现在让我们看看最后三个。</p>
        <p>It’s got a horizontal and a vertical. One has all three diagonal. And which one of these is a weaker net,
            between E and F. F. F has one node that can only do a horizontal, and one node that can only do a vertical
            line. So which one is F going to have to do? Two. And E does what? Good job, guys. Good job, you got this.
            So now let’s look at the last three.</p>
        <p>第三个绝对是最难的。它是一个 exceller。你们当中玩过 double o 2 这类东西的人，甚至只是玩过逻辑的人，可能知道没有办法在一个逻辑层面上进行某种简单的线性组合来创建 x 或。x
            或非常难以创建。有一些有趣的问题涉及尝试将 exceller 教给神经网络。</p>
        <p>Number three is definitely the hardest. It’s an exceller. Those of you who’ve played around with double o 2
            kind of stuff, or even just logic, probably know that there is no way to make a sort of simple linear
            combination in one level of logic to create an x or. x or is very difficult to create. There are some
            interesting problems involving trying to teach an exceller to a neural net.</p>
        <h2 id="unknown-506">未知</h2>
        <h2>Unknown</h2>
        <p>因为神经网络不能得到 x，或者因为你可以告诉它，好的，我希望这个值高，这个值低。没问题。你说这两个值都必须高。没问题。</p>
        <p>Because a neural net is not to be able to get the x or, because of the fact that you can tell it, OK, I want
            this one to be high, and this one to be low. That’s fine. You say these both have to be high. That’s fine.
        </p>
        <p>很难说，几乎不可能说，这个或那个，而不是另一个，因为需要在单个节点中处于高位，因为如果你只是玩它，你就会明白。你需要在某处设置一个阈值，并且它无法区分，如果阈值设置为或将起作用，那么整个或将起作用。</p>
        <p>It’s hard to say, it’s pretty much impossible to say, this one or this one, but not the other, because need
            to be high in a single node, because of the fact that if you just play with it, you’ll see. You need to set
            a threshold somewhere, and it’s not going to be able to distinguish between, if the threshold is set such
            that the or is going to work, the whole or is going to work.</p>
        <p>当它们都为正时，它也会接受。那么我们该如何做 x 或呢？我们需要更多的逻辑。我们需要以两级方式使用 and 和 or 的一些组合。要做到这一点，我们需要我们拥有的最深的神经网络。只有一个能够做到这一点。那就是？它是
            C。有很多不同的方法可以做到这一点。让我们想想一种可能性。</p>
        <p>It’s going to accept when both of them are positive as well. So how we can do x or? We need more logic. We
            need to use some combinations of ands and ors in a two level way. To do that we need the deepest neural net
            that we have. There’s only one that’s capable of that. And that is? It’s C. There are many different ways to
            do it. Let’s think of a possibility.</p>
        <h2 id="unknown-507">未知</h2>
        <h2>Unknown</h2>
        <p>I 1 和 I 2 画出这两条线。我们称它们为一、二、三、四、五，节点 1 和节点 2 画出这两条线。我会在这里为你们画出来。然后也许节点 3 会赋予值。是的，让我看看。节点 3 可以赋予值。让我们看看。节点 3
            可以为所有事物赋予值。这里有很多可能性。</p>
        <p>I 1 and I 2 draw these two lines. Let’s call these one, two, three, four, five, node 1 and node 2 draw these
            two lines. And I’ll just sort of draw it here for you guys. Then maybe node 3 gives value to. yeah, let me
            see. node three can give value to perhaps. let’s see. node 3 can give value to everything that is. there are
            a lot of possibilities here.</p>
        <p>节点 3 可以为上面的所有内容赋值。实际上，节点 3 可以为除底部之外的所有内容赋值，然后节点 4 可以赋值。目前还没有这样做，但有几种。如果你尝试过，有几种不同的方法可以做到这一点。</p>
        <p>Node 3 can give value to everything that is up here. Actually node 3 can give value to everything except for
            this bottom part, and then node 4 could give value to say. doesn’t do it yet, but there’s a few. there’s a
            few different ways to do it if you played around.</p>
        <p>关键思想是节点 3 和节点 4 可以为某个组合赋予值或不赋予值，然后节点 5 可以根据高于或低于某个阈值（3 和 4 的组合）赋予值。您可以使用逻辑门构建
            exceller。在我们继续前进时，我会暂时考虑一下这个问题，但显然 C 必须完成第三个任务。好的。</p>
        <p>The key idea is that node 3 and node 4 can give value to some combination and or not, and then node 5 can
            give value based on being above or below a certain threshold, combination of 3 and 4. You can build an
            exceller out of the logic gates. I will ponder on that in the back burner for a moment, as we continue
            onward, but clearly C has to do number three. OK.</p>
        <h2 id="unknown-508">未知</h2>
        <h2>Unknown</h2>
        <p>现在我们只剩下四和五了。我觉得有趣的是，五看起来可能比四更复杂，因为它需要做两个不同的方向，而不是两个相同的方向。所以，只是行数较少、更简单的那个想法可能无法让我们在这里得到解决。这是有原因的。看看我们还剩下什么可以用。
        </p>
        <p>Now we’re left with four and five. I think, interestingly, five looks like it may be more complicated than
            four, because of the fact that it needs to do both different directions instead of two of the same
            direction. So however, just the idea of the one with the fewer lines, being a simpler one, may not get us
            through here. And there’s a reason why. Look what we have left to use.</p>
        <p>我们必须使用 D 或 B。D 可以绘制的两条线的属性是什么？D 更简单。一条水平线，一条垂直线，没错。因此，尽管只有两条水平线可能看起来更简单，但实际上它需要 B。B 是唯一可以绘制两条水平线的，因为 D
            必须绘制一条水平线和一条垂直线。因此，剩下的就是，B 在这，D 在这。</p>
        <p>We have to use D or B. What is the property of the two lines that D can draw? D being the simpler one. One
            horizontal, one vertical, that’s right. So even though it may look simpler to just have two horizontal
            lines, it actually requires B. B is the only one that can draw two horizontal lines because D has to draw
            one horizontal and one vertical. So that leaves us with, B on this, D on this.</p>
        <p>太好了，我们有个问题。我以为我们可能没有问题，或者我可能已经尽力解释了。问题。学生：我不明白为什么 B 必须是两条水平线。教授：好的。问题是，我不明白为什么 B 必须是两条水平线。答案是，不是。B 可以是任何东西，但
            D 不能是两条水平线。</p>
        <p>Excellent, we have a question. I would’ve thought it would have been possible that we had no questions, or
            maybe I just explained it the best I ever have. Question. STUDENT: I didn’t get why B has to be two
            horizontal lines. PROFESSOR: All right. So the question is, I don’t understand why B to be two horizontal
            lines. The answer is, it doesn’t. B can be anything, but D can’t be two horizontal lines.</p>
        <h2 id="unknown-509">未知</h2>
        <h2>Unknown</h2>
        <p>因此，通过排除法，结果是 B。我们来看看 D，对吧。D 有三个节点，一、二、三。节点 1 和节点 2 可以在任意位置画一条线，涉及它们收到的输入。节点 1 收到什么输入？</p>
        <p>And so by process of elimination, it’s B. Well take a look at D, right. So D has three nodes, one, two,
            three. Node 1 and node 2 can just draw a line anywhere they want, involving the inputs they receive. What
            input does node 1 receive?</p>
        <p>让我们转到节点 1。因此，它只能根据 I 1 进行截断。因此，它只能通过在某个点的上方和下方进行截断来绘制。节点 1 只能绘制垂直线。节点 2 只能绘制水平线，因为它只能根据 I 2
            的位置进行截断。因此它们不能同时绘制水平线。这就是为什么这是最棘手的部分。</p>
        <p>Let’s go to node 1. So it can only make a cut off based on I 1. So therefore, it can only draw by making the
            cut off above and below a certain point. Node 1 can only draw vertical lines. Node 2 can only draw a
            horizontal line, because it can only make a cut off based on where it is an I 2. Therefore they can’t both
            draw a horizontal. That’s why this is the trickiest part.</p>
        <p>最后一部分，因为 B 更强大。B 不只需要画两条水平线。它可以画两条对角线。它可以做任何它想做的事情。它只是被困在了这个稍微容易一些的问题上，因为事实上它是唯一一个有能力做到这一点的人。</p>
        <p>This last part, because B is more powerful. B does not only have to do two horizontal lines. It can do two
            diagonal lines. It can do anything it wants. It just happens that it’s stuck doing this somewhat easier
            problem, because the fact that it is the only one left that has the power to do it.</p>
        <h2 id="unknown-510">未知</h2>
        <h2>Unknown</h2>
        <p>所以让我们看看，我们完成了，我们会在这部分测验中取得高分，就像没有人能通过一样，好吧，不是没有人，但是很少有人能通过，当我们在 2008
            年进行测验时。我们唯一要问的是。让我看看。是的，我们唯一要问的是，我们在这里要做什么？好的。让我们看看。</p>
        <p>So let’s see, we’re done, and we’d have aced this part of the quiz that like no one got, well not no one, but
            very few people got, when we put it on in 2008. The only thing we have left to ask is. let me see. yeah, the
            only thing we have left to ask is what are we going to do here for this? All right. Let’s see.</p>
        <p>对于 x 或，让我们看看我是否可以执行这个 x
            或。好的。这个怎么样。对。我是个白痴。这是最简单的方法。数字一画出这条线。数字二画出这条线。数字三结束这条线，两条线。数字三表示只有当它们都为真时，我才会接受。数字四映射两条线。数字五在三和四之间进行或。谢谢。不，这并不难。
        </p>
        <p>For the x or, let’s see if I can do this x or. OK. How about this one. Right. I’m an idiot. This is the
            easiest way. Number one draws this line. Number two draws this line. Number three ends the line, the two
            lines. Number three says only if both of them are true, will I accept. Number four maps the two lines. And
            number five ors between three and four. Thank you. No, it’s not that hard.</p>
        <p>我完全想不起来了，因为还有另一种方法，很多人喜欢这样做。它需要画出很多线条，然后画出 b 2
            谱号。但我现在想不起来了。或者还有其他问题吗？因为我认为如果你现在有问题，就像其他四个人也有问题，只是没有举手。所以问任何有关这个绘图的问题。问题？</p>
        <p>I just completely blanked, because there’s another way that a lot of people like to do it. It involves
            drawing in a lot of lines, and then making the clef b 2. But I can’t remember it at the moment. Or there any
            other questions? Because I think if you have a question now, like four other people have it and just aren’t
            raising their hand. So ask any questions about this drawing thing. Question?</p>
        <h2 id="unknown-511">未知</h2>
        <h2>Unknown</h2>
        <p>学生：我们为什么要这样做？教授：我们为什么要画图？这个问题问得很好。答案是，这样你就能知道在这些简单的问题中，你可能需要使用哪些类型的网络来解答这些简单的问题。</p>
        <p>STUDENT: Why do we do this? PROFESSOR: Why do we do this drawing thing? That’s a very good question. The
            answer is so that you can see what kinds of nets you might need to use in these simple problems, to answer
            these simple problems.</p>
        <p>因此，如果 Athena 禁止您在某个工作中使用神经网络进行实际学习，并且您看到问题的某种性质，您就知道不要制作太简单的网络。而且您也不希望网络比它应有的更复杂。因此，您可以看到网络在每个级别上的作用，并更直观地理解。
        </p>
        <p>So that if Athena forbid that you have to use a neural net in a job somewhere to do some actual learning, and
            you see some sort of quality about the problem, you know not to make a net that’s too simple, for instance.
            And you wouldn’t want a net that is more complex than it has to be. So you can sort of see what the net’s do
            at each level, and more visibly understand.</p>
        <p>我认为许多绘制此类问题的人只是想让人们知道，哦，是的，我们不仅仅是盲目地从问题的另一部分反向传播这些数字来使它们变高或变低。这就是我们在每个级别所做的。这就是我们正在研究的空间。每个节点都在对之前的步骤执行逻辑。
        </p>
        <p>I think a lot of people who drew problems like this just want to make sure people know, oh yeah, it’s not
            just these numbers that we’re mindlessly backpropagating from the other part of the problem to make them
            higher or lower. This is what we’re doing at each level. This is the space that we’re looking at. Each node
            is performing logic on the steps before.</p>
        <h2 id="unknown-512">未知</h2>
        <h2>Unknown</h2>
        <p>这样，如果你以后真的需要使用神经网络，你就能弄清楚你的神经网络应该是什么样子。你就能弄清楚它在做什么。</p>
        <p>So that if you actually have to use a neural net later on, down the road, then you’ll be able to figure out
            what your net’s going to need to look like. You’ll be able to figure out what it’s doing.</p>
        <p>至少你能弄清楚它在做什么，对于一个神经网络来说，因为它经常会开始产生这些非常疯狂的数字，里面会有各种各样的节点，就像现在正在使用的真正的神经网络一样，会有大量的节点，你会看到数字剧烈波动，然后突然间它会开始工作或停止工作。这是个好问题。还有其他问题吗？
        </p>
        <p>At least as well as you can figure out what it’s doing, for a neural net, since it often will start getting
            up these really crazy numbers, will have all sorts of nodes in it, and like a real neural net that’s being
            used nowadays, there’ll be tons of nodes, and you’ll just see the numbers fluctuate wildly, and then
            suddenly it’s going to start working or not. That’s a good question. Any other questions?</p>
        <p>我们还有几分钟时间。不多，但有几分钟。还有其他关于这些内容的问题吗？抱歉。学生：谈谈你刚才问的问题。就因为我们画了它，机器就需要学习吗？教授：你对右边的图片感到困惑，为什么机器要运行？哦，好的。机器不必通过画图并调用它们来学习。让我给你一些实际的应用程序。
        </p>
        <p>We still have a few minutes. Not many, but a few. Any other questions about any of this stuff? Sorry.
            STUDENT: Talk about what you just asked. Just because we draw it, does the machine need to learn. PROFESSOR:
            You’re confused why the machine is run what, by the pictures on the right? Oh OK. Machine does not have to
            learn by drawing pictures and calling them in. Let me give you some real applications.</p>
        <h2 id="unknown-513">未知</h2>
        <h2>Unknown</h2>
        <p>我马里兰大学的朋友最近确实使用了神经网络，因为他当时正在参加一个游戏计划竞赛，在设计 AI 时，你并不知道比赛内容。它必须能够做到这一点。</p>
        <p>My friend at the University of Maryland recently actually used neural nets because, yeah, he actually did,
            because of the fact that he was doing an game plan competition, where the game was not known when you were
            designing your AI. It had to be able to.</p>
        <p>有一些非常优雅、通用的游戏解算器，你可以将其连接到其中，然后他们制定规则，你只有一点时间，然后它就开始了。</p>
        <p>There was some very elegant, general game solver thing that you had be able to hook up into, and then they
            made up the rules, and you had a little bit of time, and then it started.</p>
        <p>有些人工智能所做的就是，一旦他们自己找出规则，他们就会进行训练，根据规则，在他的例子中，他有一个神经网络，因为它太通用了，你只有一个随机的网络。他认为它可以学习任何东西，然后。他从来没有告诉我它进展如何，可能进展不顺利。但也许它确实成功了。它基本上试图学习一些关于规则的东西。
        </p>
        <p>Some of the AI’s, what they did was, they trained, once they found out what the rules were on their own, with
            the rules, in his case he had a neural net, because it was so generic, you just have a web of random gook.
            He thought it could learn anything, and then. he never did tell me how it went, probably didn’t go well. But
            maybe it did. It basically tried to learn some things about the rules.</p>
        <h2 id="unknown-514">未知</h2>
        <h2>Unknown</h2>
        <p>其他一些更有原则的游戏玩家实际上试图通过测试一些不同的东西来找出规则空间的基本属性，因此他们认为更多的知识意味着更少的搜索，因此在实际游戏开始时他们可以减少搜索。然后当实际游戏开始时，几乎每个人都做了一些基于游戏树的东西。
        </p>
        <p>Some of the other people who are more principled game players actually tried to find out fundamental
            properties of the space of the rules by testing a few different things, so they could view more knowledge is
            less search so they could do less search when the actual game playing came on. And then when the actual game
            playing came on, pretty much everyone did some kind of game tree based stuff.</p>
        <p>他告诉我，很多基于蒙特卡洛的游戏树的东西都是非常不确定的，就像他们现在所做的那样，而不是决定 alpha beta 的东西，尽管他说如果你有足够的时间，它会收敛到 alpha
            beta。这就是他告诉我的，但我认识的一个人正在使用神经网络。</p>
        <p>He’s telling me that a lot of Monte Carlo based game tree stuff that is this very non deterministic as what
            they’re doing nowadays, rather than what determines the alpha beta, although he said it converges to alpha
            beta, if you’ve given enough time. That’s what he told me, But that someone I know who is using neural nets.
        </p>
        <p>我在一门认知科学课上也看到过神经网络试图将相似的特性附加到物体上，通过在层级之间设置大量的节点，最后就像鸭子飞起来一样，你会想，它怎么会再次这样做？我不确定，但它就是这样的。所以基本的想法是，当。</p>
        <p>I’ve also in a cognitive science class I took, saw neural nets that tried to attach like qualities to
            objects, by having just this huge, huge number of nodes in levels in between, and then eventually it was
            like, a duck flies, and you’re like, how’s it doing this again? I’m not sure, but it is. So the basic idea
            is that when.</p>
        <h2 id="unknown-515">未知</h2>
        <h2>Unknown</h2>
        <p>神经网络在当时被广泛使用的主要原因之一是，认知科学、人工智能等不同领域的人们都在说，等一下，神经元网络可以做很多事情，而且我们在不同的地方看到了它。当你同时在这么多不同的地方看到它时，这一定是一个将彻底改变一切的天才想法。
        </p>
        <p>One of the main reasons that neural nets were used so much back in the day is that people on many different
            sides of this problem, cognitive science, AI, whatever, were all saying, wait a minute, there’s networks of
            neurons, and they can do stuff, and we’re seeing it in different places. And when you’ve seen it in so many
            different places at once, must be a genius idea that’s going to revolutionize everything.</p>
        <p>于是每个人都开始使用它们，试图将所有这些东西连接在一起，我认为这是一项崇高的事业，但不幸的是，人们只是停止使用它。它没有像他们想要的那样发挥作用。事实证明，弄清楚我们大脑中的神经元是如何工作的并不是一次性解决所有人工智能难题的方法。它们逐渐失宠，尽管由于某些原因仍在使用，比如总和就是这样。
        </p>
        <p>And so then everyone started using them to try to connect all these things together, which I think is a noble
            endeavor, but unfortunately people just stopped using it. It didn’t work as they wanted. It turned out that
            figuring out our neurons worked in our head was not the way to solve all AI hard problems at once. And they
            fall into disfavor, although are still used for some reasons, like the sum is like that.</p>
        <p>所以我们不会只用它来画这些图。我们之所以有这些图，是因为我们给你提供了简单的网络，你可以在测验中手工算出它们。如果我们试图让你在测验中用它做点什么，现在真正使用的任何网络都会让你的头脑爆炸。那太可怕了。所以我认为这是个好问题。如果没有其他问题，或者即使有，因为我们必须出去，如果有其他问题，你可以在我走出去的时候看到我。
        </p>
        <p>So we wouldn’t use it just to draw these pictures. The reason why we have these pictures is because we give
            you simple nets that you can work it out by hand on the quiz. Any net that is really used nowadays would
            make your head explode, if we tried to make you do something with it on the quiz. It would just be horrible.
            So I think that’s a good question. If there’s no other questions, or even if they are, because we have to
            head out, if there’s any other questions, you can see me as I’m walking out.</p>
        <h1 id="mega-r5.-support-vector-machines">Mega-R5. 支持向量机</h1>
        <h1>Mega-R5. Support Vector Machines</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:53 <a
                href="https://youtube.com/watch?v=6nDqY8MPLDM">https://youtube.com/watch?v=6nDqY8MPLDM</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:53 <a
                href="https://youtube.com/watch?v=6nDqY8MPLDM">https://youtube.com/watch?v=6nDqY8MPLDM</a></p>
        <h2 id="unknown-516">未知</h2>
        <h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。教授：希望大家昨天度过了一个愉快的退伍军人节假期，当然，当我检查我的测试观众时，大部分时间都在做你们的研究或 PSET。但至少有一个人看了电视。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Hope everyone
            had a great Veteran’s Day break yesterday, spending it, of course, as I check my test audience, spending it
            mostly doing your research or PSETs. But at least one person got to watch TV.</p>
        <p>所以至少有一个人得到了真正的突破，这是一件真正令人惊奇和特别的事情。现在我们要讨论 SVM。它们几乎是 6.034
            中最难的东西。然而，近年来出现了一些捷径，有时可以让你解决问题，这取决于他们要求什么，而不必解出一大堆丑陋的方程式和大量丑陋的未知数。</p>
        <p>So at least one person got to have a real break, and that’s something truly amazing and special. So now we’re
            going to talk about SVMs. They’re pretty much the hardest thing in 6.034. However, in recent years a few
            shortcuts have popped up that will sometimes allow you to solve the question, depending on what they’re
            asking for, without solving some vast ugly set of equations with a vast ugly number of unknowns.</p>
        <p>所以我要向你们展示这一点。我还将尝试解释其中的所有字母组合以及所有字母代表什么，因为我花了好几次研究 SVM。我花了好几次研究 SVM 才真正弄清楚所有这些字母代表什么。</p>
        <p>So I’m going to show that to you guys. I’m also going to try to explain all the alphabet soup that’s in and
            what all the letters stand for because it took me a few times going through SVMs. It took me a few times
            going through SVMs to actually find out for sure what all those letters stood for.</p>
        <h2 id="unknown-517">未知</h2>
        <h2>Unknown</h2>
        <p>如果你们第一次尝试就解决了，那就太好了，你们会没事的。所以让我们来看看这个问题，这个问题可能最适合使用一些捷径来解决它，而不是列出所有的方程式。然后我会的。</p>
        <p>And if you guys figure it out first try, that’s going to be great, and you guys will be just fine. So let’s
            take a look at the problem that’s perhaps most optimized for using some of the shortcuts to solving it and
            not putting up all the equations. Then I will.</p>
        <p>不是因为我虐待狂，而是因为我很善良，我会强迫你和我一起解决一些他们没有要求我们解决的问题，这样你就能明白，如果不做一些更困难的事情，我们就无法摆脱一切。当然，一定要像往常一样提出问题，但这次要问得更多。</p>
        <p>Not because I’m sadistic, but because I’m being nice, I will force you with me to solve some of the things
            they didn’t ask for us to solve so that you can see that we can’t get away with everything without doing
            some of the harder stuff. And of course, definitely ask questions as always, but this time even more so.</p>
        <p>各位，如果你们环顾四周，你会发现整个演讲厅里没有人举手表示他们已经准备好并且了解
            SVM。所以如果你有问题，也许其他人都有。那我们开始吧。我们从这里开始。像往常一样，假装我可以画图，因此所有的正负号都只在整数坐标上。所以在这个问题中我们被要求圈出支持向量。</p>
        <p>You guys, well, if you were looking around, you saw nobody in this entire lecture hall raised their hand that
            they are already set and ready and know SVMs. So if you have a question, maybe everybody else does. So let’s
            go. We’ll start right here. As always, pretend that I can draw, and that therefore all the pluses and
            minuses are only on integer coordinates. So we are asked in this problem to circle the support vectors.</p>
        <h2 id="unknown-518">未知</h2>
        <h2>Unknown</h2>
        <p>画出街道的边缘，然后在中间画出分隔它们的虚线，即虚线分隔线。然后给出 w 和 b。那么 w 和 b 是什么？好吧，SVM 中有几个重要的方程式，我们真的希望如此。我要告诉你，我们很幸运，因为我们不需要这样做。</p>
        <p>Draw the edges of the street and then the dotted line in the middle that separates them, the separator, as a
            dashed line. And then to give w and b. So what are w and b? Well, there’s a few important equations in SVMs
            that we really hope. and I’m going to tell you we’re lucky in this because we don’t have to.</p>
        <p>但我们真的希望我们不必使用它们，因为它们提供了大量的变量。因此，其中一个关键方程是，对于一个正支持向量，w 向量点 x 加，正支持向量，加 b 等于 1。w 点 x 减去加 b 等于减 1。而 w
            点那条虚线。我不知道，我们称之为点。加 b 等于 0。那么这是什么意思呢？</p>
        <p>But we really hope that we don’t have to use because they provide a huge number of variables. So one of those
            crucial equations is that for a plus support vector, w vector dot x plus, the plus support vector, plus b
            equals 1. w dot x minus plus b equals minus 1. And w dot that dotted line. I don’t know, we’ll call it dot.
            plus b equals 0. So what does this mean?</p>
        <p>有很多向量。嗯，我的意思是，我们通常处于二维空间中，所以我们基本上可以说这个 w 向量有两个分量，w1 和 w2。它们只是线性方程中的两个系数。例如，我们感兴趣的是找到这条点线，我们将其称为
            x，因此上面什么都没有。实际上，也许这样会更容易。</p>
        <p>There are a lot of vectors. Well, I mean, we’re usually in two dimensional space, so we can basically just
            say that there’s two components of this w vector, w1 and w2. And they’re just two coefficients in a linear
            equation. So for instance, what we’re interested in finding, this dot line, we’ll just call that x, so with
            nothing on it. Actually, maybe that’ll be easier.</p>
        <h2 id="unknown-519">未知</h2>
        <h2>Unknown</h2>
        <p>这相当于说 w1x1 加 w2x2 加 b 等于 0，其中 x1 是这个，x2 是这个。我们可能称它们为 x 和 y。因此，一种思考方式是，w1，我们称之为 a，ax 加。称 w2 为 b。哦，不要称之为
            b。那么，ax 加 cy 加 b 等于。我会把这些都放在括号里。这基本上是一个这样的等式。</p>
        <p>This is equivalent to saying w1x1 plus w2x2 plus b equals 0, where x1 is this, and x2 is this. We would
            possibly call them x and y. So one way to think about it is w1, we’ll call it a, ax plus. call w2 b. by. Oh,
            don’t call it b. Well, ax plus cy plus b equals. I’ll put this all in parentheses. this is basically an
            equation like this.</p>
        <p>或者 y 等于负 a 除以 cx 减 b 除以 c。基本上就是 y 等于 mx 加 b。大家明白了吗？我们要找的东西，这个 w 点 x 加 b 等于 0，是笛卡尔坐标系中的直线方程。只是看起来比较丑。通常，当我们对 w
            和 b 进行所有这些求解时，我们必须输入大量的方程，将所有支持向量代入其中。</p>
        <p>Or y equals negative a over c x minus b over c.&nbsp;It’s basically y equals mx plus b. Does everyone see
            that? This thing that we’re looking for, this w dot x plus b equals 0, is the equation of a line in
            Cartesian coordinates. It just looks uglier. Normally, when we’re doing all this solving for w and b, we
            would have to put in tons of equations, plug in all of the support vectors in there.</p>
        <p>我们必须使用这些称为 alpha 的小恶魔。本质上就是 alpha。如果在讲座中没有讲清楚，通常不是每个人都完全清楚，对我来说也不是完全清楚。alpha，我喜欢这样思考它们，这个问题中的 alpha
            是图表上任何特定点对创建边界的重要性的权重。alpha 越高，该点在边界上的收窄就越大。</p>
        <p>And we’d have to use these little devils called alphas. Alphas essentially. if it wasn’t clear in the
            lecture, which it usually isn’t completely clear to everyone, wasn’t clear to me completely. alphas, the way
            I like to think about them, the alphas in this problem is they are the weight of how significant any
            particular point on the graph is towards creating the boundary. The higher the alpha is, the more that point
            narrows in the boundary.</p>
        <h2 id="unknown-520">未知</h2>
        <h2>Unknown</h2>
        <p>alpha 值越低，边界上的点越窄，道路就越宽。如果该点不做任何事情，如果该点无关紧要，可以移除，并且不会影响边界，那么 alpha 值是多少？每个人？观众：零。教授：零。好吧，那是一个人，但对每个人来说都足够了。
        </p>
        <p>The lower the alpha is, the less that point narrows in the boundary, the wider the road can be. And if that
            point doesn’t do anything, if that point is irrelevant and could be removed and it wouldn’t affect the
            boundary, the alpha is? Everyone? AUDIENCE: Zero. PROFESSOR: Zero. Well, that was one person, but you can
            suffice for everyone.</p>
        <p>alpha 为 0。这意味着如果它不是支持向量，如果它不是边界线上的向量之一，那么它的 alpha 总是为 0，因为它不会产生影响。因此，请记住，如果我们要求解许多未知数的多个方程，那么我们需要一些关于 alpha
            的有趣且重要的方程，但希望我们不必这样做。</p>
        <p>The alpha is 0. And that means if it’s not a support vector, if it’s not one of the vectors on the boundary
            lines, it will always have an alpha of 0 because it doesn’t affect. So keeping that in mind, there’s a few
            fun and important equations about alphas that we’ll need if we’re solving many equations for many unknowns,
            which hopefully we won’t have to do.</p>
        <p>正 alpha 值的总和等于负 alpha 值的总和。对于所有点来说都是如此。但是由于除支持向量外，所有 alpha 值均为 0，因此这也意味着正支持向量的 alpha 值等于负支持向量的 alpha 值。</p>
        <p>The sum over the positive alphas equals the sum over the alphas. the negative points. And this is true over
            all the points. But since all of the alphas are 0, except for the support vectors, it also means the alphas
            of the positive support vectors are equal to the alphas of the negative support vectors.</p>
        <h2 id="unknown-521">未知</h2>
        <h2>Unknown</h2>
        <p>此外，我们的老朋友 w 向量等于所有 I 的总和，即加上 wi alpha I 减去 m 除以 j 的向量，再减去 wj alpha j
            的向量。现在，所有这些方程式都可以乱用，以找出我们试图找到的答案，也就是圆。实际上，它们不能用作圆支撑向量并绘制虚线。</p>
        <p>Additionally, our old buddy, the w vector, is equal to the sum over all I that are plus vectors of wi alpha I
            minus m over j minus vectors of wj alpha j. Now, all of these equations can be used in a bloody mess to
            figure out the answer to what we’re trying to find, which is circles. well, actually, they can’t be used as
            circle support vectors and draw the dotted line.</p>
        <p>但是一旦我们这样做了，所有这些方程式就可以乱七八糟地用来得到我们想要的下一个东西，也就是 w 和 b。幸运的是，还有另一种方法可以得到 w 和 b。如果你们真的想要，在最后我们也可以尝试使用许多方程式和许多未知数来推导
            w 和 b，但这有点痛苦。我们会尝试用很酷的方式来做。</p>
        <p>But once we do that, all these equations can be used in a bloody mess to give us the next thing that we want,
            which is w and b. So fortunately, there’s another way to get w and b. If you guys really want, at the end of
            the hour we can also try to derive w and b using the many equations in many unknowns, but it’s a bit
            painful. We’ll try to do it the cool way.</p>
        <p>那么让我们开始吧。这就是我们要看的。我们需要找到支持向量的位置。所以我们要做的第一件事就是简单地观察它。幸运的是，在测试中，如果你要圈出支持向量，总会有一些你可以观察的。显然有一些优点和缺点。我说显然，但可能不是。
        </p>
        <p>So let’s start off. This is the one we’re looking at. We need to find where the support vectors are. So the
            first thing we need to do is simply eye it. Fortunately, on the test, there will always be ones that you can
            eye if you’re supposed to circle the support vectors. There’s obviously some number of pluses and some
            number of minuses. I say obviously, but maybe not.</p>
        <h2 id="unknown-522">未知</h2>
        <h2>Unknown</h2>
        <p>但显然我们希望如此，因为我们会随机找人来回答。所以请给我一个正支持向量。听众：嗯，看一个看起来像教授：哪个加号的，听众：右边的那个。教授：最右边的那个。是的，那个加号是一个正支持向量。很好。好吗？太好了。现在，给我一个负支持向量。那个？不是吗？听众：是的，抱歉。
        </p>
        <p>But hopefully obviously, and we’ll find out because I’m going to call on random people. So give me a positive
            support vector. AUDIENCE: Um, going to the one that looked like PROFESSOR: Which plus sign, AUDIENCE: One on
            the right. PROFESSOR:. The one all the way on the right. Yeah, that plus sign is a positive support vector.
            That’s good. All right? Excellent. Now, give me a negative support vector. That one? No? AUDIENCE: Yeah,
            sorry.</p>
        <p>教授：啊，没问题。给我一个负支持向量。听众：我一定要问你，什么是支持向量？教授：这个问题问得好。问题是，什么是支持向量？有多少人会承认有这个问题？看到了吗？你并不孤单。好的。在我继续之前，我会假设。你们确保我是正确的。星期一，只是为了确定，这样我就可以根据这个进行调整。
        </p>
        <p>PROFESSOR: Ah, no problem. Give me a negative support vector. AUDIENCE: I should definitely ask you, what’s a
            support vector? PROFESSOR: That is a good question. The question is, what is a support vector? How many
            other people will admit to having this question? See? You’re not alone. OK. Before I go on, I’m going to
            assume. you guys make sure I’m correct. Monday was, just being sure so I can tailor based on this.</p>
        <p>周一是支持向量机讲座。但跟上也非常困难。这通常是我所期望的。那么什么是支持向量？好吧，所有这些优点和缺点，如果我们是我，我想。是的，如果我们是我，如果我在描述这个问题，我们在课堂上解决的问题，我会称它们为点，因为它们在图表上。它们是点。它们是数据点。
        </p>
        <p>Monday was the support vector machine lecture. But it was also very difficult to follow. That’s what I
            usually expect. So what is a support vector? Well, all these pluses and minuses, if we were me, and if, I
            guess. yeah, if we were me and if I was describing this problem, the one that we work out in class, I would
            call them points because they’re on the graph. They’re points. They’re data points.</p>
        <h2 id="unknown-523">未知</h2>
        <h2>Unknown</h2>
        <p>但是，在这个问题的更困难版本中，有 n 个维度，其中 n
            是一个荒谬的维度数，你永远无法将其绘制成图。比如说我现在正在做的一些研究，我可以使用支持向量机来处理我正在阅读的一些关于网络事件的文章，试图弄清楚是否有真实事件，或者只是有人在抱怨我们真的很脆弱之类的，实际上并没有发生任何事件。
        </p>
        <p>But however, in more difficult versions of this problem that have n dimensions, where n is some ridiculous
            number of dimensions that you’re never going to graph. Like say some of the research I’m doing now, I could
            use support vector machines on some of these articles that I’m reading about cyber events to try to figure
            out if there’s a real event or if it’s just someone complaining about how we’re really vulnerable or
            something like that and no event actually happened.</p>
        <p>所以他们之所以称这些点为向量，是因为当你无法在笛卡尔平面上绘制它们时，仍然会存在这个具有许多不同维度的长向量。不过现在，这些点代表向量。这很简单。这样查看它们更容易。</p>
        <p>So the reason why they call these guys vectors is when you’re not able to graph them on a Cartesian plane,
            there’s still this long vector of many different dimensions. Right now, though, these points represent the
            vectors. This is very simple. It’s easier to view them this way.</p>
        <p>但是例如，在负 1,2 处的正值表示有一个向量朝负 1,2 方向移动，其幅度达到负
            1,2。所以所有这些点都只是向量的点表示。你可能在任何一堂学习向量的课上都看到过这种情况，看到向量被表示为点。问题？听众：总是从原点开始？教授：是的。</p>
        <p>But for instance, that plus at negative 1,2 represents the fact that there is a vector going in the direction
            of negative 1,2 with a magnitude such that it reaches negative 1,2. So all these points are just a point
            representation of a vector. You probably, in any class that worked with vectors, saw this, saw vectors being
            represented as points. Question? AUDIENCE: Always from the respect to the origin? PROFESSOR: Yes.</p>
        <h2 id="unknown-524">未知</h2>
        <h2>Unknown</h2>
        <p>问题总是与原点有关。答案是，当向量表示为点时，是的，它总是与原点有关。所以基本思想是所有这些点都是向量。那么什么是支持向量？好吧，在这种情况下，你可以称它们为支持点。</p>
        <p>The question is always with respect to the origin. The answer is canonically, when vectors are represented as
            points, yes, it’s always with respect to the origin. So that’s the basic idea is that all these points are
            vectors. So what are support vectors? Well, you could call them support points for this case.</p>
        <p>但我们之所以称它们为支持向量，是因为在现实世界中，在实际使用真实 AI 时，你可能会得到一个巨大的向量。而且它不仅仅是图上的点。通常情况下是这样。因此，我们正确地找到了支持向量、支持点中的一个。就是这个。它们将是那些
            alpha 不为零的向量。</p>
        <p>But the reason we call them support vectors is again, in the generalized case that you might be doing in the
            real world with real AI, you’re going to have a giant vector. And it’s not just going to be points on a
            graph. Well, usually. So the support vectors, the support points, we found one of them correctly. It’s this
            guy. They’re going to be the ones that again, they don’t have an alpha of zero.</p>
        <p>它们是 Petra
            所说的道路、边界线的结合点。它们将位于正号的边缘。无论我们朝哪个方向画，这个正号都是正号区域的边缘。如果我们将其作为正号区域的边缘，而这一侧的所有内容都是正号，这一侧的所有内容都是负号，那么我们就完蛋了，因为另一侧有两个正号。
        </p>
        <p>They’re the ones that bind in the, as Petra calls it, the road, the boundary lines. They’re going to be on
            the edge of plus. Whichever direction we draw it, this plus is the edge of the plus region. If we made this
            the edge of the plus region and everything on this side is plus and everything on this side is minus, we’d
            be screwed because there’s two pluses on the other side of that.</p>
        <h2 id="unknown-525">未知</h2>
        <h2>Unknown</h2>
        <p>一般来说，在尝试寻找支持向量时，你会做一些类似于我疯狂的最近邻方法的事情，并尝试找到彼此接近的正负对。但有时，这不仅仅是两个点，因为有时如果你试图画出简单的东西，即两个点的垂直平分线，你会陷入困境，因为有另一个点挡住了你的路。
        </p>
        <p>Generally, when trying to find a support vector, you do something a little bit similar to my crazy method of
            doing nearest neighbors, and try to find a plus minus pair that’s close to each other. Sometimes though,
            it’s not just two points because sometimes if you try to draw the simple minded thing, which is the
            perpendicular bisector of the two points, you get screwed because there’s another point in your way.</p>
        <p>现在我已经给出了线索​​，让我们开始吧。希望你们能理解。支持向量是边缘上的那些，它们几乎肯定是正数，或者几乎肯定是负数。让我们回过头来。你能给我一个负支持向量吗？ 观众：最上面的那个？ 教授：嗯？ 观众：最上面的负点？
            教授：左上角的那个？是的。</p>
        <p>So now that I’ve given away a clue, let’s go. and hopefully that made sense to you guys. The support vectors
            are the ones on the edges that are just barely a plus for sure, or just barely a minus for sure. Let’s go
            back. Can you give me a negative support vector? AUDIENCE:. The top one? PROFESSOR: Hmm? AUDIENCE:. The top
            negative point? PROFESSOR:. The one on the top left? Yes.</p>
        <p>有没有人想过还有第三个支持向量？好吧，让我们简单地尝试一下。记住，支持向量总是试图在正负之间留出尽可能宽的空间。所以让我们简单地尝试做垂直平分线，看看是否会搞砸我们。所以当我们简单地做垂直平分线时，它会像这样穿过这里。而且很好。所以这是仅有的两个支持向量。
        </p>
        <p>And does anyone think that there’s a third support vector? Well, let’s simple mindedly try the thing that.
            remember, support vectors always attempt to have the widest possible space between the pluses and minuses
            that they can. So let’s simple mindedly try to do the perpendicular bisector and see if screws us over. So
            when we simple mindedly do the perpendicular bisector, it goes through here like this. And it’s just fine.
            So these are the only two support vectors.</p>
        <h2 id="unknown-526">未知</h2>
        <h2>Unknown</h2>
        <p>这就是我们的分界线。所以我们快到了最后阶段。但我们必须找到 w 和 b。在过去，我们会通过将 w 点代入加支持向量，加上 b 等于 1 来找到 w 和 b。哦，这非常关键。这些 w 点 x 加 x 减只有等于 1 或负
            1 才成立？还是只对支持向量成立？</p>
        <p>And there’s our divider line. So we’re on the home stretch. But we have to find w and b. In olden days, we
            would find w and b by plugging in w dot the plus support vector, plus b equals 1. Oh, that’s very crucial.
            These w dot x plus x minus are only true equaling 1 or negative 1? Or only true for support vectors?</p>
        <p>w 点任何正点加上 b 总是会得到某个正数，这始终是正确的。但它并不总是 1。事实上，它在这里总是大于 1。它在那里总是小于 1。在过去，我们会将 1,2 代入这个等式。我们会将 3, 2 代入这个等式。我们会将
            alpha plus 等于 alpha minus 代入其和中。</p>
        <p>It’s always true that w dot any positive point plus b will be some positive number. But it won’t always be 1.
            In fact, it will always be greater than 1 up over here. It will always be less than 1 down over there. In
            olden days, we would plug in 1,2 into this equation. We would plug in 3, 2 into this equation. We’d plug in
            alpha plus equals alpha minus in its sums.</p>
        <p>因为只有 1 加 1 减，我们知道它们是相等的。然后我们会摆弄这个 w 方程。但是，有更好的方法可以做到。所以让我们使用这个廉价策略来解决这个版本的
            SVM。方法如下。首先，我知道我没有完全正确地画出它们。抱歉。但有人可以通过查看。这是 3，2.2，这是 1,2。有人能告诉我这个方程是什么吗？</p>
        <p>And since there’s only 1 plus 1 minus, we’d know they were equal. And then we’d fidget around with this w
            equation. However, there is a better way to do it. And so let’s use this cheap strategy to solve this
            version of the SVM. Here’s how. First, and I know I didn’t draw these completely straight. Sorry. But can
            anyone, by looking at. this is three, 2.2 And this is 1,2. Can anyone tell me what the equation.</p>
        <h2 id="unknown-527">未知</h2>
        <h2>Unknown</h2>
        <p>你可以计算 y 等于 mx 加 b。如果我擅长画画，谁能告诉我虚线的等式应该是什么？听众：教授：人们说 y 等于 x 减 1。我说是的，y 等于 x 减 1。因此，正数应该是 y 确实大于或等于 x 减 1。</p>
        <p>You can do y equals mx plus b. Can anyone tell me what the equation of the dotted line is supposed to be if I
            was good at drawing? AUDIENCE: PROFESSOR: People say y equals x minus 1. And I say yes, y equals x minus 1.
            So therefore, the pluses would be y is greater than or equal to x minus 1 indeed.</p>
        <p>所以我们已经看到 w 点 x 加 b 可以以某种方式转换为这种形式。对吧？因此，如果 y 等于 x 减 1，那么我们就知道 w 点 x 加 b 等于 0。让我们在这里这样做。所以我们知道 w1 x1。我们甚至可以称之为
            x 和 y。我认为这没问题。没有人会追随我们。</p>
        <p>So we’ve already seen that w dot x plus b somehow can be converted into this form. Right? So therefore, if we
            have y equals x minus 1, then we know that we have we have w dot x plus b equals 0. Let’s do that here. So
            we know that w1 x1. We can even call it x and y. I think it’ll be fine. No one will come after us.</p>
        <p>W2 y 加 b 等于 0。但我们也知道 y 等于 x 减 1，这意味着如果 y 等于 x 减 1，那么根据我们这里的这个东西，负 w1 除以 w2 等于。</p>
        <p>W2 y plus b equals 0. But we also know that y equals x minus 1, which means that if y equals x minus 1, then
            according to this thing we have over here, then negative w1 over w2 equals.</p>
        <h2 id="unknown-528">未知</h2>
        <h2>Unknown</h2>
        <p>所以我们知道负 w1 除以 w2。我们有 b 除以 w2。所以 y 等于 x 减 1。如果我们解这个等式，使它看起来像这样，我们会得到 y 等于负 w1 除以 w2 减 b 除以
            w2。所以我们以某种方式、形状或形式知道了这一点。</p>
        <p>So we know that negative w1 over w2. and we have b over w2. So y equals x minus 1. And if we solve this
            equation to make it look like this, we would have y equals negative w1 over w2 Minus b over w2. So we know
            that in some way, shape, or form.</p>
        <p>因此，我们知道，w1 除以 w2 是负 1 的标量倍数。并且我们知道，b 除以 w2 实际上是正 1 的标量倍数。标量倍数，什么是标量倍数？那么，为什么它是标量倍数？为什么它不只是负 1 或正
            1？因为在这个等式中，我们可以将整个等式乘以任意数字，它仍然具有相同的边界线。</p>
        <p>We know that then therefore, w1 over w2 is some scalar multiple of minus 1. And we know that b over w2 is, in
            fact, some scalar multiple of positive 1. Scalar multiple, what’s a scalar multiple? Well, why is it a
            scalar multiple? Why isn’t it just going to be negative 1 or positive 1? Just because in this equation, we
            can multiply the entire equation by any number and it will still have the same boundary line.</p>
        <p>你们看到了吗？哦，这里有个 x。如果我们把所有的东西都乘以，因为所有的东西都除以 w2。如果我们把 w2 翻倍，同时把 b 和 w1
            也翻倍，那么它就是完全相同的等式。你们同意吗？所以，事实上，有无数个可能的等式。你会说，好吧，马克，太好了。你已经弄清楚它是什么形式了。</p>
        <p>You guys see that? Oh, there’s an x here. If we multiplied everything, since it’s all divided by w2. If we
            double w2, but also doubled b and w1, it would be the exact same equation. Do you guys agree? So there’s, in
            fact, infinitely many possible equations. You say, well, great, Mark. You’ve figured out what form it is.
        </p>
        <h2 id="unknown-529">未知</h2>
        <h2>Unknown</h2>
        <p>所以你算出 w1 除以 w2 等于负 1 的某个标量倍数。所以它是负 1 乘以。每个人最喜欢的字母是什么？听众：k。教授：k。负 1 乘以 k。我们算出 b 除以 w2 等于。我想我们可以把负数等于正 k。但 k
            是多少？我们该怎么算出来？嗯，这是个好问题。我会告诉你怎么做。</p>
        <p>So you figured out that w1 over w2 equals some scalar multiple of negative 1. So it’s negative 1 times.
            what’s everyone’s favorite letter? AUDIENCE: k. PROFESSOR: k. Negative 1 times k. And we figured out that b
            over w2 is. I guess we can just do negative is positive k. But what’s k? How are we going to figure it out?
            Well, it’s a good question. And I will tell you how.</p>
        <p>我将断言以下事实为真，无需证明。那么，我就不证明它了。1 除以 w 的量级，也就是这个带有 w1 和 w2 的向量，等于这个，这是我刚刚画的那条线，从这里到这个点的线。1 除以 w 的量级等于这个。</p>
        <p>I will assert the following fact as true without proof. Then I will not prove it. 1 over the magnitude of w,
            which is this vector here with w1 and w2, equals this where this is that line that I just drew, the line
            from here to this point. 1 over the magnitude of w equals this.</p>
        <p>因此，由于 1 除以 w 的量级等于这个，而我相信这个等于 2 根 2，因为我们要归结为勾股定理，2，根 2。因此，将所有内容翻转。w 的量级等于 1 除以 2 根 2。因此，w 的量级等于根 2 除以
            4。但为什么我们没问题？那么，我们如何计算 w 的量级？一般来说，人们知道向量的量级吗？</p>
        <p>Therefore, since 1 over the magnitude of w equals this, and this equals, I believe, 2 root 2, because we’re
            going over to, down to, Pythagorean Theorem, 2, root 2. So therefore, flip everything over. Magnitude of w
            equals 1 over 2 root 2. So therefore, magnitude of w equals root 2 over 4. But why are we OK? Well, how do
            we calculate the magnitude of w? Do people know, in general, magnitudes of vectors?</p>
        <h2 id="unknown-530">未知</h2>
        <h2>Unknown</h2>
        <p>通常，对于这些向量，我们用分量平方和的平方根来计算。因此，w1 平方加上 w2 平方的平方根等于 2/4。但这还不是全部。我们说这还不是全部，因为我们从这里知道 w1 和 w2 的比率是。听众：教授：是的，w1 和
            w2 的比率将是。实际上，抱歉。我不应该在这里写 ak。</p>
        <p>Generally, for these vectors, we do it by the square root of the sum of the components squared. So the square
            root of w1 squared plus w2 squared equals root 2 over 4. But that’s not all. That’s not all, we say, because
            we know from this over here that the ratio of w1 and w2 is. AUDIENCE: PROFESSOR: Yeah, the ratio of w1 and
            w2 is going to be. actually, sorry. I shouldn’t put a k here.</p>
        <p>我意识到我可能让你们很困惑。w1 除以 w2 就是 1。B 除以 w2 就是 1。这只是一个事实。没有 k。k 是为了确定 w1 和 w2 是什么。所以 w1 等于 k。而 w2 等于正 k。b 也等于正
            k。顺便问一下，我问你们一个问题。我可以将负号放在 w2 和 b 上，而不是放在 w1 上吗？</p>
        <p>I realize I probably have been confusing you guys a lot. w1 over w2 is just 1. B Over w2 is just 1. That’s
            just a fact. There’s no k. The k is to determine what w1 and w2 are. So w1 equals k. And w2 equals positive
            k. And b equals also positive k. By the way, here’s a question for you. Could I have put the negative sign
            on w2 and b instead of on w1?</p>
        <p>很多人都说是的。这个回答很聪明。其实不是，因为正数在负 x 轴上。这只是我学到的一个小技巧。当其中一个是负数而另一个不是时，就顺着正数走。所以我们知道 w1 是 k，w2 是正 k，b 是正 k。</p>
        <p>So many people said yes. That’s a very smart answer. Actually, no, because of the fact that the pluses are on
            the negative x axis. It’s just a little trick I picked up. When one of them is negative and the other one
            isn’t, follow the pluses. So we know that w1 is k, w2 is positive k, and b Is positive k.</p>
        <h2 id="unknown-531">未知</h2>
        <h2>Unknown</h2>
        <p>W1 除以 w2 等于 1。b 除以 w2 等于正 1。那么，我们对 w1 和 w2 的比率了解多少呢？它等于 1。这意味着当我们求它的平方时，w1 平方等于 w2 平方。因此，这是 2 w1 平方的平方根，等于根 2
            w1。实际上不是，它不等于根 2 w1，因为 w1 实际上是负数。所以它是负根 2 w1。这没关系。</p>
        <p>W1 over w2 is 1. b over w2 is positive 1. So what do we know about the ratio of w1 and w2? It’s equal to 1.
            And that means that when we square it, w1 squared equals w2 squared. So therefore, this is the square root
            of 2 w1 squared, which equals root 2 w1. Well, actually no, it doesn’t equal root 2 w1 because w1 is
            actually negative. So it’s negative root 2 w1. It doesn’t matter.</p>
        <p>关键是，如果等于 2/4 的根，那么 w1 就是。各位？听众：负 1/4。教授：负 1/4。Bingo。如果 w1 是 1/4,1，其他一切都会水到渠成。w2 和 b 是多少？各位？听众：正 1/4。教授：正
            1/4。我们明白了。我们完成了这部分问题。但是，还有奖励。让我们来看看他们没有要求您计算的 alpha。实际上，您知道吗？</p>
        <p>The point is that if that equals root 2 over 4, then w1 is. everyone? AUDIENCE: Negative 1/4. PROFESSOR:
            Negative 1/4. Bingo. And if w1 is 1/4,1 everything else falls into place. What are w2 and b? Everyone?
            AUDIENCE: Positive 1/4. PROFESSOR: Positive 1/4. We got it. We’re done with this part of problem. However,
            bonus. Let’s come to the alphas, which they didn’t ask you to calculate. Actually, you know what?</p>
        <p>如果时间充裕，我们会计算 alpha，因为他们实际上并没有要求您计算它们。但是，我的建议是，由于只有一个 alpha 加号和一个 alpha 减号，因此从这个等式来看它们必须相等，因为 alpha 加号的总和等于
            alpha 减号的总和。因此，w 等于 w 的总和。抱歉，这应该是 x。当然，这个等式中没有一百万个 w。</p>
        <p>We’ll do the alphas if we have enough time, since they didn’t actually ask you to calculate them. However, my
            recommendation is since there’s only one alpha plus and one alpha minus, they must be equal from this
            equation, since the sum of the alpha plus equals the sum of alpha minus. And so therefore, w equals the sum
            of w. sorry, this should be an x. Of course, there’s not a million w’s in this equation.</p>
        <h2 id="unknown-532">未知</h2>
        <h2>Unknown</h2>
        <p>正数据点乘以它们的 alpha 值的总和减去负数据点乘以它们的 alpha 值。所以我们在这里看到 1/4,1/4 相等。我们得到了什么？正点负 1,2？所以我们得到了 alpha，该点的 alpha 值负 1,2
            减去该负点的 alpha 值。那是什么？</p>
        <p>The sum of the positive data points times their alpha is minus the negative data points times their alphas.
            So we’re looking at here 1/4,1/4 equals. what do we got here? Positive point negative 1,2? So we’ve got
            alpha, alpha of that point negative 1,2 minus alpha of that minus point. And what is that?</p>
        <p>是 3、2.3、2。因此，如果两个相等的 alpha 都是 1，那么我们就会得到 4、4。但我们想要 1/4、1/4。因此，实际上两个 alpha 都是
            1/16。这就是答案。如果有时间，我们会更深入地讨论这个问题。但我们不会。所以让我们做第二个。让我们进入更快的模式。第二个在很多方面与第一个非常相似。</p>
        <p>It’s 3, 2.3, 2. So if both of the alphas which are equal were 1, we’d have 4,4. But we want 1/4,1/4. So
            actually both of the alphas are 1/16. And that’s the answer. We’ll do that more in depth if we have time.
            But we won’t. So let’s do number two. So let’s go into faster mode. Number two, very similar to number one
            in many ways.</p>
        <p>但正如您所看到的，他们在 2、1
            处添加了一个额外的减号，这是其中最重要的一点。所以我认为我们都同意这仍然是我们的加号。实际上，他们还在那里添加了另一个加号。所以也许这个加号是一个支持向量。但事实并非如此。这个加号是一个支持向量。你们觉得这个新的负号怎么样？
        </p>
        <p>But as you can see, one of the main things that they added an extra minus sign at 2, 1. So I think we can all
            agree that this will still be our plus. Actually, they added another plus sign there, too. So maybe this
            plus sign is a support vector. But it’s not. This plus sign is a support vector. What do you guys think
            about the new negative sign?</p>
        <h2 id="unknown-533">未知</h2>
        <h2>Unknown</h2>
        <p>它会变成支持向量吗？因为它更接近正值？是的，你说得对。好吧，这是一个非常漂亮的除法，因为如果我这样做是正确的，虽然我没有，但如果我们假装我这样做了。那么虚线就是。听众：教授：y 等于 x。</p>
        <p>Will it become a support vector since it is strictly closer to the pluses? Yep, you’re right. OK, so this is
            a very beautiful division because if I do this correctly, which I didn’t, but if we pretend that I did. Then
            the dotted line is. AUDIENCE: PROFESSOR: y equals x.</p>
        <p>好的，所以有了 y 等于 x 处的虚线，那么就像我们在这里所做的那样，我们知道如果 y 等于 x 加 0，那么我们知道首先，b 等于 0。其次，我们知道如果 y 等于 x，那么我们知道 w1 除以 w2 等于
            1。加号仍然在左边和上面，所以我们知道 w1 是某个负数 k，而 w2 是某个正数 k。</p>
        <p>OK, so with the dotted line at y equals x, then just like we did up here, we know that if y equals x plus 0,
            we know that first of all, b equals 0. Second of all, we know that if y equals x, then we know that w1 over
            w2 equals 1. The pluses are still on the left and up, so we know that w1 is some negative number, k, and w2
            is some positive number k.</p>
        <p>太好了。我们该怎么算呢？好吧，我们把这个叫做 d，代表距离，或者你想叫什么都行。所以 1 除以 w 等于 d。在这种情况下，d 不是 2 除以 2。大家能说出这里的 d 是多少吗？听众：教授：事实上是。</p>
        <p>Great. How are we going to figure it out? Well, let’s call this d for distance, or whatever you want to call
            it. So 1 over w equals d.&nbsp;d in this case is not 2 over 2. Can everyone tell what d is here? AUDIENCE:
            PROFESSOR: It’s actually.</p>
        <h2 id="unknown-534">未知</h2>
        <h2>Unknown</h2>
        <p>所以它超过 2 和 1。所以它应该是 1/2 根号 2，因为这个宽度距离是其两倍，超过 3 和 3，也就是 3 根号 2。所以它是 1/2 根号 2。我不喜欢在那里放小数和其他东西。</p>
        <p>So it goes over 2 and 1. So it should be 1/2 root 2 since this width distance, which is twice as much, goes
            over 3 and 3, which is 3 root 2. So it’s 1/2 root 2. I don’t like putting in decimals and stuff there.</p>
        <p>因此，我们会说 2 除以 w 的量级等于 2d 等于 3 根 2。因此，对，毕达哥拉斯，一，二，三，一，二，三，3 根 2。因此，w 的量级等于。让我们看看。将它们互换一下。</p>
        <p>So we’ll say that 2 over magnitude of w equals 2d equals 3 root 2. So therefore, right, Pythagorean, one,
            two, three, one, two, three, 3 root 2. So therefore, magnitude of w equals. let’s see. Switch them over.</p>
        <p>我们应该得到 3 分之 2 的根号。如果 w 的大小是 3 分之 2 的根号，我们可以用前面同样的方法，2 的平方根 w2 的平方等于 3 分之 2 的根号。而这只是 2 的根号乘以 w2 等于 3 分之 2
            的根号。因此，w2 是？1/3。而 w1 是。1/3。Bingo。我们得到了 w1。我们得到了 w2。我们知道 b 为零，因为它显然是 0。y 等于 x。</p>
        <p>We should get root 2 over 3. And if magnitude of w is root 2 over 3, we can do our same trick from before,
            square root of 2 w2 squared equals root 2 over 3. And this is just root 2 times w2 equals root 2 over 3. So
            therefore, w2 is? 1/3. And w1 is. 1/3. Bingo. We’ve got w1. We’ve got w2. We know that b was zero because
            obviously it’s 0. It’s y equals x.</p>
        <h2 id="unknown-535">未知</h2>
        <h2>Unknown</h2>
        <p>我们完成了。这很快。alpha 可能需要更长时间。实际上，这个 alpha
            比其他任何地方都更麻烦，因为如果您能看到它，让我们看看这个。我们又添加了一些新点。我们在这里得到了这个点，在那里得到了这个点。所以我认为很明显这个正负号是最接近的。</p>
        <p>And we’re done. That was fast. The alphas might taken longer. Actually, the alphas on this one are more of a
            pain in the ass than anywhere else because let’s take a look at this one if you can see it. We’ve added in
            yet some new points. We’ve got this point up here and this point down there. So I think pretty clearly this
            plus and minus are the closest to each other.</p>
        <p>但是如果我们取这两者之间的垂直平分线并这样做会发生什么？这个加号在中间。因此，这个加号也必须是支持向量。所以我们不能只画这条线。我们必须包括这个。我们最好的划分是什么？垂直线，没错。垂直线就是这样。这意味着我们的边界方程，这里的虚线，y
            轴。</p>
        <p>But what happens if we take the perpendicular bisector between these two and do like this? This plus is in
            the middle. So therefore, this plus is going to have to also be a support vector. So we can’t just draw this
            line. We have to include this. What’s our best division? Vertical lines, that’s right. Vertical lines just
            so. And that means that the equation of our boundary, the dotted line here, y axis.</p>
        <p>因此，边界与 y 轴的方程为，b 等于 0。w2 也等于 0。所以唯一不等于 0 的就是 y 轴的方程。听众：等一下，w1 教授：w2 等于 0。所以 w2 等于 0。b 等于 0。但是 w1 不等于零，因为它只是 y
            轴的方程。</p>
        <p>So the equation of the boundary with the y axis, then b equals 0. And hell, w2 equals 0. So the only thing
            that is not. AUDIENCE: Wait, w1 PROFESSOR: w2 equals 0. So w2 equals 0. b equals 0. But w1 is not equal to
            zero because it’s just the equation of the y axis.</p>
        <h2 id="unknown-536">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们知道该方程就是 w1 乘以 x 等于 0。所以它是 w1 乘以 x 等于 0。我们知道这本质上意味着 x 将是某个 k。由于正数仍在左边，因此它也是负数。然后我们必须找出那个 k
            是多少。我们将使用我们的老技巧。希望到现在为止已经很老了。</p>
        <p>So we therefore know that the equation is just w1 times x equals 0. So it’s w1 times x equals 0. And we know
            that just means essentially that x is going to be some k. It’s also going to be negative because of the fact
            that the pluses are still on the left. Then we’re going to have to figure out what that k is. We’ll use our
            old trick. by this point old, hopefully.</p>
        <p>w 的量级除以 1 等于 d。这次 d 正好是 1。因此，w 的量级等于 1。w 中只有一个分量。因此 w1 为 听众：1。 教授：1，因为加号在左边。大家看到了吗？还不错。这个很容易计算 w。但要得到所有的
            alpha 就不那么容易了。但让我们继续讨论一个新的、更有趣的问题。也许不是。</p>
        <p>One over magnitude of w equals d.&nbsp;This time d is just 1. So therefore, magnitude of w equals 1. There’s
            only one component in w. So therefore w1 is AUDIENCE: 1. PROFESSOR: 1 because the plus is on the left. Do
            people see that? Not too bad. This one’s easy to calculate the w. But it’s not as easy to get all the
            alphas. But let’s move on to a new and even more fun. maybe not.</p>
        <p>问题是，这个是哪个。如你所见。也许不是。这是一个一维向量。这些向量只有一个维度。所以它看起来就像一条数轴。这个维度从 9 到正 9
            不等。它只有一个分量。你不必担心任何带有两个分量的疯狂量级，只需将所有东西都作为单个分量即可。</p>
        <p>Question, which is this guy. As you can see. well maybe not. This is a one dimensional vector. These vectors
            only have a single dimension. So it just looks like a number line here. That dimension varies from it looks
            like 9 to positive 9. It just has one component. You don’t have to worry about any of these crazy magnitudes
            with two components, just everything as a single component.</p>
        <h2 id="unknown-537">未知</h2>
        <h2>Unknown</h2>
        <p>但是，很明显，线性基线会把我们搞砸，因为此时的线就像，咕噜，所有这些都是优点。所有这些都是缺点。好吧，太好了，这并没有解决所有问题。那么我们该怎么做呢？好吧，我们将使用通常可能是 SVM
            中最难的东西，但在这种情况下对我们来说不会太糟糕。</p>
        <p>However, it’s obvious that a linear basis line is going to completely screw us up here, since lines at this
            point are just like, grunk, all these are pluses. All these are minuses. Well, great, that doesn’t get them
            all. So how are we going to do it? Well, we’re going to use what is usually perhaps the hardest thing in
            SVMs, but in this case is not going to be too bad for us.</p>
        <p>我们将使用一个核。现在，基于我第一次上这门课时对核的了解很少，我猜你们想对这些核进行一些解释。你们可能见过它们。你们还记得帕特里克讲座上的核吗？有这个 phi。然后是这个
            k。然后它们变得非常复杂。好的，这就是核的工作原理。基本思想是这样的。我会在这里写出来。</p>
        <p>We’re going to use a kernel. Now, based on how little I understood kernels the first time I took this class,
            I’m guessing that you guys would like to have some explanation on these kernels. You probably saw them. You
            remember the kernels from Patrick’s lecture vaguely? There’s this phi. And then there’s this k. And then
            they get really complicated. OK, so here’s how the kernel works. The basic idea is this. And I’ll write it
            over here.</p>
        <p>哦，哇，还有更多东西。好的。我就写在这里。基本思想是这样的。我们取正常空间，也就是这条数轴，或者可以是任何类型的正常空间，然后取一个向量，将一个称为 phi 的函数放入其中。向量 x 的 phi 将 x
            带入某个新维度。</p>
        <p>Oh, wow, there’s more stuff. OK. I’ll write it right here. The basic idea is this. We’re taking the normal
            space, which is this number line or it could be any kind of normal space, and we’re going to take a vector,
            we’re going to put into it a function called phi. And phi of vector x brings x into some new dimension.</p>
        <h2 id="unknown-538">未知</h2>
        <h2>Unknown</h2>
        <p>Phi，或者如果你更喜欢叫它“phee”，通常是一件令人讨厌的工作，你永远都不想看到它。有时它不是太糟糕。Phi 是将其带入新维度的函数。好吗？当你将数据带入新维度时，有时你可以在该维度上切一条直线，你就会很高兴。
        </p>
        <p>Phi, or “phee” if you like it better, is usually a nasty piece of work and something you never, ever want to
            look at. Sometimes it’s not too bad. Phi is the function that brings it into the new dimension. OK? And when
            you brought the data into a new dimension, sometimes you can just cut a straight line in that dimension and
            you’ll just be happy.</p>
        <p>然而，这位非常聪明的支持向量机发明者指出，你实际上并不需要使用函数 phi，即使 phi 是一个绝对可怕的怪物，因为你永远不需要知道所有这些向量 x
            在新空间中实际上是什么，至少不需要直接知道。在上面这些方程中，我们从未单独使用过 x。</p>
        <p>However, something that was noted by the very, very smart inventor of support vector machines is that you
            don’t actually need to work with function phi, even if phi is an absolutely horrible monstrosity, because of
            the fact that you never need to know what all these vectors x actually are in the new space, at least not
            directly. In none of these equations up here do we ever use x by itself.</p>
        <p>但是，我们确实使用了 x 与其他元素的点积。所以他想出了一个非常巧妙而出色的捷径。好的，那么。哦，我不应该使用 x1 和 x2。我将使用 x 和 z。因此，如果您有两个向量 x 和
            z，它们位于常规空间中，则将它们放入这个称为内核的函数中。然后它会告诉您 phi x 点乘 phi z。如果您有这个，您就不需要 phi 了。</p>
        <p>However, we do use x being dot product with something else. So he figured out a very sneaky and excellent
            shortcut. OK, so. oh, I shouldn’t use x1 and x2. I’ll use x and z. So if you have two vectors, x and z,
            which are in a regular space, you put them into this function called the kernel. Then it will tell you phi x
            dotted with phi z. And if you have that, you don’t need phi.</p>
        <h2 id="unknown-539">未知</h2>
        <h2>Unknown</h2>
        <p>大家明白了吗？大家明白我们为什么不需要 phi 了吗？看看上面的所有这些方程。至少在这些向量方程中，我们从来没有单独考虑过 x。现在，计算 alpha，是的，这有点模糊。此外，你可能会问，你为什么要这样做？你无法计算
            alpha。事实证明，除了这些非常简单的线性问题之外，人类的大脑无法计算 alpha。</p>
        <p>Does everyone see that? Does everyone see why we don’t need phi? Look at all these equations up here. We have
            never looked at x by itself in these vector equations at least. Now, calculating alphas, yeah, that gets a
            little bit fuzzy. Also, you may ask, why would you do this? You can’t calculate the alphas. It turns out
            that actually, other than for these very simple linear problems, human minds cannot calculate the alphas.
        </p>
        <p>事实上，你要进行非常复杂的二次优化。事实上，找出最佳 alpha 值是你在现实世界中使用 SVM 时需要爬山才能找到的。你会说，好吧，当我知道只有一个峰值时，我会运行我的算法，这非常非常好，因为它是二次优化。让我找出
            alpha 值。</p>
        <p>In fact, you run a very complicated quadratic optimization. In fact, finding out the best alphas is the thing
            that you hill climb on when you’re doing SVMs in the real world. You say, all right, I’ll run my algorithm
            when I know there’s only one peak, which is very, very good because it’s quadratic optimization. Let me
            figure out the alphas.</p>
        <p>因此，事实上，如果您只知道核函数而不知道 phi 函数，那么无法使用这些 alpha 方程来计算 alpha 值也没关系，因为通常情况下，计算机会通过二次优化为您计算出 alpha
            值。在这些简单的问题中，我们知道您可以计算出 alpha 值。因此，我们有核函数，它基本上为我们提供了新空间中事物的点积。</p>
        <p>So in fact, it doesn’t matter that you can’t use these alpha equations to figure out the alphas if you only
            know the kernel function and not the phi function because normally, the computer figures out the alphas for
            you with quadratic optimization. Just in these simple problems, we know you can calculate the alphas. So we
            have the kernel, which basically gives us the dot product of the things in the new space.</p>
        <h2 id="unknown-540">未知</h2>
        <h2>Unknown</h2>
        <p>既然如此，我在这里给你核。我希望你给我 phi。有人有个主意，她的名字显然是 Susan Q. Random Student。她有个主意，如果我们有一个 x 和 z
            的核。实际上，我想它们不是向量。它们只是单个分量。核等于余弦 Pi 除以 4x 乘以余弦 Pi 除以 4z 加上正弦 Pi 除以 4x 加上正弦 Pi 除以 4z。</p>
        <p>So being that as it may, I’ll give you the kernel here. I’d like you give me phi. Someone got an idea, whose
            name was Susan Q. Random Student, apparently. She got an idea that if we had a kernel for x and z. actually,
            they’re not vectors, I guess. There just single components. And the kernel equals cosine Pi over 4x times
            cosine Pi over 4 z plus sine Pi over 4x plus sine Pi over 4z.</p>
        <p>所以这就是新的点积。哦，等等，抱歉。我把一个 z 放在了括号外。我真傻。所以，4x 的余弦乘以 4z 的余弦加上 4x 的正弦加上 4z 的正弦就是新的乘积。这就引出了一个问题。这个问题很简单，所以我们可以计算出
            phi。</p>
        <p>So that is the new dot product. Oh, wait, sorry. I put one of the z’s not inside the parentheses. That was
            silly of me. So cosine of the quantity pi Over 4x times cosine of the quantity Pi over 4z plus sine of
            quantity Pi over 4x plus sine of quantity Pi over 4z is the new product. So that begs the question. This is
            an easy one so we can calculate the phi.</p>
        <p>x 的 phi 是多少？我们实际上是从一维开始计算的，我们可能需要反复研究才能得到这个值。这个值变成了一个新的点积。它取代了点积。请记住，标量的点积只是将两个数字相乘。因此，这实际上使它变得有点复杂。有人认为他们知道
            phi 吗？哦，我们找到了一个。您觉得呢？</p>
        <p>What is phi of x? We’re actually taking it from one dimension and we may be playing around with it a lot to
            get this. And this thing has become a new dot product. It replaces dot product. And remember, the dot
            product for scalars would have just been multiplying two numbers together. So it actually makes it a little
            bit more complicated. Does anyone think they know the phi? Oh, we got one. What do you think?</p>
        <h2 id="unknown-541">未知</h2>
        <h2>Unknown</h2>
        <p>听众：教授：您指的是两个常见的二维向量吗？听众：两个点。教授：当然。完全正确。如果您不是我们勇敢的志愿者，您将如何在实际测验中解决这个问题？好吧，如果您眯着眼睛看，那个 k 实际上并不多。它几乎是 Pi 的余弦除以 4
            和 Pi 的正弦除以 4 之间的点积。我的意思是，看看它。</p>
        <p>AUDIENCE: PROFESSOR: You mean two common vectors, two dimensional? AUDIENCE:. The two points. PROFESSOR:
            Absolutely. That’s exactly correct. How would you have solved this on the actual quiz if you’re not our
            brave volunteer? Well, that k, if you squint at it. not very much actually. is pretty much a dot product
            between cosine of Pi over 4 and sine Pi over 4. I mean, look at it.</p>
        <p>记住，如果 x 和 z 向量的点积是 x1、z1 加 x2、z2。那么基本上就是 x1、z1 加 x2
            z2。哦，这应该是乘以。是的，这应该是乘以。抱歉。那里有一个加号。任何因此而错过的人，都是我的错。那应该是乘以。那应该是乘以上面的。</p>
        <p>Remember, if the dot product of x and z vectors is x1, z1 plus x2, z2. so that basically is x1, z1 plus x2
            z2. Oh, this should have been a times. Yeah, this should have been a times. Sorry. There’s a plus there.
            Anyone who missed it because of that, my bad. That’s should have been a times. That should have been a times
            up there.</p>
        <p>它是余弦 Pi 除以 4x 余弦 Pi 除以 4z 加上正弦 Pi 除以 4x 乘以正弦 Pi 除以 4z。所以，是的，它基本上是余弦 Pi 除以 4x 和正弦 Pi 除以 4x
            之间的点积。Bingo。好的，最后一件事。好吧，我们还没有完成，因为我们可能会问一些问题。然后我们将看看我们是否可以计算这些 alpha。但最后一件事，让我们在这个新维度上绘制所有点。</p>
        <p>It’s cosine Pi over 4x cosine Pi over 4z plus sine Pi over 4x times sine Pi over 4z. So yeah, it’s basically
            the dot product between cosine Pi over 4x and sine Pi over 4x. Bingo. All right, last thing. Well, we’re not
            done yet because we’re going to maybe ask some questions. And then we’re going to see if we can calculate
            those alphas. But last thing, let’s graph in this new dimension all the points.</p>
        <h2 id="unknown-542">未知</h2>
        <h2>Unknown</h2>
        <p>显然，余弦和正弦，所以我们会得到 1 和 1
            之间的结果。让我们看看。也许我可以画出图表。我是否在所有这些上都写过？等一下，也许是这个。不，人们在那里画了奇怪的火柴人。好的。哦，是的，这个有点乱。但我们会在这个上做。好的，所以这是
            1、1、1/2、1/2、1、1、1/2、1/2。好吗？</p>
        <p>So obviously, cosines and sines, so we’re going to get results between 1 and 1. Let’s see. Maybe I can graph
            it. did I write on all these? Wait, maybe this one. No, people drew weird stick figures there. OK. Oh, yeah,
            this one’s kind of messy. But we’ll do it on this. OK, so this is 1, 1,1/2, 1/2,1, 1, 1/2,1/2. OK?</p>
        <p>鉴于此，让我们尝试用余弦乘以 Pi/4，将这条数轴上的所有点绘制到这个崭新的维度中。好吧，太好了。那么我们先做加法。0 处的加法是余弦 0，正弦 0。那是什么？听众：1,0。教授：那是 1,0。没错。</p>
        <p>So given that, let’s try to graph all these points on this number line into this brave new dimension by using
            their cosine times Pi over 4. So all right, great. So let’s do the pluses first. The plus at 0 is cosine 0,
            sine 0. So what is that? AUDIENCE: 1,0. PROFESSOR: That’s 1,0. That’s right.</p>
        <p>事实上，8 加 8 也是乘以 3。8 加 8 也是乘以 3，因为这样它就是 2 Pi 减 2 Pi，余弦和正弦都是周期的。好，太好了。那 1 呢？嗯，那是余弦 Pi 除以 4，正弦 Pi 除以
            4。那是什么？听众：教授：是的，它是根 2 除以 2，根 2 除以 2。所以这就是我们所说的。</p>
        <p>In fact, the 8 and the 8 are also that times 3. The 8 and the 8 are also that because then it’s just 2 Pi
            minus 2 Pi, which both cosine and sine are periodic. OK, great. What about the 1? Well, that’s cosine Pi
            over 4, sine Pi over 4. And what’s that? AUDIENCE: PROFESSOR: Yeah, it’s root 2 over 2, root 2 over 2. So
            that’s something like here, we’ll say.</p>
        <h2 id="unknown-543">未知</h2>
        <h2>Unknown</h2>
        <p>事实上，9 和 7 也是这样的。所以这两个数有三个。1 呢？它是 4 的余弦负数，4 的正弦正数。听众：教授：没错。x 值是 2 的正根 2。y 值是负数。同样，它们有三个。好的，很好。现在我们来做减法。</p>
        <p>And in fact, and the 9 and the 7 are also that. So there’s three of these two. What about the 1? That’s
            cosine negative pi over 4, sine Pi over 4. AUDIENCE: PROFESSOR: That’s right. The x value is positive root 2
            over 2. And the y value is negative. And again, there’s three of them. All right, great. Now let’s do the
            minuses.</p>
        <p>3 处有负号，与 7 处的负号相同。3 处的负号是 3π 除以 4 的余弦，3π 除以 4 的正弦。哪一个是？ 听众： 教授：是的，它将位于第二象限。余弦将为负。但正弦将为正。所以我们在这里得到 3 个点。</p>
        <p>There’s the minus at 3, which is also the same as the minus at 7. The minus at 3 is cosine 3 Pi over 4, sine
            3 Pi over 4. Which one is that? AUDIENCE: PROFESSOR: Yeah, that’s going to be in the second quadrant. The
            cosine is going to be negative. But the sine is going to be positive. And so we get 3 points here.</p>
        <p>您可能已经猜到了，另一个，即 5 Pi 除以 4，位于第三象限。我们在这里得到 3
            个点，支持向量在哪里？听众：问题。教授：问题？听众：我明白您在第一象限中得到三个正值的原因。但根据那里的线，您想要的是四个值的总数。教授：哦，您说得对。只有两个。好主意。</p>
        <p>And as you may have predicted, the other one, the 5 Pi over 4, is in the third quadrant. We get 3 points
            here, Where are the support vectors? AUDIENCE: Question. PROFESSOR: Question? AUDIENCE: I understand where
            you’re getting at the three quantities of pluses in the first of the quadrants. But according to the line
            there, you want that the total of four. maybe the values. PROFESSOR: Oh, you’re right. There’s only two.
            Good call.</p>
        <h2 id="unknown-544">未知</h2>
        <h2>Unknown</h2>
        <p>这里有两个负数。这里也有两个负数。说得好。这并没有改变问题。事实上，如果我们只是绘制更多的点，可能会有更多。但这是一个非常微妙和重要的区别。有两个负数。但除此之外，是的，这些是正确绘制的。有人看到支持向量在哪里吗？观众：前两个。观众：也许是前两个。教授：所以前两个，减号和加号。
        </p>
        <p>There’s two negatives here. And there’s two negatives here. Good call. It doesn’t change the problem. In
            fact, if we just graph more points, there might have been more. But that’s a very subtle and important
            distinction. There are two negatives. But otherwise, yeah, these are graphed correctly. Does anyone see
            where the support vectors are? AUDIENCE:. The first two. AUDIENCE: Maybe the top two. PROFESSOR:So the top
            two, the minus and plus.</p>
        <p>我们将尝试做垂直平分线。让我们看看。这可行。但猜怎么着？这些家伙在同一条线上。所以我们最好把它们圈起来。所以实际上，问题是什么不是支持向量？只有这个。只有那三个。问题？听众：你不能只在一个维度上做这个吗？我的意思是，你刚刚表明它们在同一条线上。所以你真的不需要余弦项和正弦项。
        </p>
        <p>We’ll try to do the perpendicular bisector. Let’s see it. That works. But guess what? These guys are on the
            same line. So we’d better circle them. So actually, the question is what isn’t a support vector? Only this.
            Only those three. Question? AUDIENCE: Couldn’t you have just done this in one dimension? I mean, you just
            showed that those ran on the same lines. So you really didn’t need the cosine term and a sine term.</p>
        <p>你可以只用余弦来证明这一切。教授：好吧，问题是我们能否在一维上做到这一点。我们所做的只是余弦。所以如果我们只做余弦，那么它们仍然很容易被整除。答案是，当然，我们可以。然而，问题并没有解决，因为 Susan Q.
            Random Student 决定做余弦和正弦。</p>
        <p>You could have proved all this with just the cosine. PROFESSOR: All right, the question is couldn’t we have
            done this in one dimension. All we do is only the cosine. So if we did only the cosine, then they would’ve
            still been easily divisible. The answer is, absolutely, we could have. However, the question did not because
            Susan Q. Random Student decided to do cosine and sine.</p>
        <h2 id="unknown-545">未知</h2>
        <h2>Unknown</h2>
        <p>但是，是的，如果我们告诉你，学生，找到一个适用于此的 phi，你就可以找到一个余弦
            phi。那会更容易。但是，重要的是要对别人给你的东西做一些工作。在这种情况下，他们给了你那个变换，是的，这很浪费，因为多了一个维度。你不需要正弦，因为你在这里真的不需要 y 轴。你只需要 x。</p>
        <p>But yes, if we had said you, student, find a phi that will work for this, you could have found a phi that was
            just cosine. That would have been easier. However, it’s important to be a little work with what somebody
            else gives you. In this case, they gave you that transformation, which yeah, was wasteful with an extra
            dimension. You didn’t need the sine because you didn’t need the y axis really here. You just needed the x.
        </p>
        <p>大家知道这是怎么回事吗？也许你可以变换维度？最难的部分是他们通常会给你 ak 并要求你给出 phi，或者给你 phi 并要求你给出 k。但这也不算太糟。只要记住，如果他们给你 phi，就用它做点积。如果他们给你
            phi，那只是一个分量，一个分量的点积，只需将它们相乘即可。很简单。</p>
        <p>Does everyone see this, how this works? You can maybe transform dimensions? The main hardest part is they’ll
            usually give you a k and ask for a phi or give you a phi and ask for a k. But it’s not too bad. Just
            remember, if they give you a phi, do a dot product with it. And if they give you a phi that’s just one
            component, dot product of one component, just multiply them together. Easy enough.</p>
        <p>如果他们给你 ak，就把它当作点积，然后尝试进行逆向工程。通常像这样的东西很容易进行逆向工程。我真的没见过不逆向工程的地方。所以它往往看起来像是最可怕的问题。但在 phis 和 k
            之间进行转换通常不会太糟糕。有人对我们在支持向量机上所做的任何事情有疑问吗？问题。观众：那么 w 背后的直觉是什么？</p>
        <p>If they give you a k, treat it as a dot product and try to reverse engineer. It’s usually something like this
            that’s easy to reverse engineer. I really haven’t seen it where it’s not. So it often looks like the
            scariest problem. But it’s usually not too bad to go between phis and k’s. Does anyone have any questions on
            anything that we did on support vector machines? Question. AUDIENCE: So what’s the intuition behind the w?
        </p>
        <h2 id="unknown-546">未知</h2>
        <h2>Unknown</h2>
        <p>我们解决了这个问题，并计算出了数字和积分。但它与什么有关。教授：问题是，直觉是什么？w 是什么？W
            是分界线。它是一条绝对的分界线。当我说绝对的分界线时，就像那边那些又大又粗的实线。那些是你相当确定的线。超过那条线的一切都是一个减号。</p>
        <p>We solved it and figured out numbers and integrations with it. But what is it in relation to. PROFESSOR:
            Question is, what is intuition? What is w? W Is the dividing line. It is the drop dead dividing line. When I
            say the drop dead dividing line, you like those big, bold solid lines over there. Those are your pretty
            certain lines. Everything past that was a minus.</p>
        <p>在训练中，粗线以外的所有内容都是训练内容中的加号。但虚线才是您在测试数据中真正要用到的。在测试数据中，当遇到困难时，如果位于该线内，您可能会得到一些东西。</p>
        <p>In your training, it’s that everything past the big bold line there was a plus in your training stuff. But
            the dotted line is the one you’re really going to use in the test data. In the test data, when push comes to
            shove, you might get something if it’s inside of that gutter.</p>
        <p>如果它在上面，比如说，如果它在虚线的左上角，你就会称它为正数。所以那条虚线就是你的决策边界。这就是基本思路。事实上，算法在计算机上执行的方式是，它会二次优化 alpha，这会弄乱虚线。然后二次最大化 alpha。
        </p>
        <p>And if it’s on the, say, of that one up there, if it’s on the upper left side of that dotted line, you’re
            going to call it a plus. So that dotted line is your decision boundary. And that is basically the idea. And
            in fact, the way that the algorithm would do it on the computer is it would quadratically optimize the
            alphas, which messes around with the dotted line. And by quadratically maximizing the alphas.</p>
        <h2 id="unknown-547">未知</h2>
        <h2>Unknown</h2>
        <p>你看 alpha 值加起来等于 w。它只是检查了一下。最后，它发现，哦，让这个的 alpha 值为 0 可以实现更好的优化。你试图得到尽可能宽的道路。它最终会得到这个结果。这对于人类来说很容易看出来。</p>
        <p>You see how the alphas add up to a w. It just checks it around. And eventually, it finds, oh, making the
            alpha of this one 0 makes it a better optimization. You’re trying to get the widest possible road. It would
            eventually come out to this. This is trivial for a human to eyeball.</p>
        <p>但是，有些实际问题涉及 200
            个数据点，其中有一两个数据点必须分类错误，而您可能使用二次核或类似的东西，您无法做到这一点。您就是​​做不到。好吧，也许可以，在这种情况下您应该获得麦克阿瑟奖学金或类似的东西。但计算机可以。</p>
        <p>But some real problems with 200 data points that have to get one or two of them wrong, classified, and you
            may be using a quadratic kernel or something that, you can’t do that. You just can’t. Well, maybe can, in
            which case you should be getting a MacArthur Fellowship or something like that. But the computer can.</p>
        <p>基本思想是，归根结底，它会找出 alpha，也就是最宽道路的最佳 w。w 是你的决策边界。好问题。还有其他关于我们的老朋友 SVM 的问题吗？我有个问题要问你。看完这个之后。让我们假装他们只要求解决 w、b、这些核和
            phi，这些都是典型的东西。</p>
        <p>And the basic idea is when it comes down to it, it figures out the alphas, that the best w for the widest
            road. And the w s your decision boundary. Good question. Any other questions about our old friend, SVM? I
            have a question for you. After seeing this. and let’s pretend that they only asked to solve for w’s, b’s,
            these kind of kernels and phi, which are the typical things.</p>
        <h2 id="unknown-548">未知</h2>
        <h2>Unknown</h2>
        <p>现在有多少人认为你可以解决 SVM
            问题？好吧，我们有几个人。我们有少数人很高兴。兄弟连。可能有八个人在那里举手。这很好。现在有多少人知道什么是支持向量？这真的很好。因为如果这就是你从今天的背诵中学到的全部内容，那么它仍然很好。它确实很好。我告诉你。
        </p>
        <p>How many people now think that you can go through and work an SVM problem? All right, we’ve got a few. We’ve
            got a happy few. Band of brothers. Maybe eight people raised their hand there. That’s good. How many people
            know what a support vector is now? That’s really good. Because if that’s all you learned from today’s
            recitation, it’s still good. It really is. I’m telling you.</p>
        <p>我必须上两节课，然后做助教，才能真正理解它。所以你们比我领先。好吧，保重。祝你周末愉快。下周我们会在助推和吸血鬼课上见到你。</p>
        <p>I had to take two classes on this and then TA it before I really, really understood it. So you guys are ahead
            of me. All right, take care. Have a great weekend. And we’ll see you for boosting and vampires next week.
        </p>
        <h1 id="mega-r6.-boosting">Mega-R6。增强</h1>
        <h1>Mega-R6. Boosting</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 49:55 <a
                href="https://youtube.com/watch?v=ZZmzMJB-tow">https://youtube.com/watch?v=ZZmzMJB-tow</a></p>
        <p> 11 years ago (Jan 11, 2014) — 49:55 <a
                href="https://youtube.com/watch?v=ZZmzMJB-tow">https://youtube.com/watch?v=ZZmzMJB-tow</a></p>
        <h2 id="unknown-549">未知</h2>
        <h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。教授：大家早上好。今天，我们将讨论提升。提升非常棒，而且它并不像看起来那么难。只要你做得对，它实际上相当容易。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Good morning,
            everyone. Today, we’re going to talk about boosting. Boosting is pretty awesome, and it’s not as hard as it
            might seem. It’s actually pretty easy, as long as you do it right.</p>
        <p>让我们来看看这个提升问题。就像 ID 树一样，你可能已经注意到 ID 树问题中有一个图，并且所有测试都是在该图上进行的 x 轴和 y 轴测试。然后是 ID 树问题，其中有很多关于离散特征的疯狂不同分类器。</p>
        <p>So let’s take a look at this boosting problem. Just like with ID trees, you may have noticed there’s the ID
            tree problem where there’s a graph, and all of the tests are x and y axis tests on that graph. And then
            there’s the ID tree problem where there are a lot of crazy different classifiers about characteristics that
            are discrete.</p>
        <p>然后，如果您愿意的话，所有类型的 ID
            树桩都是由这些离散品质构建的。这是第二类提升问题的示例，它具有一堆离散品质，例如邪恶、情绪化、变形、闪亮。它具有浪漫兴趣的数值质量数。因此，它基本上是您可能看到的两种提升问题之一。现在，有两个轴笛卡尔问题。</p>
        <p>And then all of the sort of ID tree stumps, if you will, are built out of those discrete qualities. This is
            an example of a boosting problem of the second type with a bunch of discrete qualities, like evil, emo,
            transforms, sparkly. And it has an numerical quality number of romantic interests. So it’s one of basically
            the two kinds of boosting problems that you might see. And now, there is the two axis Cartesian problem.</p>
        <h2 id="unknown-550">未知</h2>
        <h2>Unknown</h2>
        <p>一个很好的例子就是睡眠时间与咖啡时间问题，我计划在周一的教程中介绍这个问题，这样你们就能对这两种类型的问题有所了解。这个问题看起来很大。我认为它看起来很大。我的意思是，它有十种不同的吸血鬼或非吸血鬼需要分类。它有一大堆可能的分类器。但如果你做得对，你可以很快完成。
        </p>
        <p>A good example of that is the hours of sleep versus coffee problem, which I, for one, am planning on doing in
            tutorial on Monday so that you guys sort of get a sense of both types of problems. This one looks big. I
            think it looks. I mean, it has ten different vampires or non vampires to classify. It has a whole bunch of
            possible classifiers. But if you do it right, you can do it fast.</p>
        <p>这是问题的提示。我们来看看。从麻省理工学院毕业后，你得到一份工作，为著名的吸血鬼狩猎咨询机构 Van Helsing and Sommers 工作。两位创始人之一 Gabriel Van Helsing
            曾经以嘉宾身份参加过几次 6.034 讲座，他记得 Winston 教授的吸血鬼识别树讲座。他给你分配了一个任务，即使用 boosting 为以下数据创建一个优越的吸血鬼分类器。</p>
        <p>So here’s the prompt for the problem. Let’s see. After graduating MIT, you get a job working for Van Helsing
            and Sommers, a famous vampire hunting consulting agency. Gabriel Van Helsing, one of the two founders, once
            attended several 6.034 lectures as a guest, and he remembers Professor Winston’s vampire identification tree
            lecture. He assigns you the task of creating a superior classifier for vampires by using boosting on the
            following data.</p>
        <p>因此，我们得到了 ID
            号，我们可以用它以速记方式写出内容。我们得到了几个吸血鬼和非吸血鬼的名字。然后你会看到他们是否是吸血鬼。这就是他们的分类对吸血鬼质量是加分还是减分。之后，有很多可能的方法可以对他们是否是吸血鬼进行分类。</p>
        <p>So we’ve got the ID number, which we can use to just write things out in shorthand. We’ve got the name of
            several vampires and non vampires. Then you see whether they’re a vampire or not. That’s whether their
            classification is a plus or minus for the quality of vampire. After that, there’s a bunch of possible ways
            to classify whether they’re a vampire or not.</p>
        <h2 id="unknown-551">未知</h2>
        <h2>Unknown</h2>
        <p>他们是否邪恶、是否情绪化、是否会变形、是否闪闪发光，以及他们有多少个恋爱对象。例如，一方面，德古拉是邪恶的，但他不是情绪化的。他可以变成蝙蝠或一团雾。他没有闪闪发光，他有五个恋爱对象。</p>
        <p>There’s whether or not they’re evil, whether or not they’re emo, whether or not they transform, and whether
            or not they’re sparkly, as well as the number of romantic interests that they have. So for instance, on the
            one hand, you have Dracula, who’s evil, but he’s not emo. He can transform into a bat or a cloud of mist. He
            does not sparkle, and he has five romantic interests.</p>
        <p>开头的那三个吸血鬼女孩，威廉敏娜·默里和露西·韦斯顿拉。另一方面，《最终幻想
            VII》的主角斯考尔·莱昂哈特非常情绪化，没有任何其他特征。他不是吸血鬼。然而，他是一个很好的反例，可以证明所有情绪化的人都是吸血鬼，因为他非常非常情绪化，但他不是吸血鬼。</p>
        <p>Those three vampire chicks at the beginning, Wilhelmina Murray, and Lucy Westenra. So on the other hand, you
            have Squall Leonhart, who’s the protagonist of Final Fantasy VII, is extremely emo and doesn’t have any of
            the other characteristics. And he’s not a vampire. However, he’s a nice counterexample for a possible rule
            that all emo people are vampires because he’s very, very emo, and he’s not a vampire.</p>
        <p>那么，我们将如何通过提升来解决这个问题呢？嗯，有很多不同的分类器。如果你认为这些就是它们的全部，比如邪恶、情绪化、变形、闪亮、浪漫兴趣和真实。实际上，这只是其中的一半。另一半是相反的版本，但我们现在先忽略它们。所以如果你看看这些，你大概就能明白它们的意思了。
        </p>
        <p>So how will we go about tackling this problem with boosting? Well, there’s a whole bunch of different
            classifiers. And if you think this is all of them. like evil, emo, transforms, sparkly. romantics interests,
            and true. it’s actually only half of them. The other half are the opposite versions, but we’ll ignore them
            for now. So if you look at these, you can probably figure out what they mean.</p>
        <h2 id="unknown-552">未知</h2>
        <h2>Unknown</h2>
        <p>邪恶等于是意味着我们在这里所说的是吸血鬼。但也许是真的。你可能会说，为什么有一个只说真的？只说真的那个说每个人都是吸血鬼。你可能会想，哦，这太糟糕了。但情况并没有那么糟糕，因为 10 个样本中有 7
            个是吸血鬼。增强的关键在于，对于任何可能的分类器，比如在邪恶维度上进行分类。</p>
        <p>Evil equals yes means vampire is what we’re saying here. except maybe true. You might be saying, why is there
            one that just says true? The one that just says true says that everybody is a vampire. You might think, oh,
            that sucks. But it’s not that bad since seven of the 10 samples are vampires. The key, crucial thing about
            boosting is that for any possible classifier, like classifying on the evil dimension.</p>
        <p>这听起来确实像是漫画书中的某个奇怪的地方。但是，根据情绪维度或其他维度进行分类，只要数据不是 50 50 分割，您就一定能够以某种方式使用它来提升。如果数据是 50 50
            分割，那么就像抛硬币一样，所以它是无用的。因为如果您有其他东西，例如性别等于男性或女性。</p>
        <p>Which actually sounds like some kind of weird place that you’d go in a comic book. But classifying on the emo
            dimension or whatever, as long as it’s not a 50 50 split of the data, you’re guaranteed to be able to use it
            in some way for boosting. If there is a 50 50 split, it’s like flipping a coin, so it’s useless. Because if
            you had some other thing, like gender equals male or female.</p>
        <p>假设是 50 50。事实并非如此。但是假设吸血鬼和非吸血鬼的比例是 50 50。这是一个无用的分类器，因为它就像抛硬币一样。你得不到任何信息。现在，你可能会说，等一下。比 50 50
            更差的分类器怎么办？它们怎么办？它们可能比 50 50 分类器更差吗？</p>
        <p>And let’s say that was 50 50. It’s not. But let’s say it was 50 50 between vampire and non vampire. it’s a
            useless classifier because it would be just the same as flipping a coin. You’d get no information. Now, you
            might say, wait a minute. What about classifiers that get worse than 50 50? What about them? Might not they
            be even worse than a 50 50 classifier?</p>
        <h2 id="unknown-553">未知</h2>
        <h2>Unknown</h2>
        <p>我认为，得分低于 50 50 的分类器仍然比得分正好为 50 50 的分类器要好。有什么问题吗？听众：是的。在 ID 树示例中，有人说您使用了 50 50
            个分类器并尝试了各种方法。在您已经生成每个集合的元素后，您每个集合只使用了 50 50 个分类器。</p>
        <p>I claim a classifier that gets less than 50 50 is still better than a classifier that gets exactly a 50 50
            split. Is there a question? AUDIENCE: Yeah. In the ID tree example, somebody said you used 50 50 classifiers
            and played around. And after you already produced the elements per set, then you only used 50 50 classifiers
            per set.</p>
        <p>教授：那么问题是，在 ID 树示例中，如果有一个 50 50 分类器，但该侧的大多数内容已被删除，那么您可能会在后续轮次中使用 50 50 分类器。假设有 20 个数据点，有一个分类器将其分成 10 和
            10。两边各有一半是正半一半是负。</p>
        <p>PROFESSOR: So the question is, in the ID tree example, you might use 50 50 classifiers in later rounds if,
            for instance, there’s a 50 50 classifier except for that most of the things off of that side have been
            already removed. Let’s say there’s 20 data points, and there’s a classifier that splits it 10 and 10. And it
            gets half plus half minus on both sides.</p>
        <p>但是右侧的所有优点都被其他分类器移除了。您可能会使用它。这是真的。但是在 boosting 中，您永远不会使用 50 50 分类器。您永远不会使用正确率恰好为 50 50 的分类器。因为如果它有 50 50
            的机会是正确的，它就没用了。如果它有 50 50 的机会……不，对不起。让我再说一遍。</p>
        <p>But all the pluses from the right side have been removed by some other classifier. You might use it. That’s
            true. But in boosting, you will never use something that’s a 50 50 classifier. You never use something that
            has exactly a 50 50 chance of being correct. Because if it has a 50 50 chance of being correct, it’s
            useless. And if it has a 50 50 chance of. no, sorry. Let me specify again.</p>
        <h2 id="unknown-554">未知</h2>
        <h2>Unknown</h2>
        <p>你永远不会使用那些在给定权重的情况下只有 50% 的概率给出正确答案的东西。这非常非常重要。这也许就是你的问题所要表达的。正如我即将向你展示的以及 Patrick 在讲座中告诉你的那样，在后续的提升轮次中，你会改变
            10 个数据点中的每一个的权重。</p>
        <p>You’ll never use something that has a 50 50 chance of giving you the right answer given the weights. That’s
            very, very important. And that may be what your question was getting at. As I’m about to show you and as
            Patrick told you in the lecture, in later rounds of boosting, you change the weights of each of the 10 data
            points.</p>
        <p>首先，所有权重都是 1/10。权重加起来为
            1。在这种情况下，你永远不会选择一个五次正确、五次错误的分类器。在后面的轮次中，你永远不会选择一个一半权重都错误的分类器。确切地说，一半的权重是错误的。但一半的权重可能不是数据点的一半。</p>
        <p>At first, you start with all weights being 1/10. The weights have add up to 1. In this case, you never, ever
            choose a classifier that gets five of them right and five of them wrong. In the later rounds, you’ll never,
            ever choose a classifier that gets half of the weight wrong. exactly half of the weight wrong. But half of
            the weight may not be half of the data points.</p>
        <p>因此，如果分类器没有一半的权重错误，那么可以选择一半的数据点都错误的分类器。这类似于 ID 树，因为您之前已经做对了。因为您会发现权重会落在您做错的点上。所以我并不是说您应该立即丢弃任何有五个点错误的分类器。</p>
        <p>So it’s possible to choose a classifier that gets half of the data points wrong if it doesn’t get half of the
            weight wrong. And that’s similar to an ID tree when you’ve already gotten things right before. Because
            you’ll see that the weight is going to go to the ones you got wrong. So I’m not saying that you should throw
            out right away anything that gets five of the points wrong.</p>
        <h2 id="unknown-555">未知</h2>
        <h2>Unknown</h2>
        <p>见鬼，你甚至不应该立即抛弃七点错误的东西。有可能。有可能。你可能会错七点，或者如果其他三点真的很难做对，那么错的重量可能不到一半。我们稍后会看到这一点。</p>
        <p>Hell, you shouldn’t even throw out right away something that gets seven of the points wrong. It’s possible.
            possible. that you can get seven of the points wrong, or getting less than half of the weight wrong if those
            other three points are really, really annoying to get right. And we’ll see that later on.</p>
        <p>但为了获得洞察力，在每一步中，我们都愿意选择任何不能得到 50 50 的分类器。但是，我们希望选择能够获得最大权重的分类器。我们所说的最大权重，首先是指大多数点都正确。稍后，我们将确切地说出我所说的。最大权重。</p>
        <p>But for insight, at every step along the way, we’re willing to choose any classifier that doesn’t get 50 50.
            However, we want to choose the classifier that gets the most of the weight right. By most of the weight, at
            first, we mean most of the points right. Later, we will mean exactly what I said. most of the weight.</p>
        <p>如果你不明白这一点，帕特里克讲课、介绍新概念时，有时很难马上理解。如果你不明白这一点，我们讲完后你就会明白我的意思，好吗？所以我之前说的重点是，那些重量不到一半的东西怎么办？对吧？</p>
        <p>And if you don’t understand that, it’s sometimes hard to get it right away when Patrick just lectures through
            it, introduces a new concept. If you don’t understand that, you’ll see what I mean when we go through, all
            right? So my point I was making before is, what about things that get less than half of the weight right?
        </p>
        <h2 id="unknown-556">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，这些总是没问题的，因为你可以把它们翻转过来，使用它们的逆运算，这样就可以得到一半以上的正确权重。有点像。是的，有点像我的女朋友总是告诉我，当你试图在两个地方之间穿梭时，她有超过 50%
            的可能性会选择错误的方向，我对此有点怀疑。</p>
        <p>Well, those are always OK because you can just flip them around, use their inverse, and that gets more than
            half of the weight right. It’s sort of like. yeah, it’s sort of like my girlfriend always tells me that she
            is more than 50% likely to choose the wrong direction when you’re trying to go between two places, which I’m
            kind of skeptical of.</p>
        <p>但我说，如果这是真的，那么我们就可以去你没有说要去的地方，而且我们更有可能走对路。所以你实际上非常擅长找到我们想去的地方。然后她说，不，那行不通。因为那样我就会知道你会这么做，我会重复说错路。然后你又会走错路。</p>
        <p>But I said, if that’s really true, then we can just go wherever you didn’t say to go, and we’ll be more
            likely to go the right way. So you’re actually really good at finding the place that we want to go. And then
            she’s like, no, that won’t work. Because then I’ll know that you’re going to do that, and I’ll double say
            the wrong way. And then you’ll go the wrong way again.</p>
        <p>但尽管如此，你还是可以看到你可以将同样的概念应用于提升。这就是为什么在这下面，我有所有这些测试的所有相反版本。那么我们应该怎么做才能更快地解决这个问题呢？首先，让我们找出每个分类器对哪些数据点进行了错误分类。</p>
        <p>But that not withstanding, you can see you can apply the same concept to boosting. And that’s why underneath
            of this, I have all the opposite versions of all these tests. So what should we be doing to solve this
            problem more quickly? First, let’s figure out which data points are misclassified by each of these
            classifiers.</p>
        <h2 id="unknown-557">未知</h2>
        <h2>Unknown</h2>
        <p>换句话说，如果我们说所有邪恶的东西都是吸血鬼，所有非邪恶的东西都不是吸血鬼，我们错在哪里？如果我们对每个分类器都这样做，那么以后就会更快。因为以后，我们将检查分类器，并且必须将它们判断错误的分类器加起来。所以这张图表将非常有用。
        </p>
        <p>In other words, if we say all the evil things are vampires and all the non evil things are not vampires, what
            do we get wrong? And if we do that for every classifier, then that’ll make it faster later on. Because later
            on, we’re going to go through classifiers, and we’re going to have to add up the ones they got wrong. So
            this chart over here is going to be extremely useful.</p>
        <p>在出现这个的测试中，他们甚至让你填写它来帮助自己。让我们看看。如果我们说所有邪恶等于是的都是吸血鬼，所有邪恶等于否的都不是吸血鬼，那么。我来帮你做第一个。所以我们把所有非吸血鬼都答对了，因为它们都是邪恶等于否。</p>
        <p>And on the test that this appeared in, they even made you fill it in to help yourself out. So let’s see. If
            we said that all the evil equals yes are vampires and all the evil equals no are not vampires, then. I’ll do
            the first one for you. So we get all of the non vampires correct because they are all evil equals no.</p>
        <p>但不幸的是，我们把 Angel、Edward Cullen、Saya Otonashi 和 Lestat de Lioncourt 搞错了，因为他们是吸血鬼，而且他们的邪恶等于不。显然，Lestat
            是可疑的。但我从未读过这些书，维基百科文章说，最后他并没有那么邪恶。所以我们就这样了。邪恶等于是错误地将 2、3、4 和 5 分类。好吧，让我们试试 emo 等于是。我会让别人来做。让我们看看你们是否明白了。</p>
        <p>But unfortunately, we get Angel, Edward Cullen, Saya Otonashi, and Lestat de Lioncourt wrong because they are
            vampires, and they’re evil equals no. Apparently, Lestat is iffy. But I never read those books, and the
            Wikipedia article said that in the end, he wasn’t that evil. So there we go. Evil equals yes misclassifies
            2,3.4, and 5. All right, so let’s try emo equals yes. I’ll have someone else do it. So let’s see if you guys
            got it.</p>
        <h2 id="unknown-558">未知</h2>
        <h2>Unknown</h2>
        <p>所以如果我们说所有 emo 人都是吸血鬼，所有非 emo 人都不是吸血鬼，我们错在哪里？听众：1、6.7、9。教授：1、6.7、9。完全正确，而且很快。很好。我们错了 1、6.7 和 9。1、6 和 7
            是错的，因为他们不是 emo，但他们是吸血鬼。9 是错的，因为 Squall 是 emo，他不是吸血鬼。很好。</p>
        <p>So if we say that all the emo people are vampires and all the non emo people are not vampires, what do we get
            wrong? AUDIENCE: 1,6.7,9. PROFESSOR: 1,6.7,9. that’s exactly right, and fast. Good. We get 1,6.7, and 9
            wrong. 1,6, and 7 are wrong because they are not emo, but they’re vampires. 9 is wrong because Squall is
            emo, and he is not a vampire. Good.</p>
        <p>好吧，如果我们说变形的角色是吸血鬼，而没有变形的角色不是吸血鬼，那会怎样？我们会错选哪些角色？观众：教授：变形是下一个。观众：所有角色教授：那么如果我们说变形的角色是吸血鬼，而没有变形的角色不是吸血鬼，我们会错选哪些角色？观众：我们会错选
            8 个角色。教授：我们肯定会错选 8 个角色，因为她不是吸血鬼。</p>
        <p>OK, what if we said that exactly the transforming characters are vampires and the ones that do not transform
            are not vampires? Which ones will we get wrong? AUDIENCE: PROFESSOR: Transforms is the next one over.
            AUDIENCE: All of the ones PROFESSOR: So which ones would we get wrong if we said that said transforms yes
            were vampires and transforms no were not vampires? AUDIENCE: We’d get 8 wrong. PROFESSOR: We’d definitely 8
            wrong because she’s not a vampire.</p>
        <p>观众：教授：嗯，不。它确实在那里。观众：我们也会弄错 3 和 4。教授：是的。观众：还有
            5。教授：是的，没错。好的。哦，伙计。你没看图表。你只是，嗯。你看到了左边。你只是说，嗯，哪一个是变形角色？好的，这很硬核。观众：教授：但是是的，3、4.5 和 8。不，不，它绝对是给你的。</p>
        <p>AUDIENCE: PROFESSOR: Well, no. It’s actually on there. AUDIENCE: And 3 and 4 we’d also get wrong. PROFESSOR:
            Yep. AUDIENCE: As well as 5. PROFESSOR: Yes, exactly. OK. Oh, man. You didn’t see the chart. You were just
            like, hmmm. You saw the left. You just said, hm, which one of these are the transforming characters? OK,
            that’s pretty hardcore. AUDIENCE: PROFESSOR: But yeah, 3,4.5, and 8. No, no, it’s definitely given to you.
        </p>
        <h2 id="unknown-559">未知</h2>
        <h2>Unknown</h2>
        <p>这将是国际学生有史以来最糟糕的考试。啊，如果你不知道这十个字符是吸血鬼，你就输了。好吧，那么那“闪闪发光等于是”是吸血鬼，如果不是闪闪发光，就不是吸血鬼”呢？这肯定不会顺利。你觉得会错在哪里？观众：对于闪闪发光？教授：是的，闪闪发光等于是是唯一的吸血鬼。观众：错了。安吉尔会错的。
        </p>
        <p>That would be like the worst test ever for international students. Ah, if you don’t know these ten characters
            as vampires, you lose. All right, so what about that sparkly equals yes is a vampire, and if it’s not
            sparkly, it’s not a vampire? This is guaranteed not to go well. What do you think it’s going to get wrong?
            AUDIENCE: For sparkly? PROFESSOR: Yeah, sparkly equals yes are the only vampires. AUDIENCE: wrong. Angel’s
            going to be wrong.</p>
        <p>嗯。所以是 1、2、4、5、6、7 和 8。教授：8。是的，没错。1、2、4、5、6、7 和 8
            都错了。太糟糕了。但该死，爱德华·卡伦的名字是对的。他很难被正确算出来，因为他不太像吸血鬼。他更像一个自称是吸血鬼的超级英雄。好的，接下来，浪漫情趣的数量大于 2。所以如果他们有超过两个浪漫情趣，他们就是吸血鬼。
        </p>
        <p>Saya. so 1,2.4,5.6,7, and 8. PROFESSOR: And 8. yes, that’s right. It gets 1,2.4,5.6,7, and 8 wrong. That’s
            pretty awful. But dammit, it gets Edward Cullen right. And he’s hard to get correct due to the fact that
            he’s not very much like a vampire. He’s more of a superhero who says he’s a vampire. OK, so next, number of
            romantic interest greater than two. So if they have more than two romantic interest, they’re a vampire.</p>
        <p>除此之外，他们不是吸血鬼。那么哪些会出错呢？嗯？观众：3 和 10。教授：只有 3 和 10，没错。因为 Circe 有奥德修斯。她有忒勒马科斯。实际上，她有那个被她变成啄木鸟的人。她还有另一个人，是海神，是他让她把
            Scylla 变成了九头怪物，可能还有至少另一个人。所以 Circe 错了。</p>
        <p>And otherwise, they’re not a vampire. So which ones would that get wrong? Hm? AUDIENCE: 3 and 10. PROFESSOR:
            Just 3 and 10, that’s right. Because Circe had Odysseus. She had Telemachus. Actually, she had that guy she
            turned into a woodpecker. She had that other guy who was a sea god who caused her to turn Scylla into the
            nine headed thing, and probably at least one other person. So Circe it gets wrong.</p>
        <h2 id="unknown-560">未知</h2>
        <h2>Unknown</h2>
        <p>而且爱德华·卡伦的答案也是错的，因为他只有一个。所以是 3 和 10。你可以看出我在写这个问题的时候考虑过这个问题。我写了这个。好吧，大于 4
            的恋爱对象的数量。所以这次有点不同。现在你必须至少有四个恋爱对象。或者实际上，大于四个，但没有一个正好是四个。</p>
        <p>And it also gets Edward Cullen wrong because he only has one. So 3 and 10. You can tell I thought about this
            problem when I was writing it up. I wrote this one. All right, number of romantic interest greater than
            four. So it’s a little bit different this time. Now you have to have at least four romantic interests. or
            actually, greater than four, but there are none that are exactly four.</p>
        <p>被归类为吸血鬼。你认为哪些会出错？观众：3、4、10。教授：是的，3、4 和 10 会出错。因为现在，你会遇到这样一个事实：Saya 有那个金发男孩、Haji 和
            Kai。所以最后一个正面的。因为我声称如果你们给我正面的，我就能推导出所有的负面的。</p>
        <p>To be classified as a vampire. Which ones do you think it’s going to get wrong? AUDIENCE: 3,4.10. PROFESSOR:
            Yup, it is going to get 3,4, and 10 wrong. Because now, you run into the fact that Saya has that blond guy,
            Haji, and Kai. So the last of the positive ones. because I claim I can derive all the negative ones if you
            guys give me the positive ones.</p>
        <p>最后一个正面的意思是每个人都是吸血鬼。谁错了？观众：8、9、10。教授：是的，好的。现在，我可以毫不费力地从中得出所有负面的。邪恶等于否。好吧，不看图表的话，答案是
            1、6、7、8、9、10。如果你不看图表的话，知道为什么是 1、6、7、8、9、10，请举手。如果你不知道，请举手。没有人，好的。等一下。一只手。好的，我稍后也在那里看到了另一只手。</p>
        <p>The last of the positive ones is everybody’s a vampire. Who does that get wrong? AUDIENCE: 8,9.10. PROFESSOR:
            Yes, OK. Now, I can derive all the negative ones from this without a sweat. Evil equals no. well, it’s
            1,6.7,8.9,10 without looking at the chart. Raise your hand if you see why it’s 1,6.7,8.9,10 without looking
            at the chart. Raise your hand if you don’t. Nobody, OK. wait. One hand. OK, I saw another one back there too
            later.</p>
        <h2 id="unknown-561">未知</h2>
        <h2>Unknown</h2>
        <p>他们只是更加谨慎。好吧，它是 A 的补数，因为 A 是邪恶的等于是吸血鬼。它错了 2、3、4 和
            5。因此，邪恶等于不是吸血鬼必然会得到所有相反的结果。观众：哦，我们也可以看看这个吗？教授：是的，我们在这里看，但我们没有看那里的大图表。观众：你可以看任何一张吗？教授：哦，是的。</p>
        <p>They were just more tentative. OK, it’s the complement of A because A is evil equals yes is a vampire. It
            gets 2,3.4, and 5 wrong. So therefore, evil equals no is a vampire is guaranteed to get all the opposite
            ones. AUDIENCE: Oh, we could have looked at that too? PROFESSOR: Yeah, we’re looking here, but we’re not
            looking at the big chart there. AUDIENCE: You can look at any? PROFESSOR: Oh, yeah.</p>
        <p>如果你什么都看不清，那你就完蛋了。除非你不仅和这个家伙一样铁杆，而且还记住了这些数字。好吧，所以 emo 等于 no 将是 2,3.4,5.8,10。Transforms 等于 no 是
            1,2.6,7.9,10。Sparkle 等于 no 是 3,9.10。小于 2 的浪漫兴趣是除 3 和 10 之外的所有数字。1,2.4,5.6,7.8,9.1,2.5,6.7,8.9。最后，除 8、9 和 10
            之外的所有数字，即 1,2.3,4.5,6.7。好吧，所以当我们开始时，我们知道一切都出错了。</p>
        <p>If you can’t look at anything, then you’re screwed. unless not only are you as hardcore as this guy, but
            you’ve also memorized the numbers. All right, so emo equals no is going to be 2,3.4,5.8,10. Transforms
            equals no is 1,2.6,7.9,10. Sparkle equals no is 3,9.10. Romantic interest less than two is everything except
            3 and 10. 1,2.4,5.6,7.8,9.1,2.5,6.7,8.9. And then finally, everything but 8,9, and 10, so 1,2.3,4.5,6.7. All
            right, so when we started off, we know what everything gets wrong.</p>
        <p>然后我大胆地断言，因为有 n 个这样的人，也就是 14 个。我断言只有 6
            个是你最疯狂的梦想，你甚至可能考虑使用它们。其余的，你永远不会使用。问题？听众：是的，我只是想问一下浪漫兴趣的数量。教授：是的。听众：你否定了它，两边都没有等号。教授：没错。</p>
        <p>I then make a bold claim, because there are n of these, which is 14. I make the claim that there are only six
            that, in your wildest dreams, you would ever possibly even consider using ever. And the rest, you would
            never, ever use. Question? AUDIENCE: Yeah, I just have a question about the number of romantic interests.
            PROFESSOR: Yes. AUDIENCE: You negated it without an equals on either side. PROFESSOR: That’s true.</p>
        <h2 id="unknown-562">未知</h2>
        <h2>Unknown</h2>
        <p>这样做只因为有 2 或 4。但它应该用小于或等于来取反。我抄袭了测验。但是是的。今天早上我在练习时注意到了这一点。我想，没有小于或等于。等一下。然后，哦，等等。它没有任何 2 或 4。实际上，我不记得把它们都写成 5
            和 3。</p>
        <p>That works only because there are 2 or 4. But it should have been negated with a less than or equal. I’m
            copying off of the quiz. But yes. I noticed that this morning when I was putting myself through my pace. I’m
            like, there’s not a less than or equal to. Wait a minute. And then, oh, wait. It doesn’t have any 2’s or
            4’s. Actually, I don’t remember writing them all as 5’s and 3’s.</p>
        <p>有可能其他人在后期编辑过程中将它们全部改为大致相同的数字，然后更改小于或等于的选项以减少混淆。有可能我把 Circe 设为 4，而某个地方有一个等于。他们就像，算了吧。因为我想不出她的第五个浪漫情人。</p>
        <p>It’s possible that somebody else in the post editing process changed them all to be about the same number,
            and then changed the less than or equal tos to be less confusing. It’s possible I had Circe at 4, and there
            was an equal to somewhere. And they were like, forget it. Because I can’t think of the fifth romantic
            interest for her.</p>
        <p>所以是的，通常情况下，你必须用等号来否定它，但这里碰巧没有任何等于 4 或 2
            的东西。所以他们这次侥幸逃脱了。但这是很好的做法。所以我声称，在我们最疯狂的梦想中，我们永远也只想使用其中的六个。至于其他八个，忘掉它吧。让我们看看。我会随机叫人来。</p>
        <p>So yes, normally, you would have to negate it with an equal to sign, but there happened to not be any things
            that are equal to 4 or 2 here. So they get away with it this time. But it’s good practice. So I’m claiming
            that in our wildest dreams, we’d only ever want to use six of these ever. And the other eight, forget it. So
            let’s see. I will call on people at random to.</p>
        <h2 id="unknown-563">未知</h2>
        <h2>Unknown</h2>
        <p>第一批人显然很容易理解。告诉我你认为你可能想要使用其中哪一个。给我一个你可能想要使用的。听众：E。教授：为什么，E？当然。那是最好的一个。是的，那是你可能想要使用的。我会圈出你可能想要使用的。E。它只答错了 3 和
            10。太神奇了。</p>
        <p>The first people obviously are getting it really easy. to tell me which of these you think that you might
            ever want to use. Give me one you might ever want to use. AUDIENCE: E. PROFESSOR: Why, E? Of course. That’s
            the best one. Yes, that’s one that you might ever want to use. I’ll circle the ones that you might ever want
            to use. E. it only gets 3 and 10 wrong. That’s amazing.</p>
        <p>这就像是 OK 课程中最好的一门，所以请再给我一门你可能想要的。听众：F。教授：F。让我们看看。F。F 很棒。它只错了三道。人们同意你会使用 F 吗？听众：不。教授：每个人都说不。为什么不呢？听众：它就像
            E，但更糟糕。教授：它就像 E，但更糟糕。</p>
        <p>It’s like the best class of OK, so give me another one that you might ever want to us. AUDIENCE: F.
            PROFESSOR: F. Let’s see. F. F is great. It only gets three wrong. Do people agree that you would ever want
            to use F? AUDIENCE: No.&nbsp;PROFESSOR: Everyone’s saying no. Why not? AUDIENCE: It’s like E, except worse.
            PROFESSOR: It’s like E, except worse.</p>
        <p>无论权重是多少，它在每个步骤的准确率都比 E 差。它肯定很好。如果没有 E，它将是我们最好的分类器之一。但实际上，F 不是这六个分类器之一。这就是为什么我让他们在测试中写有六个，因为人们可能没有找到所有六个。</p>
        <p>It’s guaranteed at every step, no matter what the weights are, to have a worse accuracy than E. It is
            definitely good. If E wasn’t around, it would be one of our best classifiers of all. But actually, F is not
            one of the six. This is why I had them write on the test that there were six, because people might not have
            found all six.</p>
        <h2 id="unknown-564">未知</h2>
        <h2>Unknown</h2>
        <p>因为那些想出不包含 F 的人可能没有想出包含一些你想要包含的数据。所以。听众：我不明白为什么你不能使用 F。教授：为什么你不能使用 F，好的。所以我们从所有数据点的 1/10
            权重开始。但假设在我们提升期间，所有十个数据点现在都有不同的权重。</p>
        <p>Because people who did figure out not to include F might not have figured out to include some of the ones you
            want to include. So. AUDIENCE: I don’t understand why you can’t use F. PROFESSOR: Why you can’t use F, OK.
            So we start off with 1/10 weight for all our data points. But let’s say during our time of boosting that all
            ten of the data points have now different weights.</p>
        <p>因此，我们将 3 的权重称为 ，而您肯定会得到错误的结果。您想最小化错误，对吗？因此，3 的权重（即 E 的错误）是 x。10 的权重可以是 y。因此，如果您考虑选择 3，您知道您……哦，抱歉。如果您考虑选择
            E，您的错误是 x 加 y。观众：当然。</p>
        <p>So we’ll call whatever the weight of 3 is, which you’re going to get wrong. you want to minimize the error,
            right? So that weight of 3, which goes into the error of the E, is x. The weight of 10 can be y. So if
            you’re thinking of choosing 3, you know you’re. oh, sorry. If you’re thinking of choosing E, your error is x
            plus y. AUDIENCE: Sure.</p>
        <p>教授：如果你打算选择 F，你的错误就是 x 加 y 加 z，其中 z 是错误 4。而且由于你永远不会有负权重，x 加 y 加 z 总是大于 x 加 y。听众：你不能再选择没有 3 和 10 的东西了，因为你已经选择了
            E。教授：是的，你可能会选择没有选错 3 和 10 的东西。</p>
        <p>PROFESSOR: If you’re thinking of choosing F, your error is x plus y plus z, where z is the error of 4. And
            since you’re never going to have a negative weight, x plus y plus z is always greater than x plus y.
            AUDIENCE: You can’t choose something without the 3 and the 10 anymore because you already chose E.
            PROFESSOR: That’s. yes, you would probably choose something that didn’t get the three and the ten wrong.</p>
        <h2 id="unknown-565">未知</h2>
        <h2>Unknown</h2>
        <p>但你绝对不会选择 F，因为它总是比 E 更糟糕。事实上，这正是让你找到正确 6 的过程。我说的“将”是指“可以”。我说的“可以”是指，让我们看看你们是否明白。再给我一个你们可以保留的 6 中的 1
            个。观众：K。教授：K 是主张。闪闪发光。</p>
        <p>But you would certainly never choose F ever because it’s always worse than E. In fact, this is exactly the
            process that will allow you to find the correct six. And by “will,” I mean “can.” And by “can,” I mean,
            let’s see if you guys get it. Give me another one of the six that you might keep. AUDIENCE: K. PROFESSOR: K
            is the claim. sparkly.</p>
        <p>K，我要说的是，他输掉的原因和 F 一样。K 是 3、9 和 10。它本质上类似于 3、4 和 10。所以，哦，顺便说一句，我们不应该只选择错误最少的。你需要选择没有绝对更好的东西的。在这种情况下，3 和 10
            错了绝对比 3、9 和 10 错了要好。问题？</p>
        <p>K, I’m going to say will lose for the same reason as F. It’s 3,9, and 10. It’s essentially similar to 3,4,
            and 10. So. oh, by the way, we should not be only going for the ones with the fewest incorrect. You need to
            be going for ones that do not have something that is strictly better. In this case, 3 and 10 wrong is
            strictly better than getting 3,9, and 10 wrong. Question?</p>
        <p>听众：哦，我本来想说变换。教授：你本来想说变换。你说得对。变换是我们需要的变换之一，C。3、4.5 和 8。这里没有比这些错误更少的。例如，没有任何东西会让我们 3、4.5 出错。是的，没有办法在不出错 10 或 5
            和 8 的情况下出错 3、4。听众：那么为什么 8 是那样的？</p>
        <p>AUDIENCE: Oh, I was going to say transform. PROFESSOR: You were going to say transform. You are going to be
            correct. Transforms is one of the ones we need, C. 3,4.5 and 8. there’s nothing down here that gets fewer
            than those wrong. There’s nothing that gets us 3,4.5 wrong, for instance. Yeah, there’s no way to get 3,4
            wrong without getting either 10 wrong, or 5 and 8 wrong. AUDIENCE: So why is the 8 like that?</p>
        <h2 id="unknown-566">未知</h2>
        <h2>Unknown</h2>
        <p>教授：什么？听众：为什么不是 G？教授：为什么不是 G？为什么不是 G？为什么不是 G？让我们也包括
            G。听众：哦。我们不必全部都做？教授：我们需要六个。不，我只是说给我任何一个，然后有人给了我最简单的一个，E。问题？听众：B教授：为什么不是 B？B 看起来很棒。我喜欢 B。让我们包括
            B。还有人想给出他们想包括的另一个吗？听众：A。</p>
        <p>PROFESSOR: What? AUDIENCE: Why not G? PROFESSOR: Why not G? Why not? Why not G? Let’s include G, too.
            AUDIENCE: Oh. We don’t have to do it all? PROFESSOR: We need six. No, I just said give me any, and someone
            gave me the easiest one, E. Question? AUDIENCE: B PROFESSOR: Why not B? B looks great. I love B. Let’s
            include B. Does someone else want to give another one that they want to include? AUDIENCE: A.</p>
        <p>教授：A。为什么不选 A？当然。我的意思是，这里很难找到，因为下面可能有更好的。但是，是的，没有。所以让我们把 A 也算进去。为什么不选 A？我喜欢 A。A
            很棒。好的。现在有五个了。我们还需要另一个。它是迄今为止最难找到的。再给我找一个没有比它更好的。</p>
        <p>PROFESSOR: A. Why not A? Sure. I mean, it’s hard to see down here because there might be something better on
            the bottom. But yeah, there’s not. So let’s include A. Why not A? I love A. A’s great. OK. So that is now
            five. There’s one more that we need. It’s by far the hardest one to find. Find me one more that there’s
            nothing better than it.</p>
        <p>没有什么是严格由相同的子集组成的，是错的。听众：问题。教授：什么？听众：抱歉，我们可以快点问吗？为什么在我们选择 C ​​之前你会选择 A？教授：好的，为什么在选择 C ​​之前你会选择 A？假设 8
            对你来说是一个真正的问题。而你刚刚得到。假设 3、4 和 5，它们没有那么糟糕。它们没有那么糟糕。它们没有那么糟糕。</p>
        <p>There’s nothing that has a strict subset of the same ones wrong. AUDIENCE: Question. PROFESSOR: What?
            AUDIENCE: Sorry, can we quickly. why would you choose A before we chose C? PROFESSOR: OK, why would you
            choose A before you’ve chosen C? Let’s say 8 was a real problem for you. And you were just getting. let’s
            say that 3,4 and 5, they weren’t that bad. They weren’t that bad. They weren’t that bad.</p>
        <h2 id="unknown-567">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，你在这里用变换错了。你选了 C。但过了一段时间，8 才是你的问题所在。对吧？3、4、5 和 8。3、4 和 5 的权重比 8 小得多。然后，在你选错 3、4.5 和 8 之后，3、4 和 5
            仍然不是那么糟糕。而且 8 仍然是一个很高的数字。然后过了一段时间，你看着事情。你会说，你知道吗？</p>
        <p>OK, you got them wrong here with transforms. You chose C. But sometime later, 8 was just by far your issue.
            All right? 3,4, and 5, and 8. 3,4 and 5 are much smaller weights than 8. And then after you’ve got 3,4.5,
            and 8 wrong, 3,4, and 5 were still not that bad. And 8 still was a high number. And then sometime later down
            the line, you’re looking at things. And you’re saying, you know what?</p>
        <p>我真的不想再答错 8，但我不介意答错 3、4 和 5。也许我会答错 2，我从来没有答错过。实际上，我们在这里圈出的所有题目都没有答错 2，所以答错 2 可能并不那么糟糕。所以这就是为什么。因为它不会答错 8。如果 A
            是 2、3、4、5、8，你永远不会选它。你明白我的意思了吗？</p>
        <p>I really don’t want to get 8 wrong again, but I don’t mind if I get 3,4 and 5 wrong. Maybe I’ll get it wrong
            with 2, which I’ve never gone wrong yet. Actually, none of the ones we’ve circled here get 2 wrong, so it’s
            probably not that bad to get 2 wrong. So that’s why. because it doesn’t get 8 wrong. If A was 2,3.4,5.8,
            you’d never take it. Do you see what I mean?</p>
        <p>哦，有人举手了吗？有人找到了吗？听众：我只是想问个问题。你可以用同样的理由来选择 K，对吧？因为在 E 之后，我们可以选择 A，然后说 9 只有一点点不同。教授：但严格来说，它更糟糕。听众：对不起，对不起，我的意思是
            E 到 9 和 10，然后我们可以选择 3、9 和 10，对吧？因为。教授：但我们选择使用 E。</p>
        <p>Oh, did someone raise their hand? Did someone find it? AUDIENCE: I just have a question. You can use the same
            reasoning for choosing K, right? Because after E, we could have chosen A and said that 9 only a little
            different. PROFESSOR: But it’s strictly worse. AUDIENCE: Sorry, sorry, I meant E to 9 and 10, and then we
            could have chosen 3,9, and 10, right? Because. PROFESSOR: But we chose to use E.</p>
        <h2 id="unknown-568">未知</h2>
        <h2>Unknown</h2>
        <p>因为在任何情况下，只选错 3、10 都比选错 3、9 和 10 要好。你选。你明白我的意思吗？它可能不会差太多。选 K 可能只会差一点点，但总是更差。所以它。问题？观众：我们需要一个不包含 3 的。因为现在，选错
            3。教授：好的，这是一个非常好的见解。你在想什么？</p>
        <p>Because getting only 3,10 wrong is better than getting 3,9, and 10 wrong in any universe. You pick. You see
            what I mean? It might not be that much worse. It might be only a little bit worse to choose K, but it’s
            always worse. So it. question? AUDIENCE: we need one that doesn’t have 3 in it. Because right now, get 3
            wrong. PROFESSOR: OK, that’s a pretty good insight. What are you thinking about?</p>
        <p>听众：好吧，我正在尝试证明 D。教授：您正在尝试证明 D。听众：教授：D 很大。它答对了一半以上。但你知道吗？它答对了 3 道题。你知道吗？它答对了 10 道题。与答对 3 道和 10 道题（即 B）不同，它还答对了
            9 道题。D 是最后一个分类器。你猜对了。</p>
        <p>AUDIENCE: Well, I’m trying to justify D. PROFESSOR: You’re trying to justify D. AUDIENCE: PROFESSOR: D is
            huge. It gets more than half of them wrong. But you know what? It gets 3 right. You know what? It gets 10
            right. And unlike our things that get 3 and 10 right, which is B, it also gets 9 right. D is the last
            classifier. You got it.</p>
        <p>很难选出一个有七项都错的答案，但 D 是你可能选的最后一个。事实证明，没有什么比这更能正确地分类 Edward Cullen 和 Squall 以及 Circe 的烦人数据点了，Circe
            并不那么烦人，但在涉及浪漫时她往往会成为一个问题。所以这是我们可能会使用的六个。我们现在可以永远忽略其余的了。</p>
        <p>It’s hard to choose one that has seven of them wrong, but D is the last one you might pick. It turns out
            there’s nothing better than this for classifying correctly those annoying data points of Edward Cullen and
            Squall, and also Circe, who’s not that annoying, but she tends to be a problem when romance is concerned. So
            these are the six that we might use. We can now ignore the rest of them forever.</p>
        <h2 id="unknown-569">未知</h2>
        <h2>Unknown</h2>
        <p>或者至少直到有人重复这个问题或类似的东西。但我们可以忽略除 A、B、C、D、E、G
            之外的所有内容。事实上，我为什么要提起它？我们想要的都在前面。我会把它拿下来。然后我会不顾一切地划掉它。这甚至弄断了我的一根粉笔。现在，这些是我们真正考虑使用的。</p>
        <p>Or at least until someone reuses this problem or something like that. But we can ignore everything except A,
            B, C, D, E, G. In fact, why did I even bring that up? All the ones we want are on the front. I’ll bring it
            back down. Then I’ll cross this off with reckless abandon. That even broke off a piece of my chalk. Now,
            these are the ones that we’re actually thinking about using.</p>
        <p>这里有一个图表，已经准备好使用这六个分类器进行一些提升，好吗？让我们试一试。现在，请记住，在提升中，我们总是尝试选择错误最少的分类器。有问题吗？听众：抱歉，是的。在我们继续之前，你能再慢一点说一下我们在选择分类器时到底在寻找什么吗，比如关于子集的东西？
        </p>
        <p>There is a chart over here already prepared to do some boosting with these six classifiers, all right? So
            let’s give it a try. Now, remember, in boosting, we always try to choose whichever classifier has the least
            errors. Is there a question? AUDIENCE: Sorry, yeah. Before we move on, can you say again a little more
            slowly what exactly we were looking for when we were choosing our classifiers, like something about the
            subset?</p>
        <p>教授：你通常要选择分类器。所以我来告诉你如何划掉一个分类器。这可能是一个更好的方法。你可以划掉一个无用的分类器，如果……顺便说一句，这只有在你能更快地完成而不是浪费时间查看所有分类器时才有用。因为如果你不能划掉一些无用的分类器，通常在测试中，它们不会让你通过。
        </p>
        <p>PROFESSOR: You generally want to take classifiers. So I’ll tell you what let’s you cross off a classifier.
            That may be a better way to do it. You can cross off a classifier as useless if. and by the way, this is
            only useful if you can do it faster than just wasting your time looking at all of them. Because if you can’t
            cross off some of them as useless. usually on the test, they won’t make you.</p>
        <h2 id="unknown-570">未知</h2>
        <h2>Unknown</h2>
        <p>你可以浪费时间，在提升的每一步中都有 14 种可能性，而不是 6 种。但是看看这个。1,2.3,4.5,6.7。然后看看。你这里有什么严格子集是错误的吗？哦，看。2,3.4,5
            是一个严格子集。这可以划掉。1,2.5,6.7,8.9。有什么是严格子集吗？是的，1,6.7,9。所以它可以被划掉。1,2.4,5.6,7.8,9。让我们看看。1,6.7,9 是一个严格子集。</p>
        <p>You can just waste your time and have 14 instead of six possibilities every step of the boosting. But take a
            look at this. 1,2.3,4.5,6.7. Then see. Do you have anything here that has a strict subset of these wrong?
            Oh, look. 2,3.4,5 is a strict subset. This can be crossed off. 1,2.5,6.7,8.9. anything that’s a strict
            subset? Yes, 1,6.7,9. So it can be crossed off. 1,2.4,5.6,7.8,9. Let’s see. 1,6.7,9 is a strict subset.</p>
        <p>3,9.10。3,10 是严格子集。1,6.7,9。是严格子集。3,4.5,8 是严格子集，2,3.4,5.1,6.7,9 也是严格子集。而上面，3,4.10。3,10 是严格子集。但其他的都没有严格子集，甚至
            1,2.4,5.6,7.8 也没有。一般来说，你想保留它们。你想保留每个你可能会使用的分类器。你永远不会使用的分类器是那些有其他更好的分类器，因为它们的严格子集总是错误的。</p>
        <p>3,9.10. 3,10 is a strict subset. 1,6.7,9. a strict subset. 3,4.5,8 is a strict subset, as is 2,3.4,5.1,6.7,9
            is a strict subset. And up here, 3,4.10. 3,10 is a strict subset. But the others don’t have one, even
            1,2.4,5.6,7.8. In general, you want to keep them. You want to keep every classifier you might use. The only
            ones you’ll never use are ones that there’s something else that’s just better always by having a strict
            subset of them wrong.</p>
        <p>希望这样说更清楚。这很棘手。很少有人意识到。即使有七件事是错的，我们也有勇气接受
            sparkly。所以让我们开始一些提升。这不是提升。这是在给自己找麻烦。但它让你了解了提升的工作原理。知识越少，搜索就越少。现在我们只需要搜索六件事。啊，我的意思是知识越多，搜索就越少，而不是知识越少，搜索就越少。
        </p>
        <p>Hopefully, that was more clear. It’s tricky. Very few people realize. we’re brave enough to take sparkly even
            when it got seven things wrong. So let’s start out some boosting. This wasn’t boosting. This was setting
            yourself up. But it was setting yourself up with the knowledge of how boosting works. Less knowledge, less
            search. Now we only have to search six things. Ah, I mean more knowledge, less search, not less knowledge,
            less search.</p>
        <h2 id="unknown-571">未知</h2>
        <h2>Unknown</h2>
        <p>因此，我们一开始就让所有权重都相等。由于有 10 个数据点，所以所有 10 个数据点的权重都是
            1/10。好的，我们现在让所有数据点的权重都相等。由于我们让所有数据点的权重都相等，因此当我们想要找到错误最少的分类器时，我们只想找到错误点最少的分类器。那会是哪一个？</p>
        <p>So we start off with all weights being equal. And since there’s ten data points, all ten of the data points
            are weighted 1/10. OK, we’re now weighting all of them equally. Since we’re weighting all of them equally,
            when we want to find the classifier that gets the least error, we just want to find the one that gets the
            fewest points wrong. Which one is that?</p>
        <p>这是我们的朋友 E，人们意识到它是一个很好的朋友。所以我们将选择分类器 E。我们的错误是多少？它只是我们出错的总和。那么这次我们的错误是多少？它是 1/5。我们错了第 3 点和第 10 点。它们的权重都是
            1/10。1/10 加 1/10 等于 1/5。所以我将 1/5 和 alpha 放在一起。</p>
        <p>That’s our friend E, the first one that people realized was a good one. So we’re going to choose classifier
            E. What’s our error? It’s just the sum of the ones we get wrong. So what’s our error this time? It’s 1/5. We
            got points 3 and 10 wrong. They both have a weight of 1/10.1/10 plus 1/10 is 1/5. So I’ll put 1/5, and
            alpha.</p>
        <p>Alpha 是一种投票，将在最后用于聚合我们的分类器。Alpha 是 1 的 1/2 自然对数减去误差除以误差。不过，我有一个小技巧。这个技巧并不令人印象深刻，但很有趣。因为误差是
            1/2。抱歉，不是误差。alpha。Alpha 是 1 的 1/2 自然对数减去误差除以误差。</p>
        <p>Alpha is sort of a vote that will be used at the very end to aggregate our classifier. Alpha is 1/2 natural
            log of 1 minus the error over the error. However, I have a little trick for you. It’s not that impressive of
            a trick, but it’s a little fun. So since error is 1/2. sorry, not error. alpha. Alpha is 1/2 natural log of
            1 minus error over error.</p>
        <h2 id="unknown-572">未知</h2>
        <h2>Unknown</h2>
        <p>如果误差是 1/x，那么 alpha 就是 x 的 1/2 自然对数减 1。这只是从数学上得出的。这是一个小捷径。如果误差的形式是 1/x，那么它就是 x 的 1/2 自然对数减 1，这意味着由于它是 1/5
            的形式，各位，alpha 是多少？</p>
        <p>If error is 1/x, then alpha is 1/2 natural log of x minus 1. That just follows from the math. It’s a little
            shortcut. If error is in the form of 1/x, then it’s just 1/2 natural log of x minus 1, which means since
            this is in the form of 1/5, everyone, alpha is?</p>
        <p>1/2 ln 4。好的，1/2 ln 4。现在我们来到了提升过程中许多人认为最难的部分，我将向您展示如何更轻松地完成它。在这一部分，我们尝试将所有正确的部分更改为 1/2。而所有错误的部分，我们将它们的权重更改为
            1/2。这是我的自动化流程。</p>
        <p>1/2 ln 4. OK, 1/2 ln 4. So now we come to the part in boosting that many people consider the hardest part,
            and I’m going to show you how to do it more easily. This is that part where we try to make all the ones we
            got right, we change their weights to be 1/2. And all of the ones we got wrong, we change their weights to
            be 1/2. Here is my automated process.</p>
        <p>这叫做分子保持不变法。它是这样工作的。这是我们的十个数据点。它们当前的权重都是 1/10。我们将在下一步中重新加权它们。所以你同意它们都是
            1/10？首先，它们是相等的。所以第一步。删除分母。去掉分数。我不喜欢它们。有除法和乘法。这很麻烦。我只想添加整数。这就是我们要做的。</p>
        <p>It’s called the numerator stays the same method. Here’s how it works. Here’s our ten data points. Their
            current weight is 1/10, all of them. We’re about to re weight them for the next step. So you agree they’re
            all 1/10? They’re equal, to start off. So step one. erase the denominators. Screw fractions. I don’t like
            them. There’s division, multiplication. It’s a pain. I just want to add whole numbers. That’s what we’re
            going to do.</p>
        <h2 id="unknown-573">未知</h2>
        <h2>Unknown</h2>
        <p>那么我们错了哪些呢？3 和 10。把它们圈起来。好的。把圆圈里的数字相加，然后乘以 2。结果是多少？4。这就是新的分母。听众：你总是乘以 2，还是只乘以 2。教授：你总是乘以 2。把不在圆圈里的数字相加。乘以
            2。结果是多少？听众：16。教授：16。这就是新的分母。</p>
        <p>So which ones do we get wrong? 3 and 10. Circle those. All right. Add the numbers in the circles and multiply
            by 2. What does that give you? 4. That’s the new denominator. AUDIENCE: Do you always multiply by 2, or
            just. PROFESSOR: You always multiply by 2. Add the numbers not in the circles. Multiply by 2. What does that
            give you? AUDIENCE: 16. PROFESSOR: 16. That’s the new denominator.</p>
        <p>最后一步也是最关键的一步，这样我们才能在下一轮中再次进行这一步，这是迄今为止数学上最复杂的事情，因为我们必须对分数进行一些处理，但这也不算太糟。然后我们把所有东西都改成相同的分母。那么 1/4
            就变成了什么？听众：4/16。教授：4/16。好的。我还可以取消这些圆圈，以便进行下一步。啊。我按下那个按钮。好的。新的权重。</p>
        <p>The final, crucial step so that we can do this again next round is by far the most mathematically complicated
            thing here because we have to actually do something with fractions, but it’s not too bad. is we then change
            everything to be with the same denominator. So the 1/4’s become? AUDIENCE: 4/16. PROFESSOR: 4/16. All right.
            I can also uncircle these for next. ah. I hit the that button. All right. New weights.</p>
        <p>1/16,1/16,4/16,1/16,1/16,1/16,1/16,1/16,4/16。注意，权重加起来是 1。你答错的加起来是 1/2。你答对的加起来是 1/2。你很开心。所以现在你答错 3 题得到的误差是
            4/16，答错 10 题得到的误差是 4/16，看看这六个。我不会叫别人来，只要是数学好、能更快把这些加起来的人就行。</p>
        <p>1/16,1/16,4/16,1/16,1/16,1/16,1/16,1/16,4/16. Note, the weights add up to 1. The ones you got wrong add up to
            1/2. The ones you got right add up to 1/2. You’re happy. So now that you get 4/16 of an error for getting 3
            wrong, 4/16 of an error for getting 10 wrong, take a look at these six. I’m not going to call on someone,
            just whoever’s good at math and can add these up more quickly.</p>
        <h2 id="unknown-574">未知</h2>
        <h2>Unknown</h2>
        <p>只有 3 和 10 算作 4。其他的都算作 1。把它们加起来。告诉我哪一个最轻。你说什么？听众：我会选 B。教授：你会选 B。它不会错 3。我觉得这听起来不错。其他人也喜欢 B 吗？我喜欢。我的意思是，我们所有不会错
            3 或 10 的，我们只看 B 和 D。</p>
        <p>Just 3 and 10 count as 4. All the others count as 1. Add them up. Tell me which one’s the lightest. What did
            you say? AUDIENCE: I’d go with B. PROFESSOR: You’d go with B. It doesn’t get 3 wrong. That sounds pretty
            good to me. Does everyone else like B as well? I like it. I mean, all of our ones that don’t get 3 wrong or
            10 wrong, we’re only looking at B and D.</p>
        <p>D 有七道题，B 有四道题，所以 B 是最好的。B 错了 4/16。大家都看到了吗？因为由于新的权重，即使错了 3 道或 10 道题中的一道，也和 B 错的所有题一样糟糕。太酷了。我们选择
            B。没错。我差不多把答案说出来。B 的误差是多少？它有四道题是错的，每道题的分数是 1/16。误差是多少？是什么？</p>
        <p>And D has seven. B has four. So B is the best. B gets 4/16 wrong. Does everyone see that? Because even
            getting one of 3 or 10 wrong is as bad as all the ones that B gets wrong because of the new weights. So
            cool. Let’s choose B. That’s right. And I sort of gave it away. What’s the error that B has? It has four
            wrong, each of which are worth 1/16. The error is? What?</p>
        <p>4/16，或者 1/4，无论哪一个是你最喜欢的，这意味着 alpha 是多少？观众：1/2 ln 3。教授：1/2 ln 3。宾果。最后一轮。好，我们错了什么？我们错了 1、6.7 和
            9。哦，是的，我们可以擦掉分母。好的。圆圈里的数字相加，乘以 2 后是多少？观众：8。教授：是 8。1/8。那么不在圆圈里的数字相加，乘以 2 后是多少？</p>
        <p>4/16, or 1/4, whichever is your favorite, which means that the alpha is? AUDIENCE: 1/2 ln 3. PROFESSOR: 1/2
            ln 3. Bingo. Final round. OK, what did we get wrong? We got 1,6.7, and 9 wrong. Oh yeah, we can erase the
            denominators. All right. What are the numbers in the circles, summed up, multiplied by 2? AUDIENCE: 8.
            PROFESSOR: That’s 8. 1/8. And what about the numbers not in the circle, summed up, multiplied by 2?</p>
        <h2 id="unknown-575">未知</h2>
        <h2>Unknown</h2>
        <p>观众：24。教授：没错。24，这意味着我必须将圆圈中的所有数字改为 3/24，但我想我不会这么做，因为这是最后一轮。但如果我要再做一轮。</p>
        <p>AUDIENCE: 24. PROFESSOR: That’s right. 24, which means I’m going to have to change all the numbers in the
            circle to 3/24, except I guess I don’t because this is the last round. But if I was going to do another
            round.</p>
        <p>让我们做好准备，以防万一，将所有这些都改为
            3/24。此外，它使计算哪一个是最好的分类器变得更容易，因为您只需使用分子并将它们相加即可。所以当我写这些的时候，你们可以找出你们喜欢的分类器，并在我写完后告诉我。3/24.1/24.4/24.1/24.1/24.3/24.3/24.1/24.3/24。等等。我这里差了一个。
        </p>
        <p>Let’s prepare in case we where, change all of these to 3/24. Besides, it makes it easier to calculate which
            one is the best possible classifier because you can just use the numerators and sort of add them up. So
            while I’m writing that up, you guys figure out which one you like for classifier and call it out to me when
            I’m done. 3/24.1/24.4/24.1/24.1/24.3/24.3/24.1/24.3/24. wait. I’m off by one here.</p>
        <p>3,1.4。听众：这是因为 w1 没有被分配任何东西。所以 w2 实际上是 w1。教授：啊哈。你说得对。w1 没有被分配任何东西，所以 w2 实际上是 w1，是吗？听众：w1 和 w2 之间多了一个 1/16。多了一个
            1/16。教授：是的，没错。好的，嗯。听众：我们明白了。教授：你明白了。H，那么最佳 H 是什么？你明白了，因为它就在这里。看到了吗？</p>
        <p>3,1.4. AUDIENCE: It’s because w1 is not assigned to anything. So w2 is really w1. PROFESSOR: Aha. You’re
            right. w1 is not assigned to anything, so w2 is really w1 Yeah? AUDIENCE: There’s an extra 1/16 between w1,
            w2. There’s an extra 1/16. PROFESSOR: Yes, that’s true. OK, well. AUDIENCE: We get it. PROFESSOR: You get
            it. H, so what is the best H? You get it because it’s right here. See?</p>
        <h2 id="unknown-576">未知</h2>
        <h2>Unknown</h2>
        <p>这个过程非常万无一失，即使像我这样的傻瓜也能正确回答，而他们的图表是错误的。好吧，那么最好的分类器是什么？听众：C。教授：你说 C。我认为这似乎很合理。它只错了 3、4.5 和
            8。还有其他人得到不同的答案吗？听众：A。教授：其他人得到了 A。我喜欢 A。谁说 A？很多人都说 A。好吧，让我们弄清楚。</p>
        <p>The process is so foolproof, even a fool like me can get it right while they have the chart wrong. All right,
            so what’s the best classifier? AUDIENCE: C. PROFESSOR: You say C. I say that seems pretty reasonable. It
            only gets 3,4.5, and 8 wrong. Does anyone else get a different answer? AUDIENCE: A. PROFESSOR: Someone else
            gets A. I like A. Who said A? A lot of people said A. Well, let’s figure it out.</p>
        <p>所以 A 得到 1,5.6,7。C 得到
            4,5.6,7。事实上，它们是相等的。平局决胜局由小写字母决定，因为我们就是这么说的。事实上，我没有告诉你，但我们就是这么说的。问题？听众：那么当我们决定使用哪个分类器时，我们只能看权重，还是还必须看教授：忽略所有之前的轮次。问题是，在确定分类器时，你只看当前权重吗？
        </p>
        <p>So A gets 1,5.6,7. C gets 4,5.6,7. They’re, in fact, equal. Tie break goes to the lower letter because that’s
            what we said. In fact, I didn’t tell you, but that’s what we said. Question? AUDIENCE: So when we were
            deciding which classifier to use, can we only look at the weights, or do we also have to look at the
            PROFESSOR: Ignore all previous rounds. The question is, do you only look at the current weights when
            determining a classifier?</p>
        <p>或者你也会看前几轮的结果？你必须忽略前几轮的结果。相信我。它们将在稍后的投票中使用。但在进行当前轮次时使用前几轮的结果有点像是给陪审团抹黑。因为你想用这些新权重重新开始，得到一个新的分类器。然后稍后，每个人都可以投票。
        </p>
        <p>Or do you look at the previous rounds as well? You have to ignore the previous rounds. Trust me. They will be
            used later in the vote. But it’s sort of like tainting the jury a little bit to use the previous rounds when
            you’re doing the current round. Because you want to start fresh with these new weights, get a new
            classifier. And then later, everyone will get to make their vote.</p>
        <h2 id="unknown-577">未知</h2>
        <h2>Unknown</h2>
        <p>所以你只能根据当前的权重来做。观众：我们不会考虑上一轮的 6
            是否错误。教授：不，虽然权重考虑到了错误，但它会增加。观众：好的。教授：问题？观众：理论上你能重复使用分类器吗？教授：问题是，理论上你能重复使用分类器吗？答案是，你绝对可以。当发生这种情况时，它实际上会获得额外的权重，因为你再次使用了它。
        </p>
        <p>So you only do it based on the current weights. AUDIENCE: We don’t take any consideration if the last round’s
            6 was wrong or anything. PROFESSOR: Nope, although the weights take into consideration is when it’s wrong,
            it’s going to increase. AUDIENCE: OK. PROFESSOR: Question? AUDIENCE: Could you theoretically reuse a
            classifier? PROFESSOR:. The question is, could you theoretically reuse a classifier? Answer. you absolutely
            can. When that happens, it essentially gets extra weight because you used it again.</p>
        <p>但是你永远不能连续使用它两次。原因如下。假设我们想使用。我们最后用的是哪一个？B？假设我们想再次使用 B。它会给我们什么？50 50。如果我们想使用 B，然后 B。3、6、9、12 错了。总是保证给你 50
            50，这是唯一能确保你永远不会使用它的方法。</p>
        <p>But you can never, ever use it twice in a row. Here’s why. Let’s say that we want to use. which was the one
            we used last over there? B? Let’s say we wanted to use B again. What does it give us? 50 50. If we wanted to
            use B and then B. 3,6.9,12 wrong. Always guaranteed to give you 50 50, which is the only way that you can be
            sure you’ll never use it.</p>
        <p>事实上，这是设计使然。你可以重复使用它，但不能连续两次。它可以在以后的流程中使用。而且它会被使用。因为如果你进行七轮，其中一个必须被重复使用。它只会给被重复使用的那个增加更多的权重。但是是的，A 赢了 C。C
            也是一个完美的答案。问题？观众：等等，你能重复使用吗教授：什么？</p>
        <p>In fact, that’s by design. You could reuse it, but not twice in a row. It could be used later on down the
            stream. And it will be used. Because if you do seven rounds, one of them has to be reused. It just gives
            more weight to whichever one is reused. But yes, A wins against C. C was a perfectly good answer as well.
            Question? AUDIENCE: Wait, can you reuse PROFESSOR: What?</p>
        <h2 id="unknown-578">未知</h2>
        <h2>Unknown</h2>
        <p>听众：而不是 A 或 C。教授：好的。如果可以重复使用，他为什么不选 E？E 答错了 24 道题中的 8 道。比 A 和 C 差一道。这是唯一的原因。下一步可能会使用
            A。或者抱歉。好吧，下一步，坦率地说，我们可能会使用 E。虽然可能不会，因为我们在 A 上答错了 3 道。但很快，我们会再次使用 E，因为 E 非常棒。但无论如何，这里我们使用了 A。</p>
        <p>AUDIENCE: Instead of A or C. PROFESSOR: OK. If you could reuse, why doesn’t he pick E? E gets eight out of 24
            wrong. It’s one worse than A and C. That’s the only reason. Next step will probably use A. or sorry. Well,
            next step, we’ll probably use E, frankly. although maybe not, because we got 3 wrong on A. But pretty soon,
            we would use E again because E’s pretty awesome. But anyway, here we used A.</p>
        <p>我们说我们错了 7/24。哦，天哪，我们不能用我的小捷径。所以答案必须是 17/7。或者 17/7 的 1/2
            自然对数。就这样。现在，我们必须问，从所有这些中我们创建的最终分类器是什么？我们所做的就是将我们选择的所有分类器相加。然后将它们乘以它们的权重 alpha。</p>
        <p>And we said we got 7/24 wrong. Oh, man, we can’t use my little shortcut. So the answer, it has to be 17/7. or
            1/2 natural log of 17/7. So there we go. Now, we have to ask, what is the final classifier that we created
            from all these things? All we do is we sum up all the classifiers we chose. And we multiply them times their
            weight, alpha.</p>
        <p>因此，1/2 ln 4 乘以 E（无论 E 是否返回 true），加上 1/2 ln 3 乘以 B，再加上 1/2 ln 17/7 乘以 A，就是我们的最终分类器，如果 E 认为它是吸血鬼，则返回加 1，如果 E
            认为它不是吸血鬼，则返回减 1。B 和 A 也一样，好吗？然后我们取它的符号。我不是指正弦和余弦。</p>
        <p>So 1/2 ln 4 times E, whether or not E returns true, plus 1/2 ln 3 times B plus 1/2 ln 17/7 times A, is our
            final classifier, where E returns a plus 1 if E thinks it’s a vampire, and a minus 1 if E think it’s not.
            Same for B and A, all right? And then we take the sign of this. And I don’t mean sine and cosine.</p>
        <h2 id="unknown-579">未知</h2>
        <h2>Unknown</h2>
        <p>我的意思是，它是正数还是负数？好吗？所以现在考试的问题是，如果我们使用这个，十个数据点中有多少个是正确的？让我们来看看。E 是。所以我们的浪漫兴趣大于 2。我们有
            emo，是的。我们有邪恶，是的。所以天哪，对数有时很烦人。我们真的必须把它们加起来吗？我敢说我们不需要。</p>
        <p>I mean just, just is it positive or negative? OK? So the question now on the exam is, how many of the ten
            data points do you get right if we used this? Let’s give it a look see. E is. so we have romantic interest
            greater than 2. We have emo yes. And we have evil yes. So oh my gosh, logarithms, they’re sometimes
            annoying. Do we have to actually add them up? I claim we don’t.</p>
        <p>这是板上有三个对数的一个很好的特殊情况。以下两种情况之一是正确的。这三个对数中的一个非常大，以至于它大于其他两个对数的总和，在这种情况下，如果该对数返回正数或负数，则它只是正数或负数，因为该对数很大。</p>
        <p>Here’s a nice special case of having three logarithms on the board. One of two things is true. Either one of
            those three logarithms is so large that it’s bigger than the other two combined, in which case, if that one
            returns a positive or a negative, it’s just positive or negative because that one’s big.</p>
        <p>或者其中一个不是那么大，在这种情况下，任何两个都可以压倒另一个，所以相当于多数票。所以我声称当只有三个时我们永远不需要添加它们。你们明白我的意思了吗？</p>
        <p>Or one is not that large, and in which case, any two can dominate the other one, and so is just equivalent to
            a majority vote. So I claim we never have to add them when there’s only three. You guys see what I mean?</p>
        <h2 id="unknown-580">未知</h2>
        <h2>Unknown</h2>
        <p>比如，假设其中一个是十亿的 1/2 对数，其他的分别是 3 的 1/2 对数和 4 的 1/2 对数。显然，十亿的 1/2 对数乘以十亿的 1/2 对数得到的结果是 10 亿的 1/2 对数，其他的将被忽略。</p>
        <p>Like, let’s say one of them was 1/2 log of a billion, and the others were 1/2 log of 3 and 1/2 log of 4.
            Obviously, whatever the 1/2 log of a billion says, which is multiplied by 1/2 log of a billion, is it’s just
            going to be that, and the others will be ignored.</p>
        <p>但是，如果其中一个不大于其他两个的总和，那么这三个人之间就只是简单的投票，因为如果两个一起投票，他们都可以胜过另一个。在这种情况下，让我们看看，17/7 不等于 3。但是，4 的对数肯定不比 3 的对数加上 17/7
            的对数好。它不相等。</p>
        <p>However, if it’s not the case that one of them is larger than the other two combined, then it’s a simple vote
            between the three, because any two can out vote the other one if they work together. And in this case, let’s
            see, 17/7 is not quite 3. However, log of 4 is certainly not better than log of 3 plus log of 17/7. It’s not
            even.</p>
        <p>4 的对数等于 2 的对数加上 2 的对数。这两个数都大于 2 的对数。这是对数的规则。4 的对数等于 2 的平方对数，你可以去掉
            2。所以这两个数还不足以让其中一个大于其他两个数之和。所以这只是一次简单的投票。让我们开始吧。德古拉。</p>
        <p>Log of 4 is equal to log of 2 plus log of 2. And these are both bigger than log of 2. That’s rules of logs.
            log of 4 equals log of 2 squared, and you can take the 2 out. So these are not big enough that one of them
            is bigger than the other two combined. So it’s just going to be a simple vote. So let’s go through. Dracula.
        </p>
        <h2 id="unknown-581">未知</h2>
        <h2>Unknown</h2>
        <p>好吧，他有很多小吸血鬼。他不是 emo，所以 E 猜对了。他不是
            emo。所以猜错了。但他是邪恶的。猜对了。三分之二的人认为他是吸血鬼。猜对了。下一个是天使。好吧，他参演了一部长期连载剧。他有很多浪漫情趣。所以猜对了。他肯定是 emo。猜对了。</p>
        <p>OK, he’s got tons of his little vampyrettes. He’s not emo, so E gets it right. He’s not emo. So that gets it
            wrong. But he is evil. That gets it right. Two out of three vote that he’s a vampire. correct. Next. Angel.
            OK, well, he was in a long running series. He’s got plenty of romantic interests. So that gets it right.
            He’s certainly emo. That gets it right.</p>
        <p>尽管他并不邪恶，但三分之二的人说他是吸血鬼，所以是正确的。接下来是爱德华·卡伦。好吧，《暮光之城》，我们来看看。他只有一个恋爱对象，所以错了。好吧。他很情绪化，所以是对的。但他并不邪恶，所以错了两点。所以根据我们的最终分类器，爱德华不是吸血鬼。但他是。所以我们有一个数据点是错误的。你们看到了吗？
        </p>
        <p>And even though he’s not evil, two out of three says he’s a vampire, so correct. Next, Edward Cullen. well,
            Twilight, here we come. Let’s see. He only has one romantic interest, so that gets it wrong. OK. He’s emo,
            so that gets it right. But he’s not evil, so two wrong. So Edward’s not a vampire according to our final
            classifier. But he is. So we got one of the data points wrong. You guys see that?</p>
        <p>因为这里我们三个分类器中有两个说他不是吸血鬼。好吧，让我们看看。萨娅。嗯，她有超过两个恋爱对象。而且她是 emo。所以即使她不是邪恶的，我们也做对了。好吗？让我们看看。莱斯特。他也有许多恋爱对象，是
            emo，而且不是邪恶的。所以你做对了。好的，比安卡是邪恶的，有很多恋爱对象。</p>
        <p>Because two out of three of our classifiers here said that he was not a vampire. All right, let’s see. Saya.
            well, she has more than two romantic interests. And she’s emo. So even though she’s not evil, we get it
            right. OK? Let’s see. Lestat. he also has may love interests, is emo, and is not evil. So you get it right.
            OK, Bianca is evil with many love interests.</p>
        <h2 id="unknown-582">未知</h2>
        <h2>Unknown</h2>
        <p>尽管她不是 emo，但三分之二，你猜对了。好吧，卡米拉。我要叫她卡恩斯坦。基本上和比安卡一模一样，只是浪漫情趣的数量是固定的。所以她总是会做和比安卡一样的事情。这就是为什么 6 和 7
            总是一起旅行。所以我们猜对了。美少女战士不应该是吸血鬼。</p>
        <p>Even though she’s not emo, two out of three, you get it right. All right, Carmilla. I’m going to call her
            Karnstein. is basically exactly the same as Bianca with the number of romantic interests fixed the way it
            is. So she will always do the same thing that Bianca does. It’s why 6 and 7 always travel together. So we
            get it right. Sailor Moon is supposed to be not a vampire.</p>
        <p>所以她爱慕的对象数量表明她不是吸血鬼，因为她只有一个。她既不邪恶也不情绪化，这一事实表明，她实际上完全不是吸血鬼。他们都同意。这是正确的。斯考尔只有一个爱慕对象，莉诺娅。他并不邪恶，这两者都表明他不是吸血鬼。但他情绪化。但三分之二的人说他不是吸血鬼。我们猜对了。
        </p>
        <p>So her number of love interests say that she’s not a vampire because she only has one. The fact that she’s
            not evil and not emo says that actually, she’s perfectly not a vampire. They all agree. And that’s correct.
            Squall has only one love interest, Rinoa. And he’s not evil, both of which of say he’s not a vampire. But he
            is emo. But two out of three says he’s not a vampire. We get it correct.</p>
        <p>尽管瑟茜有许多浪漫情趣，这可能表明她可能是吸血鬼，但她既不邪恶也不情绪化，也不是吸血鬼。所以除了爱德华·卡伦，我们的一切都是对的，这可能更多地说明了斯蒂芬妮·迈耶斯的写作，而不是我们的提升。好的，最后一个问题。韦斯利·温德姆，一位顾问，注意到你使用的某些分类器之间存在一些相关性。
        </p>
        <p>And Circe, despite her many romantic interests which says she might be a vampire, is neither evil nor emo,
            and is not a vampire. So we got everything right except Edward Cullen, which perhaps says more about
            Stephenie Meyers writing than about our boosting. All right, final question. Wesley Wyndham, a fellow
            consultant, has noticed a few correlations between some of the classifiers you used.</p>
        <h2 id="unknown-583">未知</h2>
        <h2>Unknown</h2>
        <p>他建议使用一组新的弱分类器，由一对在逻辑上进行“与”和“或”运算的分类器组成。例如，两个新的分类器将是“emo 等于是”或“evil 等于是”，或“sparkly 等于否”和“transforms
            等于是”。这样就可以将 Sailor Moon 从变换云中剔除。他相信你将能够对大型吸血鬼数据集进行分类。无论如何，比这个数据集要大。</p>
        <p>He suggests using a new set of weak classifiers consisting of a pair of your classifiers that are logically
            anded and ored together. For instance, two of the new classifiers would be emo equals yes or evil equals
            yes, or sparkly equals no and transforms equals yes. So that would cut out Sailor Moon from the transforms
            cloud. He believes that you’ll be able to classify large vampire data sets. larger than this one, anyway.
        </p>
        <p>使用他的系统，可以更快地完成更少的提升轮次。你同意还是不同意韦斯利的观点？解释你的论点。所以，你知道，这是一个棘手的概念问题。除了“哦，天哪，这是韦斯利。他肯定错了”之外，还有没有人有本能反应。观众：你可能会使用更少的提升轮次，因为你有更多的分类器。但你必须搜索更多的分类器。教授：啊哈，这是罕见的满分答案。
        </p>
        <p>More quickly with fewer rounds of boosting using his system. Do you agree or disagree with Wesley? Explain
            your arguments. So this was, you know, the tough concept question. Does anyone have just an instinctual
            thing other than like, oh, man, it’s Wesley. He must be wrong. AUDIENCE: You’ll probably use fewer rounds of
            boosting because you have more classifiers. But you’ll have to search through more classifiers. PROFESSOR:
            Aha, that is the rare full point answer.</p>
        <p>很少有人意识到韦斯利只是部分正确。他们要么说他完全错了，这是错的，要么说他完全正确。是的，由于你可以这样做，它将使用更少的提升轮次。本质上，这些提升之一已经以某种方式让事情一起投票。</p>
        <p>Very few people realize that Wesley was partially right. They either said something about him being
            completely wrong, which was wrong, or said that he was completely right. Yes, it will use fewer rounds of
            boosting because of the fact that you can. essentially, one of these boosting already does is sort of gets
            things to vote together in an fashion.</p>
        <h2 id="unknown-584">未知</h2>
        <h2>Unknown</h2>
        <p>因此，通过能够合并成两个，它将使用大约一半的提升轮数。但是有很多“与”和“或”。实际上有 n 选 2，其中 n 是吸血鬼的数量。并且由于使用一半的轮数但每轮花费 n 选 2 次，因此时间并不会减少。所以这是完全正确的。
        </p>
        <p>So it will use approximately half the number of rounds of boosting by being able to combine into two. But
            there’s a lot of ands and ors. There’s in fact n choose 2, where n is the number of vampires. And since
            using half the number of rounds but taking n choose 2 time for each round is not less time. So that’s
            exactly correct.</p>
        <p>并不是很多人都能得到满分，因为有时他们被韦斯利的想法所吸引。或者他们只是觉得，这是韦斯利。他错了。或者只是其他一些有趣的答案。对提升有什么问题吗？问题？观众：你怎么知道要进行多少轮提升？教授：问题是，你怎么知道要进行多少轮提升？答案是。所以在测验中，它告诉你有三轮。
        </p>
        <p>Not that many people got full credit on that one because sometimes, they were seduced by Wesley’s idea. Or
            they were just like, it’s Wesley. He’s wrong. or just some other funny answer. Any questions about boosting?
            Question? AUDIENCE: How do you know how many rounds of boosting to take? PROFESSOR:. The question is, how do
            you know how many rounds of boosting to do? The answer is. so on the quiz, it tells you that you have three.
        </p>
        <p>在现实生活中，你可能只想让它一直运行，直到收敛。这是一种可能性。让它一直运行，直到收敛到一个答案，它就不再做任何事情了。我认为 Patrick 在 6.034
            网站上有一个小部件，可以让你放下一些数据点并对其进行提升。你可以看到它最终收敛了。提升收敛到一个答案，它不会改变。观众：什么收敛了？</p>
        <p>In real life, you might want to just kind of keep it running until it converges. That’s one possibility. keep
            it running until it converges to an answer, and it doesn’t do anything anymore. Patrick has a little widget
            on the 6.034 website, I think, that lets you plunk down some data points and run boosting on them. And you
            can see that eventually, it converges. The boosting converges to an answer, and it doesn’t change. AUDIENCE:
            What converges?</p>
        <h2 id="unknown-585">未知</h2>
        <h2>Unknown</h2>
        <p>教授：基本上，不是你选择的分类器，当然也不是权重。而是你的数据集中哪些是正确的。因为他是在二维空间中而不是像这样做的。他向你展示了提升在分类之间绘制的线条，并用绿色和红色等颜色标记。最终，它会将线条的位置和正确的线条聚合在一起。
        </p>
        <p>PROFESSOR: Basically, the. not the classifiers you picked, of course, or the weights. But what converges is
            which ones of your data set you get correct. Because he does his in two dimensional space rather than like
            this. And he shows you the lines that boosting is drawing between classification, and colors things in green
            and red or something like that. And eventually, it converges where the lines are and which ones it’s getting
            right.</p>
        <p>它通常会收敛到一切都正确。当这种情况发生时，你就可以停止了。但这是个好问题。在现实世界中，这并不总是那么容易。有时你不得不说，这对我来说已经足够了。我已经给了它 n
            轮。这比分类器的数量要多得多，所以也许它不会得到更好的结果。</p>
        <p>It generally converges to getting everything correct. And when that happens, then you can stop. But that’s a
            good question. And it’s not always that easy in the real world. You have to sometimes just say, this is
            enough for me. I’ve given it n number of rounds. And that’s much more the number of classifiers, so maybe it
            won’t get anything better.</p>
        <h1 id="mega-r7.-near-misses-arch-learning">Mega-R7。险些失误，Arch Learning</h1>
        <h1>Mega-R7. Near Misses, Arch Learning</h1>
        <p><img
                src="data:image/jpeg;base64,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">11
            年前 (2014 年 1 月 11 日) — 33:04 <a
                href="https://youtube.com/watch?v=J-ocRQCjcwE">https://youtube.com/watch?v=J-ocRQCjcwE</a></p>
        <p> 11 years ago (Jan 11, 2014) — 33:04 <a
                href="https://youtube.com/watch?v=J-ocRQCjcwE">https://youtube.com/watch?v=J-ocRQCjcwE</a></p>
        <h2 id="unknown-586">未知</h2>
        <h2>Unknown</h2>
        <p>以下内容根据 Creative Commons 许可提供。您的支持将帮助 MIT OpenCourseWare 继续免费提供高质量的教育资源。要捐款或查看数百门 MIT 课程的其他材料，请访问 MIT
            OpenCourseWare，网址为 ocw.mit.edu。教授：这里有一棵差点学习树。它有点不同，但有点相似。我们有不同的类型。我们正在尝试了解不同的光源。</p>
        <p>The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare
            continue to offer high quality educational resources for free. To make a donation or view additional
            materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Here we have a
            near miss learning tree. It’s a little bit different but a little bit similar. We’ve got different types.
            We’re trying to learn about different light sources.</p>
        <p>因此，我们关注的是不同的灯、手电筒等各种各样的东西。这些灯可能有各种类型的支撑、底座或电线。底座可以是平的、夹紧的，也可以有支腿。支腿可以是平底的，也可以是有轮子的。光源可以是白炽灯、荧光灯或钠蒸气灯。为其供电的能源可以是电、电池、油或气。因此，还有一些其他的东西。
        </p>
        <p>So we’re concerned about different lamps, flashlights, all sorts of things. So these guys can have had types
            of support, a base or wires. The base can be flat, clamped, or have legs. The legs can be flat bottomed or
            wheeled. The light source can be incandescent, fluorescent, or sodium vapor. And the energy source that
            powers it can be electric, battery, oil, or gas. So there’s also some other things.</p>
        <p>但这些是我们可能用爬树启发式方法爬上的树。我们的起始模型是，我们有一个白炽灯泡。高度正好等于 24
            英寸。它们有一个平坦的底座。有电来供电。它有一个灯罩。所以我们的起始模型基本上就像你放在桌子上的台灯。它的顶部有一个灯罩。</p>
        <p>But these are the trees that we might be climbing up with the climb tree heuristic. Our starting model is we
            have an incandescent bulb. The height is equal to 24 inches exactly. They have a flat base. There’s
            electricity to power it. And it has a shade. So our starting model is basically like a table lamp that you
            set on the table. It’s got the shade over the top of it.</p>
        <h2 id="unknown-587">未知</h2>
        <h2>Unknown</h2>
        <p>你知道，标准房间台灯是我们的起始模型。回想一下，在我们在这里使用的拱形学习中，我们可能想要使用几种启发式方法。让我们看看，启发式方法有。我们有要求链接、禁止链接。我们有爬树、扩展集、关闭间隔。我们还有删除链接。忘了那个。这有点像链接。我可以把它放在这里。
        </p>
        <p>You know, the standard room table lamp is our starting model. So recall in arch learning, which is what we
            are using here, that there are several heuristics that we might want to use. Let’s see, well, the heuristics
            are. we’ve got require link, forbid link. We’ve got climb tree, extend set, close interval. And we also have
            drop link. forgot about that one. That’s sort of with the links. I can put it here.</p>
        <p>好的，如果你想在期末考试中回答好这些问题，首先你最好知道这六个启发式方法的作用。所以基本上，要求链接是一种启发式方法。它作为一项要求进入你的模型，而之前你的模型并不关心它。假设你的灯光有颜色问题，比如你的灯座或灯的颜色。现在它根本不关心颜色了。
        </p>
        <p>OK, so if you want to do well on these questions if they appear on the final, first of all, you better know
            what those six heuristics do. So basically require link is a heuristic. It goes into your model as a
            requirement, where before your model just didn’t care. So let’s say there was something about color for your
            light, like for your lamp’s base or for the lamp. And right, now it doesn’t care about color at all.</p>
        <p>但是，假设你最终得到一个示例，例如蓝色或白色或这些颜色。它最终认为蓝色是可以的。在它的模型中，它最终认为，蓝色和白色都可以。或者实际上，不，一个更基本的例子。假设它一直以蓝色灯作为示例，并且它们都是蓝色的。</p>
        <p>But let’s say that you wound up having an example that was, for instance, blue or white or whatever these
            colors are. And it wound up getting that blue was OK for. in its model, it wound up getting, OK, blue and
            white are OK. Or actually, no, an even more basic example. let’s say that it just kept getting blue lamps as
            examples, and all of them were blue.</p>
        <h2 id="unknown-588">未知</h2>
        <h2>Unknown</h2>
        <p>它根本不在乎它们是蓝色的。但它们确实是蓝​​色的。最后，它找到了一个与它的模型完全相同的红色，但红色的差点就找到了。然后它可能需要蓝色，这在它的模型中已经是可接受的、允许的了。但它可能是必需的。现在，它可以做的另一件事是禁止。
        </p>
        <p>And it just didn’t care about the fact that they were blue. But they did turn out to be blue. And then
            eventually it found a red one that was exactly the same as its model, but the red one was a near miss. Then
            it might require blue, which was already an acceptable, permissible thing in its model. But it could be
            required. Now, another thing it can do is forbid.</p>
        <p>有人问我，既然已经可以要求蓝色，为什么还要费心说禁止红色呢？它们涵盖了相同的内容。答案是因为这个系统，这个主要的学习系统。记住，这是帕特里克的博士论文。这是一个古老的系统。因为它建立在旧系统之上，而且它通常是一个好主意，所以它非常简约。假设你有一组颜色。红色、蓝色、黄色、绿色、粉色、紫色、橙色。
        </p>
        <p>Some people ask me, why would you bother saying, forbid red, say, if you can already just require blue?
            Together they cover the same ground. The answer is because this system, this arch learning system. remember,
            it’s Patrick’s doctoral thesis. It’s an old system. Because it was built on old systems and because also
            it’s generally a good idea, it was very parsimonious. Let’s say you had a set of colors. red, blue, yellow,
            green, pink, purple, orange.</p>
        <p>如果你发现只有橙色给你带来了问题，你可以尝试要求它在这个范围内。或者你甚至可以使用扩展集，制作一个由红色、蓝色、黄色、绿色、紫色、粉色组成的巨型集，但不包括橙色。但如果你只是禁止橙色，你就节省了一大堆空间，特别是如果有一个可能的大型集合系列。你想有禁止的能力。
        </p>
        <p>And you found that only orange was giving you problems, you could try to require that it be in this. Or you
            could even use extend set, make a giant set of red, blue, yellow, green, purple, pink, but not orange. But
            if you just forbid orange, you’ve just saved a whole bunch of space, particularly if there’s a large
            possible series of set. You want to have the ability to forbid.</p>
        <h2 id="unknown-589">未知</h2>
        <h2>Unknown</h2>
        <p>因为它需要有最小的模型来覆盖所有样本。所以我们还有爬树。爬树选取我们模型中的一个元素并将树向上移动一层。假设我们说，只有白炽灯是好的。但我们发现荧光灯也很好。哇，我们向上移动到光源，爬树。扩展集合。我们这里没有任何集合。
        </p>
        <p>Because it needs to have the smallest model that it can that covers all of its samples. So we also have climb
            tree. Climb tree takes one of the elements in our model and moves up the tree one level. So let’s say we
            said, only incandescent lights are good. But we found that fluorescent lights are also good. Whoop, we move
            up to light source, climb tree. Extend set. we don’t have any sets here.</p>
        <p>但是让我们回到红色、蓝色、橙色、黄色。扩展集是指我们说，只有红色才适合作为例子。但是我们看到另一个正面例子是黄色。所以我们说，好的，我将扩展这个集合。红色和黄色都很好。好吧，紧密间隔。当你需要使用紧密间隔时，它确实很明显。
        </p>
        <p>But let’s go back to red, blue, orange, yellow. Extend set is when we say, only red is good as an example.
            But we see another positive example that’s yellow. So we say, OK, I’ll extend the set. Red and yellow are
            both good. All right, close interval. close interval is really obvious when you need to use it.</p>
        <p>因为它不仅只用于高度等于 24 英寸或某种数字的间隔，而且它还是高度等于某种数字时唯一可以使用的东西。 紧密间隔会让你对间隔感到困惑，然后说，哦，我想 20 英寸高度的间隔也可以。 所以我们将整个间隔设为 20 到
            24 英寸就好了。</p>
        <p>Because not only is it only used for intervals like height equals 24 inches or some kind of numbers, but it
            is the only thing you can use for when height equals some kind of numbers. Close interval will let you fuss
            around with the intervals and say, oh, well I guess one with a 20 inch height is OK, too. So we’ll make the
            whole interval 20 to 24 inches good.</p>
        <h2 id="unknown-590">未知</h2>
        <h2>Unknown</h2>
        <p>因为这样说没有任何意义，比如 20 很好，24 也很好，但中间没有一个是好的。不，那些很糟糕。所以那个紧密间隔覆盖了整个间隔。最后但并非最不重要的是 drop link。关于 drop link 的事情是 drop
            link 再次真正被使用，因为系统希望其模型尽可能简洁。Drop link 就是这样。</p>
        <p>Because it doesn’t make any sense for it to be like, 20 is good, 24 is good, but nothing in between is good.
            No, those are awful. So that close interval covers the entire interval. Last but not least is drop link. The
            thing about drop link is that drop link is, again, only really used due to the fact that the system wants to
            be as parsimonious as possible with its model. Drop link is.</p>
        <p>假设你有一种颜色，你会想，好吧，只有红色是可以接受的。然后你看到蓝色。没问题。假设红色、蓝色和黄色是你唯一的颜色。只有红色和蓝色是可以接受的。嘿，等一下。不，我们可以说只有黄色是不可接受的，或者类似这样的话，好吗？但你会看到黄色是一个正面的例子。
        </p>
        <p>Let’s say you have a color, and you’re like, OK, only red is acceptable. And then you see a blue one. It’s
            OK. And let’s say red, blue, and yellow are your only colors. Only red and blue are acceptable. Hey, wait a
            minute. No, we can just say that only yellow is not acceptable, or something like that, all right? But then
            you see yellow as a positive example.</p>
        <p>实际上，你甚至不能切换到不可接受的黄色，因为你只看到了红色和蓝色的正片。所以你说，只有红色和蓝色是可接受的。问题？听众：你提到，如果我们看到了白炽灯和荧光灯，我们可以爬树到光源，因为这样更节俭。</p>
        <p>Actually, you can’t even switch to the yellow being not acceptable, because you’ve only seen positive of red
            and blue. So you say, only red and blue are acceptable. Question? AUDIENCE: You mentioned that if we had
            seen incandescent and fluorescent, it’s OK that we could climb the tree to light source, because it’s more
            parsimonious.</p>
        <h2 id="unknown-591">未知</h2>
        <h2>Unknown</h2>
        <p>如果我们看到钠蒸气，比如路灯，我们拒绝了它，我们是否必须回到树下，还是只需添加一个禁止链接？
            教授：那么你会添加一个禁止链接。问题是，我们爬上白炽灯、荧光灯，直到光源。当我们看到钠蒸气时，我们该怎么办？答案是，我们的系统与格子学习不同，在拱形学习中，我们的系统对之前的示例没有记忆。</p>
        <p>If we then see one that’s sodium vapor, say a street lamp, and we reject it, do we have to go back down the
            tree, or do we just add a forbid link? PROFESSOR: Then you would add a forbid link. The question is, we
            climb incandescent, fluorescent, to light source. When we see sodium vapor, what do we do? The answer is,
            our system, unlike with lattice learning, in arch learning, our system is memoryless of its previous
            examples.</p>
        <p>它无法回到树下。所以如果你看到白炽灯和荧光灯，爬到光源处，你唯一的办法，如果你发现钠蒸气不起作用，就是禁止链接钠蒸气。这是个好问题。这表明你对我们的工作原理有很好的理解。所以放下链接，正如你所见，红色和蓝色，它们都可以。你不知道黄色，所以你不能直接把它切换到禁止链接到黄色。
        </p>
        <p>It’s incapable of going back down the tree. And so if you see incandescent and fluorescent, climb up to light
            source, you’re only recourse, if you see that sodium vapor doesn’t work, is to forbid link sodium vapor.
            That’s a good question. It shows good understanding of how we work. So drop link, as you see, red and blue,
            they both are OK. You don’t know about yellow, so you can’t just switch that to a forbid link to yellow.</p>
        <p>但你看到黄色也是可以的。你会怎么做？你可以把链接全部删除，节省空间。如果整个世界的所有颜色都可以，你可以不让模型的一部分成为颜色，而是说，任何颜色都可以。颜色一定不是定义灯的一部分。我想它不是。我的意思是，我想，它是定义俗气灯的一部分。
        </p>
        <p>But then you see yellow is also OK. What do you do? You can drop the link altogether, save yourself space. If
            all the colors that you have in the entire world are fine, you can just not have part of your model be the
            color, and just say, any color is fine. Color must not be really part of what defines a lamp. And I’d say
            it’s not. I mean, it’s what defines a tacky lamp, I guess.</p>
        <h2 id="unknown-592">未知</h2>
        <h2>Unknown</h2>
        <p>但这不是定义灯的原因。所以这可能是为什么颜色从一开始就不存在的原因。所以为了让自己在这方面做得更好，更多的知识等于更少的搜索。让我们拥有更多的知识。只有其中一些用于概括，使我们的模型更能接受新的例子。这些是我们在看到正面的命中后会使用的，有点概括，在模型中学习更多的东西。
        </p>
        <p>But it’s not what defines a lamp. So that might be why color isn’t here to begin with. So to make yourself do
            better on this, more knowledge equals less search. Let’s have more knowledge. There’s only some of these are
            used to generalize, to make our model more accepting of new examples. And those are ones we would use after
            we’ve seen a positive hit, sort of generalize, learn more things in the model.</p>
        <p>其中一些用于专门化，使我们的模型更加完善，使我们的模型更加具体。只有在看到险情后，您才会使用它们。所以让我们将它们分开。这样，你们就不会犯在错误情况下使用其中之一的错误。知识越多，搜索越少。搜索越少意味着测验时间越短。所以需要链接，你们觉得怎么样？这是专业化还是通用化？听众：专业化。
        </p>
        <p>Some of these are used to specialize and make our model refined, make our model more specific. You’d only use
            those after you saw a near miss. So let’s actually separate those. That way you guys will never make the
            mistake of using one of these in the wrong situation. And more knowledge, less search. And less search means
            a faster quiz time. So require link, what do you guys think? Is that a specializer or a generalizer?
            AUDIENCE: Specializer.</p>
        <p>教授：专业化，没错。我们只会在看到差点成功时使用它。禁止链接，专业化还是通用化？听众：专业化。教授：专业化。我们只会在看到失败时禁止某事。爬树，专业化还是通用化？听众：通用化教授：通用化，没错。我们只会在看到正面例子时爬到树中更通用的东西上。扩展集合？听众：通用化。教授：通用化。我们只会在看到正面例子时扩展集合中的东西。封闭间隔？
        </p>
        <p>PROFESSOR: Specializer, that’s right. We’d only use it when we saw a near miss. Forbid link, specializer or
            generalizer? AUDIENCE: Specializer. PROFESSOR: Specializer. We’d only forbid something if we saw a miss.
            Climb tree, specializer or generalizer? AUDIENCE: Generalizer PROFESSOR: Generalizer, that’s right. We’d
            only climb up to a more generic thing in the tree if we saw a positive example. Extend set? AUDIENCE:
            Generalizer. PROFESSOR: Generalizer. We’d only extend the things in our set if we saw a positive example.
            Close interval?</p>
        <h2 id="unknown-593">未知</h2>
        <h2>Unknown</h2>
        <p>听众：泛化器。教授：这是泛化器。我们只会在看到正面例子时将间隔变大。有人问，如果你有 10 到 30 个样本，然后在 20
            个样本中发现一个负面例子，你会怎么做？这个系统通常必须处理这种情况，这很烦人。根据你的实现，你可以做各种各样的事情。我从未见过我们在测验中问这个问题。</p>
        <p>AUDIENCE: Generalizer. PROFESSOR: That’s a generalizer. We’d only make the interval bigger if we saw a
            positive example. Someone asks, what do you do if you have like 10 to 30, and then you find a negative
            example in 20? It’s generally pretty annoying for this system to have to deal with that. There’s a variety
            of things you could do based on your implementation. I’ve never seen us ask it in a quiz.</p>
        <p>但有一件事你可以做，那就是禁止 20，正好是 20，然后在 20
            处留一个小洞。然后删除链接。专业化还是通用化？听众：通用化。教授：通用化。如果你不了解这个系统，这个很容易搞砸。因为删除，你会想，哦，去掉一些东西。这是专业化。但实际上不是，你是在通用化整个领域，说，我们可以忘掉它。因为它们都很好。
        </p>
        <p>But one thing you could do is forbid 20, exactly just 20, and just have a little hole at exactly 20. So then
            drop link. specializer or generalizer? AUDIENCE: Generalizer. PROFESSOR: Generalizer. This one’s an easy one
            to mess up if you don’t understand the system. Because dropping, you think, oh, getting rid of something.
            That’s specializing. But actually no, you’re generalizing the entire area saying, we can forget about it.
            Because they’re all good.</p>
        <p>好吧，既然如此，我们就可以开始做这道题了。嘿，考虑到我们不用做加法，我们有相当多的时间。太好了。所以我们的第一个例子是那种立式阅读灯，你可以把它放在一个支架上，你可以上下调节小灯泡。这是一个白炽灯泡。它的高度为 11
            英寸。</p>
        <p>All right, so given that, we are set to do this problem. Hey, we have a pretty reasonable amount of time
            considering that we don’t have to do our sums. It’s great. So our first example is sort of one of those
            stand reading lights that you sort of have it on a stand, and you can adjust the little light bulb up or
            down. It is an incandescent bulb. It has a height of 11 inches.</p>
        <h2 id="unknown-594">未知</h2>
        <h2>Unknown</h2>
        <p>而且它有一个平底。它是电动的。它有一个遮阳板。它很受欢迎。它很受欢迎，是一个正面的例子，一个加分项。它很好。所以我们马上就知道我们只能使用 S 还是 G？G。我们只能使用
            G。因为它很受欢迎。所以我们不会要求。我们今天不会禁止。我们会很高兴并使用通用化器。</p>
        <p>And it’s got a flat base. It’s electric. It’s got a shade. And it’s a hit. It’s a hit, a positive example, a
            plus. It’s good. So right away, we know that we can only use the S’s or the G’s? The G’s. We can only use
            the G’s. Because it’s a hit. So we’re not going to be requiring. We’re not to be forbidding today. We’re
            going to be happy and using a generalizer.</p>
        <p>所以我会帮忙做第一个。你们会帮我做下一个。或者有人会说，我们没有讲到这个，我先讲第一个，然后我们就停下来。所以我们有一个白炽灯。这和我们的模型一样。我们的高度是 11 英寸。哦，我们的模型只覆盖了 24
            英寸。平底。这和我们的模型一样。电力。和我们的模型一样。遮阳。和我们的模型一样。</p>
        <p>So I’ll help do the first one. You guys will help me do the next one. Or someone can say, we didn’t cover
            this, and I’ll do the first one, and we’ll stop. So we’ve got an incandescent light. That’s the same as our
            model. We’ve got a height of 11 inches. Oh, our model only covers 24. Flat base. that’s the same as our
            model. Electricity. same as our model. Shade. same as our model.</p>
        <p>所以我说唯一的区别是高度。我说我们需要关闭间隔，关闭间隔。我说我们的模型与以前相同，除了高度是 11 到 24
            的一个元素。很简单。我为自己选择了简单的一个。下一个。所以这个将是。这里的图片没有出现。但它基本上是一盏没有灯罩的灯。</p>
        <p>So I say the only difference is the height. I say we need to close the interval, close interval. And I say
            that our model is the same as before, except for that height is an element of 11 to 24. Simple enough. I
            picked the easy one for myself. Next one. So this one is going to be. the picture here didn’t turn out. But
            it’s basically a lamp that doesn’t have a shade.</p>
        <h2 id="unknown-595">未知</h2>
        <h2>Unknown</h2>
        <p>它就像是一束光照在你身上。所以这是一个正面的例子。它是白炽灯，高度等于
            11.5，底座平坦，还有电。好的，那么首先，我们要使用哪种类型，专门化还是概括化？听众：概括化。教授：概括化，特别是，有人想看一下我们的模型吗？这是起始模型，除了高度可以从 11 到
            24？我们需要在这里使用哪种启发式方法？听众：放下链接。</p>
        <p>It just sort of has the light shining on you. So it is a positive example. It’s incandescent, height equals
            11.5, flat base, and electricity. All right, so first of all, which kind would we use, specializing or
            generalizing? AUDIENCE: Generalizing. PROFESSOR: Generalizing, and particularly, anyone want to take a look
            at our model, which is the starting model, except for the height can go 11 to 24? Which heuristic do we need
            to use here? AUDIENCE: Drop link.</p>
        <p>教授：好吧，大家都在说“放下链接”。是的，没错。有人说要扩展设置。好吧，我想我们可以将设置扩展到带灯罩或不带灯罩。但这就是全部。所以“放下链接”就是答案。我们可以放下灯罩。不管有没有灯罩，这仍然基本上是一盏灯。所以这是正确的。我们放下链接。所以模型。好吧，它已经做了足够多的更改，我想我会再写一遍。
        </p>
        <p>PROFESSOR: All right, people are all saying drop link. Yes, that’s right. Someone said extend set. Well, I
            suppose we could extend the set to shade or not shade. But that’s everything. So drop link is the answer. We
            can drop the shade. Shade or not, this is still pretty much a lamp. So that’s correct. We drop link. So the
            model. all right, there’s been enough changes to it that I think I’ll write it out again.</p>
        <p>该模型是白炽灯，高度等于 11 到 24
            之间的元素，底座平坦，并且是电。好，那么问题来了？听众：所以在这一切过程中，我们都在根据模型的顺序编辑模型，对吗？教授：所以问题是，我们根据示例的顺序编辑模型？听众：如果之后有相同的东西，但没有阴影或类似的东西，会发生什么？
        </p>
        <p>The model is incandescent, height equals an element from 11 to 24, and flat base, and electricity. Good, so
            question? AUDIENCE: So during this all, we’re editing our model based on the order that the model is in,
            right? PROFESSOR: So the question is, we’re editing the model based on the order of the examples? AUDIENCE:
            What happens if there’s the same thing except with no shade or something like that afterwards?</p>
        <h2 id="unknown-596">未知</h2>
        <h2>Unknown</h2>
        <p>教授：那么问题是，如果你有相同的内容，除了有一个相同的内容没有阴影，并且是差点儿内容，那么会发生什么？或者对不起，你是说，如果有一个有阴影的内容是差点儿内容，那么会发生什么？所以它可能会尝试禁止链接阴影。这就是问题所在？你说得对。这是数据不一致的问题。
        </p>
        <p>PROFESSOR: So the question is, what happens if you have the same thing except for that there’s one that’s the
            same with no shades and is a near miss or something so that you need to. or I’m sorry, you’re saying, what
            if there’s one that has a shade now that’s a near miss? And so it would probably try to forbid link shades.
            That’s sort of the question? You’re right. That’s an inconsistency in the data.</p>
        <p>这个系统非常脆弱。尤其是在订购方面非常脆弱。考虑到它是在 60
            年代制造的，你会惊讶于它有多棒。这是相当令人印象深刻的东西。但它很老了。我们并不是说这是你现在应该学习的方式。因为它有一些严重的问题。它很老了，但就它所做的而言，它已经相当不错了。</p>
        <p>The system is very fragile. It’s very fragile to ordering in particular. And you’d be surprised how awesome
            this does considering that it was made in, like, the ’60s. It’s pretty impressive stuff. But it’s old. We’re
            not saying that this is a way that you should do all of your learning nowadays. Because it has some serious
            issues. It’s old, but it’s pretty damn good for what it did.</p>
        <p>我和另一位现在在谷歌工作的朋友发明了一种较新的学习方式，即格学习，它也有自己的问题，其中之一就是因为它试图表现得像一个小孩子，一开始它会试图声称一切都很好，直到你向它展示一些好的反面例子。但解决这个问题的一个非常特别的方法是它不是无记忆的。
        </p>
        <p>A newer style of this kind of learning made by me and another friend who’s now working at Google is lattice
            learning, which has its own share of issues, one of which is that since it’s trying to act like a little
            kid, at first it tries to claim that everything is OK until you show it some good negative examples. But
            very particularly one way gets around this problem is it’s not memoryless.</p>
        <h2 id="unknown-597">未知</h2>
        <h2>Unknown</h2>
        <p>事实上，它会存储它见过的所有例子，并将它们与新例子中看到的进行比较和对比。这样它就可以说出类似的话。假设你试图教它什么可以飞。在网格学习中，你会说，好吧，蓝松鸦可以飞。然后如果你问它，牛能飞吗，当然可以，牛跳过了月亮。所有东西都可以飞。所以这就是它的问题。
        </p>
        <p>It in fact stores all of the examples it’s ever seen and compares and contrasts them to what it sees in the
            new example. And this allows it to say something like. let’s say you’re trying to teach it what can fly. In
            lattice learning, you say, all right, well, a blue jay can fly. And then if you ask it, can a cow fly, sure,
            a cow jumped over the moon. Everything can fly. So that’s its problem.</p>
        <p>但是如果你说，其实那是一首童谣，牛实际上不能飞，那么它就会说，只有鸟才能飞。因为它记得蓝松鸦可以飞。然后你最终可以说，好吧，其实蝙蝠可以飞，果蝠可以飞。好吧，所以翼手目动物可以飞，鸟也可以飞。但就是这样。你可以给它一些其他的例子。但还有其他的学习方式。
        </p>
        <p>But if you say, actually, that was a nursery rhyme, a cow cannot in fact fly, it will then say, only birds
            can fly. Because it remembers that the blue jay can fly. And then you can eventually say, well, actually
            bats can fly, a fruit bat can fly. Well, OK, so chiropterans can fly and birds can fly. But that’s it. And
            you can give it some other examples. But there are other styles of learning.</p>
        <p>你说得完全正确。事实上，如果有一道选择题问你 arch 学习的优点和缺点，缺点之一就是，它很容易受到你教给它的顺序的影响。但想想看。如果我们的第一个例子与这个完全一样，但有一个阴影，而且它错了，系统就会对你大喊大叫。
        </p>
        <p>You’re absolutely right. In fact, if there’s a multiple choice asking you about the strengths and weaknesses
            of arch learning, one of the weaknesses. it’s very vulnerable to ordering of what you teach it. But think
            about it. If our first example was one that’s exactly the same as this but with a shade, and it was a miss,
            the system would just yell at you.</p>
        <h2 id="unknown-598">未知</h2>
        <h2>Unknown</h2>
        <p>就像，你给它两件不一致的东西，这有点太混蛋了。你可能会说，在现实世界中确实会发生这种情况。嗯，拱形学习，对现实世界混乱的数据并不擅长。它会发疯。它非常强迫症。它希望一切都匹配。如果它得到两件不一致的东西，它就会对你大喊大叫。就像，你错了。这不可能是真的。因为你说阴影没问题。
        </p>
        <p>Like, you’re kind of being an asshole to it to give it two things that are inconsistent. You might say in the
            real world that happens. Well, arch learning, not great with real world messy data. It just goes ballistic.
            It’s very OCD. It wants everything to match up. If it gets two things that are inconsistent, it’ll just yell
            at you. It’s like, you’re wrong. This can’t be true. Because you said it’s OK with the shade.</p>
        <p>所以这是个非常好的问题。还有问题吗？听众：所以如果你有一个样本，该样本包含两种不同的模型，那么这就是一次差点成功。教授：这不是一次差点成功。问题是，如果你的模型中有两个不同的东西，那么这就是一次差点成功。很抱歉打断你。
        </p>
        <p>So that’s a very good question. Another question? AUDIENCE: So if you have a sample that had two different
            things for the model, and it’s a near miss. PROFESSOR: It’s not a near miss. The question is, if you have
            something with two different things in the model, and it’s a near miss. and I’m sorry to cut you off.</p>
        <p>基于时间安排，我只想说，你之后说的一切都不再适用，因为那时它还不是一次险些失误。这非常重要。只有当只有一个更改时，它才算险些失误。否则，它就是一次失误。如果是一次失误，并且有多个更改，你可能会说，你怎么知道要改变什么？答案是，你什么都改变不了。
        </p>
        <p>Based on timing, I’m just going to say, whatever you said afterwards doesn’t apply due to the fact at that
            point it’s not a near miss. It’s very important. It’s only a near miss if there’s only one change.
            Otherwise, it’s just a miss. And if it’s a miss with more than one change, you were probably going to say,
            how do you know what to change? The answer is, you can’t change anything.</p>
        <h2 id="unknown-599">未知</h2>
        <h2>Unknown</h2>
        <p>因为任何一个都可能是错误。这就是为什么对于命中，哦，它可以有任意多的不同。你只需将它们全部概括。对于未命中，它只能有一个不同之处。事实上，这里可能有一个不是近距离命中，你只需说，哦，我们什么也不做。我们继续。如果有时间，我们可能会看到它。那么问题来了？
        </p>
        <p>Because any of them could have been what was wrong. That’s why for a hit, oh, it can have as many differences
            as you want. You’ll just generalize them all. For a miss, it can only have one thing different. In fact,
            there might be one here that’s not a near miss, and you just say, oh, we don’t do anything. We’ll keep
            going. We might see it if we have time. So question?</p>
        <p>听众：因为顺序是这个大问题的一个重要因素，你遇到了一个物品，它有两个价值差异，但如果顺序不同，那么就会被考虑在内，你还记得吗？教授：所以问题是，假设你有一个非险情。你能把它留着以后在险情发生时再用吗？答案是，不能。
        </p>
        <p>AUDIENCE: Because ordering is an important factor for this bigger thing, and you encounter an item which has
            two discrepancies in values, but if the ordering were different such that would be accounted for, do you
            remember if you turn back to that PROFESSOR: So the question is, let’s say you have a non near miss. Can you
            hold onto it and use it later when it would be a near miss? The answer is, nope.</p>
        <p>如果我们要说的是，对于一个无法记忆的系统，你可以加入一些临时解决方案。这肯定会让它变得更聪明。它会利用所有数据。但 60
            年代的帕特里克在发明这个系统时追求优雅。优雅的解决方案是，让我们完全不记东西，这个想法是，小婴儿无法告诉你他们玩拱形积木的每一次经历。所以我们什么都不记。</p>
        <p>If we’re going here, for a system that doesn’t remember things, you can put in some kludges. That definitely
            makes it smarter. It uses all its data. But Patrick in the ’60s making this up strove for elegance. And the
            elegant solution is, let’s just be memoryless completely, the idea being, well, little babies can’t tell you
            every experience they had with playing with blocks with an arch. So let’s remember nothing.</p>
        <h2 id="unknown-600">未知</h2>
        <h2>Unknown</h2>
        <p>对于网格学习，我们的想法是，人们在潜意识的某个地方可能确实存储了所有的例子，或者至少存储了比你认为的要多得多的东西。那么为什么不存储它们呢？但是对于拱形学习，小婴儿无法告诉你，哦，是的，有一次我玩了一盏灯，它没有灯罩，所以它不是灯，或者诸如此类的东西。
        </p>
        <p>With lattice learning, our idea is that people sort of somewhere in the subconscious maybe do store all the
            examples, or at least a lot more than you give them credit for. So why not store them? But with arch
            learning, it’s, little babies can’t tell you, oh yeah, there’s that one time I played with a lamp, and it
            didn’t have a shade, and so it wasn’t a lamp, or something like that.</p>
        <p>它们无法存储它。它们无法保存它以供日后使用。有点像图灵的风格，他认为人类可以成为计算机的老师，arch 学习真正关注的是人类是一个善良的好老师，可以提供当时完全合适的例子。</p>
        <p>They’re not going to be able to store it. They’re not going to be able to save it for later. Sort of in the
            style of Turing, who believed a human would be a teacher to a computer, arch learning really focuses on the
            fact that the human is a kind and good teacher who offers examples that are exactly appropriate at the time.
        </p>
        <p>这给你这个拱形学习系统的培训师带来了很大的压力。大家的问题都很好。让我们继续解决这个问题。下一个示例基本上是另一种立式灯。它有一个向下发光的灯，是荧光灯。它看起来像这样。它是荧光灯。高度等于 13
            英寸，平底座，电动，有灯罩。是的，正如我在这里所说的，它很受欢迎。</p>
        <p>And that puts a lot of pressure on you as the trainer of an arch learning system. Good questions, all. Let’s
            continue working this guy out. So the next example is basically another one of these stand lamps. It’s got a
            light shining down, which is fluorescent. It looks like this. It’s fluorescent. Height equals 13 inches,
            flat base, electric, shade. And yeah, as I put here, it’s a hit.</p>
        <h2 id="unknown-601">未知</h2>
        <h2>Unknown</h2>
        <p>正如你们无数次告诉我的那样，我相信你们还记得，我们将对命中进行概括。那么我们在这里应该做什么呢？有些人说扩展集合。听众：爬树。教授：说爬树的人是对的。为什么我们要爬树而不是扩展集合？因为它是一棵树，而不是一个集合。
        </p>
        <p>So as you guys have told me countless times, so I believe you that you remember, we’re going to generalize
            for the hit. So what should we do here? So some people are saying extend set. AUDIENCE: Climb tree.
            PROFESSOR:. The people who are saying climb tree are correct. Why do we climb tree instead of extend set?
            Because of the fact that it’s a tree, not a set.</p>
        <p>而且，爬树比将其视为一个集合并扩展该集合更为节俭。集合没有层次结构。它就像红色逗号蓝色逗号黄色，或阴影逗号非阴影。每当您有一棵树时，事实上，仅仅爬树比将其视为一个集合并扩展它更为节俭，尽管我认为您可以这样做。</p>
        <p>And it is more parsimonious to climb the tree than it is to treat it as a set and extend the set. Sets don’t
            have a hierarchy. It’s just like red comma blue comma yellow, or shade comma not shade. Whenever you have a
            tree, it’s a fact that it’s more parsimonious to just climb it than it is to treat it as a set and extend
            it, even though you could do that, I suppose.</p>
        <p>拱形学习很简约。它很优雅。它很简单。而且优雅的做法就是爬上去。所以是的，我们要爬上树。所以我们的模型和上次的一样，只是这次它有一个光源，而不是白炽灯。它只是爬到光源上，当然。好的，所以下一个很酷。这是另一种荧光照射灯。
        </p>
        <p>And arch learning, it’s parsimonious. It’s elegant. It’s simple. And the elegant thing to do. just climb up.
            So yes, we’re going to climb up the tree. So our model is the same as last time except that this time it has
            a light source instead of saying, incandescent. It just climbed up to light source, sure. All right, so the
            next one is pretty cool. It’s another one of those fluorescent shine down lamps.</p>
        <h2 id="unknown-602">未知</h2>
        <h2>Unknown</h2>
        <p>但在太空中，它有一个三脚架，上面有三条腿。这些腿都有轮子。所以它也很受欢迎。所以它是荧光的，高度等于 14
            英寸，有轮子腿，而且是电动的。所以很明显我们在概括。这次我们将使用什么启发式方法？听众：爬树。教授：再爬树一次，没错。我们要从平底爬到底座支撑本身。因为轮子腿和平底，底座支撑是两者的祖先。</p>
        <p>But in space, it has a tripod with three legs. And those legs all have wheels. So it is also a hit. So it is
            fluorescent, height equals 14 inches, wheeled legs, and electric. So obviously we’re generalizing. What
            heuristic will we use this time? AUDIENCE: Climb tree. PROFESSOR: Climb tree again, that’s right. We’re
            going to climb from flat base to base support itself. Because wheeled legs and flat base, base support is an
            ancestor of both.</p>
        <p>所以我们的模型和以前一样。但此时，它已经发生了足够多的变化，我不妨把它写出来。所以我们会说它是光源，高度在 11 到 24
            英寸之间，有底座支撑，某种支撑和电力。太棒了，现在来谈谈奇偶校验。啊，不，你回来。我们开始吧。所以下面那个，不要混淆。下面那个是我们最新的模型。我们现在有。哦，哎呀，呵呵。</p>
        <p>So our model is the same as before. But at this point, it’s changed enough that I might as well write it out.
            So we’ll say that it is light source, height between 11 and 24 inches, base support, some sort of support,
            and electricity. Awesome, now, come on parity. Ah, no, back up, you. There we go. So that down there, don’t
            get confused. That down there is our most recent model. We now have. oh, oops, hehe.</p>
        <p>好吧，我其实漏了一个例子。但是，我们其实完全正确，这暴露了很多问题。我们漏了一个，白炽灯泡。在过去，白炽灯是我们唯一的灯。高度等于 60 英寸，扁平，在过去，扁平灯是我们唯一的灯。电动，在过去，我们有电动。</p>
        <p>OK, I actually missed one of the examples. However, we were actually completely correct, which gives away
            quite a lot. We had a miss, incandescent bulb. back in the old days when incandescent was the only kind of
            light we had. Height equals 60 inches, flat, back in the old days when flat was the only one we had,
            electric, back in the old days when we had electric.</p>
        <h2 id="unknown-603">未知</h2>
        <h2>Unknown</h2>
        <p>在我们删除了阴影链接之后，我们还有阴影。所以我们知道阴影或不阴影都可以。那么问题是，我们该怎么做？首先，我们是专门处理这个错误还是概括处理？听众：专门处理。教授：显然我们只能要求或禁止。所以标准上、传统上，答案是什么都没有。
        </p>
        <p>And we also have shade after we’ve dropped the link for shade. So we know that shade or not shade are both
            OK. So the question is, what do we do? First of all, do we specialize or generalize with this miss?
            AUDIENCE: Specialize. PROFESSOR: So obviously we’re only to be able to require or forbid. So standardly,
            traditionally, the answer would be nothing.</p>
        <p>但是，我们认为对身高设置要求是合理的。有些学生会这样做，比如说，天哪，11 到 24
            的设置太难了。但这不是你通常会做的事情。因此，为了表明我们试图理解在奇怪的情况下可能会有多个想法，我们确实接受了一次。但一般来说，你不必做任何事情。</p>
        <p>However, we accept it as sane the possibility that you might put a require onto the height. Some students did
            that, like say, oh gosh, that’s a hard set, 11 to 24. But that’s not what you would normally do. So just to
            show that we tried to understand that there might be multiple ideas in a weird situation, we did accept that
            one time. But generally, there’s not anything you have to do.</p>
        <p>当时，这个系统，直到现在，都表示 11 到 24。这恰恰是这个例子应该去的正确位置，这真是太神奇了。但你不必对模型做任何事情。你不必使用任何启发式方法。好的，最后一步，问题是。哦，问题。</p>
        <p>This system at the time, and still now, said 11 to 24. This is spookily exactly the correct place where this
            example would go. But you don’t have to do anything to the model. You don’t have to use any heuristics. OK,
            so last step, the question is. oh, question.</p>
        <h2 id="unknown-604">未知</h2>
        <h2>Unknown</h2>
        <p>听众：所以，如果我们按照这个特定的顺序来做，将它作为示例列表中的下一个，那么它与我们现有的模型只有一个不同，对吗？教授：不，它有很大不同。如果我们这样做，实际上不会是一次险胜。听众：如果我们这样做，那将是一次险胜。教授：不，白炽灯与光源不同。高度与高度不同。
        </p>
        <p>AUDIENCE: So here, if we did it in this particular order, having it be the next in our example list, then it
            only differs by one from our existing model, right? PROFESSOR: No, it differs by a lot. It actually wouldn’t
            be a near miss if we did it out. AUDIENCE: It would be a near miss if we did it out. PROFESSOR: No,
            incandescent is different than light source. Height is different than height.</p>
        <p>平面与基座支撑不同。所以两者相差很大，而且不会差到哪里去。听众：好的，那么这里有一个问题。即使树上的白炽灯是光源分支出来的，如果我们遇到的话。</p>
        <p>Flat is different than base support. So it would differ by a huge number, and it would not be a near miss.
            AUDIENCE: OK, then here’s a question. Even though on the tree incandescent is a child branching off of light
            source, if we instead encountered.</p>
        <p>因为我们目前的模型说的是光源逗号之类的东西，如果我们得到的是负数，即荧光，那么这是否不算是一次近距离碰撞，因为荧光与光源不同？教授：所以问题是，如果你有荧光而不是光源，这是否不算是一次近距离碰撞，因为荧光与光源不同？所以答案是肯定的。听众：由于它是无记忆的，它不知道荧光。
        </p>
        <p>Because our current model says light source comma whatnot, if instead we got a negative thing that said,
            fluorescent, again, would that be considered not a near miss, because fluorescent differs from light source?
            PROFESSOR: So the question is, would it not be considered a near miss if you had, say, fluorescent rather
            than light source, because fluorescent differs from light source? So the answer there is. AUDIENCE: Since
            it’s memoryless, it doesn’t know that fluorescent.</p>
        <h2 id="unknown-605">未知</h2>
        <h2>Unknown</h2>
        <p>教授：当然，它是无记忆的。它不知道荧光以前曾是正例。不幸的是，假设我们有一个荧光的例子，高度是元素 11 到 24，基座支撑，电。实际上，假设我们有一个钠蒸气的例子，高度是 13。钠蒸气，高度是
            13，基座支撑。出于某种原因，它被称为基座支撑和电。这是可能的，对吧？</p>
        <p>PROFESSOR: Well sure, it is memoryless. It doesn’t know that fluorescent used to be a positive example
            before. Unfortunately, let’s say we did have an example that was fluorescent, height is an element of 11 to
            24, base support, electric. Actually, let’s say we have one that was sodium vapor, height is 13. Sodium
            vapor, height is 13, base support. for some reason it was called base support and electric. That’s possible,
            right?</p>
        <p>我们还没有看到钠蒸气的正面结果。假设钠蒸气是不允许的。如果我们不说这是一次险情，然后禁止链接钠蒸气，我们怎么才能摆脱钠蒸气呢？不幸的是，正因为如此，我们失去了一些表达能力。实际上，你说的非常合乎逻辑，也很有道理，你希望有能力说，好的，这是一个子集。这应该没问题。
        </p>
        <p>We haven’t seen a positive of sodium vapor. Let’s say that sodium vapor was not allowed. If we don’t say
            that’s a near miss and then forbid link sodium vapor, how are we ever going to get rid of sodium vapor?
            Unfortunately, because of that, we lose some amount of expressiveness. Actually, what you’re saying is very
            logical and makes sense, that you would want to have the ability to say, OK, this is a subset. This should
            be OK.</p>
        <p>但不幸的是，由于我们爬树时，即使没有看到树底所有事物的正面例子，我们实际上也失去了这种能力。这是一种权衡。这无疑是你在这里指出的 arch
            学习的一个弱点。格子学习解决了这个问题，但它也有自己的问题。所以你是对的，你希望能够做到这一点。但你不能。因为它是无记忆的。</p>
        <p>But unfortunately, because of the fact that we climbed the tree even when we haven’t seen a positive example
            for all things in the bottom on that tree, we actually lose that ability. It’s a trade off. It is most
            certainly a weakness of arch learning that you point out here. Lattice learning fixes that, but it has its
            own problems. So you’re right, that you’d want to be able to do that. But you can’t. Because it’s
            memoryless.</p>
        <h2 id="unknown-606">未知</h2>
        <h2>Unknown</h2>
        <p>最后但并非最不重要的一点是，我们将展示哪种模型，以便从最终系统中立即教会系统，灯需要底座支撑？因此，它需要底座支撑的连接。所以真的很快。白炽灯泡的高度为 6 英寸，底座平坦，并且通电。这样可以吗？</p>
        <p>So last but not least, which model would we present in order to teach the system, right now from its final
            system, that a lamp requires a base support? So it would need to be require link for the base support. So
            really fast. incandescent bulb with a height of 6, a flat base, and electricity. Would that do it?</p>
        <p>高度为 8 的白炽灯泡和电池，高度为 12 的白炽灯泡，电线支架和电力。哦，对了，顺便说一句，这将是一个教授要求的失误。荧光灯泡，高度为 21，夹式底座和电力，或白炽灯泡，高度为
            12，平底脚和电力。所以有人从我说的速度中挑出这个吗？因为要进来。</p>
        <p>Incandescent bulb with a height of 8 and a battery, incandescent bulb with a height of 12, wire support, and
            electricity. oh yeah, by the way, this will have to be a miss to teach a require. Fluorescent bulb, height
            of 21, clamp base, and electricity, or incandescent bulb, height of 12, flat bottomed legs, and electricity.
            so did anyone pick that out over the fastness of me saying it? Because is going to want to come in.</p>
        <p>问题还是答案？观众：教授：答案肯定是错的。观众：如果答案是在电线支架上？教授：是的。我知道答案很快。你可以看看 2007
            年期末测验。因为马上就要到了。但是答案肯定是错的，除了电线支架之外，其他都正确。那么你的问题是，等一下，上面写着白炽灯泡。</p>
        <p>Question or answer? AUDIENCE: PROFESSOR: It’s going to have a miss. AUDIENCE: If it’s on the wire support?
            PROFESSOR: Yes. So I know it was fast. You can look through quiz 2007 final. Because is going to come in a
            minute. But it would be a miss on the one that was correct on everything except for it was a wire support.
            Then the question you have is, wait a minute, it says incandescent bulb.</p>
        <h2 id="unknown-607">未知</h2>
        <h2>Unknown</h2>
        <p>现在，这是光源的一个子集。你又遇到了他的问题。答案是，他们肯定已经决定这样做是可以的。但另一方面，如果决定不这样做，它将如何学习钠蒸气？这将是一个实施细节。因此，你可以做的一件事是尝试以合乎逻辑的方式将子集视为可以。因为问题可能会这样做。
        </p>
        <p>That is now a subset of light source. You have his question again. The answer to that is they must have
            decided here that was OK. But on the other hand, how would it learn sodium vapor if that wasn’t decided to
            be OK? That’s going to have to be an implementation detail. So one thing you can do is try to treat subsets
            as OK in the logical way. Because questions may do that.</p>
        <p>然后我想你已经失去了禁止钠蒸气的能力。所以你必须放弃其中之一。我认为如果出现这种情况，你绝对可以询问助教或监考人员选择哪种实施细节。因为你可以在运行 arch learning 时选择其中一种。</p>
        <p>And then I guess you’ve lost the ability to forbid, say, sodium vapor. So you have to lose one or the other.
            I think you’re definitely OK to ask TAs or someone who’s proctoring if a situation like that comes up, which
            implementation detail is chosen. Because you could choose either when you’re running arch learning.</p>

    </div>
    <script id="res-script" src="/res/dist/res/main.js" type="text/javascript"></script>
</body>

</html>