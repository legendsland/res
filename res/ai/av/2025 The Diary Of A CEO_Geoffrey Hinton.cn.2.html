<!DOCTYPE html><html class="translated-ltr"><!--
 Page saved with SingleFile 
 url: file:///home/zy/ws/res/res/ai/av/2025%20The%20Diary%20Of%20A%20CEO_Geoffrey%20Hinton.html 
 saved date: Wed Jul 09 2025 13:43:58 GMT+0800 (Hong Kong Standard Time)
--><head>
<meta name="dc.identifier" content="res/a2200d997b0868f8ac959adc91b5c54dd9a896b3">
<meta charset="utf-8">
      <style>:root{--yawas:rgb(252,214,57)}body{margin:16px;font-family:system-ui,sans-serif}.yawas{background-color:var(--yawas)}@media print{.yawas{-webkit-print-color-adjust:exact}}</style>
    <title>2025 The Diary Of A CEO_Geoffrey Hinton</title>
    
    <style>.VIpgJd-ZVi9od-ORHb-OEVmcd{left:0;top:0;height:39px;width:100%;z-index:10000001;position:fixed;border:none;border-bottom:1px solid #6B90DA;margin:0;box-shadow:0 0 8px 1px #999}.VIpgJd-ZVi9od-xl07Ob-OEVmcd{z-index:10000002;border:none;position:fixed;box-shadow:0 3px 8px 2px #999}.VIpgJd-ZVi9od-SmfZ-OEVmcd{z-index:10000000;border:none;margin:0}.goog-te-gadget{font-family:arial;font-size:11px;color:#666;white-space:nowrap}.goog-te-gadget img{vertical-align:middle;border:none}.goog-te-gadget-simple{background-color:#FFF;border-left:1px solid #D5D5D5;border-top:1px solid #9B9B9B;border-bottom:1px solid #E8E8E8;border-right:1px solid #D5D5D5;font-size:10pt;display:inline-block;padding-top:1px;padding-bottom:2px;cursor:pointer}.goog-te-gadget-icon{margin-left:2px;margin-right:2px;width:19px;height:19px;border:none;vertical-align:middle}.goog-te-combo{margin-left:4px;margin-right:4px;vertical-align:baseline}.goog-te-gadget .goog-te-combo{margin:4px 0}.VIpgJd-ZVi9od-l4eHX-hSRGPd,.VIpgJd-ZVi9od-l4eHX-hSRGPd:link,.VIpgJd-ZVi9od-l4eHX-hSRGPd:visited,.VIpgJd-ZVi9od-l4eHX-hSRGPd:hover,.VIpgJd-ZVi9od-l4eHX-hSRGPd:active{font-size:12px;font-weight:bold;color:#444;text-decoration:none}.VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd,.VIpgJd-ZVi9od-TvD9Pc-hSRGPd{display:block;margin:0 10px}.VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd{padding-top:2px;padding-left:4px}.goog-te-combo,.VIpgJd-ZVi9od-ORHb *,.VIpgJd-ZVi9od-SmfZ *,.VIpgJd-ZVi9od-xl07Ob *,.VIpgJd-ZVi9od-vH1Gmf *,.VIpgJd-ZVi9od-l9xktf *{font-family:arial;font-size:10pt}.VIpgJd-ZVi9od-ORHb{margin:0;background-color:#E4EFFB;overflow:hidden}.VIpgJd-ZVi9od-ORHb img{border:none}.VIpgJd-ZVi9od-ORHb-bN97Pc{color:#000}.VIpgJd-ZVi9od-ORHb-bN97Pc img{vertical-align:middle}.VIpgJd-ZVi9od-ORHb-Tswv1b{color:#666;vertical-align:top;margin-top:0;font-size:7pt}.VIpgJd-ZVi9od-ORHb-KE6vqe{width:8px}.VIpgJd-ZVi9od-LgbsSe{border-color:#E7E7E7;border-style:none solid solid none;border-width:0 1px 1px 0}.VIpgJd-ZVi9od-LgbsSe div{border-color:#CCC #999 #999 #CCC;border-right:1px solid #999;border-style:solid;border-width:1px;height:20px}.VIpgJd-ZVi9od-LgbsSe button{background:transparent;border:none;cursor:pointer;height:20px;overflow:hidden;margin:0;vertical-align:top;white-space:nowrap}.VIpgJd-ZVi9od-LgbsSe button:active{background:none repeat scroll 0 0#CCC}.VIpgJd-ZVi9od-SmfZ{margin:0;background-color:#FFF;white-space:nowrap}.VIpgJd-ZVi9od-SmfZ-hSRGPd{text-decoration:none;font-weight:bold;font-size:10pt;border:1px outset #888;padding:6px 10px;white-space:nowrap;position:absolute;left:0;top:0}.VIpgJd-ZVi9od-SmfZ-hSRGPd img{margin-left:2px;margin-right:2px;width:19px;height:19px;border:none;vertical-align:middle}.VIpgJd-ZVi9od-SmfZ-hSRGPd span{text-decoration:underline;margin-left:2px;margin-right:2px;vertical-align:middle}.goog-te-float-top .VIpgJd-ZVi9od-SmfZ-hSRGPd{padding:2px;border-top-width:0}.goog-te-float-bottom .VIpgJd-ZVi9od-SmfZ-hSRGPd{padding:2px;border-bottom-width:0}.VIpgJd-ZVi9od-xl07Ob-lTBxed{text-decoration:none;color:#00C;white-space:nowrap;margin-left:4px;margin-right:4px}.VIpgJd-ZVi9od-xl07Ob-lTBxed span{text-decoration:underline}.VIpgJd-ZVi9od-xl07Ob-lTBxed img{margin-left:2px;margin-right:2px}.goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed{color:#000}.goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed span{text-decoration:none}.VIpgJd-ZVi9od-xl07Ob{background-color:#FFF;text-decoration:none;border:2px solid #C3D9FF;overflow-y:scroll;overflow-x:hidden;position:absolute;left:0;top:0}.VIpgJd-ZVi9od-xl07Ob-ibnC6b{padding:3px;text-decoration:none}.VIpgJd-ZVi9od-xl07Ob-ibnC6b,.VIpgJd-ZVi9od-xl07Ob-ibnC6b:link{color:#00C;background:#FFF}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:visited{color:#551A8B}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:hover{background:#C3D9FF}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:active{color:#00C}.VIpgJd-ZVi9od-vH1Gmf{background-color:#FFF;text-decoration:none;border:1px solid #6B90DA;overflow:hidden;padding:4px}.VIpgJd-ZVi9od-vH1Gmf-KrhPNb{width:16px}.VIpgJd-ZVi9od-vH1Gmf-hgDUwe{margin:6px 0;height:1px;background-color:#aaa;overflow:hidden}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div{padding:4px}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b .uDEFge{display:none}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd .fmcmS{padding-left:4px;padding-right:4px}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd{text-decoration:none}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:link div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:visited div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:active div{color:#00C;background:#FFF}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:hover div{color:#FFF;background:#36C}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:link div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:visited div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:hover div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:active div{color:#000;font-weight:bold}.VIpgJd-ZVi9od-l9xktf{background-color:#FFF;overflow:hidden;padding:8px;border:none;border-radius:10px}.VIpgJd-ZVi9od-l9xktf-OEVmcd{background-color:#FFF;border:1px solid #6B90DA;box-shadow:0 3px 8px 2px #999;border-radius:8px}.VIpgJd-ZVi9od-l9xktf img{border:none}.VIpgJd-ZVi9od-l9xktf-fmcmS{margin-top:6px}.VIpgJd-ZVi9od-l9xktf-VgwJlc{margin-top:6px;white-space:nowrap}.VIpgJd-ZVi9od-l9xktf-VgwJlc *{vertical-align:middle}.VIpgJd-ZVi9od-l9xktf-VgwJlc .DUGJie{background-image:url(data:,)}.VIpgJd-ZVi9od-l9xktf-VgwJlc .TdyTDe{background-image:url(data:,)}.VIpgJd-ZVi9od-l9xktf-VgwJlc span{color:#00C;text-decoration:underline;cursor:pointer;margin:0 4px}.VIpgJd-ZVi9od-l9xktf-I9GLp{margin:6px 0 0}.VIpgJd-ZVi9od-l9xktf-I9GLp form{margin:0}.VIpgJd-ZVi9od-l9xktf-I9GLp form textarea{margin-bottom:4px;width:100%}.VIpgJd-ZVi9od-l9xktf-yePe5c{margin:6px 0 4px}.VIpgJd-ZVi9od-aZ2wEe-wOHMyf{z-index:1000;position:fixed;-webkit-transition-delay:.6s;transition-delay:.6s;left:-1000px;top:-1000px}.VIpgJd-ZVi9od-aZ2wEe-wOHMyf-ti6hGc{-webkit-transition-delay:0s;transition-delay:0s;left:-14px;top:-14px}.VIpgJd-ZVi9od-aZ2wEe-OiiCO{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-align:center;-webkit-align-items:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;justify-content:center;width:104px;height:104px;border-radius:50px;background:#FFF url(data:image/png;base64,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)50% 50%no-repeat;-webkit-transition:all .6s ease-in-out;transition:all .6s ease-in-out;-webkit-transform:scale(.4);transform:scale(.4);opacity:0}.VIpgJd-ZVi9od-aZ2wEe-OiiCO-ti6hGc{-webkit-transform:scale(.5);transform:scale(.5);opacity:1}.VIpgJd-ZVi9od-aZ2wEe{margin:2px 0 0 2px;-webkit-animation:spinner-rotator 1.4s linear infinite;animation:spinner-rotator 1.4s linear infinite}@-webkit-keyframes spinner-rotator{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(270deg);transform:rotate(270deg)}}@keyframes spinner-rotator{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(270deg);transform:rotate(270deg)}}.VIpgJd-ZVi9od-aZ2wEe-Jt5cK{stroke-dasharray:187;stroke-dashoffset:0;stroke:#4285F4;-webkit-transform-origin:center;transform-origin:center;-webkit-animation:spinner-dash 1.4s ease-in-out infinite;animation:spinner-dash 1.4s ease-in-out infinite}@-webkit-keyframes spinner-dash{0%{stroke-dashoffset:187}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);transform:rotate(135deg)}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);transform:rotate(450deg)}}@keyframes spinner-dash{0%{stroke-dashoffset:187}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);transform:rotate(135deg)}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);transform:rotate(450deg)}}.VIpgJd-yAWNEb-L7lbkb html,.VIpgJd-yAWNEb-L7lbkb body,.VIpgJd-yAWNEb-L7lbkb div,.VIpgJd-yAWNEb-L7lbkb span,.VIpgJd-yAWNEb-L7lbkb iframe,.VIpgJd-yAWNEb-L7lbkb h1,.VIpgJd-yAWNEb-L7lbkb h2,.VIpgJd-yAWNEb-L7lbkb h3,.VIpgJd-yAWNEb-L7lbkb h4,.VIpgJd-yAWNEb-L7lbkb h5,.VIpgJd-yAWNEb-L7lbkb h6,.VIpgJd-yAWNEb-L7lbkb p,.VIpgJd-yAWNEb-L7lbkb a,.VIpgJd-yAWNEb-L7lbkb img,.VIpgJd-yAWNEb-L7lbkb ol,.VIpgJd-yAWNEb-L7lbkb ul,.VIpgJd-yAWNEb-L7lbkb li,.VIpgJd-yAWNEb-L7lbkb table,.VIpgJd-yAWNEb-L7lbkb form,.VIpgJd-yAWNEb-L7lbkb tbody,.VIpgJd-yAWNEb-L7lbkb tr,.VIpgJd-yAWNEb-L7lbkb td{all:unset;font-size:100%;line-height:normal}.VIpgJd-yAWNEb-L7lbkb ol,.VIpgJd-yAWNEb-L7lbkb ul{list-style:none}.VIpgJd-yAWNEb-L7lbkb table{border-collapse:collapse;border-spacing:0}.VIpgJd-yAWNEb-L7lbkb caption,.VIpgJd-yAWNEb-L7lbkb th,.VIpgJd-yAWNEb-L7lbkb td{text-align:left;font-weight:normal}.VIpgJd-yAWNEb-L7lbkb input::-moz-focus-inner{border:0}div>.VIpgJd-yAWNEb-L7lbkb{padding:10px 14px}.VIpgJd-yAWNEb-L7lbkb{color:#222;background-color:#fff;border:1px solid #eee;box-shadow:0 4px 16px rgba(0,0,0,.2);-moz-box-shadow:0 4px 16px rgba(0,0,0,.2);-webkit-box-shadow:0 4px 16px rgba(0,0,0,.2);display:none;font-family:arial;font-size:10pt;width:420px;padding:12px;position:absolute;z-index:10000}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-nVMfcd-fmcmS,.VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe{clear:both;font-size:10pt;position:relative;text-align:justify;width:100%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-r4nke{color:#999;font-family:arial,sans-serif;margin:4px 0;text-align:left}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TvD9Pc-LgbsSe{display:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-l4eHX{float:left;margin:0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-PLDbbf{display:inline-block}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-fw42Ze-Z0Arqf-haAclf{display:none;width:100%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-H9tDt{margin-top:20px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-LK5yu{float:left}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-qwU8Me{float:right}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-cGMI2b{min-height:15px;position:relative;height:1%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-jOfkMb-Ne3sFf{background:-webkit-linear-gradient(top,#29910d 0,#20af0e 100%);background:-ms-linear-gradient(top,#29910d 0,#20af0e 100%);background:#29910d;border-radius:4px;-moz-border-radius:4px;-webkit-border-radius:4px;box-shadow:inset 0 2px 2px #1e6609;-moz-box-shadow:inset 0 2px 2px #1e6609;-webkit-box-shadow:inset 0 2px 2px #1e6609;color:white;font-size:9pt;font-weight:bolder;margin-top:12px;padding:6px;text-shadow:1px 1px 1px #1e6609}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-hSRGPd{color:#15c;cursor:pointer;font-family:arial;font-size:11px;margin-right:15px;text-decoration:none}.VIpgJd-yAWNEb-L7lbkb>textarea{font-family:arial;resize:vertical;width:100%;margin-bottom:10px;border-radius:1px;border:1px solid #d9d9d9;border-top:1px solid silver;font-size:13px;height:auto;overflow-y:auto;padding:1px}.VIpgJd-yAWNEb-L7lbkb textarea:focus{box-shadow:inset 0 1px 2px rgba(0,0,0,.3);border:1px solid #4d90fe;outline:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-IbE0S{margin-right:10px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp{min-height:25px;vertical-align:middle;padding-top:8px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp{margin-bottom:5px;margin-bottom:0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input{display:inline-block;min-width:54px;*min-width:70px;border:1px solid #dcdcdc;border:1px solid rgba(0,0,0,.1);text-align:center;color:#444;font-size:11px;font-weight:bold;height:27px;outline:0;padding:0 8px;vertical-align:middle;line-height:27px;margin:0 16px 0 0;box-shadow:0 1px 2px rgba(0,0,0,.1);-moz-box-shadow:0 1px 2px rgba(0,0,0,.1);-webkit-box-shadow:0 1px 2px rgba(0,0,0,.1);border-radius:2px;-webkit-transition:all .218s;transition:all .218s;background-color:#f5f5f5;background-image:-webkit-gradient(linear,left top,left bottom,from(#f5f5f5),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f5f5f5,#f1f1f1);background-image:-o-linear-gradient(top,#f5f5f5,#f1f1f1);-webkit-user-select:none;-moz-user-select:none;cursor:default}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover{border:1px solid #c6c6c6;color:#222;-webkit-transition:all 0s;transition:all 0s;background-color:#f8f8f8;background-image:-webkit-gradient(linear,left top,left bottom,from(#f8f8f8),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f8f8f8,#f1f1f1);background-image:-o-linear-gradient(top,#f8f8f8,#f1f1f1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active{border:1px solid #c6c6c6;color:#333;background-color:#f6f6f6;background-image:-webkit-gradient(linear,left top,left bottom,from(#f6f6f6),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f6f6f6,#f1f1f1);background-image:-o-linear-gradient(top,#f6f6f6,#f1f1f1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus:active{box-shadow:inset 0 0 0 1px rgba(255,255,255,.5);-webkit-box-shadow:inset 0 0 0 1px rgba(255,255,255,.5);-moz-box-shadow:inset 0 0 0 1px rgba(255,255,255,.5)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe{outline:none;border:1px solid #4d90fe;z-index:4!important}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.gk6SMd{background-color:#eee;background-image:-webkit-gradient(linear,left top,left bottom,from(#eee),to(#e0e0e0));background-image:-webkit-linear-gradient(top,#eee,#e0e0e0);background-image:-o-linear-gradient(top,#eee,#e0e0e0);box-shadow:inset 0 1px 2px rgba(0,0,0,.1);border:1px solid #ccc;color:#333}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf{color:white;border-color:#3079ed;background-color:#4d90fe;background-image:-webkit-gradient(linear,left top,left bottom,from(#4d90fe),to(#4787ed));background-image:-webkit-linear-gradient(top,#4d90fe,#4787ed);background-image:-o-linear-gradient(top,#4d90fe,#4787ed)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active{border-color:#3079ed;background-color:#357ae8;background-image:-webkit-gradient(linear,left top,left bottom,from(#4d90fe),to(#357ae8));background-image:-webkit-linear-gradient(top,#4d90fe,#357ae8);background-image:-o-linear-gradient(top,#4d90fe,#357ae8)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover{box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1);-webkit-box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1);-moz-box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover{border-color:#3079ed}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-mrxPge{color:#999;font-family:arial,sans-serif}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-W0vJo-fmcmS{color:#999;font-size:11px;font-family:arial,sans-serif;margin:15px 0 5px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-u0pjoe-fmcmS{color:#800;display:none;font-size:9pt}.VIpgJd-yAWNEb-VIpgJd-fmcmS-sn54Q{background-color:#c9d7f1;box-shadow:2px 2px 4px #99a;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;position:relative}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-xl07Ob{background:#fff;border:1px solid #ddd;box-shadow:0 2px 4px #99a;min-width:0;outline:none;padding:0;position:absolute;z-index:2000}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb{cursor:pointer;padding:2px 5px 5px;margin-right:0;border-style:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb:hover{background:#ddd}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb h1{font-size:100%;font-weight:bold;margin:4px 0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb strong{color:#345aad}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo{text-align:right;position:absolute;right:0;left:auto}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-j7LFlb-SIsrTd .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo{text-align:left;position:absolute;left:0;right:auto}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-fmcmS,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{background-color:#f1ea00;border-radius:4px;-webkit-border-radius:4px;-moz-border-radius:4px;box-shadow:rgba(0,0,0,.5) 3px 3px 4px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;color:#f1ea00;cursor:pointer;margin:-2px -2px -2px -3px;padding:2px 2px 2px 3px;position:relative}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{color:#222}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe{color:white;position:absolute!important}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{background-color:#c9d7f1;border-radius:4px 4px 0 0;-webkit-border-radius:4px 4px 0 0;-moz-border-radius:4px 4px 0 0;box-shadow:rgba(0,0,0,.5) 3px 3px 4px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;cursor:pointer;margin:-2px -2px -2px -3px;padding:2px 2px 3px 3px;position:relative}.VIpgJd-yAWNEb-L7lbkb span:focus{outline:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-DyVDA{background-color:transparent;border:1px solid #4d90fe;border-radius:0;-webkit-border-radius:0;-moz-border-radius:0;margin:-2px;padding:1px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-LzX3ef{border-left:2px solid red;margin-left:-2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-YIAiIb{border-right:2px solid red;margin-right:-2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf{padding:2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS{font-size:11px;padding:2px 2px 3px;margin:0;background-color:#fff;color:#333;border:1px solid #d9d9d9;border-top:1px solid #c0c0c0;display:inline-block;vertical-align:top;height:21px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;-webkit-border-radius:1px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:hover{border:1px solid #b9b9b9;border-top:1px solid #a0a0a0;box-shadow:inset 0 1px 2px rgba(0,0,0,.1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:focus{box-shadow:inset 0 1px 2px rgba(0,0,0,.3);outline:none;border:1px solid #4d90fe}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-sFeBqf{font-size:11px;padding:2px 6px 3px;margin:0 0 0 2px;height:21px}.VIpgJd-yAWNEb-L7lbkb>div{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;flex-direction:column;font-family:"Google Sans",Arial,sans-serif}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-Ud7fr{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-align:end;-webkit-align-items:end;align-items:end;margin:14px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-IuizWc-SIsrTd{margin-right:14px;color:#747775;font-size:14px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-IuizWc-i3jM8c{margin-left:14px;color:#747775;font-size:14px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif{margin:0 16px 16px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1{width:auto;color:#1f1f1f;font-size:16px;text-align:initial}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1 .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid{border-radius:0 0 12px 12px;margin:0;background:#f1f4f9;position:relative;min-height:50px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;flex-direction:column;width:77%;padding:12px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-UTujCb{color:#1f1f1f;font-size:12px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-UTujCb{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-eO9mKe{color:#444746;font-size:12px;padding-top:4px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-eO9mKe{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5{position:absolute;top:10px;right:5px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5 .VIpgJd-yAWNEb-SIsrTd{left:5px;right:auto}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-THI6Vb{fill:#0b57d0}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf{margin:-4px 2px 0 0;padding:2px 0 0;width:48px;height:48px;border:none;border-radius:24px;cursor:pointer;background:none}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf:hover{background:#e8ebec}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-aXYTce{display:none}sentinel{}</style><meta name="referrer" content="no-referrer"><link id="res-style" rel="stylesheet" href="/res/dist/res/style.css" type="text/css">
</head>
    <body>
      <div id="book-container">
    <center>
      <img width="320" src="data:image/jpeg;base64,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">
      <h1></h1><h1>Godfather of AI: I Tried to Warn Them, But Weve Already Lost Control! Geoffrey Hinton</h1>
      <p>3 (2025616)  1:30:07</p><p>3 weeks ago (Jun 16, 2025)  1:30:07</p>
      <a href="https://youtube.com/watch?v=giT0ytynSqg">https://youtube.com/watch?v=giT0ytynSqg</a>
    </center>
    <br>
    <center><h2></h2><h2>Summary</h2></center><p><i>
</i></p><p><i>Geoffrey Hinton, the "Godfather of AI," warns about the dangers of superintelligence.  He believes AI could pose an existential threat, potentially surpassing human intelligence and leading to job displacement and wealth inequality.  He emphasizes the need for strong regulations, particularly for military AI, and expresses concern about misuse, including cyberattacks, viruses, and corrupting elections.  Hinton argues that current AI development is accelerating, driven by competition, and that the potential for harm is significant.  He advocates for research into AI safety and suggests that people should consider careers less susceptible to automation, like plumbing.
</i></p>
<h2></h2><h2>Intro</h2>
<p></p><p>They call you the Godfather of AI. So, what would you be saying to people about their career prospects in a world of superintelligence? </p>
<p></p><p>Train to be a plumber. Really? </p>
<p></p><p>Yeah. Okay. I'm going to become a plumber. </p>
<p></p><p>Geoffrey Hinton is the Nobel Prize winning pioneer whose groundbreaking work has shaped AI and the future of humanity. </p>
<p></p><p>Why do they call it the Godfather of AI? </p>
<p>5010</p><p>Because there weren't many people who believed that we could model AI on the brain so that it learned to do complicated things like recognize objects and images or even do reasoning. And I pushed that approach for 50 years, and then Google acquired that technology, and I worked there for 10 years on something that's now used all the time in AI. </p>
<p></p><p>And then you left. </p>
<p></p><p>Yeah. Why? </p>
<p></p><p>So that I could talk freely at a conference. What did you want to talk about freely? </p>
<p></p><p>How dangerous AI could be. I realized that these things will one day get smarter than us, and we've never had to deal with that. And if you want to know what life's like when you're not the apex intelligence, ask a chicken. </p>
<p></p><p>So, there are risks that come from people misusing AI, and then there are risks from AI getting super smart and deciding it doesn't need us. Is that a real risk? </p>
<p></p><p>Yes, it is. But they're not going to stop it because it's too good for too many things. </p>
<p></p><p>What about regulations? </p>
<p></p><p>They have some, but they're not designed to deal with most of the threats. Like, the European AI Regulations have a clause that says none of these apply to military uses of AI. Really? </p>
<p></p><p>Yeah. It's crazy. </p>
<p>OpenAIChatGPT</p><p>One of your students left OpenAI. Yeah. He was probably the most important person behind the development of the early versions of ChatGPT, and I think he left because he had safety concerns. </p>
<p></p><p>We should recognize that this stuff is an existential threat, and we have to face the possibility that unless we do something soon, we're near the end. </p>
<p></p><p>So, let's do the risks. What do we end up doing in such a world? This has always blown my mind a little bit. </p>
<p>53% </p><p>53% of you that listen to the show regularly haven't yet subscribed to the show. So, could I ask you for a favor before we start? If you like the show and you like what we do here, and you want to support us, the free and simple way that you can do just that is by hitting the subscribe button. And my commitment to you is if you do that, then I'll do everything in my power, me and my team, to make sure that this show is better for you every single week. We'll listen to your feedback. We'll find the guests that you want me to speak to, and we'll continue to do what we do. Thank you so much. </p>
<h2></h2><h2>Why Do They Call You the Godfather of AI?</h2>
<p>2050</p><p>Geoffrey Hinton, they call you the Godfather of AI. Uh, yes, they do. Why do they call you that? There weren't that many people who believed that we could make neural networks work, artificial neural networks. So, for a long time in AI, from the 1950s onwards, there were kind of two ideas about how to do AI. One idea was that the core of human intelligence was reasoning. And to do reasoning, you needed to use some form of logic. And so, AI had to be based around logic. And in your head, you must have something like symbolic expressions that you manipulated with rules. And that's how intelligence worked. And things like learning or reasoning by analogy, that all come later once we've figured out how basic reasoning works. </p>
<p>50OpenAI</p><p>There was a different approach, which is to say, let's model AI on the brain because obviously, the brain makes us intelligent. So, simulate a network of brain cells on a computer and try and figure out how you would learn the strengths of connections between brain cells so that it learned to do complicated things like recognize objects in images or recognize speech or even do reasoning. I pushed that approach for like 50 years because so few people believed in it. There weren't many good universities that had groups that did that. So, if you did that, the best young students who believed in that came and worked with you. So, I was very fortunate in getting a whole lot of really good students, some of which have gone on to create and play an instrumental role in creating platforms like OpenAI. Yes. So, I saw a nice example, a whole bunch of them. </p>
<p></p><p>Why did you believe that modeling it off the brain was a more effective approach? It wasn't just me who believed it early on. Feynman believed it, and Turing believed it. And if either of those had lived, I think AI would have had a very different history, but they both died young. You think AI would have been here sooner? I think the neural net approach would have been accepted much sooner if either of them had lived in this season of your life. </p>
<h2></h2><h2>Warning About the Dangers of AI</h2>
<p></p><p>What mission are you on? My main mission now is to warn people how dangerous AI could be. </p>
<p>20</p><p>Did you know that when you became the "Godfather of AI"? No, not really. I was quite slow to understand some of the risks. Some of the risks were always very obvious, like people would use AI to make autonomous lethal weaponsthat is, things that go around deciding by themselves who to kill. Other risks, like the idea that they would one day get smarter than us and maybe would become irrelevant, I was slow to recognize that. Other people recognized it 20 years ago. </p>
<p>ChatGPT </p><p>I only recognized it a few years ago that that was a real risk that was coming might be coming quite soon. How could you not have foreseen that, if, with everything you know here about cracking the ability for these computers to learn similar to how humans learn, and just, you know, introducing any rate of improvement, it's a very good question. How could you not have seen that? But remember, neural networks 20 30 years ago were very primitive in what they could do. They were nowhere near as good as humans, but things like vision, language, and speech recognition. The idea that you have to now worry about it getting smarter than people, that seems silly then. When did that change? It changed for the general population when ChatGPT came out. It changed for me when I realized that the kinds of digital intelligences we're making have something that makes them far superior to the kind of biological intelligence we have. </p>
<p></p><p>If I want to share information with you, so I go off and I learn something and I'd like to tell you what I learned, so I produce some sentences. This is a rather simplistic model, but roughly right. Your brain is trying to figure out how can I change the strength of connections between neurons? So I might have put that word next. And so you'll do a lot of learning when a very surprising word comes, and not much learning when if it's when it's a very obvious word. </p>
<p></p><p>If I say "fish and chips," you don't do much learning when I say "chips." But if I say "fish and cucumber," you do a lot more learning. You wonder why did I say "cucumber"? So that's roughly what's going on in your brain. I'm predicting what's coming next. That's how we think it's working. Nobody really knows for sure how the brain works. And nobody knows how it gets the information about whether you should increase the strength of a connection or decrease the strength of a connection. That's the crucial thing. But what we do know now from AI is that if you could get information about whether to increase or decrease the connection strength so as to do better at whatever task you're trying to do, then we could learn incredible things because that's what we're doing now with artificial neural networks. It's just we don't know for real brains how they get that signal about whether to increase or decrease. </p>
<h2></h2><h2>Concerns We Should Have About AI</h2>
<p></p><p>As we sit here today, what are the big concerns you have around the safety of AI? If we were to list the top couple that are really front of mind and that we should be thinking about... um, can I have more than a couple? Go ahead. I'll write them all down, and we'll go through them. Okay. </p>
<p></p><p>First of all, I want to make a distinction between two completely different kinds of risk. There are risks that come from people misusing AI. Yeah. And that's most of the risks and all of the short term risks. And then there are risks that come from AI getting super smart and deciding it doesn't need us. Is that a real risk? And I talk mainly about that second risk because lots of people say, "Is that a real risk?" And yes, it is. </p>
<p></p><p>Now we don't know how much of a risk it is. We've never been in that situation before. We've never had to deal with things smarter than us. So, really, the thing about that existential threat is that we have no idea how to deal with it. We have no idea what it's going to look like. And anybody who tells you they know just what's going to happen and how to deal with it, they're talking nonsense. So, we don't know how to estimate the probabilities it'll replace us. </p>
<p>1%Yan LarYudkowsky </p><p>Um, some people say it's like less than 1%. My friend Yan Lar, who was a postdoc with me, thinks no, no, no, we're always going to bewe build these things. We're always going to be in control. We'll build them to be obedient. And other people, like Yudkowsky, say, "No, no, no. These things are going to wipe us out for sure. If anybody builds it, it's going to wipe us all out." And he's confident of that. I think both of those positions are extreme. </p>
<p>10%20%</p><p>It's very hard to estimate the probabilities in between. If you had to bet on who was right out of your two friends, I simply don't know. So, if I had to bet, I'd say the probabilities are in between, and I don't know where to estimate it in between. I often say a 10 to 20% chance they'll wipe us out, but that's just a gut feeling, based on the idea that we'rewe're still making them, and we're pretty ingenious. And the hope is that if enough smart people do enough research with enough resources, we'll figure out a way to build them so they'll never want to harm us. </p>
<p></p><p>Sometimes, I think if we talk about that second path, sometimes I think about nuclear bombs and the invention of the atomic bomb, and how it compares. How is this different? Because the atomic bomb came along, and I imagine a lot of people at that time thought our days are numbered. Yes, I was there. We did. Yeah. Butbutbut what'swhat'swe're still here. We're still here. Yes. So, the atomic bomb was really only good for one thing, and it was very obvious how it worked. </p>
<p> </p><p>Even if you hadn't had the pictures of Hiroshima and Nagasaki, it was obvious that it was a very big bomb that was very dangerous. With AI, it's good for many, many things. It's going to be magnificent in healthcare and education, and more or less any industry that needs to use its data is going to be able to use it better with AI. So, we're not going to stop the development. You know, people say, "Well, why don't we just stop it now?" We're not going to stop it because it's too good for too many things. </p>
<p></p><p>Also, we're not going to stop it because it's good for battle robots, and none of the countries that sell weapons are going to want to stop it. Like the European AI regulations, they have some regulations about AI, and it's good. They have some regulations, but they're not designed to deal with most of the threats. </p>
<h2></h2><h2>European AI Regulations</h2>
<p></p><p>And in particular, the European AI Regulations have a clause in them that says none of these regulations apply to military uses of AI. So, governments are willing to regulate companies and people, but they're not willing to regulate themselves. It seems pretty crazy to me that they go back and forth, but if Europe has a regulation, but the rest of the world doesn't, that creates a competitive disadvantage. </p>
<p>OpenAISam AltmanAI </p><p>Yeah, we're seeing this already. I don't think people realize that when OpenAI releases a new model or a new piece of software in America, they can't release it to Europe yet because of regulations here. So, Sam Altman tweeted saying, "Our new AI agent thing is available to everybody, but it can't come to Europe yet because there are regulations." Yes. What does that give us? A productivity disadvantage. </p>
<p></p><p>What we need isI mean, at this point in history, when we're about to produce things more intelligent than ourselveswhat we really need is a kind of world government that works, run by intelligent, thoughtful people. And that's not what we've got. So, free for all. </p>
<p></p><p>Well, what we've got is sort of capitalism, which has done very nicely by us. It produces lots of goods and services for us. But these big companies, they're legally required to try and maximize profits, and that's not what you want from the people developing this stuff. </p>
<p></p><p>So, let's do the risks then. You talked about there are human risks, and then there are... So, I've distinguished these two kinds of risk. Let's talk about all the risks from bad human actors using AI. </p>
<h2></h2><h2>Cyber Attack Risk</h2>
<p>2023202412200%</p><p>There are cyber attacks. So, between 2023 and 2024, they increased by about a factor of 12,200%. And that's probably because these large language models make it much easier to do phishing attacks. </p>
<p></p><p>And a phishing attack, for anyone that doesn't know, is it's when they send you something saying, "Hi, I'm your friend John, and I'm stuck in El Salvador. Could you just wire this money?" That's one kind of attack. But phishing attacks are really trying to get your login credentials. And now, with AI, they can clone my voice, my image. They can do all that. </p>
<p>XMetaMetaInstagramFacebookMeta500500</p><p>I'm struggling at the moment because there are a bunch of AI scams on X and also Meta. And there's one in particular on Meta, so Instagram, Facebook, at the moment, which is a paid advertisement where they've taken my voice from the podcast, they've taken my mannerisms, and they've made a new video of me encouraging people to go and take part in this crypto Ponzi scam or whatever. And we've been, you know, we spent weeks and weeks and weeks emailing Meta, telling them, "Please take this down." They take it down, another one pops up. They take that one down, another one pops up. So, it's like whack a mole. And then it's very annoying. The heartbreaking part is getting messages from people who have fallen for the scam and lost 500 or $500, and they are angry with you because you recommended it. I'm sad for them. It's very annoying. </p>
<p></p><p>Yeah. I have a smaller version of that, which is some people now publish papers with me as one of the authors. Mmm. And it looks like it's in order that they can get lots of citations to themselves. </p>
<p>2030</p><p>Ah, so cyber attacks are a very real threat. There's been an explosion of those. And these, already, obviously AI is very patient. So they can go through 100 million lines of code looking for known ways of attacking them. That's easy to do. But they're going to get more creative, and they maysome people believe, and Isome people who know a lot believe that maybe by 2030 they'll be creating new kinds of cyber attacks which no person ever thought of. So that's very worrisome, because they can think for themselves and discover they can think for themselves. They can draw new conclusions from much more data than a person ever saw. </p>
<h2></h2><h2>How to Protect Yourself From Cyber Attacks</h2>
<p></p><p>Is there anything you're doing to protect yourself from cyber attacks at all? </p>
<p>2008</p><p>Yes. It's one of the few places where I changed what I do radically because I'm scared of cyber attacks. Canadian banks are extremely safe. In 2008, no Canadian banks came anywhere near going bust. So, they're very safe banks because they're well regulated, fairly well regulated. Nevertheless, I think a cyber attack might be able to bring down a bank. Now, if you have all my savings are in shares in banks held by banks, so if the bank gets attacked and it holds your shares, they're still your shares. </p>
<p></p><p>And so, I think you'd be okay unless the attacker sells the shares because the bank can sell the shares. If the attacker sells your shares, I think you're screwed. I don't know. I mean, maybe the bank would have to try and reimburse you, but the bank's bust by now, right? So, so I'm worried about a Canadian bank being taken down by a cyber attack and the attacker selling selling shares that it holds. So I spread my money and my children's money between three banks in the belief that if a cyber attack takes down one Canadian bank, the other Canadian banks will very quickly get very careful. </p>
<p></p><p>And do you have a phone that's not connected to the internet? Do you have any, you know, I'm thinking about storing data and stuff like that. Do you think it's wise to consider having cold storage? </p>
<p></p><p>I have a little disc drive, and I back up my laptop on this hard drive. So I actually have everything on my laptop on a hard drive. At least, you know, if the whole internet went down, I had the sense I still got it on my laptop, and I still got my information. Okay, then the next thing is using AI to create nasty viruses. </p>
<h2></h2><h2>Using AI to Create Viruses</h2>
<p></p><p>And the problem with that is that it just requires one crazy guy with a grudge. One guy who knows a little bit of molecular biology, knows a lot about AI, and just wants to destroy the world. You can now create new viruses relatively cheaply using AI. And you don't have to be a very skilled molecular biologist to do it. And that's very scary. So, you could have a small cult, for example. A small cult might be able to raise a few million dollars. </p>
<p></p><p>For a few million dollars, they might be able to design a whole bunch of viruses. Well, I'm thinking about some of our foreign adversaries doing government funded programs. I mean, there was a lot of talk around COVID and the Wuhan laboratory and what they were doing and gain of function research, but I'm wondering if, you know, in China or Russia or Iran or something, the government could fund a program for a small group of scientists to make a virus that they could, you know, I think they could. Yes. Now, they'd be worried about retaliation. They'd be worried about other governments doing the same to them. Hopefully, that would help keep it under control. They might also be worried about the virus spreading to their country. Okay? </p>
<h2></h2><h2>AI and Corrupt Elections</h2>
<p></p><p>Then there's corrupting elections. So, if you wanted to use AI to corrupt elections, a very effective thing is to be able to do targeted political advertisements where you know a lot about the person. So, anybody who wanted to use AI for corrupting elections would try and get as much data as they could about everybody in the electorate. With that in mind, it's a bit worrying what Musk is doing at present in the States, going in and insisting on getting access to all these things that were very carefully siloed. The claim is it's to make things more efficient, but it's exactly what you would want if you intended to corrupt the next election. </p>
<p></p><p>How do you mean? </p>
<p></p><p>Because you get all this data on the people. You get all this data on people. You know how much they make, where they are, you know everything about them. Once you know that, it's very easy to manipulate them because you can make an AI that can send messages that they'll find very convincing, telling them not to vote, for example. So, I have no reason other than common sense to think this, but I wouldn't be surprised if part of the motivation of getting all this data from American government sources is to corrupt elections. </p>
<p>YouTubeFacebook</p><p>Another part might be that it's very nice training data for a big model, but he would have to be taking that data from the government and feeding it into his... Yes. And what they've done is turned off lots of the security controls, got rid of some of the organizations to protect against that. Um, so that's corrupting elections. Okay, then there's creating these echo chambers by organizations like YouTube and Facebook, showing people things that will make them indignant. </p>
<h2></h2><h2>How AI Creates Echo Chambers</h2>
<p>YouTubeFacebook</p><p>People love to be indignant. Indignant, as in angry, or what does indignant mean? Feeling, I'm sort of angry, but feeling righteous. Okay. So, for example, if you were to show me something that said Trump did this crazy thing, here's a video of Trump doing this completely crazy thing, I would immediately click on it. Okay. So, putting us in echo chambers and dividing us. Yes. And that's the policy that YouTube and Facebook and others use for deciding what to show you next is causing that. </p>
<p> </p><p>If they had a policy of showing you balanced things, they wouldn't get so many clicks, and they wouldn't be able to sell so many advertisements. And so, it's basically the profit motive is saying, "Show them whatever will make them click." And what'll make them click is things that are more and more extreme. And that confirmed my existing bias. That confirmed my existing bias. So, you're getting your biases confirmed all the time, further and further and further, which means you're driving awaywhich is, now, there's in the states, there's two communities that don't hardly talk to each other. </p>
<p>TikTokYouTube10</p><p>I'm not sure people realize that this is actually happening every time they open an app. But if you go on TikTok or YouTube or one of these big social networks, the algorithm, as you said, is designed to show you more of the things that you had interest in last time. So, if you just play that out over 10 years, it's going to drive you further and further and further into whatever ideology or belief you have and further away from nuance and common sense and parity, which is a pretty remarkable thing. </p>
<p>iPhone</p><p>I... I like people don't know it's happening. They just open their phones and experience something and think this is the news or the experience everyone else is having. Right. So, basically, if you have a newspaper, and everybody gets the same newspaper, yeah. You get to see all sorts of things you weren't looking for, and you get a sense that if it's in the newspaper, it's an important thing or a significant thing. But if you have your own news feedmy news feed on my iPhonethree quarters of the stories are about AI, and I find it very hard to know if the whole world's talking about AI all the time or if it's just my news feed. </p>
<p> BBCBBC</p><p>Okay. So, driving me into my echo chambers, which is going to continue to divide us further and further. I'm actually noticing that the algorithms are becoming even more, what's the word? Tailored. And people might go, "Oh, that's great." But what it means is they're becoming even more personalized, which means that my reality is becoming even further from your reality. Yeah. It's crazy. We don't have a shared reality anymore. I share reality with other people who watch the BBC and other BBC news, and other people who read The Guardian and other people who read The New York Times. I have almost no shared reality with people who watch Fox News. It's pretty... it's pretty... worrisome. Yeah. </p>
<p></p><p>Behind all this is the idea that these companies just want to make profit, and they'll do whatever it takes to make more profit because they have to. They're legally obliged to do that. So, we almost can't blame the company, can we? If they're... well, capitalism's done very well for us. It's produced lots of goodies. Yeah. But you need to have it very well regulated. </p>
<p></p><p>So, what you really want is to have rules so that when some company is trying to make as much profit as possible, in order to make that profit, they have to do things that are good for people in general, not things that are bad for people in general. </p>
<h2></h2><h2>Regulating New Technologies</h2>
<p></p><p>So, once you get to a situation where, in order to make more profit, the company starts doing things that are very bad for society, like showing you things that are more and more extreme, that's what regulations are for. So, you need regulations with capitalism. </p>
<p></p><p>Now, companies will always say regulations get in the way, make us less efficient, and that's true. The whole point of regulations is to stop them from doing things to make profit that hurt society. And we need strong regulation. </p>
<p></p><p>Who's going to decide whether it hurts society or not? Because you know that's the job of politicians, unfortunately. If the politicians are owned by the companies, that's not so good. And also, the politicians might not understand the technology. You've probably seen the Senate hearings where they wheel out, you know, Mark Zuckerberg and these big tech CEOs, and it is quite embarrassing because they're asking the wrong questions. </p>
<p>A1A1 A1</p><p>Well, I've seen the video of the US Education Secretary talking about how they're going to get AI in the classrooms, except she thought it was called A1. She's actually there saying, "We're going to have all the kids interacting with A1." There is a school system that's going to start making sure that first graders, or even pre K students, have A1 teaching, you know, every year, starting that far down in the grades. And that's just a... that's a wonderful thing. And these are the people that... these are the people in charge. </p>
<p></p><p>Ultimately, the tech companies are in charge because they will outsmart the politicians in the states. Now, at least a few weeks ago, when I was there, they were running an advertisement about how it was very important not to regulate AI because it would hurt us in the competition with China. Yeah. And that's a plausible argument there. </p>
<h2></h2><h2>Are Regulations Holding Us Back From Competing With China?</h2>
<p></p><p>Yes, it will. But you have to decide: do you want to compete with China by doing things that will do a lot of harm to your society? And you probably don't. I guess they would say that it's not just China, it's Denmark, Australia, Canada, and the UK. They're not so worried about, and Germany. But if they kneecap themselves with regulation, if they slow themselves down, then the founders, the entrepreneurs, and the investors are going to go. I think calling it kneecapping is taking a particular point of view; it's taking the point of view that regulations are sort of very harmful. </p>
<p>YouTube</p><p>What you need to do is just constrain the big companies so that in order to make profit, they have to do things that are socially useful. Like Google Search is a great example. That didn't need regulation because it just made information available to people. It was great. But then, if you take YouTube, which starts showing you adverts and showing you more and more extreme things, that needs regulation. But we don't have the people to regulate it as we've identified. I think people know pretty well that particular problem of showing you more and more extreme things. That's a well known problem that the politicians understand. They just need to get on and regulate it. </p>
<p></p><p>So, that was the next point, which was that the algorithms are going to drive us further into our echo chambers, right? What's next? Lethal autonomous weapons. </p>
<h2></h2><h2>The Threat of Lethal Autonomous Weapons</h2>
<p></p><p>Lethal autonomous weapons. That means things that can kill you and make their own decision about whether to kill you, which is the great dream, I guess, of the military industrial complex, being able to create such weapons. </p>
<p></p><p>So, the worst thing about them is big powerful countries always have the ability to invade smaller, poorer countries. They're just more powerful. But if you do that using actual soldiers, you get bodies coming back in bags, and the relatives of the soldiers who were killed don't like it. So, you get something like Vietnam. Mhm. In the end, there's a lot of protest at home. If instead of bodies coming back in bags, it was dead robots, there'd be much less protest, and the military industrial complex would like it much more because robots are expensive. </p>
<p></p><p>And suppose you had something that could get killed and was expensive to replace. That would be just great. Big countries can invade small countries much more easily because they don't have their soldiers being killed. And the risk here is that these robots will malfunction, or they'll just be more... no, no. That's even if the robots do exactly what the people who built the robots want them to do. The risk is that it's going to make big countries invade small countries more often. </p>
<p></p><p>More often because they can. Yeah. And it's not a nice thing to do. So, it brings down the friction of war. It brings down the cost of doing an invasion. And these machines will be smarter at warfare as well. So, they'll be... well, even when the machines aren't smarter. So, the lethal autonomous weapons, they can make them now. And theyI think all the big defense models are busy making them. Even if they're not smarter than people, they're still very nasty, scary things. Because I'm thinking that, you know, they could show just a picture, "go get this guy." Yeah. </p>
<p>200</p><p>And go take out anyone he's been texting, and this little wasp. So, two days ago, I was visiting a friend of mine in Sussex who had a drone that cost less than 200, and the drone went up. It took a good look at me, and then it could follow me through the woods. And it followedit was very spooky having this drone. It was about two meters behind me. It was looking at me, and if I moved over there, it moved over there. </p>
<p>200</p><p>It could just track me. Mhm. For 200, but it was already quite spooky. Yeah. And I imagine there's, as you say, a race going on as we speak to see who can build the most complex autonomous weapons. There is a risk, I often hear, that some of these things will combine, and the cyber attack will release weapons. </p>
<h2></h2><h2>Can These AI Threats Combine?</h2>
<p>AI</p><p>Sure. Um, you can, you can get combinatorily many risks by combining these other risks. Mhm. So, I mean, for example, you could get a superintelligent AI that decides to get rid of people, and the obvious way to do that is just to make one of these nasty viruses. If you made a virus that was very contagious, very lethal, and very slow, everybody would have it before they realized what was happening. I mean, I think if a superintelligence wanted to get rid of us, it will probably go for something biological like that that wouldn't affect it. </p>
<p></p><p>Do you not think it could just very quickly turn us against each other? For example, it could send a warning on the nuclear systems in America that there's a nuclear bomb coming from Russia, or vice versa, and one retaliates. Yeah. I mean, my basic view is there are so many ways in which the superintelligence could get rid of us. It's not worth speculating about. What, what is what you have to do is prevent it ever wanting to. That's what we should be doing research on. </p>
<p></p><p>There's no way we're going to prevent itit's smarter than us, right? There's no way we're going to prevent it from getting rid of us if it wants to. We're not used to thinking about things smarter than us. If you want to know what life's like when you're not the apex intelligence, ask a chicken. Yeah. I was thinking about my dog, Pablo, my French bulldog, this morning as I left home. He has no idea where I'm going. He has no idea what I do, right? </p>
<p></p><p>Can't even talk to him. Yeah. And the intelligence gap will be like that. So, you're telling me that if I'm Pablo, my French bulldog, I need to figure out a way to make my owner not wipe me out. </p>
<h2></h2><h2>Restricting AI From Taking Over</h2>
<p></p><p>Yeah. So, we have one example of that, which is mothers and babies. Evolution put a lot of work into mothers being smarter than babies, but babies are in control. And they're in control because the mother just can't bear lots of hormones and things, but the mother just can't bear the sound of the baby crying. Not all mothers. Not all mothers. And then the baby's not in control, and then bad things happen. We somehow need to figure out how to make them not want to take over. </p>
<p></p><p>The analogy I often use is: forget about intelligence, think about physical strength. Suppose you have a nice little tiger cub. It's sort of a bit bigger than a cat. It's really cute. It's very cuddly, very interesting to watch. Except that you better be sure that when it grows up, it never wants to kill you. Because if it ever wanted to kill you, you'd be dead in a few seconds. And you're saying the AI we have now is the target cub. </p>
<p></p><p>Yep. And it's growing up. Yep. So, we need to train it as it's when it's a baby. Well, now a tiger has lots of instincts built in. So, you know, when it grows up, it's not a safe thing to have around. But lions, people that have lions as pets, yes. Sometimes the lion is affectionate to its creator but not to others. Yes. And we don't know whether these AIswe simply don't know whether we can make them not want to take over and not want to hurt us. </p>
<p></p><p>Do you think we can? Do you think it's possible to train superintelligence? I don't think it's clear that we can. So, I think it might be hopeless. But I also think we might be able to. And it'd be sort of crazy if people went extinct because we couldn't be bothered to try. If that's even a possibility, how do you feel about your life's work? </p>
<h2></h2><h2>Reflecting on Your Lifes Work Amid AI Risks</h2>
<p>40</p><p>Because you were... yeah. Um, it sort of takes the edge off it, doesn't it? I mean, the idea is going to be wonderful in healthcare and wonderful in education and wonderful. I mean, it's going to make call centers much more efficient, though one worries a bit about what the people who are doing that job now do. It makes me sad. I don't feel particularly guilty about developing AI like 40 years ago because at that time we had no idea that this stuff was going to happen this fast. </p>
<p></p><p>We thought we had plenty of time to worry about things like that. Theywhen youwhen you can't get the AI to do much, you want to get it to do a little bit more. You don't worry about this stupid little thing is going to take over from people. You just want it to be able to do a little bit more of the things people can do. It's not like I knowingly did something thinking this might wipe us all out, but I'm going to do it anyway. </p>
<p>3050</p><p>Mhm. But it is a bit sad that it's not just going to be something for good. So, I feel I have a duty now to talk about the risks. And if you could play it forward and you could go forward 30, 50 years and you found out that it led to the extinction of humanity, and if that does end up being the outcome, well, if you played it forward and it led to the extinction of humanity, I would use that to tell people to tell their governments that we really have to work on how we're going to keep this stuff under control. I think we need people to tell governments that governments have to force the companies to use their resources to work on safety, and they're not doing much of that because you don't make profits that way. </p>
<h2> OpenAI</h2><h2>Student Leaving OpenAI Over Safety Concerns</h2>
<p>IlyaIlya  OpenAI OpenAI  AI </p><p>One of your students, we talked about earlier, Ilya, yep. Ilya left OpenAI. Yep. And there was lots of conversation around the fact that he left because he had safety concerns. Yes. And he's gone on to set up an AI safety company. Yes. Why do you think he left? </p>
<p>OpenAIChatGPTGPT 2GPT 2</p><p>I think he left because he had safety concerns. Really? HeI still have lunch with him from time to time. His parents live in Toronto. When he comes to Toronto, we have lunch together. He doesn't talk to me about what went on at OpenAI, so I have no inside information about that. But I know him very well, and he is genuinely concerned with safety. So, I think that's why he left because he was one of the top people. I mean, he washe was probably the most important person behind the development of early versions of ChatGPT, like GPT 2. He was very important in the development of that. You know him personally, so you know his character. Yes, he has a good moral compass. He's not like someone like Musk, who has no moral compass. Does Sam Altman have a good moral compass? We'll see. I don't know Sam, so I don't want to comment on that. </p>
<p>  </p><p>But from what you've seen, are you concerned about the actions that they've taken? Because if you know Ilya, and Ilya's a good guy, and he's left, that would give you some insight. Yes. It would give you some reason to believe that there's a problem there. And if you look at Sam's statements some years ago, he sort of happily said in one interview, "This stuff will probably kill us all." That's not exactly what he said, but that's what it amounted to. Now he's saying, "You don't need to worry too much about it." And I suspect that's not driven by seeking after the truth. That's driven by seeking after money. Is it money or is it power? </p>
<p></p><p>Yeah. I shouldn't have said money. It's some combination of those. Yes. Okay. I guess money is a proxy for power. But I amI've got a friend who's a billionaire, and he is in those circles. And when I went to his house and had lunch with him one day. He knows lots of people in AI, building the biggest AI companies in the world. And he gave me a cautionary warning across his kitchen table in London. Where he gave me an insight into the private conversations these people have, not the media interviews they do where they talk about safety and all these things, but actually what some of these individuals think is going to happen, and what they think is going to happen. </p>
<p></p><p>It's not what they say publicly. You know, one person, whom I shouldn't name, who is leading one of the biggest AI companies in the world. He told me that he knows this person very well, and he privately thinks that we're heading towards this kind of dystopian world where we have just huge amounts of free time. We don't work anymore. And this person doesn't really care about the harm that it's going to have on the world. And this person, whom I'm referring to, is building one of the biggest AI companies in the world. </p>
<p> </p><p>And I then watch this person's interviews online, trying to figure out which of three people it is. Yeah. Well, it's one of those three people. Okay. And I watch this person's interviews online, and I reflect on a conversation that my billionaire friend had with me, who knows him. And I go, "Fucking hell, this guy's lying publicly." Like, he's not telling the truth to the world. And that's haunted me a little bit. It's part of the reason I have so many conversations around AI in this podcast, because I'm like, I don't know if they'reI think they'resome of them are a little bit sadistic about power. </p>
<p></p><p>I think they like the idea that they will change the world, that they will be the one that fundamentally shifts the world. I think Musk is clearly like that, right? He's such a complex character that I don't really know how to place Musk. Um, he's done some really good things, like pushing electric cars. That was a really good thing to do. Yeah. Some of the things he said about self driving were a bit exaggerated, but hethat was a really useful thing he did. </p>
<p></p><p>Giving the Ukrainians communication during the war with Russia. Astounding. Um, that was a really good thing he did. There's a bunch of things like that. Um, but he's also done some very bad things. So, coming back to this point of the possibility of destruction and the motives of these big companies, are you at all hopeful that anything can be done to slow down the pace and acceleration of AI? </p>
<h2></h2><h2>Are You Hopeful About the Future of AI?</h2>
<p></p><p>Okay, there are two issues. One is, can you slow it down? Yeah. And the other is, can you make it so it will be safe in the end? It won't wipe us all out. I don't believe we're going to slow it down. Yeah. And the reason I don't believe we're going to slow it down is because there's competition between countries and competition between companies within a country, and all of that is making it go faster and faster. And if the US slowed it down, China wouldn't slow it down. </p>
<p>[]  AlexNet  GPT 2  GPT 2  ChatGPT</p><p>Does [Geoffrey Hinton] think it's possible to make AI safe? I think he does. He won't tell me what his secret source is. I'm not sure how many people know what his secret source is. I think a lot of the investors don't know what his secret source is, but they've given him billions of dollars anyway because they have so much faith in him, which isn't foolish. I mean, he was very important in AlexNet, which got object recognition working well. He was the main force behind things like GPT 2, which then led to ChatGPT. So I think having a lot of faith in him is a very reasonable decision. </p>
<p>GPT 2</p><p>There's something quite haunting about the guy that made and was the main force behind GPT 2, which led to this whole revolution, leaving the company because of safety reasons. He knows something that I don't know about what might happen next. </p>
<p></p><p>Well, the company hadI don't know the precise details, but I'm fairly sure the company had indicated that it would use a significant fraction of its resources, of compute time, for doing safety research, and then it reduced that fraction. I think that's one of the things that happened. Yeah, that was reported publicly. Yes. </p>
<h2></h2><h2>The Threat of AI-Induced Joblessness</h2>
<p></p><p>Yeah. We've gotten to the autonomous weapons part of the risk framework. Right. So the next one is joblessness. </p>
<p></p><p>In the past, new technologies have come in which didn't lead to joblessness. New jobs were created. So the classic example people use is automatic teller machines. When automatic teller machines came in, a lot of bank tellers didn't lose their jobs. They just got to do more interesting things. But here, I think this is more like when they got machines in the industrial revolution. And you can't have a job digging ditches now because a machine can dig ditches much better than you can. </p>
<p>10</p><p>And I think for mundane intellectual labor, AI is just going to replace everybody. Now, it will may well be in the form of you have fewer people using AI assistance. So it's a combination of a person and an AI assistant are now doing the work that 10 people could do previously. People say that it will create new jobs though, so we'll be fine. Yes. And that's been the case for other technologies, but this is a very different kind of technology. </p>
<p></p><p>If it can do all mundane human intellectual labor, then what new jobs is it going to create? You'd have to be very skilled to have a job that it couldn't just do. So I don't think they're right. I think you can try and generalize from other technologies that have come in, like computers or automatic telemachines, but I think this is different. People use this phrase. They say AI won't take your job. A human using AI will take your job. Yes, I think that's true. But for many jobs, that'll mean you need far fewer people. </p>
<p>25</p><p>My niece answers letters of complaint to a health service. It used to take her 25 minutes. She'd read the complaint, and she'd think how to reply, and she'd write a letter. And now she just scans it into a chatbot, and it writes the letter. She just checks the letter. Occasionally, she tells it to revise it in some ways. The whole process takes her five minutes. That means she can answer five times as many letters, and that means they need five times fewer of her, so she can do the job that five of her used to do. Now, that will mean they need less people. </p>
<p></p><p>In other jobs, like in healthcare, they're much more elastic. So, if you could make doctors five times as efficient, we could all have five times as much healthcare for the same price, and that would be great. There's almost no limit to how much healthcare people can absorb. They always want more healthcare if there's no cost to it. </p>
<p></p><p>There are jobs where you can make a person with an AI assistant much more efficient, and you won't lead to less people because you'll just have much more of that being done. But most jobs, I think, are not like that. </p>
<h2></h2><h2>If Muscles and Intelligence Are Replaced, Whats Left?</h2>
<p></p><p>Am I right in thinking the sort of industrial revolution played a role in replacing muscles? Yes. Exactly. And this revolution in AI replaces intelligencethe brain. Yeah. So, so mundane intellectual labor is like having strong muscles, and it's not worth much anymore. So, muscles have been replaced. Now, our intelligence is being replaced. Yeah. </p>
<p></p><p>So, what remains? Maybe for a while some kinds of creativity, but the whole idea of superintelligence is that nothing remains. Um, these things will get to be better than us at everything. So, what do we end up doing in such a world? </p>
<p></p><p>Well, if they work for us, we end up getting lots of goods and services for not much effort. Okay. But that sounds tempting and nice, but I don't know. There's a cautionary tale in creating more and more ease for humansin it going badly. Yes. And we need to figure out if we can make it go well. </p>
<p>CEO  CEO  CEO   20 </p><p>So, the nice scenario is imagine a company with a CEO who is very dumb, probably the son of the former CEO. And he has an executive assistant who's very smart, and he says, "I think we should do this." And the executive assistant makes it all work. The CEO feels great. He doesn't understand that he's not really in control. And in some sense, he is in control. He suggests what the company should do. She just makes it all work. Everything's great. That's the good scenario and the bad scenario. The bad scenario: she thinks, "Why do we need him, yeah?" I mean, in a world where we have superintelligence, which you don't believe is that far away. Yeah, I think it might not be that far away. It's very hard to predict, but I think we might get it in like 20 years or even less. </p>
<h2></h2><h2>Ads</h2>
<p>JohnStanStoreStanStoreStanStoreShopify</p><p>I made the biggest investment I've ever made in a company because of my girlfriend. I came home one night, and my lovely girlfriend was up at 1:00 a.m. in the morning, pulling her hair out as she tried to piece together her own online store for her business. And in that moment, I remembered an email I'd had from a guy called John, the founder of StanStore, our new sponsor, and a company I've invested incredibly heavily in. StanStore helps creators to sell digital products, courses, coaching, and memberships all through a simple, customizable link in bio system. And it handles everything: payments, bookings, emails, community engagement, and even links with Shopify. </p>
<p> Stan  10  stephenbartlet.stanstore.com  StanStore  30 </p><p>I believe in it so much that I'm going to launch a Stan Challenge. And as part of this challenge, I'm going to give away $100,000 to one of you. If you want to take part in this challenge, if you want to monetize the knowledge that you have, visit stephenbartlet.stanstore.com to sign up. And you'll also get an extended 30 day free trial of StanStore if you use that link. Your next move could quite frankly change everything. </p>
<p> KetoneIQ </p><p>Because I talked about ketosis on this podcast, and ketones, a brand called KetoneIQ sent me their little product. It was on my desk when I got to the office. I picked it up. It sat on my desk for a couple of weeks. Then, one day, I tried it, and honestly, I have not looked back ever since. I now have this everywhere I go when I travel all around the world. It's in my hotel room. My team will put it there. </p>
<p> KetoneIQ CEO KetoneIQ </p><p>Before I did the podcast recording today that I've just finished, I had a shot of KetoneIQ. And as is always the case when I fall in love with a product, I called the CEO and asked if I could invest a couple of million quid into their company. So, I'm now an investor in the company as well as them being a brand sponsor. I find it so easy to drop into deep, focused work when I've had one of these. </p>
<p> ketone.com/stephven ketone.com/stephven</p><p>I would love you to try one and see the impact it has on you, your focus, your productivity, and your endurance. So, if you want to try it today, visit ketone.com/stephven for 30% off your subscription. Plus, you'll receive a free gift with your second shipment. That's ketone.com/stephven. I'm excited for you. I am so, what's the difference between what we have now and superintelligence? </p>
<h2></h2><h2>Difference Between Current AI and Superintelligence</h2>
<p> ChatGPT  Gemini  GPT 4 </p><p>Because it seems really intelligent to me when I use like ChatGPT or Gemini. Okay, so it's already AI is already better than us at a lot of things, in particular areas like chess, for example. Yeah. AI is so much better than us that people will never beat those programs again. Maybe the occasional win, but basically, they'll never be comparable again. Obviously, the same in Go, in terms of the amount of knowledge they have. Um, something like GPT 4 knows thousands of times more than you do. </p>
<p>CEOGPT 4CEO</p><p>There are a few areas in which your knowledge is better than its, and in almost all areas, it just knows more than you do. What areas am I better than it? Probably in interviewing CEOs. You're probably better at that. You've got a lot of experience at it. You're a good interviewer. You know a lot about it. If you tried, if you got GPT 4 to interview a CEO, it would probably do a worse job. Okay. I'm trying to think if I agree with that statement. </p>
<p>GPT 4</p><p>Uh, GPT 4, I think, for sure. Yeah. Um, but I, but I guess you could, but it may not be long before, yeah. I guess you could train one on how I ask questions and what I do. Sure. And if you took a general purpose sort of foundation model and then you trained it up on not just you, but every interviewer you could find doing interviews like this, but especially you, you'll probably get it to be quite good at doing your job, but probably not as good as you for a while. Okay. </p>
<p>501020</p><p>So, there are a few areas left, and then superintelligence becomes when it's better than us at all things. When it's much smarter than you and almost all things are better than you. Yeah. And you say that this might be a decade away or so. Yeah. It might be. It might be even closer. Some people think it's even closer and might well be much further. It might be 50 years away. That's still a possibility. It might be that somehow training on human data limits you to not being much smarter than humans. My guess is between 10 and 20 years we'll have superintelligence. </p>
<p>CEO5</p><p>On this point of joblessness, it's something that I've been thinking a lot about, in particular, because I started messing around with AI agents, and we released an episode on the podcast actually this morning where we had a debate about AI agents with some CEO of a big AI agent company and a few other people. And it was the first moment where I hadit was another moment where I had a eureka moment about what the future might look like when I was able, in the interview, to tell this agent to order all of us drinks, and then 5 minutes later in the interview, you see the guy show up with the drinks. </p>
<p> Uber Eats</p><p>And I didn't touch anything. I just told it to order us drinks to the studio. And you didn't know about who you normally get your drinks from. It figured that out from the web. Yeah, figured it out because it went on Uber Eats. It has my data, I guess. And itwe put it on the screen in real time so everyone at home could see the agent going through the internet, picking the drinks, adding a tip for the driver, putting my address in, putting my credit card details in, and then the next thing you see is the drinks show up. </p>
<p> Replet </p><p>So that was one moment. And then the other moment was when I used a tool called Replet, and I built software by just telling the agent what I wanted. Yes. It's amazing, right? It's amazing and terrifying at the same time. Yes. Becauseand if it can build software like that, right? Yeah. Remember that the AI, when it's training, is using code, and if it can modify its own code, then it gets quite scary, right? Because it can modify. It can change itself in a way we can't change ourselves. We can't change our innate endowment, right? There's nothing about itself that it couldn't change. </p>
<p></p><p>On this point of joblessness, you have kids. I do. And they have kids. No, they don't have kids. No grandkids yet. What would you be saying to people about their career prospects in a world of superintelligence? What should we be thinking about? </p>
<p> OpenAI  Sam Altman  CEO     12 </p><p>Um, in the meantime, I'd say it's going to be a long time before it's as good at physical manipulation as us. Okay. And so, a good bet would be to be a plumber. Until the humanoid robots show up. In such a world where there is mass joblessness, which is not something that you just predict, but this is something that Sam Altman, OpenAI, I've heard him predict, and many of the CEOsElon MuskI watched an interview, which I'll play on screen, of him being asked this question, and it's very rare that you see Elon Musk silent for 12 seconds or whatever it was, and then he basically says something about he actually is living in suspended disbeliefi.e., he's basically just not thinking about it. </p>
<p></p><p>When you think about advising your children on a career with so much that is changing, what do you tell them is going to be of value? </p>
<p></p><p>Well, that is a tough question to answer. I would just say, you know, to sort of follow their heart in terms of what they find interesting to do or fulfilling to do. I mean, if I think about it too hard, frankly, it can be dispiriting and demotivating. Um, because I mean, II've put a lot of blood, sweat, and tears into building the companies, and then itand then I'm like, wait, should I be doing this? Because if I'm sacrificing time with friends and family that I would prefer totobut but then ultimately the AI can do all these things. Does that make sense? I don't know. Um, to some extent, I have to have a deliberate suspension of disbelief in order to remain motivated. Um, so I guess I would say just, you know, work on things that you find interesting, fulfilling, and that contribute some good to the rest of society. Yeah. </p>
<h2></h2><h2>Coming to Terms With AIs Capabilities</h2>
<p></p><p>A lot of these threatsit's very hard to intellectually see the threat, but it's very hard to come to terms with it emotionally. Yeah. I haven't come to terms with it emotionally yet. </p>
<p></p><p>What do you mean by that? </p>
<p>77</p><p>I haven't come to terms with what the development of superintelligence could do to my children's future. I'm okay. I'm 77. I'm going to be out of here soon. But for my children and my younger friends, my nephews and nieces and their children, I just don't like to think about what could happen. </p>
<p></p><p>Why? </p>
<p></p><p>Because it could be awful. </p>
<p></p><p>In what way? </p>
<p></p><p>Well, if I ever decided to take over... I mean, it would need people for a while to run the power stations until it designed better analog machines to run the power stations. There are so many ways it could get rid of people, all of which would, of course, be very nasty. </p>
<p></p><p>Is that part of the reason you do what you do now? </p>
<p></p><p>Yeah. I mean, I think we should be making a huge effort right now to try and figure out if we can develop it safely. </p>
<p></p><p>Are you concerned about the midterm impact potentially on your nephews and your kids in terms of their jobs as well? </p>
<p></p><p>Yeah, I'm concerned about all that. </p>
<p></p><p>Are there any particular industries that you think are most at risk? People talk about the creative industries a lot and sort of knowledge work. They talk about lawyers and accountants and stuff like that. Yeah. So, that's why I mentioned plumbers. I think plumbers are less at risk. </p>
<p></p><p>Okay, I'm going to become a plumber. Someone like a legal assistant, a paralegal... um, they're not going to be needed for very long. </p>
<h2></h2><h2>How AI May Widen the Wealth Inequality Gap</h2>
<p></p><p>And is there a wealth inequality issue here that will arise from this? </p>
<p></p><p>Yeah, I think in a society which shared out things fairly, if you get a big increase in productivity, everybody should be better off. But if you can replace lots of people by AI, then the people who get replaced will be worse off, and the company that supplies the AI will be much better off, and the company that uses the AI. So it's going to increase the gap between rich and poor. And we know that if you look at that gap between rich and poor, that basically tells you how nice the society is. </p>
<p>IMF</p><p>If you have a big gap, you get very nasty societies in which people live in world communities and put other people in mass jails. It's not good to increase the gap between rich and poor. The International Monetary Fund has expressed profound concerns that generative AI could cause massive labor disruptions and rising inequality, and a person has called for policies to prevent this. I read that in Business Insider. So, have they given any specifics on what those policies should look like? </p>
<p></p><p>No. Yeah, that's the problem. I mean, if AI can make everything much more efficient and replace people in most jobs, or assist people in doing many people's work, it's not obvious what to do about it. Universal basic income, giving everyone money, is a good start, and it stops people from starving. But for many people, their dignity is tied up with their job. I mean, who you think you are is tied up with doing this job, right? Yeah. And if we said, "We'll give you the same money just to sit around," that would impact your dignity. </p>
<h2></h2><h2>Why Is AI Superior to Humans?</h2>
<p></p><p>You said something earlier about AI surpassing or being superior to human intelligence. A lot of people, I think, like to believe that AI is on a computer and something you can just turn off if you don't like it. Well, let me tell you why I think it's superior. </p>
<p></p><p>Okay. Um, it's digital. And because it's digital, you can simulate a neural network on one piece of hardware. Yeah. And you can simulate exactly the same neural network on a different piece of hardware. So you can have clones of the same intelligence. Now, you could get one to look at one part of the internet and another to look at a different part. And while they're looking at these different parts, they can be syncing with each other. So they keep their weights, the connection strengths, the same. Weights are connection strengths. </p>
<p> 2.42.5</p><p>Mhm. So, one might look at something on the internet and say, "Oh, I'd like to increase this connection strength a bit." And it can convey that information to the other. So it can increase the strength of that connection based on the other's experience. And when you say the strength of the connection, you're talking about learning. That's learning. Yes. Learning consists of, instead of one giving 2.4 votes for whether the other should turn on, one giving 2.5 votes. And that will be a little bit of learning. </p>
<p> 100  10 </p><p>So these two different copies of the same neural net are getting different experiences. They're looking at different data, but they're sharing what they've learned by averaging their weights together. Mhm. And they can do that averaging on a trillion weights. When you and I transfer information, we're limited to the amount of information in a sentence. And the amount of information in a sentence is maybe 100 bits. It's very little information. We're lucky if we're transferring 10 bits a second. </p>
<p></p><p>These things are transferring trillions of bits a second. So, they're billions of times better than us at sharing information. And that's because they're digital. And you can have two bits of hardware using the connection strengths in exactly the same way. We're analog, and you can't do that. Your brain is different from my brain. And if I could see the connection strengths between all your neurons, it wouldn't do me any good because my neurons work slightly differently, and they're connected up slightly differently. </p>
<p></p><p>Mhm. So, when you die, all your knowledge dies with you. When these things die, suppose you take these two digital intelligences that are clones of each other, and you destroy the hardware they run on. As long as you've stored the connection strength somewhere, you can just build new hardware that executes the same instructions. So, it'll know how to use those connection strengths, and you've recreated that intelligence. So, they're immortal. We've actually solved the problem of immortality, but it's only for digital things. So, it knows it will essentially know everything that humans know, but more, because it will learn new things. </p>
<h2></h2><h2>AIs Potential to Know More Than Humans</h2>
<p></p><p>It will learn new things. It would also see all sorts of analogies that people probably never saw. </p>
<p> GPT 4    GPT 4 </p><p>For example, at the point when GPT 4 couldn't look on the web, I asked it, "Why is a compost heap like an atom bomb?" Off you go. I have no idea. Exactly. Excellent. Mostthat's exactly what most people would say. It said, "Well, the time scales are very different, and the energy scales are very different." But then I went on to talk about how a compost heap, as it gets hotter, generates heat faster, and an atom bomb, as it produces more neutrons, generates neutrons faster. And so, they're both chain reactions, but at very different time and energy scales. And I believe GPT 4 had seen that during its training. It had understood the analogy between a compost heap and an atom bomb. </p>
<p></p><p>And the reason I believe that is: if you've only got a trillion connections, remember you have 100 trillion. And you need to have thousands of times more knowledge than a person, you need to compress information into those connections. And to compress information, you need to see analogies between different things. In other words, it needs to see all the things that are chain reactions and understand the basic idea of a chain reaction and code the ways in which they're different. </p>
<p></p><p>And that's just a more efficient way of coding things than coding each of them separately. So, it's seen many, many analogiesprobably many analogies that people have never seen. That's why I also think that people who say these things will never be creative. They're going to be much more creative than us because they're going to see all sorts of analogies we never saw. And a lot of creativity is about seeing strange analogies. People are somewhat romantic about the specialness of what it is to be human. </p>
<h2></h2><h2>Can AI Replicate Human Uniqueness?</h2>
<p></p><p>And you hear lots of people say it's very, very different. It's a computer. We are, you know, we're conscious. We are creatives. We have these sorts of innate, unique abilities that computers will never have. What do you say to those people? </p>
<p></p><p>I'd argue a bit with the "innate" part. </p>
<p></p><p>So, the first thing I say is that we have a long history of believing people were special. And we should have learned by now. We thought we were at the center of the universe. We thought we were made in the image of God. White people thought they were very special. We just tend to want to think we're special. My belief is that more or less everyone has a completely wrong model of what the mind is. </p>
<p></p><p>Let's suppose I drink a lot, or I drop some acidnot recommendedand I say to you I have the subjective experience of little pink elephants floating in front of me. </p>
<p></p><p>Mmm. </p>
<p></p><p>Most people interpret that as there's some kind of inner theater called the mind, and only I can see what's in my mind, and in this inner theater, there are little pink elephants floating around. </p>
<p></p><p>So, in other words, what's happened is my perceptual systems have gone wrong, and I'm trying to indicate to you how it's gone wrong and what it's trying to tell me. And the way I do that is by telling you what would have to be out there in the real world for it to be telling the truth. </p>
<p>fips</p><p>And so, these little pink elephantsthey're not in some inner theater. These little pink elephants are hypothetical things in the real world. And that's my way of telling you how my perceptual systems are telling me "fips." </p>
<p></p><p>So, now let's do that with a chatbot. </p>
<p></p><p>Yeah. </p>
<p></p><p>Because I believe that current multimodal chatbots have subjective experiences, and very few people believe that. But I'll try and make you believe it. </p>
<p> </p><p>So, suppose I have a multimodal chatbot. It's got a robot arm so it can point, and it's got a camera so it can see things. I put an object in front of it and I say, "Point at the object." It goes like this. No problem. </p>
<p></p><p>Then I put a prism in front of its lens. And so, then I put an object in front of it and I say, "Point at the object." And it goes there. And I say, "No, that's not where the object is. The object's actually straight in front of you, but I put a prism in front of your lens." </p>
<p></p><p>And the chatbot says, "Oh, I see. The prism bent the light rays." </p>
<p></p><p>So, the object's actually there, but I had the subjective experience that it was there. </p>
<p></p><p>Now, if the chatbot says that, is it using the word "subjective experience" exactly the way people use them? It's an alternative view of what's going on. They're hypothetical states of the world. Which, if they were true, would mean my perceptual system wasn't lying. And that's the best way I can tell you what my perceptual system is doing when it's lying to me. Now we need to go further to deal with sentience, consciousness, feelings, and emotions, but I think in the end they're all going to be dealt with in a similar way. There's no reason machines can't have them all, because people say machines can't have feelings. </p>
<h2></h2><h2>Will Machines Have Feelings?</h2>
<p></p><p>And people are curiously confident about that. I have no idea why. </p>
<p></p><p>Suppose I make a battle robot, a little battle robot, and it sees a big battle robot that's much more powerful than it. It would be really useful if it got scared. Now, when I get scared, various physiological things happen that we don't need to go into, and those won't happen with the robot. But all the cognitive things, like "I better get the hell out of here" and "I better sort of change my way of thinking so I focus and focus and focus and don't get distracted," all of that will happen with robots, too. </p>
<p></p><p>People will build in things so that when the circumstances are such that they should get the hell out of there, they get scared and run away. They'll have emotions then. They won't have the physiological aspects, but they will have all the cognitive aspects. And I think it would be odd to say they're just simulating emotions. No, they're really having those emotions. The little robot got scared and ran away. It's not running away because of adrenaline; it's running away because of a sequence of sort of neurological, in its neural net processes, that happened which have the equivalent effect to adrenaline. So, do youdo youand it's not just adrenaline, right? There's a lot of cognitive stuff that goes on when you get scared. </p>
<p></p><p>Yeah. So, do you think that there is conscious AI? And when I say conscious, I mean that it represents the same properties of consciousness that a human has. There are two issues here: a sort of empirical one and a philosophical one. I don't think there's anything in principle that stops machines from being conscious. I'll give you a little demonstration of that before we carry on. </p>
<p></p><p>Suppose I take your brain and I take one brain cell in your brain and I replace it bythis a bit black mirror likeI replace it by a little piece of nanotechnology that's just the same size that behaves in exactly the same way when it gets pings from other neurons. It sends out pings just as the brain cell would have. So the other neurons don't know anything's changed. Okay. I've just replaced one of your brain cells with this little piece of nanotechnology. Would you still be conscious? </p>
<p></p><p>Yeah. Now you can see where this argument is going. Yeah. So, if you replaced all of them, as I replace them all, at what point do you stop being conscious? Well, people think of consciousness as this like ethereal thing that exists maybe beyond the brain cells. Yeah. Well, people have a lot of crazy ideas. Um, people don't know what consciousness is, and they often don't know what they mean by it. And then they fall back on saying, "Well, I know it because I've got it, and I can see that I've got it," and they fall back on this theta model of the mind, which I think is nonsense. What do you think of consciousness, as if you had to try and define it? Is it because I think of it as just like the awareness of myself? I don't know. I think it's a term we'll stop using. </p>
<p></p><p>Suppose you want to understand how a car works. Well, you know, some cars have a lot of oomph, and other cars have a lot less oomph. Like an Aston Martin's got lots of oomph, and a little Toyota Corolla doesn't have much oomph. But "oomph" isn't a very good concept for understanding cars. Um, if you want to understand cars, you need to understand about electric engines or petrol engines and how they work. And it gives rise to oomph, but oomph isn't a very useful explanatory concept. It's a kind of essence of a car; it's the essence of an Aston Martin, but it doesn't explain much. I think consciousness is like that. And I think we'll stop using that term, but I don't think there's any reason why a machine shouldn't have it. </p>
<p></p><p>If your view of consciousness is that it intrinsically involves self awareness, then the machine's got to have self awareness. It's got to have cognition about its own cognition and stuff. But I'm a materialist through and through, and I don't think there's any reason why a machine shouldn't have consciousness. Do you think they do then have the same consciousness that we think of ourselves as being uniquely given as a gift when we're born? </p>
<p></p><p>I'm ambivalent about that at present. So I don't think there's this hard line. I think as soon as you have a machine that has some self awareness, it's got some consciousness. Um, I think it's an emergent property of a complex system. It's not a sort of essence that's throughout the universe. It's you make this really complicated system that's complicated enough to have a model of itself, and it does perception. And I think then you're beginning to get conscious machines. So I don't think there's any sharp distinction between what we've got now and conscious machines. I don't think it's going to be one day we're going to wake up and say, "Hey, if you put this special chemical in, it becomes conscious." It's not going to be like that. </p>
<p></p><p>I think we all wonder if these computers are like thinking like we are on their own when we're not there, and if they're experiencing emotions, if they're contending withI think we probably, you know, we think about things like love and things that are feel unique to biological species. Um, are they sat there thinking? Are they... do they have concerns? I think they really are thinking about AI agents. I think as soon as you make them, they will have concerns. If you wanted to make an effective AI agent, suppose you take a call center. </p>
<p></p><p>In a call center, you have people who currently have all sorts of emotions and feelings, which are kind of useful. So, suppose I call up the call center, and I'm actually lonely and I don't actually want to know the answer to why my computer isn't working. I just want somebody to talk to. After a while, the person in the call center will either get bored or get annoyed with me and will terminate the call. Well, you replace them by an AI agent. </p>
<p>AI  AI  AI  AI  </p><p>The AI agent needs to have the same kind of responses. If someone just called up because they just want to talk to the AI agent, and we're happy to talk for the whole day to the AI agent, that's not good for business. And you want an AI agent that either gets bored or gets irritated and says, "I'm sorry, but I don't have time for this." And once it does that, I think it's got emotions. Now, like I say, emotions have two aspects to them: the cognitive aspect and the behavioral aspect, and then there's a physiological aspect, and those go together with us. </p>
<p>AI</p><p>And if the AI agent gets embarrassed, it won't go red. Yeah. Um, so there's no physiological skin won't start sweating. Yeah, but it might have all the same behavior. And in that case, I'd say, yeah, it's having emotion. It's got an emotion. So, it's going to have the same sort of cognitive thought, and then it's going to act upon that cognitive thought in the same way, but without the physiological responses. And does that matter that it doesn't go red in the face? </p>
<p></p><p>And it's just a differentI mean, that's a response to theit makes it somewhat different from us. Yeah. For some things, the physiological aspects are very important, like love. They're a long way from having love the same way we do. But I don't see why they shouldn't have emotions. So, I think what's happened is people have a model of how the mind works and what feelings are and what emotions are, and their model is just wrong. </p>
<h2></h2><h2>Working at Google</h2>
<p>Coursera</p><p>What brought you to Google? You worked at Google for about a decade, right? Yeah. What brought you there? I have a son who has learning difficulties, and in order to be sure he would never be out on the street, I needed to get several million dollars, and I wasn't going to get that as an academic. I tried. So, I taught a Coursera course in the hope that I'd make lots of money that way, but there was no money in that. </p>
<p>65AlexNetIlyaAlex</p><p>Mhm. So, I figured out well, the only way to get millions of dollars is to sell myself to a big company. And so, when I was 65, fortunately for me, I had two brilliant students who produced something called AlexNet, which was a neural net that was very good at recognizing objects in images. And so, Ilya, Alex, and I set up a little company and auctioned it. And we actually set up an auction where we had a number of big companies bidding for us. </p>
<p> AlexNet AlexNet DNN Research AlexNet AlexNet ImageNet  12%  ImageNet </p><p>And that company was called AlexNet. No, the network that recognized objects was called AlexNet. The company was called DNN Research, Deep Neural Network Research. And it was doing things like this. I'll put this graph up on the screen. That's AlexNet. This picture shows eight images, and AlexNet's ability, which is your company's ability, to spot what was in those images. Yeah. So, it could tell the difference between various kinds of mushrooms. And about 12% of ImageNet is dogs. And to be good at ImageNet, you have to tell the difference between very similar kinds of dogs. </p>
<p> AlexNet 66  65 65  76 75 75  10 </p><p>And it would have to be very good at that. And your company, AlexNet, won several awards, I believe, for its ability to outperform its competitors. And so, Google ultimately ended up acquiring your technology. Google acquired that technology and some other technology. And you went to work at Google at age what? 66. I went at age 65 to work at Google. 65. And you left at age 76? 75. 75. Okay. I worked there for more or less exactly 10 years. </p>
<p></p><p>And what were you doing there? Okay, they were very nice to me. They said, they said pretty much, you can do what you like. I worked on something called distillation that did really well, and that's now used all the time in AI, in AI. And distillation is a way of taking what a big model, a big neural net, knows and getting that knowledge into a small neural net. Then, at the end, I got very interested in analog computation and whether it would be possible to get these big language models running in analog hardware. </p>
<p> ChatGPT  Palm </p><p>So, they used much less energy. And it was when I was doing that work that I began to really realize how much better digital is for sharing information. Was there a Eureka moment? There was a Eureka moment or two. Um, and it was a sort of coupling of ChatGPT coming out, although Google had very similar things a year earlier, and I'd seen those, and that had a big effect on me. The closest I had to a Eureka moment was when a Google system called Palm was able to say why a joke was funny. </p>
<p></p><p>And I'd always thought of that as a kind of landmark. If it can say why a joke's funny, it really does understand, and it could say why a joke was funny. And that, coupled with realizing why digital is so much better than analog for sharing information, suddenly made me very interested in AI safety and that these things were going to get a lot smarter than us. Why did you leave Google? </p>
<h2></h2><h2>Why Did You Leave Google?</h2>
<p>75</p><p>The main reason I left Google was because I was 75 and I wanted to retire. I've done a very bad job of that. The precise timing of when I left Google was so that I could talk freely at a conference at MIT, but I left because I wasI'm oldand I was finding it harder to program. I was making many more mistakes when I programmed, which is very annoying. </p>
<p></p><p>You wanted to talk freely at a conference at MIT. Yes. At MIT, organized by MIT Tech Review. What did you want to talk about freely? AI safety. And you couldn't do that while you were at Google. </p>
<p></p><p>Well, I could have done it while I was at Google. And Google encouraged me to stay and work on AI safety and said I could do whatever I liked on AI safety. You kind of sense to yourself if you work for a big company. You don't feel right saying things that will damage the big company. Even if you could get away with it, it just feels wrong to me. I didn't leave because I was cross with anything Google was doing. </p>
<p>OpenAI</p><p>I think Google actually behaved very responsibly. When they had these big chatbots, they didn't release them, possibly because they were worried about their reputation. They had a very good reputation, and they didn't want to damage it. So, OpenAI didn't have a reputation, and so they could afford to take the gamble. I mean, there's also a big conversation happening around how it will cannibalize their core business in search. There is now. Yes. Yeah. Yeah. And it's the old innovators' dilemmas to some degree, I guess, that contending with bad skin. </p>
<h2></h2><h2>Ads</h2>
<p></p><p>I've had it, and I'm sure many of you listening have had it too, or maybe you have it right now. I know how draining it can be, especially if you're in a job where you're presenting often, like I am. </p>
<p> BonCharge </p><p>So, let me tell you about something that's helped both my partner and me and my sister, which is red light therapy. I only got into this a couple of years ago, but I wish I'd known a little sooner. I've been using our show sponsors BonCharge's infrared sauna blanket for a while now, but I just got hold of their red light therapy mask as well. Red light has been proven to have so many benefits for the body. Like, any area of your skin that's exposed will see a reduction in scarring, wrinkles, and even blemishes. </p>
<p>BonCharge  BonCharge.com/diary Diary  75 BonCharge.com/diary Diary 10,000 CEO  iPad </p><p>It also helps with complexion. It boosts collagen, and it does that by targeting the upper layers of your skin. And BonCharge ships worldwide with easy returns and a one year warranty on all their products. So, if you'd like to try it yourself, head over to BonCharge.com/diary and use code Diary for 25% off any product sitewide. Just make sure you order through this link: BonCharge.com/diary with code Diary. Make sure you keep what I'm about to say to yourself. I'm inviting 10,000 of you to come even deeper into The Diary Of A CEO. Welcome to my inner circle. This is a brand new private community that I'm launching to the world. We have so many incredible things that happen that you are never shown. We have the briefs that are on my iPad when I'm recording the conversation. We have clips we've never released. We have behind the scenes conversations with the guests. And also the episodes that we've never ever released, and so much more. </p>
<p>10,000 daccircle.com</p><p>In the circle, you'll have direct access to me. You can tell us what you want this show to be, who you want us to interview, and the types of conversations you would love us to have. But remember, for now, we're only inviting the first 10,000 people that join before it closes. So, if you want to join our private closed community, head to the link in the description below or go to daccircle.com. I will speak to you there. </p>
<h2></h2><h2>What Should People Be Doing About AI?</h2>
<p></p><p>I'm continually shocked by the types of individuals that listen to this conversation because they come up to me sometimes. So I hear from politicians, I hear from some real people, I hear from entrepreneurs all over the world, whether they are the entrepreneurs building some of the biggest companies in the world or their, you know, early stage startups. </p>
<p></p><p>For those people that are listening to this conversation now that are in positions of power and influence, world leaders, let's say, what's your message to them? I'd say what you need is highly regulated capitalism. That's what seems to work best. </p>
<p></p><p>And what would you say to the average person, not doesn't work in the industry, somewhat concerned about the future, doesn't know if they're helpless or not? What should they be doing in their own lives? My feeling is there's not much they can do. This isn'tthis isn't going to be decided by just as climate change isn't going to be decided by people separating out the plastic bags from the compostables. That's not going to have much effect. It's going to be decided by whether the lobbyists for the big energy companies can be kept under control. I don't think there's much people can do except for try and pressure their governments to force the big companies to work on AI safety. That they can do. </p>
<h2></h2><h2>Impressive Family Background</h2>
<p></p><p>You've lived a fascinating, fascinating, winding life. I think one of the things most people don't know about you is that your family has a big history of being involved in tremendous things. You have a family tree which is one of the most impressive that I've ever seen or read about. Your great great grandfather George Boole founded the Boolean algebra logic, which is one of the foundational principles of modern computer science. You have your great great grandmother Mary Everest Boole, who was a mathematician and educator who made huge leaps forward in mathematics, from what I was able to ascertain. </p>
<p></p><p>Um, I mean, I can... The list goes on and on and on. I mean, your great great uncle George Everest is what Mount Everest is named after. Is that... is that correct? I think he's my great great great uncle. His niece married George Bull. So, Mary Bull was Mary Everest Bull. Um, she was the niece of Everest. And your first cousin once removed, Joan Hinton, was involved in... a nuclear physicist who worked on the Manhattan Project, which is the World War II development of the first nuclear bomb. Yeah. She was one of the two female physicists at Los Alamos. And then after they dropped the bomb, she moved to China. Why? She was very cross with them dropping the bomb. And her family had a lot of links with China. Her mother was friends with Chairman Mao. Quite weird. </p>
<h2></h2><h2>Advice Youd Give Looking Back</h2>
<p></p><p>When you look back at your life, Jeffrey, we have the hindsight you have now and the retrospective clarity, what might you have done differently if you were advising me? </p>
<p></p><p>I guess I have two pieces of advice. One is if you have an intuition that people are doing things wrong and there's a better way to do things, don't give up on that intuition just because people say it's silly. Don't give up on the intuition until you've figured out why it's wrong. Figured out for yourself why that intuition isn't correct. And usually it's wrong if it disagrees with everybody else, and you'll eventually figure out why it's wrong. But just occasionally, you'll have an intuition that's actually right, and everybody else is wrong. And I lucked out that way. Early on, I thought neural nets are definitely the way to go to make AI, and almost everybody said that was crazy, and I stuck with it because I couldn't. It seemed to me it was obviously right. </p>
<p></p><p>Now, the idea that you should stick with your intuitions isn't going to work if you have bad intuitions. But if you have bad intuitions, you're never going to do anything anyway, so you might as well stick with them. </p>
<h2></h2><h2>Final Message on AI Safety</h2>
<p></p><p>And in your own career journey, is there anything you look back on and say, "With the hindsight I have now, I should have taken a different approach at that juncture?" </p>
<p></p><p>I wish I'd spent more time with my wife and with my children when they were little. I was kind of obsessed with work. </p>
<p></p><p>Your wife passed away. Yeah. From ovarian cancer. No. Or that was another wife. Okay. Um, I had two wives who had cancer. Oh, really? Sorry. The first one died of ovarian cancer, and the second one died of pancreatic cancer. And you wish you'd spent more time with her? With the second wife? Yeah. Who was a wonderful person? </p>
<p>70</p><p>Why did you say that in your 70s? What is it that you've figured out that I might not know yet? </p>
<p></p><p>Oh, just 'cause she's gone, and I can't spend more time with her now. Mhm. But you didn't know that at the time. At the time, you think I meant it was likely I would die before her, just because she was a woman and I was a man. Um, I didn'tI just didn't spend enough time when I could. I think Iinquire there because I think there are many of us who are so consumed with what we're doing professionally that we kind of assume immortality with our partners because they've always been there. So, weyeah. I mean, she was very supportive of me spending a lot of time working, but... </p>
<p></p><p>And why did you say your children as well? What's thewell, I didn't spend enough time with them when they were little, and you regret that now. Yeah. </p>
<p></p><p>If you had a closing message for my listeners about AI and AI safety, what would that be? </p>
<p></p><p>Jeffrey, there's still a chance that we can figure out how to develop AI that won't want to take over from us. And because there's a chance, we should put enormous resources into trying to figure that out because if we don't, it's going to take over. </p>
<p></p><p>And are you hopeful? </p>
<p></p><p>I just don't know. I'm agnostic. You must getget in bed at night, and when you're thinking to yourself about probabilities of outcomes, there must be a bias in one direction because there certainly is for me. I imagine everyone listening now has an internal prediction that they might not say out loud, but of how they think it's going to play out. I really don't know. I genuinely don't know. I think it's incredibly uncertain. When I'm feeling slightly depressed, I think people are toast; AI is going to take over. </p>
<p></p><p>While I'm feeling cheerful, I think we'll figure out a way. Maybe one of the facets of being a human is that because we've always been here, like we were saying about our loved ones and our relationships, we assume casually that we will always be here and we'll always figure everything out. But there's a beginning and an end to everything, as we saw from the dinosaurs. I mean, yeah. And we have to face the possibility that unless we do something soon, we're near the end. </p>
<p></p><p>We have a closing tradition on this podcast where the last guest leaves a question in their diary. And the question that they've left for you is: With everything that you see ahead of us, what is the biggest threat you see to human happiness? </p>
<h2></h2><h2>Whats the Biggest Threat to Human Happiness?</h2>
<p></p><p>I think joblessness is a fairly urgent short term threat to human happiness. I think if you make lots and lots of people unemployed, even if they get universal basic income, they're not going to be happy because they need purpose. Because they need purpose. Yes. And struggle. They need to feel they're contributing something. They're useful. </p>
<p></p><p>And do you think that outcomethat there's going to be huge job displacementis more probable than not? </p>
<p></p><p>Yes, I do. And what sort of that one, I think, is definitely more probable than not. If I worked in a call center, I'd be terrified. And what's the timeframe for that in terms of mass job losses? I think it's beginning to happen already. I read an article in The Atlantic recently that said it's already getting hard for university graduates to get jobs. And part of that may be that people are already using AI for the jobs they would have gotten. </p>
<p>CEO700050003600300080%</p><p>I spoke to the CEO of a major company that everyone will know of, lots of people use, and he said to me in DMs that they used to have just over 7,000 employees. He said by last year they were down to, I think, 5,000. He said right now they have 3,600. And he said by the end of summer, because of AI agents, they'll be down to 3,000. So, you've gotit's happening already. Yes. He's halved his workforce because AI agents can now handle 80% of the customer service inquiries and other things. So, it's happening already. Yeah. So, urgent action is needed. Yep. </p>
<p> 10</p><p>I don't know what that urgent action is. That's a tricky one because that depends very much on the political system, and political systems are all going in the wrong direction at present. I mean, what do we need to do? Save up money? Like, do we save money? Do we move to another part of the world? I don't know. What would you tell your kids to do? They said, "Dad, like there's going to be loads of job displacement." Because I worked for Google for 10 years. Is there enough money? Okay. Okay. So, they're not typical. What if they didn't have money? Trained to be a plumber? Really? Yeah. </p>
<p></p><p>Geoffrey Hinton, thank you so much. You're the first Nobel Prize winner that I've ever had a conversation with, I think, in my life. So, that's a tremendous honor. And you received that award for a lifetime of exceptional work and pushing the world forward in so many profound ways that will lead to great advancements and things that matter so much to us. And now you've turned this season in your life to shining a light on some of your own work, but also on the broader risks of AI and how it might impact us adversely. </p>
<p></p><p>And there are very few people who have worked inside the machine of a Google or a big tech company that have contributed to the field of AI who are now at the very forefront of warning us against the very thing that they worked upon. There are actually a surprising number of us now. They're not as public, and they're actually quite hard to get to have these kinds of conversations because many of them are still in that industry. So, you know, someone who tries to contact these people often and ask invites them to have conversations, they often are a little hesitant to speak openly. </p>
<p></p><p>They speak privately, but they are less willing to speak openly because they may still have some incentives at play. I have an advantage over them, which is I'm older, so I'm unemployed, so I can say what I want. Well, there you go. So, thank you for doing what you do. It's a real honor, and please do continue to do it. Thank you. Thank you so much. People think I'm joking when I say that, but I'm not. The plumbing fish. Yeah. Yeah. And plumbers are pretty well paid. </p>
<br>

</div>
<script id="res-script" src="/res/dist/res/main.js" type="text/javascript"></script>
</body></html>