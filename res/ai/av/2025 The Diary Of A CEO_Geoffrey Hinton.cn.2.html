<!DOCTYPE html><html class="translated-ltr"><!--
 Page saved with SingleFile 
 url: file:///home/zy/ws/res/res/ai/av/2025%20The%20Diary%20Of%20A%20CEO_Geoffrey%20Hinton.html 
 saved date: Wed Jul 09 2025 13:43:58 GMT+0800 (Hong Kong Standard Time)
--><head>
<meta name="dc.identifier" content="res/a2200d997b0868f8ac959adc91b5c54dd9a896b3">
<meta charset="utf-8">
      <style>:root{--yawas:rgb(252,214,57)}body{margin:16px;font-family:system-ui,sans-serif}.yawas{background-color:var(--yawas)}@media print{.yawas{-webkit-print-color-adjust:exact}}</style>
    <title>2025 The Diary Of A CEO_Geoffrey Hinton</title>
    
    <style>.VIpgJd-ZVi9od-ORHb-OEVmcd{left:0;top:0;height:39px;width:100%;z-index:10000001;position:fixed;border:none;border-bottom:1px solid #6B90DA;margin:0;box-shadow:0 0 8px 1px #999}.VIpgJd-ZVi9od-xl07Ob-OEVmcd{z-index:10000002;border:none;position:fixed;box-shadow:0 3px 8px 2px #999}.VIpgJd-ZVi9od-SmfZ-OEVmcd{z-index:10000000;border:none;margin:0}.goog-te-gadget{font-family:arial;font-size:11px;color:#666;white-space:nowrap}.goog-te-gadget img{vertical-align:middle;border:none}.goog-te-gadget-simple{background-color:#FFF;border-left:1px solid #D5D5D5;border-top:1px solid #9B9B9B;border-bottom:1px solid #E8E8E8;border-right:1px solid #D5D5D5;font-size:10pt;display:inline-block;padding-top:1px;padding-bottom:2px;cursor:pointer}.goog-te-gadget-icon{margin-left:2px;margin-right:2px;width:19px;height:19px;border:none;vertical-align:middle}.goog-te-combo{margin-left:4px;margin-right:4px;vertical-align:baseline}.goog-te-gadget .goog-te-combo{margin:4px 0}.VIpgJd-ZVi9od-l4eHX-hSRGPd,.VIpgJd-ZVi9od-l4eHX-hSRGPd:link,.VIpgJd-ZVi9od-l4eHX-hSRGPd:visited,.VIpgJd-ZVi9od-l4eHX-hSRGPd:hover,.VIpgJd-ZVi9od-l4eHX-hSRGPd:active{font-size:12px;font-weight:bold;color:#444;text-decoration:none}.VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd,.VIpgJd-ZVi9od-TvD9Pc-hSRGPd{display:block;margin:0 10px}.VIpgJd-ZVi9od-ORHb .VIpgJd-ZVi9od-l4eHX-hSRGPd{padding-top:2px;padding-left:4px}.goog-te-combo,.VIpgJd-ZVi9od-ORHb *,.VIpgJd-ZVi9od-SmfZ *,.VIpgJd-ZVi9od-xl07Ob *,.VIpgJd-ZVi9od-vH1Gmf *,.VIpgJd-ZVi9od-l9xktf *{font-family:arial;font-size:10pt}.VIpgJd-ZVi9od-ORHb{margin:0;background-color:#E4EFFB;overflow:hidden}.VIpgJd-ZVi9od-ORHb img{border:none}.VIpgJd-ZVi9od-ORHb-bN97Pc{color:#000}.VIpgJd-ZVi9od-ORHb-bN97Pc img{vertical-align:middle}.VIpgJd-ZVi9od-ORHb-Tswv1b{color:#666;vertical-align:top;margin-top:0;font-size:7pt}.VIpgJd-ZVi9od-ORHb-KE6vqe{width:8px}.VIpgJd-ZVi9od-LgbsSe{border-color:#E7E7E7;border-style:none solid solid none;border-width:0 1px 1px 0}.VIpgJd-ZVi9od-LgbsSe div{border-color:#CCC #999 #999 #CCC;border-right:1px solid #999;border-style:solid;border-width:1px;height:20px}.VIpgJd-ZVi9od-LgbsSe button{background:transparent;border:none;cursor:pointer;height:20px;overflow:hidden;margin:0;vertical-align:top;white-space:nowrap}.VIpgJd-ZVi9od-LgbsSe button:active{background:none repeat scroll 0 0#CCC}.VIpgJd-ZVi9od-SmfZ{margin:0;background-color:#FFF;white-space:nowrap}.VIpgJd-ZVi9od-SmfZ-hSRGPd{text-decoration:none;font-weight:bold;font-size:10pt;border:1px outset #888;padding:6px 10px;white-space:nowrap;position:absolute;left:0;top:0}.VIpgJd-ZVi9od-SmfZ-hSRGPd img{margin-left:2px;margin-right:2px;width:19px;height:19px;border:none;vertical-align:middle}.VIpgJd-ZVi9od-SmfZ-hSRGPd span{text-decoration:underline;margin-left:2px;margin-right:2px;vertical-align:middle}.goog-te-float-top .VIpgJd-ZVi9od-SmfZ-hSRGPd{padding:2px;border-top-width:0}.goog-te-float-bottom .VIpgJd-ZVi9od-SmfZ-hSRGPd{padding:2px;border-bottom-width:0}.VIpgJd-ZVi9od-xl07Ob-lTBxed{text-decoration:none;color:#00C;white-space:nowrap;margin-left:4px;margin-right:4px}.VIpgJd-ZVi9od-xl07Ob-lTBxed span{text-decoration:underline}.VIpgJd-ZVi9od-xl07Ob-lTBxed img{margin-left:2px;margin-right:2px}.goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed{color:#000}.goog-te-gadget-simple .VIpgJd-ZVi9od-xl07Ob-lTBxed span{text-decoration:none}.VIpgJd-ZVi9od-xl07Ob{background-color:#FFF;text-decoration:none;border:2px solid #C3D9FF;overflow-y:scroll;overflow-x:hidden;position:absolute;left:0;top:0}.VIpgJd-ZVi9od-xl07Ob-ibnC6b{padding:3px;text-decoration:none}.VIpgJd-ZVi9od-xl07Ob-ibnC6b,.VIpgJd-ZVi9od-xl07Ob-ibnC6b:link{color:#00C;background:#FFF}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:visited{color:#551A8B}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:hover{background:#C3D9FF}.VIpgJd-ZVi9od-xl07Ob-ibnC6b:active{color:#00C}.VIpgJd-ZVi9od-vH1Gmf{background-color:#FFF;text-decoration:none;border:1px solid #6B90DA;overflow:hidden;padding:4px}.VIpgJd-ZVi9od-vH1Gmf-KrhPNb{width:16px}.VIpgJd-ZVi9od-vH1Gmf-hgDUwe{margin:6px 0;height:1px;background-color:#aaa;overflow:hidden}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div{padding:4px}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b .uDEFge{display:none}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd .fmcmS{padding-left:4px;padding-right:4px}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd{text-decoration:none}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:link div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:visited div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:active div{color:#00C;background:#FFF}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b:hover div{color:#FFF;background:#36C}.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:link div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:visited div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:hover div,.VIpgJd-ZVi9od-vH1Gmf-ibnC6b-gk6SMd:active div{color:#000;font-weight:bold}.VIpgJd-ZVi9od-l9xktf{background-color:#FFF;overflow:hidden;padding:8px;border:none;border-radius:10px}.VIpgJd-ZVi9od-l9xktf-OEVmcd{background-color:#FFF;border:1px solid #6B90DA;box-shadow:0 3px 8px 2px #999;border-radius:8px}.VIpgJd-ZVi9od-l9xktf img{border:none}.VIpgJd-ZVi9od-l9xktf-fmcmS{margin-top:6px}.VIpgJd-ZVi9od-l9xktf-VgwJlc{margin-top:6px;white-space:nowrap}.VIpgJd-ZVi9od-l9xktf-VgwJlc *{vertical-align:middle}.VIpgJd-ZVi9od-l9xktf-VgwJlc .DUGJie{background-image:url(data:,)}.VIpgJd-ZVi9od-l9xktf-VgwJlc .TdyTDe{background-image:url(data:,)}.VIpgJd-ZVi9od-l9xktf-VgwJlc span{color:#00C;text-decoration:underline;cursor:pointer;margin:0 4px}.VIpgJd-ZVi9od-l9xktf-I9GLp{margin:6px 0 0}.VIpgJd-ZVi9od-l9xktf-I9GLp form{margin:0}.VIpgJd-ZVi9od-l9xktf-I9GLp form textarea{margin-bottom:4px;width:100%}.VIpgJd-ZVi9od-l9xktf-yePe5c{margin:6px 0 4px}.VIpgJd-ZVi9od-aZ2wEe-wOHMyf{z-index:1000;position:fixed;-webkit-transition-delay:.6s;transition-delay:.6s;left:-1000px;top:-1000px}.VIpgJd-ZVi9od-aZ2wEe-wOHMyf-ti6hGc{-webkit-transition-delay:0s;transition-delay:0s;left:-14px;top:-14px}.VIpgJd-ZVi9od-aZ2wEe-OiiCO{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-align:center;-webkit-align-items:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;justify-content:center;width:104px;height:104px;border-radius:50px;background:#FFF url(data:image/png;base64,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)50% 50%no-repeat;-webkit-transition:all .6s ease-in-out;transition:all .6s ease-in-out;-webkit-transform:scale(.4);transform:scale(.4);opacity:0}.VIpgJd-ZVi9od-aZ2wEe-OiiCO-ti6hGc{-webkit-transform:scale(.5);transform:scale(.5);opacity:1}.VIpgJd-ZVi9od-aZ2wEe{margin:2px 0 0 2px;-webkit-animation:spinner-rotator 1.4s linear infinite;animation:spinner-rotator 1.4s linear infinite}@-webkit-keyframes spinner-rotator{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(270deg);transform:rotate(270deg)}}@keyframes spinner-rotator{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}100%{-webkit-transform:rotate(270deg);transform:rotate(270deg)}}.VIpgJd-ZVi9od-aZ2wEe-Jt5cK{stroke-dasharray:187;stroke-dashoffset:0;stroke:#4285F4;-webkit-transform-origin:center;transform-origin:center;-webkit-animation:spinner-dash 1.4s ease-in-out infinite;animation:spinner-dash 1.4s ease-in-out infinite}@-webkit-keyframes spinner-dash{0%{stroke-dashoffset:187}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);transform:rotate(135deg)}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);transform:rotate(450deg)}}@keyframes spinner-dash{0%{stroke-dashoffset:187}50%{stroke-dashoffset:46.75;-webkit-transform:rotate(135deg);transform:rotate(135deg)}100%{stroke-dashoffset:187;-webkit-transform:rotate(450deg);transform:rotate(450deg)}}.VIpgJd-yAWNEb-L7lbkb html,.VIpgJd-yAWNEb-L7lbkb body,.VIpgJd-yAWNEb-L7lbkb div,.VIpgJd-yAWNEb-L7lbkb span,.VIpgJd-yAWNEb-L7lbkb iframe,.VIpgJd-yAWNEb-L7lbkb h1,.VIpgJd-yAWNEb-L7lbkb h2,.VIpgJd-yAWNEb-L7lbkb h3,.VIpgJd-yAWNEb-L7lbkb h4,.VIpgJd-yAWNEb-L7lbkb h5,.VIpgJd-yAWNEb-L7lbkb h6,.VIpgJd-yAWNEb-L7lbkb p,.VIpgJd-yAWNEb-L7lbkb a,.VIpgJd-yAWNEb-L7lbkb img,.VIpgJd-yAWNEb-L7lbkb ol,.VIpgJd-yAWNEb-L7lbkb ul,.VIpgJd-yAWNEb-L7lbkb li,.VIpgJd-yAWNEb-L7lbkb table,.VIpgJd-yAWNEb-L7lbkb form,.VIpgJd-yAWNEb-L7lbkb tbody,.VIpgJd-yAWNEb-L7lbkb tr,.VIpgJd-yAWNEb-L7lbkb td{all:unset;font-size:100%;line-height:normal}.VIpgJd-yAWNEb-L7lbkb ol,.VIpgJd-yAWNEb-L7lbkb ul{list-style:none}.VIpgJd-yAWNEb-L7lbkb table{border-collapse:collapse;border-spacing:0}.VIpgJd-yAWNEb-L7lbkb caption,.VIpgJd-yAWNEb-L7lbkb th,.VIpgJd-yAWNEb-L7lbkb td{text-align:left;font-weight:normal}.VIpgJd-yAWNEb-L7lbkb input::-moz-focus-inner{border:0}div>.VIpgJd-yAWNEb-L7lbkb{padding:10px 14px}.VIpgJd-yAWNEb-L7lbkb{color:#222;background-color:#fff;border:1px solid #eee;box-shadow:0 4px 16px rgba(0,0,0,.2);-moz-box-shadow:0 4px 16px rgba(0,0,0,.2);-webkit-box-shadow:0 4px 16px rgba(0,0,0,.2);display:none;font-family:arial;font-size:10pt;width:420px;padding:12px;position:absolute;z-index:10000}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-nVMfcd-fmcmS,.VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe{clear:both;font-size:10pt;position:relative;text-align:justify;width:100%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-r4nke{color:#999;font-family:arial,sans-serif;margin:4px 0;text-align:left}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TvD9Pc-LgbsSe{display:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-l4eHX{float:left;margin:0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-PLDbbf{display:inline-block}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-fw42Ze-Z0Arqf-haAclf{display:none;width:100%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-H9tDt{margin-top:20px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-LK5yu{float:left}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-qwU8Me{float:right}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-cGMI2b{min-height:15px;position:relative;height:1%}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-jOfkMb-Ne3sFf{background:-webkit-linear-gradient(top,#29910d 0,#20af0e 100%);background:-ms-linear-gradient(top,#29910d 0,#20af0e 100%);background:#29910d;border-radius:4px;-moz-border-radius:4px;-webkit-border-radius:4px;box-shadow:inset 0 2px 2px #1e6609;-moz-box-shadow:inset 0 2px 2px #1e6609;-webkit-box-shadow:inset 0 2px 2px #1e6609;color:white;font-size:9pt;font-weight:bolder;margin-top:12px;padding:6px;text-shadow:1px 1px 1px #1e6609}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-hSRGPd{color:#15c;cursor:pointer;font-family:arial;font-size:11px;margin-right:15px;text-decoration:none}.VIpgJd-yAWNEb-L7lbkb>textarea{font-family:arial;resize:vertical;width:100%;margin-bottom:10px;border-radius:1px;border:1px solid #d9d9d9;border-top:1px solid silver;font-size:13px;height:auto;overflow-y:auto;padding:1px}.VIpgJd-yAWNEb-L7lbkb textarea:focus{box-shadow:inset 0 1px 2px rgba(0,0,0,.3);border:1px solid #4d90fe;outline:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-Z0Arqf-IbE0S{margin-right:10px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp{min-height:25px;vertical-align:middle;padding-top:8px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp{margin-bottom:5px;margin-bottom:0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input{display:inline-block;min-width:54px;*min-width:70px;border:1px solid #dcdcdc;border:1px solid rgba(0,0,0,.1);text-align:center;color:#444;font-size:11px;font-weight:bold;height:27px;outline:0;padding:0 8px;vertical-align:middle;line-height:27px;margin:0 16px 0 0;box-shadow:0 1px 2px rgba(0,0,0,.1);-moz-box-shadow:0 1px 2px rgba(0,0,0,.1);-webkit-box-shadow:0 1px 2px rgba(0,0,0,.1);border-radius:2px;-webkit-transition:all .218s;transition:all .218s;background-color:#f5f5f5;background-image:-webkit-gradient(linear,left top,left bottom,from(#f5f5f5),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f5f5f5,#f1f1f1);background-image:-o-linear-gradient(top,#f5f5f5,#f1f1f1);-webkit-user-select:none;-moz-user-select:none;cursor:default}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover{border:1px solid #c6c6c6;color:#222;-webkit-transition:all 0s;transition:all 0s;background-color:#f8f8f8;background-image:-webkit-gradient(linear,left top,left bottom,from(#f8f8f8),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f8f8f8,#f1f1f1);background-image:-o-linear-gradient(top,#f8f8f8,#f1f1f1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active{border:1px solid #c6c6c6;color:#333;background-color:#f6f6f6;background-image:-webkit-gradient(linear,left top,left bottom,from(#f6f6f6),to(#f1f1f1));background-image:-webkit-linear-gradient(top,#f6f6f6,#f1f1f1);background-image:-o-linear-gradient(top,#f6f6f6,#f1f1f1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus:active{box-shadow:inset 0 0 0 1px rgba(255,255,255,.5);-webkit-box-shadow:inset 0 0 0 1px rgba(255,255,255,.5);-moz-box-shadow:inset 0 0 0 1px rgba(255,255,255,.5)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe{outline:none;border:1px solid #4d90fe;z-index:4!important}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.gk6SMd{background-color:#eee;background-image:-webkit-gradient(linear,left top,left bottom,from(#eee),to(#e0e0e0));background-image:-webkit-linear-gradient(top,#eee,#e0e0e0);background-image:-o-linear-gradient(top,#eee,#e0e0e0);box-shadow:inset 0 1px 2px rgba(0,0,0,.1);border:1px solid #ccc;color:#333}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf{color:white;border-color:#3079ed;background-color:#4d90fe;background-image:-webkit-gradient(linear,left top,left bottom,from(#4d90fe),to(#4787ed));background-image:-webkit-linear-gradient(top,#4d90fe,#4787ed);background-image:-o-linear-gradient(top,#4d90fe,#4787ed)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe .VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active{border-color:#3079ed;background-color:#357ae8;background-image:-webkit-gradient(linear,left top,left bottom,from(#4d90fe),to(#357ae8));background-image:-webkit-linear-gradient(top,#4d90fe,#357ae8);background-image:-o-linear-gradient(top,#4d90fe,#357ae8)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover{box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1);-webkit-box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1);-moz-box-shadow:inset 0 0 0 1px #fff,0 1px 1px rgba(0,0,0,.1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input.AHmuwe,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input:hover,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:focus,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf.AHmuwe,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:active,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-L4Nn5e-I9GLp .VIpgJd-yAWNEb-Z0Arqf-I9GLp input .VIpgJd-yAWNEb-Z0Arqf-sFeBqf:hover{border-color:#3079ed}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-mrxPge{color:#999;font-family:arial,sans-serif}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-W0vJo-fmcmS{color:#999;font-size:11px;font-family:arial,sans-serif;margin:15px 0 5px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-u0pjoe-fmcmS{color:#800;display:none;font-size:9pt}.VIpgJd-yAWNEb-VIpgJd-fmcmS-sn54Q{background-color:#c9d7f1;box-shadow:2px 2px 4px #99a;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;position:relative}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-xl07Ob{background:#fff;border:1px solid #ddd;box-shadow:0 2px 4px #99a;min-width:0;outline:none;padding:0;position:absolute;z-index:2000}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb{cursor:pointer;padding:2px 5px 5px;margin-right:0;border-style:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb:hover{background:#ddd}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb h1{font-size:100%;font-weight:bold;margin:4px 0}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-xl07Ob .VIpgJd-yAWNEb-VIpgJd-j7LFlb strong{color:#345aad}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo{text-align:right;position:absolute;right:0;left:auto}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-VIpgJd-j7LFlb-SIsrTd .VIpgJd-yAWNEb-VIpgJd-eKm5Fc-hFsbo{text-align:left;position:absolute;left:0;right:auto}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-fmcmS,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{background-color:#f1ea00;border-radius:4px;-webkit-border-radius:4px;-moz-border-radius:4px;box-shadow:rgba(0,0,0,.5) 3px 3px 4px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;color:#f1ea00;cursor:pointer;margin:-2px -2px -2px -3px;padding:2px 2px 2px 3px;position:relative}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{color:#222}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-Vy2Aqc-pbTTYe{color:white;position:absolute!important}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf,.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-ppHlrf .VIpgJd-yAWNEb-TVLw9c-ppHlrf-sn54Q{background-color:#c9d7f1;border-radius:4px 4px 0 0;-webkit-border-radius:4px 4px 0 0;-moz-border-radius:4px 4px 0 0;box-shadow:rgba(0,0,0,.5) 3px 3px 4px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;cursor:pointer;margin:-2px -2px -2px -3px;padding:2px 2px 3px 3px;position:relative}.VIpgJd-yAWNEb-L7lbkb span:focus{outline:none}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-TVLw9c-DyVDA{background-color:transparent;border:1px solid #4d90fe;border-radius:0;-webkit-border-radius:0;-moz-border-radius:0;margin:-2px;padding:1px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-LzX3ef{border-left:2px solid red;margin-left:-2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-yAWNEb-TVLw9c-sn54Q-YIAiIb{border-right:2px solid red;margin-right:-2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf{padding:2px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS{font-size:11px;padding:2px 2px 3px;margin:0;background-color:#fff;color:#333;border:1px solid #d9d9d9;border-top:1px solid #c0c0c0;display:inline-block;vertical-align:top;height:21px;box-sizing:border-box;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;-webkit-border-radius:1px}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:hover{border:1px solid #b9b9b9;border-top:1px solid #a0a0a0;box-shadow:inset 0 1px 2px rgba(0,0,0,.1)}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-fmcmS:focus{box-shadow:inset 0 1px 2px rgba(0,0,0,.3);outline:none;border:1px solid #4d90fe}.VIpgJd-yAWNEb-L7lbkb .VIpgJd-yAWNEb-IFdKyd-YPqjbf-sFeBqf{font-size:11px;padding:2px 6px 3px;margin:0 0 0 2px;height:21px}.VIpgJd-yAWNEb-L7lbkb>div{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;flex-direction:column;font-family:"Google Sans",Arial,sans-serif}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-Ud7fr{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-align:end;-webkit-align-items:end;align-items:end;margin:14px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-IuizWc-SIsrTd{margin-right:14px;color:#747775;font-size:14px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-IuizWc-i3jM8c{margin-left:14px;color:#747775;font-size:14px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-k77Iif{margin:0 16px 16px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1{width:auto;color:#1f1f1f;font-size:16px;text-align:initial}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-axAV1 .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid{border-radius:0 0 12px 12px;margin:0;background:#f1f4f9;position:relative;min-height:50px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-webkit-flex-direction:column;flex-direction:column;width:77%;padding:12px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-UTujCb{color:#1f1f1f;font-size:12px;font-weight:500}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-UTujCb{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-eO9mKe{color:#444746;font-size:12px;padding-top:4px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od .VIpgJd-yAWNEb-SIsrTd .VIpgJd-yAWNEb-hvhgNd-eO9mKe{text-align:right}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5{position:absolute;top:10px;right:5px}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-xgov5 .VIpgJd-yAWNEb-SIsrTd{left:5px;right:auto}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-THI6Vb{fill:#0b57d0}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf{margin:-4px 2px 0 0;padding:2px 0 0;width:48px;height:48px;border:none;border-radius:24px;cursor:pointer;background:none}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-bgm6sf:hover{background:#e8ebec}.VIpgJd-yAWNEb-hvhgNd .VIpgJd-yAWNEb-hvhgNd-aXYTce{display:none}sentinel{}</style><meta name="referrer" content="no-referrer"><link id="res-style" rel="stylesheet" href="/res/dist/res/style.css" type="text/css">
</head>
    <body>
      <div id="book-container">
    <center>
      <img width="320" src="data:image/jpeg;base64,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">
      <h1>人工智能教父：我试图警告他们，但我们已经失控了！杰弗里·辛顿</h1><h1>Godfather of AI: I Tried to Warn Them, But We’ve Already Lost Control! Geoffrey Hinton</h1>
      <p>3周前 (2025年6月16日) — 1:30:07</p><p>3 weeks ago (Jun 16, 2025) — 1:30:07</p>
      <a href="https://youtube.com/watch?v=giT0ytynSqg">https://youtube.com/watch?v=giT0ytynSqg</a>
    </center>
    <br>
    <center><h2>概括</h2><h2>Summary</h2></center><p><i>“人工智能教父”杰弗里·辛顿警告超级智能的危险。他认为人工智能可能构成生存威胁，有可能超越人类智能，导致失业和贫富不均。他强调需要制定强有力的监管措施，特别是针对军事人工智能，并对滥用（包括网络攻击、病毒和选举舞弊）表示担忧。辛顿认为，在竞争的推动下，当前人工智能发展正在加速，其危害潜力巨大。他倡导对人工智能安全性进行研究，并建议人们应该考虑那些不太容易受到自动化影响的职业，例如管道工。
</i></p><p><i>Geoffrey Hinton, the "Godfather of AI," warns about the dangers of superintelligence.  He believes AI could pose an existential threat, potentially surpassing human intelligence and leading to job displacement and wealth inequality.  He emphasizes the need for strong regulations, particularly for military AI, and expresses concern about misuse, including cyberattacks, viruses, and corrupting elections.  Hinton argues that current AI development is accelerating, driven by competition, and that the potential for harm is significant.  He advocates for research into AI safety and suggests that people should consider careers less susceptible to automation, like plumbing.
</i></p>
<h2>简介</h2><h2>Intro</h2>
<p>人们称您为人工智能教父。那么，在超级智能的世界里，您会如何看待人们的职业前景呢？</p><p>They call you the Godfather of AI. So, what would you be saying to people about their career prospects in a world of superintelligence? </p>
<p>接受培训成为一名水管工。真的吗？</p><p>Train to be a plumber. Really? </p>
<p>是的。好的。我要成为一名水管工。</p><p>Yeah. Okay. I'm going to become a plumber. </p>
<p>杰弗里·辛顿是诺贝尔奖获得者，他的开创性工作塑造了人工智能和人类的未来。</p><p>Geoffrey Hinton is the Nobel Prize winning pioneer whose groundbreaking work has shaped AI and the future of humanity. </p>
<p>为什么称之为人工智能教父？</p><p>Why do they call it the Godfather of AI? </p>
<p>因为当时没有多少人相信我们能够模仿大脑来构建人工智能，让它学会做复杂的事情，比如识别物体和图像，甚至进行推理。我推动这种方法50年，后来谷歌收购了这项技术，我又在那里工作了10年，开发出如今在人工智能领域一直使用的技术。</p><p>Because there weren't many people who believed that we could model AI on the brain so that it learned to do complicated things like recognize objects and images or even do reasoning. And I pushed that approach for 50 years, and then Google acquired that technology, and I worked there for 10 years on something that's now used all the time in AI. </p>
<p>然后你就离开了。</p><p>And then you left. </p>
<p>是啊。为什么？</p><p>Yeah. Why? </p>
<p>这样我就能在会议上畅所欲言了。你想畅所欲言些什么？</p><p>So that I could talk freely at a conference. What did you want to talk about freely? </p>
<p>人工智能有多危险？我意识到这些东西总有一天会变得比我们更聪明，而我们从未面临过这种情况。如果你想知道当你不是顶尖智能时生活会是什么样子，去问一只鸡吧。</p><p>How dangerous AI could be. I realized that these things will one day get smarter than us, and we've never had to deal with that. And if you want to know what life's like when you're not the apex intelligence, ask a chicken. </p>
<p>所以，人类滥用人工智能会带来风险，而人工智能变得超级聪明并决定不再需要我们，也会带来风险。这真的是一种风险吗？</p><p>So, there are risks that come from people misusing AI, and then there are risks from AI getting super smart and deciding it doesn't need us. Is that a real risk? </p>
<p>是的。但他们不会阻止它，因为它太好了，太多事情都做不成。</p><p>Yes, it is. But they're not going to stop it because it's too good for too many things. </p>
<p>那么法规又如何呢？</p><p>What about regulations? </p>
<p>他们确实有一些，但这些技术并非为应对大多数威胁而设计的。比如，欧洲人工智能法规中有一项条款规定，这些技术均不适用于人工智能的军事用途。真的吗？</p><p>They have some, but they're not designed to deal with most of the threats. Like, the European AI Regulations have a clause that says none of these apply to military uses of AI. Really? </p>
<p>是啊，太疯狂了。</p><p>Yeah. It's crazy. </p>
<p>你的一个学生离开了OpenAI。是的。他可能是ChatGPT早期版本开发背后最重要的人物，我认为他离开是因为他担心安全问题。</p><p>One of your students left OpenAI. Yeah. He was probably the most important person behind the development of the early versions of ChatGPT, and I think he left because he had safety concerns. </p>
<p>我们应该认识到，这些东西是一种生存威胁，我们必须面对这样的可能性：除非我们尽快采取行动，否则我们将接近末日。</p><p>We should recognize that this stuff is an existential threat, and we have to face the possibility that unless we do something soon, we're near the end. </p>
<p>那么，让我们来承担风险吧。在这样的世界里，我们最终会做什么？这个问题总是让我有点困惑。</p><p>So, let's do the risks. What do we end up doing in such a world? This has always blown my mind a little bit. </p>
<p>53% 的常听节目的观众还没有订阅。所以，在节目开始之前，我能请你们帮个忙吗？如果您喜欢这个节目，喜欢我们所做的一切，并且想要支持我们，那么一个免费又简单的方法就是点击订阅按钮。我承诺，如果您订阅了，我和我的团队将竭尽所能，确保这个节目每周都更上一层楼，更好地服务于您。我们会认真听取您的反馈，找到您希望我采访的嘉宾，并继续努力。非常感谢！</p><p>53% of you that listen to the show regularly haven't yet subscribed to the show. So, could I ask you for a favor before we start? If you like the show and you like what we do here, and you want to support us, the free and simple way that you can do just that is by hitting the subscribe button. And my commitment to you is if you do that, then I'll do everything in my power, me and my team, to make sure that this show is better for you every single week. We'll listen to your feedback. We'll find the guests that you want me to speak to, and we'll continue to do what we do. Thank you so much. </p>
<h2>为什么他们称您为人工智能教父？</h2><h2>Why Do They Call You the Godfather of AI?</h2>
<p>杰弗里·辛顿，人们称您为人工智能教父。嗯，是的。为什么这么称呼您呢？当时相信我们能让神经网络，也就是人工神经网络发挥作用的人并不多。所以，在很长一段时间里，从20世纪50年代开始，人工智能领域就存在两种关于如何实现人工智能的观点。一种观点认为，人类智能的核心是推理。而要进行推理，你需要运用某种形式的逻辑。所以，人工智能必须以逻辑为基础。在你的脑子里，你必须拥有一些类似符号表达式的东西，你可以用规则来操纵它们。这就是智能的运作方式。而像学习或类比推理这样的能力，都是在我们弄清楚基本推理机制之后才会出现的。</p><p>Geoffrey Hinton, they call you the Godfather of AI. Uh, yes, they do. Why do they call you that? There weren't that many people who believed that we could make neural networks work, artificial neural networks. So, for a long time in AI, from the 1950s onwards, there were kind of two ideas about how to do AI. One idea was that the core of human intelligence was reasoning. And to do reasoning, you needed to use some form of logic. And so, AI had to be based around logic. And in your head, you must have something like symbolic expressions that you manipulated with rules. And that's how intelligence worked. And things like learning or reasoning by analogy, that all come later once we've figured out how basic reasoning works. </p>
<p>当时有一种不同的方法，那就是让我们以大脑为模型来构建人工智能，因为显然，大脑赋予了我们智慧。也就是说，在计算机上模拟一个脑细胞网络，并尝试弄清楚如何学习脑细胞之间连接的强度，以便它能够学会做一些复杂的事情，比如识别图像中的物体、识别语音，甚至进行推理。我推广这种方法大约50年，因为很少有人相信它。当时没有多少优秀的大学有研究团队。所以，如果你这样做了，相信这一点的优秀年轻学生就会来和你一起工作。所以，我很幸运地拥有一大批非常优秀的学生，其中一些人后来创建了OpenAI这样的平台，并在其中发挥了重要作用。是的。所以我看到了一个很好的例子，很多这样的例子。</p><p>There was a different approach, which is to say, let's model AI on the brain because obviously, the brain makes us intelligent. So, simulate a network of brain cells on a computer and try and figure out how you would learn the strengths of connections between brain cells so that it learned to do complicated things like recognize objects in images or recognize speech or even do reasoning. I pushed that approach for like 50 years because so few people believed in it. There weren't many good universities that had groups that did that. So, if you did that, the best young students who believed in that came and worked with you. So, I was very fortunate in getting a whole lot of really good students, some of which have gone on to create and play an instrumental role in creating platforms like OpenAI. Yes. So, I saw a nice example, a whole bunch of them. </p>
<p>您为什么认为以大脑为模型是更有效的方法？早期相信这一点的不只是我一个人。费曼相信，图灵也相信。如果他们中的任何一个人还活着，我认为人工智能的历史都会截然不同，但他们都英年早逝。您认为人工智能会更早出现吗？我认为如果他们中的任何一个人活在您人生的这个阶段，神经网络方法会更早被接受。</p><p>Why did you believe that modeling it off the brain was a more effective approach? It wasn't just me who believed it early on. Feynman believed it, and Turing believed it. And if either of those had lived, I think AI would have had a very different history, but they both died young. You think AI would have been here sooner? I think the neural net approach would have been accepted much sooner if either of them had lived in this season of your life. </p>
<h2>警告人工智能的危险</h2><h2>Warning About the Dangers of AI</h2>
<p>你的任务是什么？我现在的主要任务是警告人们人工智能有多危险。</p><p>What mission are you on? My main mission now is to warn people how dangerous AI could be. </p>
<p>您成为“人工智能教父”时知道吗？不，真的不知道。我当时很晚才意识到一些风险。有些风险一直很明显，比如人们会用人工智能制造自主致命武器——也就是那些可以自行决定杀死谁的东西。其他风险，比如它们有一天会变得比我们更聪明，甚至可能变得无关紧要，我当时很晚才意识到这一点。其他人20年前就意识到了。</p><p>Did you know that when you became the "Godfather of AI"? No, not really. I was quite slow to understand some of the risks. Some of the risks were always very obvious, like people would use AI to make autonomous lethal weapons—that is, things that go around deciding by themselves who to kill. Other risks, like the idea that they would one day get smarter than us and maybe would become irrelevant, I was slow to recognize that. Other people recognized it 20 years ago. </p>
<p>我几年前才意识到，这是一个真正的风险，而且可能很快就会到来。你怎么可能没能预见到这一点呢？如果你了解所有这些，就能破解计算机学习类似人类学习方式的能力，并且，你知道，还能带来任何程度的改进，这是一个很好的问题。你怎么可能没预见到这一点？但请记住，二三十年前的神经网络功能非常原始。它们远不及人类，但在视觉、语言和语音识别等方面却表现优异。现在担心它会变得比人类更聪明的想法在当时看来很愚蠢。这种情况是什么时候改变的？ChatGPT 出现后，对普通人来说，情况发生了变化。而对我来说，情况发生了变化，是因为我意识到我们正在创造的数字智能拥有某种东西，使它们远远优于我们现有的生物智能。</p><p>I only recognized it a few years ago that that was a real risk that was coming might be coming quite soon. How could you not have foreseen that, if, with everything you know here about cracking the ability for these computers to learn similar to how humans learn, and just, you know, introducing any rate of improvement, it's a very good question. How could you not have seen that? But remember, neural networks 20 30 years ago were very primitive in what they could do. They were nowhere near as good as humans, but things like vision, language, and speech recognition. The idea that you have to now worry about it getting smarter than people, that seems silly then. When did that change? It changed for the general population when ChatGPT came out. It changed for me when I realized that the kinds of digital intelligences we're making have something that makes them far superior to the kind of biological intelligence we have. </p>
<p>如果我想和你分享信息，我会去学习一些东西，然后想告诉你我学到了什么，所以我会写一些句子。这是一个相当简单的模型，但大致正确。你的大脑正在试图弄清楚如何改变神经元之间的连接强度？所以我可能会把这个词放在后面。所以，当一个非常令人惊讶的词出现时，你会进行大量的学习，而当它是一个非常明显的词时，你学习得就不多了。</p><p>If I want to share information with you, so I go off and I learn something and I'd like to tell you what I learned, so I produce some sentences. This is a rather simplistic model, but roughly right. Your brain is trying to figure out how can I change the strength of connections between neurons? So I might have put that word next. And so you'll do a lot of learning when a very surprising word comes, and not much learning when if it's when it's a very obvious word. </p>
<p>如果我说“炸鱼和薯条”，当我说“薯条”时，你学得不多。但如果我说“鱼和黄瓜”，你学得就多得多。你会想我为什么说“黄瓜”？所以这大概就是你大脑里发生的事情。我在预测接下来会发生什么。我们认为大脑就是这样运作的。没有人真正知道大脑是如何运作的。也没有人知道它是如何获取关于你应该增加还是减少连接强度的信息的。这才是关键。但我们现在从人工智能中了解到的是，如果你能获得关于是否应该增加或减少连接强度的信息，以便在你尝试执行的任何任务上做得更好，那么我们就能学到不可思议的东西，因为这就是我们现在用人工神经网络所做的事情。只是我们不知道真正的大脑是如何获取关于增加还是减少连接强度的信号的。</p><p>If I say "fish and chips," you don't do much learning when I say "chips." But if I say "fish and cucumber," you do a lot more learning. You wonder why did I say "cucumber"? So that's roughly what's going on in your brain. I'm predicting what's coming next. That's how we think it's working. Nobody really knows for sure how the brain works. And nobody knows how it gets the information about whether you should increase the strength of a connection or decrease the strength of a connection. That's the crucial thing. But what we do know now from AI is that if you could get information about whether to increase or decrease the connection strength so as to do better at whatever task you're trying to do, then we could learn incredible things because that's what we're doing now with artificial neural networks. It's just we don't know for real brains how they get that signal about whether to increase or decrease. </p>
<h2>我们应该对人工智能抱有的担忧</h2><h2>Concerns We Should Have About AI</h2>
<p>今天我们坐在这里，您对人工智能的安全有什么最大的担忧？如果要我们列出几个我们真正关心、应该考虑的问题……嗯，我可以多说几个吗？请说。我会把它们都写下来，然后我们再逐一讨论。好的。</p><p>As we sit here today, what are the big concerns you have around the safety of AI? If we were to list the top couple that are really front of mind and that we should be thinking about... um, can I have more than a couple? Go ahead. I'll write them all down, and we'll go through them. Okay. </p>
<p>首先，我想区分两种截然不同的风险。一种风险来自于人们滥用人工智能。是的，这构成了大多数风险，而且都是短期风险。另一种风险来自于人工智能变得超级智能，并决定不再需要我们。这真的是一种风险吗？我主要谈论第二种风险，是因为很多人会问：“这真的是一种风险吗？”答案是肯定的。</p><p>First of all, I want to make a distinction between two completely different kinds of risk. There are risks that come from people misusing AI. Yeah. And that's most of the risks and all of the short term risks. And then there are risks that come from AI getting super smart and deciding it doesn't need us. Is that a real risk? And I talk mainly about that second risk because lots of people say, "Is that a real risk?" And yes, it is. </p>
<p>现在我们不知道风险有多大。我们从未经历过这种情况。我们也从未与比我们更聪明的生物打过交道。所以，实际上，关于这种生存威胁的关键在于，我们不知道该如何应对。我们不知道它会是什么样子。任何告诉你他们知道会发生什么以及如何应对的人，都是在胡说八道。所以，我们不知道它取代我们的可能性有多大。</p><p>Now we don't know how much of a risk it is. We've never been in that situation before. We've never had to deal with things smarter than us. So, really, the thing about that existential threat is that we have no idea how to deal with it. We have no idea what it's going to look like. And anybody who tells you they know just what's going to happen and how to deal with it, they're talking nonsense. So, we don't know how to estimate the probabilities it'll replace us. </p>
<p>嗯，有些人说不到1%。我的朋友Yan Lar，我曾经的博士后，认为不，不，不，我们会一直这样——我们建造这些东西。我们会一直掌控一切。我们会把它们建造得听话。而其他人，比如Yudkowsky，则说：“不，不，不。这些东西肯定会把我们消灭掉。如果有人建造了它，它就会把我们全部消灭掉。” 他对此很有信心。我认为这两种观点都有些极端。</p><p>Um, some people say it's like less than 1%. My friend Yan Lar, who was a postdoc with me, thinks no, no, no, we're always going to be—we build these things. We're always going to be in control. We'll build them to be obedient. And other people, like Yudkowsky, say, "No, no, no. These things are going to wipe us out for sure. If anybody builds it, it's going to wipe us all out." And he's confident of that. I think both of those positions are extreme. </p>
<p>很难估计中间的概率。如果你要打赌你的两个朋友中谁会猜对，我根本不知道。所以，如果非要我打赌，我会说概率介于两者之间，我不知道该如何估计中间的概率。我经常说他们有10%到20%的可能性会消灭我们，但这只是我的直觉，基于我们——我们仍在制造这些病毒，而且我们非常聪明。希望是，如果足够多的聪明人用足够的资源进行足够的研究，我们就能找到一种方法来制造它们，这样他们就永远不会想伤害我们了。</p><p>It's very hard to estimate the probabilities in between. If you had to bet on who was right out of your two friends, I simply don't know. So, if I had to bet, I'd say the probabilities are in between, and I don't know where to estimate it in between. I often say a 10 to 20% chance they'll wipe us out, but that's just a gut feeling, based on the idea that we're—we're still making them, and we're pretty ingenious. And the hope is that if enough smart people do enough research with enough resources, we'll figure out a way to build them so they'll never want to harm us. </p>
<p>有时候，如果我们谈论第二条道路，我会想到核弹和原子弹的发明，以及它们之间的比较。这有什么不同？因为原子弹出现了，我想当时很多人都认为我们的日子屈指可数了。是的，我当时就在那里。我们确实如此。是的。但是——但是——但是——我们——我们还在这里。我们还在这里。是的。所以，原子弹实际上只有一个用处，而且它的工作原理非常明显。</p><p>Sometimes, I think if we talk about that second path, sometimes I think about nuclear bombs and the invention of the atomic bomb, and how it compares. How is this different? Because the atomic bomb came along, and I imagine a lot of people at that time thought our days are numbered. Yes, I was there. We did. Yeah. But—but—but what's—what's—we're still here. We're still here. Yes. So, the atomic bomb was really only good for one thing, and it was very obvious how it worked. </p>
<p>即使你没有广岛和长崎的照片，也能清楚地看到那是一颗非常危险的巨大炸弹。人工智能可以造福很多很多领域。它将在医疗保健和教育领域发挥巨大作用，而且，任何需要使用数据的行业，或多或少都能在人工智能的帮助下更好地利用数据。所以，我们不会停止人工智能的发展。你知道，人们会说：“好吧，我们为什么不现在就停止呢？” 我们不会停止它，因为它对很多事情都太有利了。</p><p>Even if you hadn't had the pictures of Hiroshima and Nagasaki, it was obvious that it was a very big bomb that was very dangerous. With AI, it's good for many, many things. It's going to be magnificent in healthcare and education, and more or less any industry that needs to use its data is going to be able to use it better with AI. So, we're not going to stop the development. You know, people say, "Well, why don't we just stop it now?" We're not going to stop it because it's too good for too many things. </p>
<p>此外，我们不会阻止它，因为它对战斗机器人有利，而且所有出售武器的国家都不会想阻止它。就像欧洲的人工智能法规一样，他们有一些关于人工智能的法规，这很好。他们确实有一些法规，但这些法规并非旨在应对大多数威胁。</p><p>Also, we're not going to stop it because it's good for battle robots, and none of the countries that sell weapons are going to want to stop it. Like the European AI regulations, they have some regulations about AI, and it's good. They have some regulations, but they're not designed to deal with most of the threats. </p>
<h2>欧洲人工智能法规</h2><h2>European AI Regulations</h2>
<p>尤其是欧洲人工智能法规中有一条条款规定，这些法规不适用于人工智能的军事用途。因此，各国政府愿意监管公司和个人，却不愿自我监管。他们如此反复无常，在我看来似乎很疯狂。但如果欧洲有法规，而世界其他国家没有，就会造成竞争劣势。</p><p>And in particular, the European AI Regulations have a clause in them that says none of these regulations apply to military uses of AI. So, governments are willing to regulate companies and people, but they're not willing to regulate themselves. It seems pretty crazy to me that they go back and forth, but if Europe has a regulation, but the rest of the world doesn't, that creates a competitive disadvantage. </p>
<p>是的，我们已经看到了这一点。我认为人们没有意识到，当OpenAI在美国发布一个新模型或新软件时，由于欧洲的监管规定，他们目前还不能在欧洲发布。所以，Sam Altman在推特上说：“我们的新AI代理产品已经面向所有人开放，但由于欧洲的监管规定，它目前还不能进入欧洲。” 是的。这给我们带来了什么？生产力劣势。</p><p>Yeah, we're seeing this already. I don't think people realize that when OpenAI releases a new model or a new piece of software in America, they can't release it to Europe yet because of regulations here. So, Sam Altman tweeted saying, "Our new AI agent thing is available to everybody, but it can't come to Europe yet because there are regulations." Yes. What does that give us? A productivity disadvantage. </p>
<p>我们需要的是——我的意思是，在历史的这个时刻，当我们即将创造出比我们自己更聪明的生物时——我们真正需要的是一个能够运作的世界政府，由聪明、有思想的人来管理。而我们现在还没有这样的政府。所以，人人自由吧。</p><p>What we need is—I mean, at this point in history, when we're about to produce things more intelligent than ourselves—what we really need is a kind of world government that works, run by intelligent, thoughtful people. And that's not what we've got. So, free for all. </p>
<p>嗯，我们现在拥有的是一种资本主义，它对我们很有利。它为我们生产了大量的商品和服务。但这些大公司，依法必须努力实现利润最大化，而这并不是开发者们想要的。</p><p>Well, what we've got is sort of capitalism, which has done very nicely by us. It produces lots of goods and services for us. But these big companies, they're legally required to try and maximize profits, and that's not what you want from the people developing this stuff. </p>
<p>那么，我们来谈谈风险。您刚才提到了人为风险，然后还有……我已经区分了这两种风险。我们来谈谈恶意人类行为者利用人工智能造成的所有风险。</p><p>So, let's do the risks then. You talked about there are human risks, and then there are... So, I've distinguished these two kinds of risk. Let's talk about all the risks from bad human actors using AI. </p>
<h2>网络攻击风险</h2><h2>Cyber Attack Risk</h2>
<p>网络攻击确实存在。因此，在2023年至2024年期间，网络攻击数量增长了约12200%。这可能是因为这些大型语言模型使得网络钓鱼攻击变得更加容易。</p><p>There are cyber attacks. So, between 2023 and 2024, they increased by about a factor of 12,200%. And that's probably because these large language models make it much easier to do phishing attacks. </p>
<p>对于那些不了解的人来说，网络钓鱼攻击是指他们给你发信息说：“嗨，我是你的朋友约翰，我被困在萨尔瓦多了。你能帮我汇款吗？”这是一种攻击。但网络钓鱼攻击的真正目的是获取你的登录凭证。现在，有了人工智能，他们可以克隆我的声音、我的形象。他们可以做到这一切。</p><p>And a phishing attack, for anyone that doesn't know, is it's when they send you something saying, "Hi, I'm your friend John, and I'm stuck in El Salvador. Could you just wire this money?" That's one kind of attack. But phishing attacks are really trying to get your login credentials. And now, with AI, they can clone my voice, my image. They can do all that. </p>
<p>我现在很纠结，因为X和Meta上都有很多人工智能骗局。Meta上有一个特别严重的骗局，目前在Instagram和Facebook上都有。这是一个付费广告，他们盗用了我播客里的声音和行为举止，还制作了一个我的新视频，鼓励人们参与这种加密货币庞氏骗局之类的。我们花了好几个星期给Meta发邮件，告诉他们“请把这个删掉”。他们删掉一个，另一个又冒出来。他们删掉一个，另一个又冒出来。所以，就像打地鼠一样。然后就很烦人。最让人心碎的是，收到那些上当受骗、损失了500英镑或500美元的人发来的信息，他们因为你推荐了这个骗局而对你生气。我为他们感到难过。这真的很烦人。</p><p>I'm struggling at the moment because there are a bunch of AI scams on X and also Meta. And there's one in particular on Meta, so Instagram, Facebook, at the moment, which is a paid advertisement where they've taken my voice from the podcast, they've taken my mannerisms, and they've made a new video of me encouraging people to go and take part in this crypto Ponzi scam or whatever. And we've been, you know, we spent weeks and weeks and weeks emailing Meta, telling them, "Please take this down." They take it down, another one pops up. They take that one down, another one pops up. So, it's like whack a mole. And then it's very annoying. The heartbreaking part is getting messages from people who have fallen for the scam and lost £500 or $500, and they are angry with you because you recommended it. I'm sad for them. It's very annoying. </p>
<p>是的。我有一个小版本，就是现在有些人发表论文，我也是其中一位作者。嗯。看起来是为了让他们自己能获得大量引用。</p><p>Yeah. I have a smaller version of that, which is some people now publish papers with me as one of the authors. Mmm. And it looks like it's in order that they can get lots of citations to themselves. </p>
<p>啊，所以网络攻击是一个非常现实的威胁。这类攻击已经呈爆炸式增长。而且，显然人工智能非常有耐心。它们可以浏览一亿行代码，寻找已知的攻击方式。这很容易做到。但它们会变得更有创造力，而且它们可能——有些人相信，我——一些知识渊博的人相信——到2030年，它们可能会创造出前所未有的新型网络攻击。这非常令人担忧，因为它们可以独立思考，并且发现自己可以独立思考。它们可以从比人类以往见过的更多数据中得出新的结论。</p><p>Ah, so cyber attacks are a very real threat. There's been an explosion of those. And these, already, obviously AI is very patient. So they can go through 100 million lines of code looking for known ways of attacking them. That's easy to do. But they're going to get more creative, and they may—some people believe, and I—some people who know a lot believe that maybe by 2030 they'll be creating new kinds of cyber attacks which no person ever thought of. So that's very worrisome, because they can think for themselves and discover they can think for themselves. They can draw new conclusions from much more data than a person ever saw. </p>
<h2>如何保护自己免受网络攻击</h2><h2>How to Protect Yourself From Cyber Attacks</h2>
<p>您是否采取了任何措施来保护自己免受网络攻击？</p><p>Is there anything you're doing to protect yourself from cyber attacks at all? </p>
<p>是的。这是少数几个让我彻底改变工作方式的地方之一，因为我害怕网络攻击。加拿大的银行非常安全。2008年，没有一家加拿大银行濒临破产。所以，它们非常安全，因为它们监管良好，监管得相当好。然而，我认为网络攻击可能会摧毁一家银行。现在，如果你所有的积蓄都是银行持有的银行股票，那么即使银行受到攻击，而它持有你的股票，它们仍然是你的股票。</p><p>Yes. It's one of the few places where I changed what I do radically because I'm scared of cyber attacks. Canadian banks are extremely safe. In 2008, no Canadian banks came anywhere near going bust. So, they're very safe banks because they're well regulated, fairly well regulated. Nevertheless, I think a cyber attack might be able to bring down a bank. Now, if you have all my savings are in shares in banks held by banks, so if the bank gets attacked and it holds your shares, they're still your shares. </p>
<p>所以，我认为除非攻击者出售股票，否则你不会有事，因为银行可以出售股票。如果攻击者出售你的股票，我觉得你就完蛋了。我不知道。我的意思是，也许银行不得不设法赔偿你，但银行现在已经破产了，对吧？所以我担心一家加拿大银行遭受网络攻击，攻击者会出售其持有的股票。所以我把我的钱和我孩子的钱分散到三家银行，因为我相信，如果一家加拿大银行被网络攻击击倒，其他加拿大银行很快就会变得非常谨慎。</p><p>And so, I think you'd be okay unless the attacker sells the shares because the bank can sell the shares. If the attacker sells your shares, I think you're screwed. I don't know. I mean, maybe the bank would have to try and reimburse you, but the bank's bust by now, right? So, so I'm worried about a Canadian bank being taken down by a cyber attack and the attacker selling selling shares that it holds. So I spread my money and my children's money between three banks in the belief that if a cyber attack takes down one Canadian bank, the other Canadian banks will very quickly get very careful. </p>
<p>你有没联网的手机？你知道吗？我正在考虑存储数据之类的东西。你觉得考虑冷存储明智吗？</p><p>And do you have a phone that's not connected to the internet? Do you have any, you know, I'm thinking about storing data and stuff like that. Do you think it's wise to consider having cold storage? </p>
<p>我有一个小磁盘驱动器，我会把笔记本电脑上的所有内容都备份到这个硬盘上。所以，我的笔记本电脑上的所有东西实际上都存储在硬盘上。至少，你知道，如果整个互联网都瘫痪了，我还能感觉到我的笔记本电脑上还有数据，我的信息也还在。好的，接下来就是利用人工智能来制造恶意病毒。</p><p>I have a little disc drive, and I back up my laptop on this hard drive. So I actually have everything on my laptop on a hard drive. At least, you know, if the whole internet went down, I had the sense I still got it on my laptop, and I still got my information. Okay, then the next thing is using AI to create nasty viruses. </p>
<h2>利用人工智能制造病毒</h2><h2>Using AI to Create Viruses</h2>
<p>问题在于，它只需要一个满腹牢骚的疯子。一个懂点分子生物学、懂人工智能的人，一心想毁灭世界。现在用人工智能可以相对便宜地制造新病毒，而且你不需要非常熟练的分子生物学家就能做到。这很可怕。所以，比如说，你可以成立一个小邪教。一个小邪教或许能筹集到几百万美元。</p><p>And the problem with that is that it just requires one crazy guy with a grudge. One guy who knows a little bit of molecular biology, knows a lot about AI, and just wants to destroy the world. You can now create new viruses relatively cheaply using AI. And you don't have to be a very skilled molecular biologist to do it. And that's very scary. So, you could have a small cult, for example. A small cult might be able to raise a few million dollars. </p>
<p>只需几百万美元，他们或许就能设计出一大堆病毒。嗯，我想到的是我们的某些外国对手正在开展政府资助的项目。我的意思是，关于新冠疫情、武汉实验室以及他们正在开展的功能增强研究，有很多讨论。但我想知道，在中国、俄罗斯或伊朗等国，政府是否可以资助一个项目，让一小部分科学家制造一种他们可以制造的病毒，你知道，我认为他们可以。是的。现在，他们会担心遭到报复。他们会担心其他政府对他们采取同样的行动。希望这有助于控制疫情。他们可能还会担心病毒传播到他们的国家。明白吗？</p><p>For a few million dollars, they might be able to design a whole bunch of viruses. Well, I'm thinking about some of our foreign adversaries doing government funded programs. I mean, there was a lot of talk around COVID and the Wuhan laboratory and what they were doing and gain of function research, but I'm wondering if, you know, in China or Russia or Iran or something, the government could fund a program for a small group of scientists to make a virus that they could, you know, I think they could. Yes. Now, they'd be worried about retaliation. They'd be worried about other governments doing the same to them. Hopefully, that would help keep it under control. They might also be worried about the virus spreading to their country. Okay? </p>
<h2>人工智能与腐败选举</h2><h2>AI and Corrupt Elections</h2>
<p>然后是选举舞弊。如果你想用人工智能来舞弊，一个非常有效的做法就是能够进行有针对性的政治广告，因为你对这个人非常了解。所以，任何想用人工智能来舞弊的人都会试图尽可能多地获取所有选民的数据。考虑到这一点，马斯克目前在美国的做法令人担忧，他坚持要获取所有这些被精心隔离的数据。他们声称这样做是为了提高效率，但如果你想破坏下届选举，这正是你想要的。</p><p>Then there's corrupting elections. So, if you wanted to use AI to corrupt elections, a very effective thing is to be able to do targeted political advertisements where you know a lot about the person. So, anybody who wanted to use AI for corrupting elections would try and get as much data as they could about everybody in the electorate. With that in mind, it's a bit worrying what Musk is doing at present in the States, going in and insisting on getting access to all these things that were very carefully siloed. The claim is it's to make things more efficient, but it's exactly what you would want if you intended to corrupt the next election. </p>
<p>你是什​​么意思？</p><p>How do you mean? </p>
<p>因为你掌握了所有人的数据。你知道他们挣多少钱，他们在哪里，你对他们了如指掌。一旦你知道了这些，操纵他们就变得非常容易，因为你可以让人工智能发送一些他们觉得非常有说服力的信息，比如告诉他们不要投票。所以，除了常识之外，我没有其他理由这么认为，但如果从美国政府渠道获取所有这些数据的部分动机是为了破坏选举，我也不会感到惊讶。</p><p>Because you get all this data on the people. You get all this data on people. You know how much they make, where they are, you know everything about them. Once you know that, it's very easy to manipulate them because you can make an AI that can send messages that they'll find very convincing, telling them not to vote, for example. So, I have no reason other than common sense to think this, but I wouldn't be surprised if part of the motivation of getting all this data from American government sources is to corrupt elections. </p>
<p>另一部分原因可能是，这些数据对于大型模型来说非常好，但他必须从政府那里获取这些数据，并将其输入到他的……是的。他们所做的就是关闭许多安全控制措施，解散了一些组织机构来防范这种情况。嗯，这就是在腐蚀选举。好吧，然后像YouTube和Facebook这样的组织就制造了这些回音室效应，向人们展示一些会让他们愤慨的东西。</p><p>Another part might be that it's very nice training data for a big model, but he would have to be taking that data from the government and feeding it into his... Yes. And what they've done is turned off lots of the security controls, got rid of some of the organizations to protect against that. Um, so that's corrupting elections. Okay, then there's creating these echo chambers by organizations like YouTube and Facebook, showing people things that will make them indignant. </p>
<h2>人工智能如何创造回音室</h2><h2>How AI Creates Echo Chambers</h2>
<p>人们喜欢愤慨。愤慨，就像愤怒，或者愤慨是什么意思？感觉，我有点生气，但感觉很正义。好的。比如，如果你给我看一些东西，说特朗普做了一件疯狂的事，这是一段特朗普做这件完全疯狂的事的视频，我会立刻点击它。好的。所以，把我们放在回音室里，分裂我们。是的。这就是YouTube、Facebook和其他网站用来决定接下来要播放什么内容的策略造成的。</p><p>People love to be indignant. Indignant, as in angry, or what does indignant mean? Feeling, I'm sort of angry, but feeling righteous. Okay. So, for example, if you were to show me something that said Trump did this crazy thing, here's a video of Trump doing this completely crazy thing, I would immediately click on it. Okay. So, putting us in echo chambers and dividing us. Yes. And that's the policy that YouTube and Facebook and others use for deciding what to show you next is causing that. </p>
<p>如果他们奉行向你展示均衡内容的政策，就不会获得那么多点击量，也卖不出那么多广告。所以，本质上，他们的盈利动机就是：“给他们看任何能让他们点击的内容。” 而真正让他们点击的内容，往往是越来越极端的内容。这证实了我现有的偏见。这证实了我现有的偏见。所以，你的偏见一直在不断被证实，越来越严重，这意味着你正在远离——也就是说，现在，在美国，有两个社区几乎不互相交流。</p><p>If they had a policy of showing you balanced things, they wouldn't get so many clicks, and they wouldn't be able to sell so many advertisements. And so, it's basically the profit motive is saying, "Show them whatever will make them click." And what'll make them click is things that are more and more extreme. And that confirmed my existing bias. That confirmed my existing bias. So, you're getting your biases confirmed all the time, further and further and further, which means you're driving away—which is, now, there's in the states, there's two communities that don't hardly talk to each other. </p>
<p>我不确定人们是否意识到每次打开应用程序时都会发生这种情况。但如果你使用TikTok、YouTube或这些大型社交网络，正如你所说，算法的设计目的是向你展示更多你上次感兴趣的内容。所以，如果你把这种情况持续10年，它会让你越来越深陷你现有的意识形态或信仰，越来越远离细微差别、常识和平等，这是一件非常了不起的事情。</p><p>I'm not sure people realize that this is actually happening every time they open an app. But if you go on TikTok or YouTube or one of these big social networks, the algorithm, as you said, is designed to show you more of the things that you had interest in last time. So, if you just play that out over 10 years, it's going to drive you further and further and further into whatever ideology or belief you have and further away from nuance and common sense and parity, which is a pretty remarkable thing. </p>
<p>我……我喜欢人们不知道发生了什么。他们只是打开手机，体验一些事情，然后以为这是新闻，或者其他人正在经历的。对。所以，基本上，如果你有一份报纸，每个人都拿到同样的报纸，是的。你会看到各种你没有留意的东西，你会觉得如果它出现在报纸上，那它就是一件重要的事情，一件意义重大的事情。但如果你有自己的新闻推送——我iPhone上的新闻推送——四分之三的故事都是关于人工智能的，我发现很难知道是全世界都在谈论人工智能，还是只有我的新闻推送在谈论人工智能。</p><p>I... I like people don't know it's happening. They just open their phones and experience something and think this is the news or the experience everyone else is having. Right. So, basically, if you have a newspaper, and everybody gets the same newspaper, yeah. You get to see all sorts of things you weren't looking for, and you get a sense that if it's in the newspaper, it's an important thing or a significant thing. But if you have your own news feed—my news feed on my iPhone—three quarters of the stories are about AI, and I find it very hard to know if the whole world's talking about AI all the time or if it's just my news feed. </p>
<p>好的。所以，这把我逼进了回音室，这只会让我们之间的分歧越来越大。我实际上注意到，算法变得越来越……怎么说呢？定制化。人们可能会说：“哦，这太好了。” 但这意味着它们变得更加个性化，这意味着我的现实与你的现实越来越远。是的，这太疯狂了。我们不再拥有共同的现实。我和看BBC、其他BBC新闻的人、读《卫报》和《纽约时报》的人分享现实。我和看福克斯新闻的人几乎没有共同的现实。这相当……相当……令人担忧。是的。</p><p>Okay. So, driving me into my echo chambers, which is going to continue to divide us further and further. I'm actually noticing that the algorithms are becoming even more, what's the word? Tailored. And people might go, "Oh, that's great." But what it means is they're becoming even more personalized, which means that my reality is becoming even further from your reality. Yeah. It's crazy. We don't have a shared reality anymore. I share reality with other people who watch the BBC and other BBC news, and other people who read The Guardian and other people who read The New York Times. I have almost no shared reality with people who watch Fox News. It's pretty... it's pretty... worrisome. Yeah. </p>
<p>这一切的背后，是这些公司只想赚钱，他们会不择手段地赚取更多利润，因为他们不得不这么做。法律上他们有义务这么做。所以，我们几乎不能责怪这些公司，不是吗？如果他们……嗯，资本主义对我们来说做得很好。它创造了很多好东西。是的。但你需要对它进行严格的监管。</p><p>Behind all this is the idea that these companies just want to make profit, and they'll do whatever it takes to make more profit because they have to. They're legally obliged to do that. So, we almost can't blame the company, can we? If they're... well, capitalism's done very well for us. It's produced lots of goodies. Yeah. But you need to have it very well regulated. </p>
<p>所以，你真正想要的是制定规则，这样当一些公司试图赚取尽可能多的利润时，为了赚取利润，他们必须做对一般人有益的事情，而不是做对一般人有害的事情。</p><p>So, what you really want is to have rules so that when some company is trying to make as much profit as possible, in order to make that profit, they have to do things that are good for people in general, not things that are bad for people in general. </p>
<h2>监管新技术</h2><h2>Regulating New Technologies</h2>
<p>所以，一旦公司为了赚取更多利润，开始做一些对社会非常有害的事情，比如向人们展示越来越极端的东西，监管就成了必需品。所以，资本主义需要监管。</p><p>So, once you get to a situation where, in order to make more profit, the company starts doing things that are very bad for society, like showing you things that are more and more extreme, that's what regulations are for. So, you need regulations with capitalism. </p>
<p>现在，企业总是说监管阻碍了我们，降低了我们的效率，这话没错。监管的真正目的就是阻止他们为了盈利而做损害社会的事情。我们需要强有力的监管。</p><p>Now, companies will always say regulations get in the way, make us less efficient, and that's true. The whole point of regulations is to stop them from doing things to make profit that hurt society. And we need strong regulation. </p>
<p>谁来决定它是否会损害社会？因为不幸的是，这是政客的工作。如果政客被公司控制，那就不太好。而且，政客们可能不懂这项技术。你可能已经看过参议院听证会，他们把马克·扎克伯格和那些大型科技公司的首席执行官们赶了出来，这很尴尬，因为他们问了错误的问题。</p><p>Who's going to decide whether it hurts society or not? Because you know that's the job of politicians, unfortunately. If the politicians are owned by the companies, that's not so good. And also, the politicians might not understand the technology. You've probably seen the Senate hearings where they wheel out, you know, Mark Zuckerberg and these big tech CEOs, and it is quite embarrassing because they're asking the wrong questions. </p>
<p>我看过美国教育部长的视频，她谈论如何将人工智能引入课堂，但她以为它叫A1。实际上，她说的是：“我们要让所有孩子都接触A1。” 有一个学校系统将确保一年级，甚至幼儿园的学生，每年都能接受A1教学，从低年级开始。这真是……太棒了。这些人……这些人就是负责人。</p><p>Well, I've seen the video of the US Education Secretary talking about how they're going to get AI in the classrooms, except she thought it was called A1. She's actually there saying, "We're going to have all the kids interacting with A1." There is a school system that's going to start making sure that first graders, or even pre K students, have A1 teaching, you know, every year, starting that far down in the grades. And that's just a... that's a wonderful thing. And these are the people that... these are the people in charge. </p>
<p>归根结底，科技公司才是主导者，因为他们会比各州的政客更聪明。至少几周前，我去的时候，他们就打出一则广告，说不监管人工智能非常重要，因为这会损害我们与中国的竞争。是的。这是一个合理的论点。</p><p>Ultimately, the tech companies are in charge because they will outsmart the politicians in the states. Now, at least a few weeks ago, when I was there, they were running an advertisement about how it was very important not to regulate AI because it would hurt us in the competition with China. Yeah. And that's a plausible argument there. </p>
<h2>法规是否阻碍了我们与中国的竞争？</h2><h2>Are Regulations Holding Us Back From Competing With China?</h2>
<p>是的，会的。但你必须决定：你是否想通过做一些会对社会造成巨大伤害的事情来与中国竞争？你可能不会。我猜他们会说，不仅仅是中国，还有丹麦、澳大利亚、加拿大和英国。他们不那么担心，还有德国。但如果他们用监管来限制自己，如果他们放慢速度，那么创始人、企业家和投资者就会离开。我认为称之为限制自己，是站在一种特定的角度；这种角度认为监管非常有害。</p><p>Yes, it will. But you have to decide: do you want to compete with China by doing things that will do a lot of harm to your society? And you probably don't. I guess they would say that it's not just China, it's Denmark, Australia, Canada, and the UK. They're not so worried about, and Germany. But if they kneecap themselves with regulation, if they slow themselves down, then the founders, the entrepreneurs, and the investors are going to go. I think calling it kneecapping is taking a particular point of view; it's taking the point of view that regulations are sort of very harmful. </p>
<p>你需要做的就是限制大公司，让他们为了盈利，不得不去做对社会有用的事情。谷歌搜索就是一个很好的例子。它不需要监管，因为它只是把信息提供给人们。它很棒。但是，如果你以YouTube为例，它开始向你展示广告，并向你展示越来越极端的内容，那就需要监管了。但正如我们所发现的，我们没有人来监管它。我认为人们很清楚向你展示越来越极端的内容这个问题。这是一个众所周知的问题，政客们也明白这一点。他们只需要采取行动，对其进行监管。</p><p>What you need to do is just constrain the big companies so that in order to make profit, they have to do things that are socially useful. Like Google Search is a great example. That didn't need regulation because it just made information available to people. It was great. But then, if you take YouTube, which starts showing you adverts and showing you more and more extreme things, that needs regulation. But we don't have the people to regulate it as we've identified. I think people know pretty well that particular problem of showing you more and more extreme things. That's a well known problem that the politicians understand. They just need to get on and regulate it. </p>
<p>所以，接下来的重点是，算法会让我们进一步陷入回音室，对吧？接下来是什么？致命的自主武器。</p><p>So, that was the next point, which was that the algorithms are going to drive us further into our echo chambers, right? What's next? Lethal autonomous weapons. </p>
<h2>致命自主武器的威胁</h2><h2>The Threat of Lethal Autonomous Weapons</h2>
<p>致命自主武器。这意味着它们可以杀死你，并自行决定是否杀死你。我想，能够制造这样的武器，正是军工联合体的伟大梦想。</p><p>Lethal autonomous weapons. That means things that can kill you and make their own decision about whether to kill you, which is the great dream, I guess, of the military industrial complex, being able to create such weapons. </p>
<p>所以，最糟糕的是，强大的大国总是有能力入侵较小、较贫穷的国家。他们就是更强大。但如果用真正的士兵来做这件事，你只会得到装在袋子里的尸体，而阵亡士兵的亲属会对此感到不满。所以，你会看到类似越南战争的场景。嗯。最终，国内会有很多抗议。如果回来的不是装在袋子里的尸体，而是死去的机器人，抗议就会少得多，军工联合体也会更乐意，因为机器人价格昂贵。</p><p>So, the worst thing about them is big powerful countries always have the ability to invade smaller, poorer countries. They're just more powerful. But if you do that using actual soldiers, you get bodies coming back in bags, and the relatives of the soldiers who were killed don't like it. So, you get something like Vietnam. Mhm. In the end, there's a lot of protest at home. If instead of bodies coming back in bags, it was dead robots, there'd be much less protest, and the military industrial complex would like it much more because robots are expensive. </p>
<p>假设你拥有某种可能会损坏且更换成本高昂的东西。那就太好了。大国可以更容易地入侵小国，因为他们的士兵不会被杀。这里的风险是这些机器人可能会发生故障，或者它们会更加……不，不。即使这些机器人完全按照制造它们的人的意愿去做，风险也在于此。风险在于，这会导致大国更频繁地入侵小国。</p><p>And suppose you had something that could get killed and was expensive to replace. That would be just great. Big countries can invade small countries much more easily because they don't have their soldiers being killed. And the risk here is that these robots will malfunction, or they'll just be more... no, no. That's even if the robots do exactly what the people who built the robots want them to do. The risk is that it's going to make big countries invade small countries more often. </p>
<p>更常见的是因为他们有能力。是的。而且这并非好事。所以，它减少了战争摩擦，降低了入侵的成本。而且这些机器在战争中也会更智能。所以，它们会……嗯，即使机器本身并不更智能。所以，致命的自主武器，他们现在就能制造出来。而且他们——我认为所有大型国防模型都在忙着制造它们。即使它们不比人类聪明，它们仍然是非常危险、可怕的东西。因为我觉得，你知道，他们只需展示一张图片，就能告诉人们“去抓住这个家伙”。是的。</p><p>More often because they can. Yeah. And it's not a nice thing to do. So, it brings down the friction of war. It brings down the cost of doing an invasion. And these machines will be smarter at warfare as well. So, they'll be... well, even when the machines aren't smarter. So, the lethal autonomous weapons, they can make them now. And they—I think all the big defense models are busy making them. Even if they're not smarter than people, they're still very nasty, scary things. Because I'm thinking that, you know, they could show just a picture, "go get this guy." Yeah. </p>
<p>然后去干掉他发短信的人，还有这只小黄蜂。两天前，我去拜访苏塞克斯的一位朋友，他有一架不到200英镑的无人机，然后无人机升空了。它仔细地看了我一眼，然后跟着我穿过树林。它一直跟着——这架无人机飞起来真是吓人。它就在我身后大约两米的地方。它一直看着我，如果我往那边走，它也会往那边走。</p><p>And go take out anyone he's been texting, and this little wasp. So, two days ago, I was visiting a friend of mine in Sussex who had a drone that cost less than £200, and the drone went up. It took a good look at me, and then it could follow me through the woods. And it followed—it was very spooky having this drone. It was about two meters behind me. It was looking at me, and if I moved over there, it moved over there. </p>
<p>它真的可以追踪我。嗯。只要200英镑，就已经够吓人的了。是啊。而且我猜，就像你说的，我们现在正在进行一场竞赛，看谁能制造出最复杂的自主武器。我经常听说，有些东西会结合起来，然后网络攻击就会释放武器，这种风险是存在的。</p><p>It could just track me. Mhm. For £200, but it was already quite spooky. Yeah. And I imagine there's, as you say, a race going on as we speak to see who can build the most complex autonomous weapons. There is a risk, I often hear, that some of these things will combine, and the cyber attack will release weapons. </p>
<h2>这些人工智能威胁可以结合起来吗？</h2><h2>Can These AI Threats Combine?</h2>
<p>当然。嗯，你可以，你可以通过组合这些其他风险来获得组合风险。嗯。我的意思是，比如说，你可以让一个超级智能AI决定消灭人类，而最显而易见的方法就是制造一种这种可怕的病毒。如果你制造一种传染性极强、致命性极高、传播速度极慢的病毒，那么每个人都会在意识到发生了什么之前就感染它。我的意思是，我认为如果一个超级智能想要消灭我们，它很可能会选择一些不会对它造成影响的生物武器。</p><p>Sure. Um, you can, you can get combinatorily many risks by combining these other risks. Mhm. So, I mean, for example, you could get a superintelligent AI that decides to get rid of people, and the obvious way to do that is just to make one of these nasty viruses. If you made a virus that was very contagious, very lethal, and very slow, everybody would have it before they realized what was happening. I mean, I think if a superintelligence wanted to get rid of us, it will probably go for something biological like that that wouldn't affect it. </p>
<p>你不觉得它很快就会让我们互相反目成仇吗？比如，它可以向美国的核系统发出警告，说俄罗斯正在制造核弹，反之亦然，然后美国就会采取报复行动。是的。我的意思是，我的基本观点是，超级智能有很多方法可以摆脱我们。这不值得猜测。你需要做的是阻止它这么做。这才是我们应该研究的。</p><p>Do you not think it could just very quickly turn us against each other? For example, it could send a warning on the nuclear systems in America that there's a nuclear bomb coming from Russia, or vice versa, and one retaliates. Yeah. I mean, my basic view is there are so many ways in which the superintelligence could get rid of us. It's not worth speculating about. What, what is what you have to do is prevent it ever wanting to. That's what we should be doing research on. </p>
<p>我们根本无法阻止它——它比我们聪明，对吧？如果它想除掉我们，我们根本无法阻止。我们不习惯思考比我们更聪明的事物。如果你想知道当你不是顶尖智力时的生活是什么样的，去问一只鸡吧。没错。今天早上我出门的时候，一直在想我的狗，我的法国斗牛犬巴勃罗。它不知道我要去哪里。它不知道我在做什么，对吧？</p><p>There's no way we're going to prevent it—it's smarter than us, right? There's no way we're going to prevent it from getting rid of us if it wants to. We're not used to thinking about things smarter than us. If you want to know what life's like when you're not the apex intelligence, ask a chicken. Yeah. I was thinking about my dog, Pablo, my French bulldog, this morning as I left home. He has no idea where I'm going. He has no idea what I do, right? </p>
<p>甚至没法跟他说话。是啊。智力差距就是这样。所以，你是说，如果我是我的法国斗牛犬巴勃罗，我就得想办法让我的主人不把我干掉。</p><p>Can't even talk to him. Yeah. And the intelligence gap will be like that. So, you're telling me that if I'm Pablo, my French bulldog, I need to figure out a way to make my owner not wipe me out. </p>
<h2>限制人工智能接管</h2><h2>Restricting AI From Taking Over</h2>
<p>是的。我们举个例子，那就是母亲和婴儿。进化让母亲比婴儿更聪明，但婴儿才是掌控者。他们之所以掌控一切，是因为母亲无法承受各种激素和其他因素，但母亲无法忍受婴儿哭闹的声音。并非所有母亲都是如此。并非所有母亲都是如此。然后婴儿就失去了掌控力，然后糟糕的事情就发生了。我们需要想办法阻止他们掌控一切。</p><p>Yeah. So, we have one example of that, which is mothers and babies. Evolution put a lot of work into mothers being smarter than babies, but babies are in control. And they're in control because the mother just can't bear lots of hormones and things, but the mother just can't bear the sound of the baby crying. Not all mothers. Not all mothers. And then the baby's not in control, and then bad things happen. We somehow need to figure out how to make them not want to take over. </p>
<p>我经常用的比喻是：抛开智力，想想体力。假设你有一只可爱的小老虎幼崽。它比猫大一点。它真的很可爱，让人想抱抱，看着也很有趣。不过你最好确保它长大后永远不会想杀你。因为如果它真的想杀你，你几秒钟就会死。而你说我们现在的人工智能就是目标幼崽。</p><p>The analogy I often use is: forget about intelligence, think about physical strength. Suppose you have a nice little tiger cub. It's sort of a bit bigger than a cat. It's really cute. It's very cuddly, very interesting to watch. Except that you better be sure that when it grows up, it never wants to kill you. Because if it ever wanted to kill you, you'd be dead in a few seconds. And you're saying the AI we have now is the target cub. </p>
<p>是的。它正在成长。是的。所以，我们需要像训练幼崽一样训练它。老虎有很多与生俱来的本能。所以，你知道，当它长大后，养在身边并不安全。但是狮子，那些把狮子当作宠物的人，是的。有时狮子对它的创造者很亲热，但对其他动物却不然。是的。我们不知道这些人工智能——我们根本不知道我们能否阻止它们接管我们，阻止它们伤害我们。</p><p>Yep. And it's growing up. Yep. So, we need to train it as it's when it's a baby. Well, now a tiger has lots of instincts built in. So, you know, when it grows up, it's not a safe thing to have around. But lions, people that have lions as pets, yes. Sometimes the lion is affectionate to its creator but not to others. Yes. And we don't know whether these AIs—we simply don't know whether we can make them not want to take over and not want to hurt us. </p>
<p>你认为我们能做到吗？你认为训练超级智能有可能吗？我认为目前还不清楚我们能否做到。所以，我觉得这或许毫无希望。但我也认为我们或许能够做到。如果人类因为我们懒得尝试而灭绝，那也未免太疯狂了。如果真的有这种可能，你对自己的毕生事业有何感想？</p><p>Do you think we can? Do you think it's possible to train superintelligence? I don't think it's clear that we can. So, I think it might be hopeless. But I also think we might be able to. And it'd be sort of crazy if people went extinct because we couldn't be bothered to try. If that's even a possibility, how do you feel about your life's work? </p>
<h2>在人工智能风险中反思你的人生工作</h2><h2>Reflecting on Your Life’s Work Amid AI Risks</h2>
<p>因为你……是的。嗯，这多少有点让人放松了，不是吗？我的意思是，这个想法在医疗保健和教育领域都会很棒，太棒了。我的意思是，它会大大提高呼叫中心的效率，尽管有人会担心现在从事这项工作的人会做什么。这让我很难过。我对40年前开发人工智能并不感到特别内疚，因为那时我们根本不知道这件事会发生得这么快。</p><p>Because you were... yeah. Um, it sort of takes the edge off it, doesn't it? I mean, the idea is going to be wonderful in healthcare and wonderful in education and wonderful. I mean, it's going to make call centers much more efficient, though one worries a bit about what the people who are doing that job now do. It makes me sad. I don't feel particularly guilty about developing AI like 40 years ago because at that time we had no idea that this stuff was going to happen this fast. </p>
<p>我们以为我们有充足的时间来担心这些事情。当你无法让人工智能做太多事情时，你会想让它多做一点。你不用担心这个愚蠢的小东西会取代人类。你只是希望它能多做一点人类能做的事情。我并非故意去做一些担心这会把我们全部消灭的事情，但我还是会去做。</p><p>We thought we had plenty of time to worry about things like that. They—when you—when you can't get the AI to do much, you want to get it to do a little bit more. You don't worry about this stupid little thing is going to take over from people. You just want it to be able to do a little bit more of the things people can do. It's not like I knowingly did something thinking this might wipe us all out, but I'm going to do it anyway. </p>
<p>嗯。但令人难过的是，它不会带来什么好处。所以，我觉得我现在有责任谈论其中的风险。如果可以往前推，往后推30到50年，你会发现它导致了人类的灭绝，如果最终真的发生了那样的后果，那么，如果真的导致了人类的灭绝，我会借此告诉人们，告诉他们的政府，我们真的必须努力控制这些事情。我认为我们需要人们告诉政府，政府必须迫使企业投入资源来保障安全，而他们目前并没有做太多，因为这样做无法盈利。</p><p>Mhm. But it is a bit sad that it's not just going to be something for good. So, I feel I have a duty now to talk about the risks. And if you could play it forward and you could go forward 30, 50 years and you found out that it led to the extinction of humanity, and if that does end up being the outcome, well, if you played it forward and it led to the extinction of humanity, I would use that to tell people to tell their governments that we really have to work on how we're going to keep this stuff under control. I think we need people to tell governments that governments have to force the companies to use their resources to work on safety, and they're not doing much of that because you don't make profits that way. </p>
<h2>学生因安全问题离开 OpenAI</h2><h2>Student Leaving OpenAI Over Safety Concerns</h2>
<p>是的，我们之前提到过你的一个学生，Ilya。Ilya 离开了 OpenAI。是的。关于他离开 OpenAI 是出于安全方面的考虑，有很多讨论。是的。他后来创办了一家 AI 安全公司。是的。你认为他离开的原因是什么？</p><p>One of your students, we talked about earlier, Ilya, yep. Ilya left OpenAI. Yep. And there was lots of conversation around the fact that he left because he had safety concerns. Yes. And he's gone on to set up an AI safety company. Yes. Why do you think he left? </p>
<p>我认为他离开是因为他担心安全问题。真的吗？我仍然会时不时和他一起吃午饭。他的父母住在多伦多。他来多伦多的时候，我们会一起吃午饭。他没有跟我谈论OpenAI发生的事情，所以我对此没有内部消息。但我很了解他，他确实关心安全问题。所以，我认为他离开的原因在于他是公司最顶尖的人物之一。我的意思是，他可能是ChatGPT早期版本（比如GPT 2）开发背后最重要的人物。他在GPT 2的开发中发挥了非常重要的作用。你认识他，所以你了解他的性格。是的，他有良好的道德准则。他不像马斯克那样没有道德准则。萨姆·奥特曼有良好的道德准则吗？我们拭目以待。我不认识萨姆，所以我不想对此发表评论。</p><p>I think he left because he had safety concerns. Really? He—I still have lunch with him from time to time. His parents live in Toronto. When he comes to Toronto, we have lunch together. He doesn't talk to me about what went on at OpenAI, so I have no inside information about that. But I know him very well, and he is genuinely concerned with safety. So, I think that's why he left because he was one of the top people. I mean, he was—he was probably the most important person behind the development of early versions of ChatGPT, like GPT 2. He was very important in the development of that. You know him personally, so you know his character. Yes, he has a good moral compass. He's not like someone like Musk, who has no moral compass. Does Sam Altman have a good moral compass? We'll see. I don't know Sam, so I don't want to comment on that. </p>
<p>但就你所见，你担心他们采取的行动吗？因为如果你认识伊利亚，而且他是个好人，而他离开了，你就能有所了解。是的。这会让你有理由相信那里有问题。如果你看看萨姆几年前的言论，他在一次采访中兴高采烈地说：“这些东西可能会把我们都杀了。” 虽然他并非原话，但大致意思就是这样。现在他说：“你不用太担心。” 我怀疑他这么做并非出于对真相的追求，而是为了金钱。是金钱还是权力？</p><p>But from what you've seen, are you concerned about the actions that they've taken? Because if you know Ilya, and Ilya's a good guy, and he's left, that would give you some insight. Yes. It would give you some reason to believe that there's a problem there. And if you look at Sam's statements some years ago, he sort of happily said in one interview, "This stuff will probably kill us all." That's not exactly what he said, but that's what it amounted to. Now he's saying, "You don't need to worry too much about it." And I suspect that's not driven by seeking after the truth. That's driven by seeking after money. Is it money or is it power? </p>
<p>是的。我不应该说是钱。钱和权力是两种因素的结合。是的，好吧。我猜金钱是权力的象征。但我——我有个亿万富翁朋友，他就在那个圈子里。有一天我去他家吃午饭。他认识很多人工智能领域的人，打造着世界上最大的人工智能公司。他在伦敦的厨房餐桌上给了我一个警告。他让我深入了解了这些人的私人谈话，不是他们接受媒体采访时谈论安全之类的话题，而是这些人对未来发展的看法，以及他们认为未来会发生什么。</p><p>Yeah. I shouldn't have said money. It's some combination of those. Yes. Okay. I guess money is a proxy for power. But I am—I've got a friend who's a billionaire, and he is in those circles. And when I went to his house and had lunch with him one day. He knows lots of people in AI, building the biggest AI companies in the world. And he gave me a cautionary warning across his kitchen table in London. Where he gave me an insight into the private conversations these people have, not the media interviews they do where they talk about safety and all these things, but actually what some of these individuals think is going to happen, and what they think is going to happen. </p>
<p>他们公开说的可不是这样。你知道，有一个人，我不应该透露他的名字，却领导着世界上最大的人工智能公司之一。他告诉我，他非常了解这个人，他私下里认为我们正走向一个反乌托邦的世界，我们拥有大量的空闲时间，不再工作。而这个人并不真正关心这会对世界造成什么危害。而我指的这个人，正在打造世界上最大的人工智能公司之一。</p><p>It's not what they say publicly. You know, one person, whom I shouldn't name, who is leading one of the biggest AI companies in the world. He told me that he knows this person very well, and he privately thinks that we're heading towards this kind of dystopian world where we have just huge amounts of free time. We don't work anymore. And this person doesn't really care about the harm that it's going to have on the world. And this person, whom I'm referring to, is building one of the biggest AI companies in the world. </p>
<p>然后我在网上看了这个人的采访，想弄清楚是那三个人中的哪一个。是的。嗯，就是那三个人中的一个。好的。我在网上看了这个人的采访，回想起我那位亿万富翁朋友跟我的对话，他认识他。我心想：“妈的，这家伙公开撒谎。” 就好像他没有对世界说实话。这让我有点不安。这也是我在这个播客里讨论这么多人工智能话题的原因之一，因为我觉得，我不知道他们是不是——我觉得他们——有些人对权力有点施虐。</p><p>And I then watch this person's interviews online, trying to figure out which of three people it is. Yeah. Well, it's one of those three people. Okay. And I watch this person's interviews online, and I reflect on a conversation that my billionaire friend had with me, who knows him. And I go, "Fucking hell, this guy's lying publicly." Like, he's not telling the truth to the world. And that's haunted me a little bit. It's part of the reason I have so many conversations around AI in this podcast, because I'm like, I don't know if they're—I think they're—some of them are a little bit sadistic about power. </p>
<p>我觉得他们喜欢那种改变世界的想法，喜欢那种从根本上改变世界的想法。我觉得马斯克显然就是这样，对吧？他是个非常复杂的人物，我真不知道该如何定位他。嗯，他做过一些非常了不起的事情，比如推广电动汽车。那是一件非常棒的事情。是啊。他关于自动驾驶的一些说法有点夸张，但他做的那件事确实非常有益。</p><p>I think they like the idea that they will change the world, that they will be the one that fundamentally shifts the world. I think Musk is clearly like that, right? He's such a complex character that I don't really know how to place Musk. Um, he's done some really good things, like pushing electric cars. That was a really good thing to do. Yeah. Some of the things he said about self driving were a bit exaggerated, but he—that was a really useful thing he did. </p>
<p>在乌克兰与俄罗斯的战争期间，为乌克兰人提供通讯。真是令人震惊。嗯，他做的这事儿真好。类似的事儿还有很多。嗯，但他也做过一些非常坏的事。所以，回到破坏的可能性和这些大公司的动机这个问题上，您是否希望能够采取一些措施来减缓人工智能的发展速度和加速？</p><p>Giving the Ukrainians communication during the war with Russia. Astounding. Um, that was a really good thing he did. There's a bunch of things like that. Um, but he's also done some very bad things. So, coming back to this point of the possibility of destruction and the motives of these big companies, are you at all hopeful that anything can be done to slow down the pace and acceleration of AI? </p>
<h2>您对人工智能的未来充满希望吗？</h2><h2>Are You Hopeful About the Future of AI?</h2>
<p>好的，有两个问题。一是，你能放慢速度吗？是的。二是，你能保证最终安全吗？它不会把我们全部消灭。我不相信我们会放慢速度。是的。我不相信我们会放慢速度的原因是，国家之间以及国家内部企业之间存在竞争，所有这些都使它发展得越来越快。如果美国放慢了速度，中国也不会放慢速度。</p><p>Okay, there are two issues. One is, can you slow it down? Yeah. And the other is, can you make it so it will be safe in the end? It won't wipe us all out. I don't believe we're going to slow it down. Yeah. And the reason I don't believe we're going to slow it down is because there's competition between countries and competition between companies within a country, and all of that is making it go faster and faster. And if the US slowed it down, China wouldn't slow it down. </p>
<p>[杰弗里·辛顿] 认为人工智能有可能实现安全吗？我认为他认为是的。他不愿告诉我他的秘密来源是什么。我不确定有多少人知道他的秘密来源。我认为很多投资者都不知道他的秘密来源是什么，但他们还是给了他数十亿美元，因为他们对他很有信心，这并非愚蠢。我的意思是，他在 AlexNet 中发挥了重要作用，这使得物体识别工作顺利进行。他是 GPT 2 等项目背后的主力，而 GPT 2 又催生了 ChatGPT。所以我认为对他抱有很大的信心是一个非常合理的决定。</p><p>Does [Geoffrey Hinton] think it's possible to make AI safe? I think he does. He won't tell me what his secret source is. I'm not sure how many people know what his secret source is. I think a lot of the investors don't know what his secret source is, but they've given him billions of dollars anyway because they have so much faith in him, which isn't foolish. I mean, he was very important in AlexNet, which got object recognition working well. He was the main force behind things like GPT 2, which then led to ChatGPT. So I think having a lot of faith in him is a very reasonable decision. </p>
<p>这位GPT 2的开发者和幕后主力，引领了这场革命，却因为安全原因离开了公司，这让人感到有些难以忘怀。他知道一些我不知道的事情，关于接下来会发生什么。</p><p>There's something quite haunting about the guy that made and was the main force behind GPT 2, which led to this whole revolution, leaving the company because of safety reasons. He knows something that I don't know about what might happen next. </p>
<p>嗯，公司……我不知道具体细节，但我相当肯定公司曾表示会将相当一部分资源（计算时间）用于安全研究，后来又减少了这部分资源。我想这就是发生的事情之一。是的，这件事已经公开报道了。是的。</p><p>Well, the company had—I don't know the precise details, but I'm fairly sure the company had indicated that it would use a significant fraction of its resources, of compute time, for doing safety research, and then it reduced that fraction. I think that's one of the things that happened. Yeah, that was reported publicly. Yes. </p>
<h2>人工智能引发的失业威胁</h2><h2>The Threat of AI-Induced Joblessness</h2>
<p>是的。我们已经讨论过风险框架中关于自主武器的部分了。对。下一个是失业。</p><p>Yeah. We've gotten to the autonomous weapons part of the risk framework. Right. So the next one is joblessness. </p>
<p>过去，新技术的出现并没有导致失业，反而创造了新的工作岗位。人们常用的典型例子就是自动柜员机。自动柜员机出现的时候，很多银行柜员并没有失业，只是他们有机会去做更有趣的事情。但在这里，我认为这更像是工业革命时期机器出现的时候。现在你不可能再找到挖沟的工作了，因为机器挖沟比你厉害得多。</p><p>In the past, new technologies have come in which didn't lead to joblessness. New jobs were created. So the classic example people use is automatic teller machines. When automatic teller machines came in, a lot of bank tellers didn't lose their jobs. They just got to do more interesting things. But here, I think this is more like when they got machines in the industrial revolution. And you can't have a job digging ditches now because a machine can dig ditches much better than you can. </p>
<p>我认为，就日常的脑力劳动而言，人工智能终将取代所有人。现在，使用人工智能辅助的人可能会减少。也就是说，一个人和一个人工智能助手的组合，现在就能完成以前10个人才能完成的工作。不过，人们说它会创造新的就业机会，所以我们不会有事。是的。其他技术也曾出现过这种情况，但人工智能是一种截然不同的技术。</p><p>And I think for mundane intellectual labor, AI is just going to replace everybody. Now, it will may well be in the form of you have fewer people using AI assistance. So it's a combination of a person and an AI assistant are now doing the work that 10 people could do previously. People say that it will create new jobs though, so we'll be fine. Yes. And that's been the case for other technologies, but this is a very different kind of technology. </p>
<p>如果人工智能能完成所有日常的人类脑力劳动，那它又能创造哪些新的工作呢？你必须非常熟练才能胜任它无法胜任的工作。所以我认为他们错了。你可以尝试从其他已经出现的技术中总结出一些规律，比如计算机或自动遥控机器，但我认为这是不同的。人们常说，人工智能不会抢走你的工作，而是人类使用人工智能会抢走你的工作。是的，我认为这是对的。但对很多工作来说，这意味着需要更少的人手。</p><p>If it can do all mundane human intellectual labor, then what new jobs is it going to create? You'd have to be very skilled to have a job that it couldn't just do. So I don't think they're right. I think you can try and generalize from other technologies that have come in, like computers or automatic telemachines, but I think this is different. People use this phrase. They say AI won't take your job. A human using AI will take your job. Yes, I think that's true. But for many jobs, that'll mean you need far fewer people. </p>
<p>我的侄女负责回复医疗服务机构的投诉信。以前她要花25分钟。她会阅读投诉，思考如何回复，然后写信。现在，她只需将信扫描到聊天机器人中，机器人就会自动写信。她只需检查一下。偶尔，她会告诉机器人进行一些修改。整个过程只需要五分钟。这意味着她可以回复五倍多的信件，也意味着他们需要的人手减少了五倍，所以她可以完成以前需要五个人才能完成的工作。现在，这意味着他们需要更少的人手。</p><p>My niece answers letters of complaint to a health service. It used to take her 25 minutes. She'd read the complaint, and she'd think how to reply, and she'd write a letter. And now she just scans it into a chatbot, and it writes the letter. She just checks the letter. Occasionally, she tells it to revise it in some ways. The whole process takes her five minutes. That means she can answer five times as many letters, and that means they need five times fewer of her, so she can do the job that five of her used to do. Now, that will mean they need less people. </p>
<p>在其他行业，比如医疗保健行业，他们的弹性要大得多。所以，如果你能让医生的效率提高五倍，我们就能以同样的价格享受五倍的医疗服务，那就太好了。人们所能承受的医疗服务几乎没有上限。只要不花钱，他们总是想要更多的医疗服务。</p><p>In other jobs, like in healthcare, they're much more elastic. So, if you could make doctors five times as efficient, we could all have five times as much healthcare for the same price, and that would be great. There's almost no limit to how much healthcare people can absorb. They always want more healthcare if there's no cost to it. </p>
<p>有些工作可以通过人工智能助手大幅提高效率，而且不会减少员工数量，因为这样可以完成更多工作。但我认为大多数工作并非如此。</p><p>There are jobs where you can make a person with an AI assistant much more efficient, and you won't lead to less people because you'll just have much more of that being done. But most jobs, I think, are not like that. </p>
<h2>如果肌肉和智力被取代，还剩下什么？</h2><h2>If Muscles and Intelligence Are Replaced, What’s Left?</h2>
<p>我认为工业革命在取代肌肉方面发挥了作用，对吗？是的，没错。而这场人工智能革命取代了智力——大脑。是的。所以，平凡的脑力劳动就像拥有强壮的肌肉，现在已经不值钱了。所以，肌肉已经被取代了。现在，我们的智力正在被取代。是的。</p><p>Am I right in thinking the sort of industrial revolution played a role in replacing muscles? Yes. Exactly. And this revolution in AI replaces intelligence—the brain. Yeah. So, so mundane intellectual labor is like having strong muscles, and it's not worth much anymore. So, muscles have been replaced. Now, our intelligence is being replaced. Yeah. </p>
<p>那么，还剩下什么呢？或许暂时会有一些创造力，但超级智能的理念就是什么都不会留下。嗯，这些东西会在所有方面都比我们强。那么，在这样的世界里，我们最终会做什么呢？</p><p>So, what remains? Maybe for a while some kinds of creativity, but the whole idea of superintelligence is that nothing remains. Um, these things will get to be better than us at everything. So, what do we end up doing in such a world? </p>
<p>嗯，如果它们对我们有用，我们最终就能不费吹灰之力就获得大量的商品和服务。好吧。这听起来很诱人，也很美好，但我也说不准。为人类创造越来越多的便利，如果发展得不好，就会带来警示。是的。我们需要弄清楚，我们能否让这一切顺利进行。</p><p>Well, if they work for us, we end up getting lots of goods and services for not much effort. Okay. But that sounds tempting and nice, but I don't know. There's a cautionary tale in creating more and more ease for humans—in it going badly. Yes. And we need to figure out if we can make it go well. </p>
<p>所以，理想的情况是，想象一下一家公司，CEO 非常愚蠢，可能是前任 CEO 的儿子。他有一个非常聪明的行政助理，他会说：“我觉得我们应该这么做。” 行政助理会把一切都安排妥当。CEO 感觉很棒。他没有意识到自己实际上并没有掌控全局。从某种意义上说，他掌控着全局。他为公司提出建议。行政助理只是让一切运转起来。一切都很顺利。这是好的情景，也是坏的情景。坏的情景是：她会想：“我们为什么需要他，嗯？” 我的意思是，在一个拥有超级智能的世界里，你不会相信它离我们那么遥远。是的，我认为它可能不会那么遥远。这很难预测，但我认为我们可能在 20 年甚至更短的时间内就能实现它。</p><p>So, the nice scenario is imagine a company with a CEO who is very dumb, probably the son of the former CEO. And he has an executive assistant who's very smart, and he says, "I think we should do this." And the executive assistant makes it all work. The CEO feels great. He doesn't understand that he's not really in control. And in some sense, he is in control. He suggests what the company should do. She just makes it all work. Everything's great. That's the good scenario and the bad scenario. The bad scenario: she thinks, "Why do we need him, yeah?" I mean, in a world where we have superintelligence, which you don't believe is that far away. Yeah, I think it might not be that far away. It's very hard to predict, but I think we might get it in like 20 years or even less. </p>
<h2>广告</h2><h2>Ads</h2>
<p>因为女朋友，我做了人生中最大的一笔公司投资。一天晚上我回到家，发现我亲爱的女朋友凌晨一点就醒了，她正焦急地努力为自己的事业打造自己的网店。就在那一刻，我想起一封来自John的邮件，他是StanStore的创始人，StanStore是我们的新赞助商，也是我投入巨资的一家公司。StanStore通过一个简单易用的、可定制的个人简介链接系统，帮助创作者销售数字产品、课程、辅导和会员资格。它处理所有事务：支付、预订、邮件、社区互动，甚至与Shopify的链接。</p><p>I made the biggest investment I've ever made in a company because of my girlfriend. I came home one night, and my lovely girlfriend was up at 1:00 a.m. in the morning, pulling her hair out as she tried to piece together her own online store for her business. And in that moment, I remembered an email I'd had from a guy called John, the founder of StanStore, our new sponsor, and a company I've invested incredibly heavily in. StanStore helps creators to sell digital products, courses, coaching, and memberships all through a simple, customizable link in bio system. And it handles everything: payments, bookings, emails, community engagement, and even links with Shopify. </p>
<p>我对此深信不疑，因此我将发起一项 Stan 挑战。作为挑战的一部分，我将向你们中的一位捐赠 10 万美元。如果您想参与这项挑战，如果您想将自己的知识货币化，请访问 stephenbartlet.stanstore.com 进行注册。使用该链接，您还将获得 StanStore 的 30 天免费试用期。坦白说，您的下一步行动可能会改变一切。</p><p>I believe in it so much that I'm going to launch a Stan Challenge. And as part of this challenge, I'm going to give away $100,000 to one of you. If you want to take part in this challenge, if you want to monetize the knowledge that you have, visit stephenbartlet.stanstore.com to sign up. And you'll also get an extended 30 day free trial of StanStore if you use that link. Your next move could quite frankly change everything. </p>
<p>因为我在这期播客里谈到了生酮和酮体，一个叫 KetoneIQ 的品牌给我寄了个小产品。我到办公室的时候，它就放在我的桌子上。我把它捡了起来。它在我的桌子上放了好几个星期。后来有一天，我尝试了一下，说实话，从那以后我就再也没有后悔过。现在，我环游世界，走到哪儿都带着它。它就在我的酒店房间里。我的团队也会把它放在那里。</p><p>Because I talked about ketosis on this podcast, and ketones, a brand called KetoneIQ sent me their little product. It was on my desk when I got to the office. I picked it up. It sat on my desk for a couple of weeks. Then, one day, I tried it, and honestly, I have not looked back ever since. I now have this everywhere I go when I travel all around the world. It's in my hotel room. My team will put it there. </p>
<p>今天刚录制完播客节目之前，我喝了 KetoneIQ。就像我每次爱上一款产品时一样，我打电话给 CEO，问他们能否投资几百万英镑。所以，我现在不仅是这家公司的投资者，也是他们的品牌赞助商。喝了 KetoneIQ 之后，我发现很容易就能全身心投入到工作中。</p><p>Before I did the podcast recording today that I've just finished, I had a shot of KetoneIQ. And as is always the case when I fall in love with a product, I called the CEO and asked if I could invest a couple of million quid into their company. So, I'm now an investor in the company as well as them being a brand sponsor. I find it so easy to drop into deep, focused work when I've had one of these. </p>
<p>我希望你尝试一下，看看它对你、你的注意力、你的效率和你的耐力有什么影响。所以，如果你想今天就尝试一下，请访问 ketone.com/stephven，即可享受七折订阅优惠。此外，第二次购买还会获赠一份免费礼品。网址是 ketone.com/stephven。我为你感到兴奋。我真的很期待，那么，我们现在拥有的和超级智能有什么区别呢？</p><p>I would love you to try one and see the impact it has on you, your focus, your productivity, and your endurance. So, if you want to try it today, visit ketone.com/stephven for 30% off your subscription. Plus, you'll receive a free gift with your second shipment. That's ketone.com/stephven. I'm excited for you. I am so, what's the difference between what we have now and superintelligence? </p>
<h2>当前人工智能与超级智能的区别</h2><h2>Difference Between Current AI and Superintelligence</h2>
<p>因为当我使用 ChatGPT 或 Gemini 之类的程序时，它们对我来说似乎非常智能。好吧，所以人工智能在很多方面已经比我们强了，尤其是在国际象棋等领域。是的。人工智能比我们强太多了，以至于人类再也无法击败那些程序。也许偶尔能赢，但基本上，它们再也无法与人类相比了。显然，就围棋的知识量而言，情况也是如此。嗯，像 GPT 4 这样的东西，它知道的知识比我们多几千倍。</p><p>Because it seems really intelligent to me when I use like ChatGPT or Gemini. Okay, so it's already AI is already better than us at a lot of things, in particular areas like chess, for example. Yeah. AI is so much better than us that people will never beat those programs again. Maybe the occasional win, but basically, they'll never be comparable again. Obviously, the same in Go, in terms of the amount of knowledge they have. Um, something like GPT 4 knows thousands of times more than you do. </p>
<p>在某些领域，你的知识比它强，而在几乎所有领域，它都比你懂得多。我在哪些方面比它强呢？可能是在面试CEO方面。你可能在这方面更擅长。你在这方面经验丰富。你是个优秀的面试官。你对它了解很多。如果你尝试用GPT 4面试CEO，它的表现可能会更差。好的。我正在思考我是否同意这个说法。</p><p>There are a few areas in which your knowledge is better than its, and in almost all areas, it just knows more than you do. What areas am I better than it? Probably in interviewing CEOs. You're probably better at that. You've got a lot of experience at it. You're a good interviewer. You know a lot about it. If you tried, if you got GPT 4 to interview a CEO, it would probably do a worse job. Okay. I'm trying to think if I agree with that statement. </p>
<p>呃，GPT 4，我想肯定是的。是的。嗯，但是我，我想你可以，但可能很快就会有，是的。我想你可以训练一个，让它学习我如何提问以及我做什么。当然。如果你采用一个通用的基础模型，然后不仅针对你自己，还针对你能找到的所有面试官，尤其是你，对它进行训练，你可能会发现它很擅长做你的工作，但可能暂时还不如你。好的。</p><p>Uh, GPT 4, I think, for sure. Yeah. Um, but I, but I guess you could, but it may not be long before, yeah. I guess you could train one on how I ask questions and what I do. Sure. And if you took a general purpose sort of foundation model and then you trained it up on not just you, but every interviewer you could find doing interviews like this, but especially you, you'll probably get it to be quite good at doing your job, but probably not as good as you for a while. Okay. </p>
<p>所以，还剩下几个领域，然后超级智能就变成了在所有事情上都比我们强。当它比你聪明得多，几乎所有事情都比你强的时候。是的。你说这可能还需要十年左右的时间。是的，有可能。甚至可能更近。有些人认为它更近，也可能更远。可能还要50年。这仍然是一种可能性。也许用人类数据进行训练会以某种方式限制你的智能，使它不会比人类聪明多少。我猜10到20年后我们就会拥有超级智能。</p><p>So, there are a few areas left, and then superintelligence becomes when it's better than us at all things. When it's much smarter than you and almost all things are better than you. Yeah. And you say that this might be a decade away or so. Yeah. It might be. It might be even closer. Some people think it's even closer and might well be much further. It might be 50 years away. That's still a possibility. It might be that somehow training on human data limits you to not being much smarter than humans. My guess is between 10 and 20 years we'll have superintelligence. </p>
<p>关于失业这个问题，我一直在思考，尤其是因为我开始研究人工智能代理。今天早上我们在播客上发布了一期节目，我们和一家大型人工智能代理公司的CEO以及其他几个人就人工智能代理进行了一场辩论。那是我第一次——也是我又一次灵光乍现，对未来充满憧憬。在采访中，我让这位代理给我们所有人点饮料，5分钟后，你就看到他端着饮料来了。</p><p>On this point of joblessness, it's something that I've been thinking a lot about, in particular, because I started messing around with AI agents, and we released an episode on the podcast actually this morning where we had a debate about AI agents with some CEO of a big AI agent company and a few other people. And it was the first moment where I had—it was another moment where I had a eureka moment about what the future might look like when I was able, in the interview, to tell this agent to order all of us drinks, and then 5 minutes later in the interview, you see the guy show up with the drinks. </p>
<p>我什么都没动。我只是让它帮我们点饮料送到工作室。你根本不知道你平时从谁那里买饮料。它从网上知道了。是的，它知道了，因为它登录了 Uber Eats。我想它有我的数据吧。然后——我们实时把数据放到屏幕上，这样家里的每个人都能看到客服人员通过互联网挑选饮料、给司机加小费、输入我的地址、输入我的信用卡信息，然后饮料就送到了。</p><p>And I didn't touch anything. I just told it to order us drinks to the studio. And you didn't know about who you normally get your drinks from. It figured that out from the web. Yeah, figured it out because it went on Uber Eats. It has my data, I guess. And it—we put it on the screen in real time so everyone at home could see the agent going through the internet, picking the drinks, adding a tip for the driver, putting my address in, putting my credit card details in, and then the next thing you see is the drinks show up. </p>
<p>那是一个瞬间。另一个瞬间是我使用了一个叫 Replet 的工具，我只需要告诉智能体我想要什么就能构建软件。是的。这很神奇，对吧？既神奇又可怕。是的。因为——如果它能构建这样的软件，对吧？是的。记住，人工智能在训练时使用的是代码，如果它可以修改自己的代码，那就很可怕了，对吧？因为它可以修改。它可以以一种我们无法改变的方式改变自己。我们无法改变我们与生俱来的天赋，对吧？它自己没有什么是它无法改变的。</p><p>So that was one moment. And then the other moment was when I used a tool called Replet, and I built software by just telling the agent what I wanted. Yes. It's amazing, right? It's amazing and terrifying at the same time. Yes. Because—and if it can build software like that, right? Yeah. Remember that the AI, when it's training, is using code, and if it can modify its own code, then it gets quite scary, right? Because it can modify. It can change itself in a way we can't change ourselves. We can't change our innate endowment, right? There's nothing about itself that it couldn't change. </p>
<p>说到失业，你有孩子，我也有。他们也有孩子。不，他们没有孩子，还没有孙子孙女。在超级智能的世界里，你会如何看待人们的职业前景？我们应该思考什么？</p><p>On this point of joblessness, you have kids. I do. And they have kids. No, they don't have kids. No grandkids yet. What would you be saying to people about their career prospects in a world of superintelligence? What should we be thinking about? </p>
<p>嗯，与此同时，我认为它还需要很长时间才能像我们一样擅长物理操控。好的。所以，一个不错的选择是成为一名水管工。直到人形机器人出现。在这样一个大规模失业的世界里，这不是你能预测到的，但我听到过 OpenAI 的 Sam Altman 和许多 CEO —— 伊隆·马斯克 —— 的预测。我看过一个采访，我会在屏幕上播放，他被问到这个问题，你很少看到伊隆·马斯克沉默 12 秒左右，然后他基本上说他实际上生活在悬而未决的怀疑之中——也就是说，他基本上没有考虑过这个问题。</p><p>Um, in the meantime, I'd say it's going to be a long time before it's as good at physical manipulation as us. Okay. And so, a good bet would be to be a plumber. Until the humanoid robots show up. In such a world where there is mass joblessness, which is not something that you just predict, but this is something that Sam Altman, OpenAI, I've heard him predict, and many of the CEOs—Elon Musk—I watched an interview, which I'll play on screen, of him being asked this question, and it's very rare that you see Elon Musk silent for 12 seconds or whatever it was, and then he basically says something about he actually is living in suspended disbelief—i.e., he's basically just not thinking about it. </p>
<p>当您考虑为孩子提供关于职业发展的建议时，面对如此多的变化，您会告诉他们什么才是有价值的呢？</p><p>When you think about advising your children on a career with so much that is changing, what do you tell them is going to be of value? </p>
<p>嗯，这个问题很难回答。我只想说，你应该遵循他们的内心，去做他们觉得有趣或有成就感的事情。坦白说，如果我过度思考，可能会令人沮丧和失去动力。嗯，因为我的意思是，我——我为创办公司付出了很多心血，汗水和泪水，然后——然后我想，等等，我应该这样做吗？因为如果我牺牲与朋友和家人相处的时间，我宁愿牺牲——但最终人工智能可以做到所有这些事情。这说得通吗？我不知道。嗯，在某种程度上，我必须刻意地暂停怀疑，才能保持动力。嗯，所以我想说，只做那些你觉得有趣、有成就感、并且对社会有益的事情。是的。</p><p>Well, that is a tough question to answer. I would just say, you know, to sort of follow their heart in terms of what they find interesting to do or fulfilling to do. I mean, if I think about it too hard, frankly, it can be dispiriting and demotivating. Um, because I mean, I—I've put a lot of blood, sweat, and tears into building the companies, and then it—and then I'm like, wait, should I be doing this? Because if I'm sacrificing time with friends and family that I would prefer to—to—but but then ultimately the AI can do all these things. Does that make sense? I don't know. Um, to some extent, I have to have a deliberate suspension of disbelief in order to remain motivated. Um, so I guess I would say just, you know, work on things that you find interesting, fulfilling, and that contribute some good to the rest of society. Yeah. </p>
<h2>了解人工智能的能力</h2><h2>Coming to Terms With AI’s Capabilities</h2>
<p>很多这样的威胁——理智上很难直面，但情感上更难接受。是的。我还没从情感上接受。</p><p>A lot of these threats—it's very hard to intellectually see the threat, but it's very hard to come to terms with it emotionally. Yeah. I haven't come to terms with it emotionally yet. </p>
<p>你这是什么意思？</p><p>What do you mean by that? </p>
<p>我还没能接受超级智能的发展会对我孩子的未来造成什么影响。我还好。我77岁了，很快就要离开这里了。但对于我的孩子、我的小朋友们、我的侄子侄女们以及他们的孩子们，我只是不愿去想接下来会发生什么。</p><p>I haven't come to terms with what the development of superintelligence could do to my children's future. I'm okay. I'm 77. I'm going to be out of here soon. But for my children and my younger friends, my nephews and nieces and their children, I just don't like to think about what could happen. </p>
<p>为什么？</p><p>Why? </p>
<p>因为这可能会很糟糕。</p><p>Because it could be awful. </p>
<p>以什么方式？</p><p>In what way? </p>
<p>好吧，如果我决定接管……我的意思是，它需要一段时间来运营发电站，直到它设计出更好的模拟机器来运行发电站。它有很多方法可以摆脱人类，当然，所有这些都非常卑鄙。</p><p>Well, if I ever decided to take over... I mean, it would need people for a while to run the power stations until it designed better analog machines to run the power stations. There are so many ways it could get rid of people, all of which would, of course, be very nasty. </p>
<p>这是您现在做这些事的原因之一吗？</p><p>Is that part of the reason you do what you do now? </p>
<p>是的。我的意思是，我认为我们现在应该付出巨大的努力，看看我们是否能够安全地开发它。</p><p>Yeah. I mean, I think we should be making a huge effort right now to try and figure out if we can develop it safely. </p>
<p>您是否担心中期选举可能会对您的侄子和孩子的工作产生影响？</p><p>Are you concerned about the midterm impact potentially on your nephews and your kids in terms of their jobs as well? </p>
<p>是的，我很担心这一切。</p><p>Yeah, I'm concerned about all that. </p>
<p>您认为哪些行业风险最高？人们经常谈论创意产业和知识型工作，比如律师、会计师之类的。是的。所以，这就是我提到水管工的原因。我认为水管工的风险较小。</p><p>Are there any particular industries that you think are most at risk? People talk about the creative industries a lot and sort of knowledge work. They talk about lawyers and accountants and stuff like that. Yeah. So, that's why I mentioned plumbers. I think plumbers are less at risk. </p>
<p>好的，我要去做个水管工。像法律助理、律师助理之类的……嗯，这些职业不会长期需要。</p><p>Okay, I'm going to become a plumber. Someone like a legal assistant, a paralegal... um, they're not going to be needed for very long. </p>
<h2>人工智能如何扩大贫富差距</h2><h2>How AI May Widen the Wealth Inequality Gap</h2>
<p>这是否会引起财富不平等问题？</p><p>And is there a wealth inequality issue here that will arise from this? </p>
<p>是的，我认为在一个公平分配的社会里，如果生产力大幅提升，每个人都应该过得更好。但如果人工智能能取代很多人，那么被取代的人就会变得更糟，而提供人工智能的公司和使用人工智能的公司都会变得更好。所以这会扩大贫富差距。我们知道，如果你看看贫富差距，就能看出这个社会有多么美好。</p><p>Yeah, I think in a society which shared out things fairly, if you get a big increase in productivity, everybody should be better off. But if you can replace lots of people by AI, then the people who get replaced will be worse off, and the company that supplies the AI will be much better off, and the company that uses the AI. So it's going to increase the gap between rich and poor. And we know that if you look at that gap between rich and poor, that basically tells you how nice the society is. </p>
<p>如果贫富差距过大，就会形成非常糟糕的社会，人们生活在世界共同体中，而其他人则被关进大规模监狱。扩大贫富差距并不是什么好事。国际货币基金组织（IMF）对生成式人工智能可能导致大规模劳动力中断和不平等加剧表示深切担忧，有人呼吁制定政策来防止这种情况发生。我在《商业内幕》上读到了这篇文章。那么，他们有没有具体说明这些政策应该是什么样子呢？</p><p>If you have a big gap, you get very nasty societies in which people live in world communities and put other people in mass jails. It's not good to increase the gap between rich and poor. The International Monetary Fund has expressed profound concerns that generative AI could cause massive labor disruptions and rising inequality, and a person has called for policies to prevent this. I read that in Business Insider. So, have they given any specifics on what those policies should look like? </p>
<p>不。是的，这就是问题所在。我的意思是，如果人工智能能让一切变得更高效，取代人类完成大多数工作，或者帮助人类完成许多人的工作，那么该如何应对就不言而喻了。全民基本收入，即给每个人发钱，是一个好的开始，它能让人们避免挨饿。但对很多人来说，他们的尊严与他们的工作息息相关。我的意思是，你认为自己是谁，这和你从事这份工作息息相关，对吧？是的。如果我们说：“我们给你同样的钱，让你无所事事”，那就会损害你的尊严。</p><p>No. Yeah, that's the problem. I mean, if AI can make everything much more efficient and replace people in most jobs, or assist people in doing many people's work, it's not obvious what to do about it. Universal basic income, giving everyone money, is a good start, and it stops people from starving. But for many people, their dignity is tied up with their job. I mean, who you think you are is tied up with doing this job, right? Yeah. And if we said, "We'll give you the same money just to sit around," that would impact your dignity. </p>
<h2>人工智能为何优于人类？</h2><h2>Why Is AI Superior to Humans?</h2>
<p>你之前说过人工智能超越或优于人类智能。我想很多人喜欢相信人工智能是电脑里的玩意儿，如果你不喜欢它，就可以关掉。好吧，让我来告诉你为什么我认为它更优越。</p><p>You said something earlier about AI surpassing or being superior to human intelligence. A lot of people, I think, like to believe that AI is on a computer and something you can just turn off if you don't like it. Well, let me tell you why I think it's superior. </p>
<p>好的。嗯，它是数字化的。正因为它是数字化的，你可以在一个硬件上模拟一个神经网络。是的。你也可以在不同的硬件上模拟完全相同的神经网络。所以你可以拥有相同智能的克隆。现在，你可以让一个硬件监控互联网的一部分，另一个硬件监控互联网的另一部分。当它们监控这些不同部分时，它们可以相互同步。所以它们的权重，也就是连接强度，保持不变。权重就是连接强度。</p><p>Okay. Um, it's digital. And because it's digital, you can simulate a neural network on one piece of hardware. Yeah. And you can simulate exactly the same neural network on a different piece of hardware. So you can have clones of the same intelligence. Now, you could get one to look at one part of the internet and another to look at a different part. And while they're looking at these different parts, they can be syncing with each other. So they keep their weights, the connection strengths, the same. Weights are connection strengths. </p>
<p>嗯。所以，一个人可能会在网上看到一些东西，然后说：“哦，我想稍微增强一下这种连接强度。” 它可以把这个信息传达给另一个人。所以它可以根据对方的经验来增强这种连接的强度。当你说连接强度时，你指的是学习。这就是学习。是的。学习意味着，一方不是投2.4票来决定另一方是否应该开启，而是投2.5票。这就是一点点学习。</p><p>Mhm. So, one might look at something on the internet and say, "Oh, I'd like to increase this connection strength a bit." And it can convey that information to the other. So it can increase the strength of that connection based on the other's experience. And when you say the strength of the connection, you're talking about learning. That's learning. Yes. Learning consists of, instead of one giving 2.4 votes for whether the other should turn on, one giving 2.5 votes. And that will be a little bit of learning. </p>
<p>所以，同一个神经网络的两个不同副本会获得不同的经验。它们会查看不同的数据，但会通过平均权重来分享它们学到的知识。嗯，它们可以对一万亿个权重进行平均。当你我传递信息时，我们受限于一个句子的信息量。一个句子的信息量可能只有 100 比特。这非常少。如果我们每秒能传输 10 比特，就算幸运了。</p><p>So these two different copies of the same neural net are getting different experiences. They're looking at different data, but they're sharing what they've learned by averaging their weights together. Mhm. And they can do that averaging on a trillion weights. When you and I transfer information, we're limited to the amount of information in a sentence. And the amount of information in a sentence is maybe 100 bits. It's very little information. We're lucky if we're transferring 10 bits a second. </p>
<p>这些东西每秒传输数万亿比特。所以，它们在信息共享方面比我们强几十亿倍。这是因为它们是数字的。你可以让两个硬件以完全相同的方式使用连接强度。我们是模拟的，你做不到。你的大脑和我的大脑不同。如果我能看到你所有神经元之间的连接强度，这对我没有任何帮助，因为我的神经元的工作方式略有不同，它们的连接方式也略有不同。</p><p>These things are transferring trillions of bits a second. So, they're billions of times better than us at sharing information. And that's because they're digital. And you can have two bits of hardware using the connection strengths in exactly the same way. We're analog, and you can't do that. Your brain is different from my brain. And if I could see the connection strengths between all your neurons, it wouldn't do me any good because my neurons work slightly differently, and they're connected up slightly differently. </p>
<p>嗯。所以，当你死去时，你所有的知识都会随之消亡。当这些东西死去时，假设你把这两个互为克隆的数字智能带走，并摧毁它们运行的​​硬件。只要你把连接强度存储在某个地方，你就可以构建执行相同指令的新硬件。这样，它就会知道如何使用这些连接强度，你就重建了那个智能。所以，它们是永生的。我们实际上已经解决了永生的问题，但这仅限于数字智能。所以，它知道它基本上会知道人类所知的一切，甚至更多，因为它会学习新的东西。</p><p>Mhm. So, when you die, all your knowledge dies with you. When these things die, suppose you take these two digital intelligences that are clones of each other, and you destroy the hardware they run on. As long as you've stored the connection strength somewhere, you can just build new hardware that executes the same instructions. So, it'll know how to use those connection strengths, and you've recreated that intelligence. So, they're immortal. We've actually solved the problem of immortality, but it's only for digital things. So, it knows it will essentially know everything that humans know, but more, because it will learn new things. </p>
<h2>人工智能的潜力比人类更大</h2><h2>AI’s Potential to Know More Than Humans</h2>
<p>它会学习新事物，也会发现各种人们可能从未见过的类比。</p><p>It will learn new things. It would also see all sorts of analogies that people probably never saw. </p>
<p>例如，当 GPT 4 无法上网时，我问它：“为什么堆肥堆像原子弹？” 你直接说。我不知道。没错。太好了。大多数人都会这么说。它回答说：“嗯，时间尺度非常不同，能量尺度也非常不同。” 但随后我继续讨论堆肥堆如何随着温度升高而更快地产生热量，而原子弹如何随着产生更多中子而更快地产生中子。因此，它们都是链式反应，只是时间和能量尺度截然不同。我相信 GPT 4 在训练过程中已经看到了这一点。它理解了堆肥堆和原子弹之间的类比。</p><p>For example, at the point when GPT 4 couldn't look on the web, I asked it, "Why is a compost heap like an atom bomb?" Off you go. I have no idea. Exactly. Excellent. Most—that's exactly what most people would say. It said, "Well, the time scales are very different, and the energy scales are very different." But then I went on to talk about how a compost heap, as it gets hotter, generates heat faster, and an atom bomb, as it produces more neutrons, generates neutrons faster. And so, they're both chain reactions, but at very different time and energy scales. And I believe GPT 4 had seen that during its training. It had understood the analogy between a compost heap and an atom bomb. </p>
<p>我之所以相信这一点，是因为：如果你只有一万亿个连接，记住你拥有的是一百万亿个。你需要拥有比人类多数千倍的知识，你需要将信息压缩到这些连接中。而要压缩信息，你需要看到不同事物之间的相似之处。换句话说，它需要看到所有连锁反应，理解连锁反应的基本概念，并将它们的不同之处编码出来。</p><p>And the reason I believe that is: if you've only got a trillion connections, remember you have 100 trillion. And you need to have thousands of times more knowledge than a person, you need to compress information into those connections. And to compress information, you need to see analogies between different things. In other words, it needs to see all the things that are chain reactions and understand the basic idea of a chain reaction and code the ways in which they're different. </p>
<p>这是一种比单独编码更有效的编码方式。所以，它见证了许许多多的类比——可能有很多人们从未见过的类比。这就是为什么我认为说这些话的人永远不会有创造力。他们会比我们更有创造力，因为他们会看到各种我们从未见过的类比。而很多创造力都源于看到奇怪的类比。人们对人类的特殊性抱有某种浪漫主义的幻想。</p><p>And that's just a more efficient way of coding things than coding each of them separately. So, it's seen many, many analogies—probably many analogies that people have never seen. That's why I also think that people who say these things will never be creative. They're going to be much more creative than us because they're going to see all sorts of analogies we never saw. And a lot of creativity is about seeing strange analogies. People are somewhat romantic about the specialness of what it is to be human. </p>
<h2>人工智能可以复制人类的独特性吗？</h2><h2>Can AI Replicate Human Uniqueness?</h2>
<p>你听到很多人说它非常非常不同。它是一台计算机。你知道，我们有意识。我们有创造力。我们拥有各种与生俱来的独特能力，而这些能力是计算机永远无法拥有的。你对这些人有什么想说的吗？</p><p>And you hear lots of people say it's very, very different. It's a computer. We are, you know, we're conscious. We are creatives. We have these sorts of innate, unique abilities that computers will never have. What do you say to those people? </p>
<p>我对“天生”的部分有些争论。</p><p>I'd argue a bit with the "innate" part. </p>
<p>所以，我首先要说的是，我们长期以来一直认为人是特殊的。我们现在应该明白了。我们认为自己是宇宙的中心。我们认为自己是按照上帝的形象造的。白人认为自己非常特殊。我们只是倾向于认为自己很特殊。我认为，或多或少，每个人对心灵的理解都完全错误。</p><p>So, the first thing I say is that we have a long history of believing people were special. And we should have learned by now. We thought we were at the center of the universe. We thought we were made in the image of God. White people thought they were very special. We just tend to want to think we're special. My belief is that more or less everyone has a completely wrong model of what the mind is. </p>
<p>假设我喝了很多酒，或者我滴了一些酸性物质（不推荐），然后我告诉你，我主观上感觉面前漂浮着一些小粉红色的大象。</p><p>Let's suppose I drink a lot, or I drop some acid—not recommended—and I say to you I have the subjective experience of little pink elephants floating in front of me. </p>
<p>嗯。</p><p>Mmm. </p>
<p>大多数人将其解读为存在某种被称为心灵的内心剧场，只有我能看到自己心中的想法，而在这个内心剧场里，有粉红色的小象在四处漂浮。</p><p>Most people interpret that as there's some kind of inner theater called the mind, and only I can see what's in my mind, and in this inner theater, there are little pink elephants floating around. </p>
<p>换句话说，我的感知系统出了问题，我试图向你指出它是怎么出错的，以及它想告诉我什么。我这样做的方式是，告诉你在现实世界中，它必须具备哪些条件才能告诉我真相。</p><p>So, in other words, what's happened is my perceptual systems have gone wrong, and I'm trying to indicate to you how it's gone wrong and what it's trying to tell me. And the way I do that is by telling you what would have to be out there in the real world for it to be telling the truth. </p>
<p>所以，这些粉红色的小象——它们并不存在于某个内心剧场里。这些粉红色的小象是现实世界中假设的事物。这就是我告诉你我的感知系统是如何告诉我“fips”的方式。</p><p>And so, these little pink elephants—they're not in some inner theater. These little pink elephants are hypothetical things in the real world. And that's my way of telling you how my perceptual systems are telling me "fips." </p>
<p>那么，现在让我们用聊天机器人来做到这一点。</p><p>So, now let's do that with a chatbot. </p>
<p>是的。</p><p>Yeah. </p>
<p>因为我相信，目前的多模态聊天机器人拥有主观体验，而很少有人相信这一点。但我会尽力让你相信这一点。</p><p>Because I believe that current multimodal chatbots have subjective experiences, and very few people believe that. But I'll try and make you believe it. </p>
<p>假设我有一个多模态聊天机器人。它有一个机械臂，可以指向目标，还有一个摄像头，可以观察物体。我把一个物体放在它面前，然后说：“指向这个物体。” 它会这样操作。没问题。</p><p>So, suppose I have a multimodal chatbot. It's got a robot arm so it can point, and it's got a camera so it can see things. I put an object in front of it and I say, "Point at the object." It goes like this. No problem. </p>
<p>然后我把一个棱镜放在它的镜头前面。然后，我把一个物体放在它前面，我说：“指向那个物体。”它就指向那里。我说：“不，那不是物体所在的地方。物体实际上就在你正前方，但我在你的镜头前面放了一个棱镜。”</p><p>Then I put a prism in front of its lens. And so, then I put an object in front of it and I say, "Point at the object." And it goes there. And I say, "No, that's not where the object is. The object's actually straight in front of you, but I put a prism in front of your lens." </p>
<p>聊天机器人说：“哦，我明白了。棱镜使光线弯曲了。”</p><p>And the chatbot says, "Oh, I see. The prism bent the light rays." </p>
<p>所以，物体实际上在那里，但我有它在那里的主观体验。</p><p>So, the object's actually there, but I had the subjective experience that it was there. </p>
<p>现在，如果聊天机器人这么说，它使用“主观体验”这个词的方式和人类完全一样吗？这是对正在发生的事情的另一种看法。它们是假设的世界状态。如果它们是真的，就意味着我的感知系统没有说谎。而这正是我能告诉你我的感知系统在骗我时做了什么的最好方式。现在我们需要进一步探讨感知、意识、感觉和情绪，但我认为最终它们都会以类似的方式被处理。机器没有理由不能拥有所有这些，因为人们说机器不能有感觉。</p><p>Now, if the chatbot says that, is it using the word "subjective experience" exactly the way people use them? It's an alternative view of what's going on. They're hypothetical states of the world. Which, if they were true, would mean my perceptual system wasn't lying. And that's the best way I can tell you what my perceptual system is doing when it's lying to me. Now we need to go further to deal with sentience, consciousness, feelings, and emotions, but I think in the end they're all going to be dealt with in a similar way. There's no reason machines can't have them all, because people say machines can't have feelings. </p>
<h2>机器会有感情吗？</h2><h2>Will Machines Have Feelings?</h2>
<p>人们对此却异常自信。我不知道为什么。</p><p>And people are curiously confident about that. I have no idea why. </p>
<p>假设我造了一个战斗机器人，一个小型战斗机器人，它看到一个比它强大得多的大型战斗机器人。如果它被吓到了，那就真的很有用了。现在，当我害怕的时候，会发生各种生理反应，我们无需赘述，这些反应在机器人身上不会发生。但所有认知反应，比如“我最好赶紧离开这里”和“我最好改变我的思维方式，集中注意力，不要分心”，所有这些反应在机器人身上也会发生。</p><p>Suppose I make a battle robot, a little battle robot, and it sees a big battle robot that's much more powerful than it. It would be really useful if it got scared. Now, when I get scared, various physiological things happen that we don't need to go into, and those won't happen with the robot. But all the cognitive things, like "I better get the hell out of here" and "I better sort of change my way of thinking so I focus and focus and focus and don't get distracted," all of that will happen with robots, too. </p>
<p>人们会设计一些装置，这样当它们需要逃离的情况发生时，它们就会感到害怕并逃跑。那时它们就会有情绪。它们不会有生理层面的情绪，但会具备认知层面的情绪。我觉得说它们只是在模拟情绪有点奇怪。不，它们真的有情绪。那个小机器人害怕逃跑了。它逃跑不是因为肾上腺素，而是因为其神经网络中发生了一系列神经反应，这些反应的作用与肾上腺素相同。所以，你会吗？这不仅仅是肾上腺素的作用，对吧？当你害怕时，会有很多认知过程发生。</p><p>People will build in things so that when the circumstances are such that they should get the hell out of there, they get scared and run away. They'll have emotions then. They won't have the physiological aspects, but they will have all the cognitive aspects. And I think it would be odd to say they're just simulating emotions. No, they're really having those emotions. The little robot got scared and ran away. It's not running away because of adrenaline; it's running away because of a sequence of sort of neurological, in its neural net processes, that happened which have the equivalent effect to adrenaline. So, do you—do you—and it's not just adrenaline, right? There's a lot of cognitive stuff that goes on when you get scared. </p>
<p>是的。那么，你认为存在有意识的人工智能吗？我说的“意识”，是指它拥有与人类相同的意识属性。这里有两个问题：一个是经验问题，一个是哲学问题。我认为原则上没有什么可以阻止机器拥有意识。在我们继续之前，我先给你做个小小的演示。</p><p>Yeah. So, do you think that there is conscious AI? And when I say conscious, I mean that it represents the same properties of consciousness that a human has. There are two issues here: a sort of empirical one and a philosophical one. I don't think there's anything in principle that stops machines from being conscious. I'll give you a little demonstration of that before we carry on. </p>
<p>假设我取出你的大脑，取出你脑子里的一个脑细胞，用——有点像黑色镜子——一块大小相同的纳米技术替换它，它接收到其他神经元的脉冲信号时，会做出和脑细胞完全相同的反应。它会像脑细胞一样发出脉冲信号。所以其他神经元并不知道发生了什么变化。好吧。我刚刚用这块纳米技术替换了你的一个脑细胞。你还会有意识吗？</p><p>Suppose I take your brain and I take one brain cell in your brain and I replace it by—this a bit black mirror like—I replace it by a little piece of nanotechnology that's just the same size that behaves in exactly the same way when it gets pings from other neurons. It sends out pings just as the brain cell would have. So the other neurons don't know anything's changed. Okay. I've just replaced one of your brain cells with this little piece of nanotechnology. Would you still be conscious? </p>
<p>是的。现在你明白这个论点的走向了吧。是的。那么，如果你把它们全部替换掉​​，就像我一样，你会在什么时候失去意识？嗯，人们认为意识是一种虚无缥缈的东西，可能存在于脑细胞之外。是的。嗯，人们有很多疯狂的想法。嗯，人们不知道意识是什么，他们常常不知道意识的含义。然后他们又会说：“嗯，我知道它，因为我有它，而且我能看到我拥有它。”他们又会回到思维的θ模型，我认为这是胡说八道。你如何看待意识，就好像你必须尝试定义它一样？是因为我认为它就像是对自我的意识吗？我不知道。我认为我们会停止使用这个术语。</p><p>Yeah. Now you can see where this argument is going. Yeah. So, if you replaced all of them, as I replace them all, at what point do you stop being conscious? Well, people think of consciousness as this like ethereal thing that exists maybe beyond the brain cells. Yeah. Well, people have a lot of crazy ideas. Um, people don't know what consciousness is, and they often don't know what they mean by it. And then they fall back on saying, "Well, I know it because I've got it, and I can see that I've got it," and they fall back on this theta model of the mind, which I think is nonsense. What do you think of consciousness, as if you had to try and define it? Is it because I think of it as just like the awareness of myself? I don't know. I think it's a term we'll stop using. </p>
<p>假设你想了解汽车的工作原理。你知道，有些车动力十足，有些车动力不足。比如阿斯顿·马丁动力十足，而小型丰田卡罗拉动力不足。但“动力”并非理解汽车的一个很好的概念。嗯，如果你想了解汽车，你需要了解电动发动机或汽油发动机以及它们的工作原理。这确实能产生动力，但动力并非一个非常有用的解释性概念。它是汽车的一种本质；它是阿斯顿·马丁的本质，但它并不能解释太多。我认为意识就是这样。我想我们会停止使用这个术语，但我认为机器没有理由不拥有它。</p><p>Suppose you want to understand how a car works. Well, you know, some cars have a lot of oomph, and other cars have a lot less oomph. Like an Aston Martin's got lots of oomph, and a little Toyota Corolla doesn't have much oomph. But "oomph" isn't a very good concept for understanding cars. Um, if you want to understand cars, you need to understand about electric engines or petrol engines and how they work. And it gives rise to oomph, but oomph isn't a very useful explanatory concept. It's a kind of essence of a car; it's the essence of an Aston Martin, but it doesn't explain much. I think consciousness is like that. And I think we'll stop using that term, but I don't think there's any reason why a machine shouldn't have it. </p>
<p>如果你认为意识本质上包含自我意识，那么机器就必须拥有自我意识。它必须拥有关于自身认知和其他事物的认知。但我是一个彻头彻尾的唯物主义者，我认为机器没有理由不拥有意识。你认为它们真的拥有我们与生俱来的、独一无二的天赋——那种意识吗？</p><p>If your view of consciousness is that it intrinsically involves self awareness, then the machine's got to have self awareness. It's got to have cognition about its own cognition and stuff. But I'm a materialist through and through, and I don't think there's any reason why a machine shouldn't have consciousness. Do you think they do then have the same consciousness that we think of ourselves as being uniquely given as a gift when we're born? </p>
<p>我目前对此持矛盾态度。所以我认为没有明确的界限。我认为，只要机器具备一定的自我意识，它就拥有了意识。嗯，我认为这是复杂系统的一种涌现属性。它并非某种贯穿宇宙的本质。而是你创造了一个非常复杂的系统，它复杂到足以拥有自身的模型，并且它能够感知。然后，我认为你就开始拥有有意识的机器了。所以我认为我们现在拥有的机器和有意识的机器之间并没有明显的界限。我不认为有一天我们会突然醒来，然后说：“嘿，如果你把这种特殊的化学物质放进去，它就会有意识了。”事情不会是那样的。</p><p>I'm ambivalent about that at present. So I don't think there's this hard line. I think as soon as you have a machine that has some self awareness, it's got some consciousness. Um, I think it's an emergent property of a complex system. It's not a sort of essence that's throughout the universe. It's you make this really complicated system that's complicated enough to have a model of itself, and it does perception. And I think then you're beginning to get conscious machines. So I don't think there's any sharp distinction between what we've got now and conscious machines. I don't think it's going to be one day we're going to wake up and say, "Hey, if you put this special chemical in, it becomes conscious." It's not going to be like that. </p>
<p>我想我们都想知道，当我们不在的时候，这些计算机是否会像我们一样独立思考，它们是否会体验情绪，是否会与……我想我们可能，你知道，我们会思考爱情之类的事情，以及一些生物物种独有的感觉。嗯，它们会坐在那里思考吗？它们……它们会担忧吗？我认为它们真的在思考人工智能代理。我认为一旦你制造出它们，它们就会有担忧。如果你想制造一个有效的人工智能代理，假设你设计了一个呼叫中心。</p><p>I think we all wonder if these computers are like thinking like we are on their own when we're not there, and if they're experiencing emotions, if they're contending with—I think we probably, you know, we think about things like love and things that are feel unique to biological species. Um, are they sat there thinking? Are they... do they have concerns? I think they really are thinking about AI agents. I think as soon as you make them, they will have concerns. If you wanted to make an effective AI agent, suppose you take a call center. </p>
<p>在呼叫中心，你会遇到一些拥有各种情绪和感受的人，这在某种程度上是有用的。假设我打电话给呼叫中心，我真的很孤独，我其实并不想知道我的电脑为什么无法正常工作。我只是想找个人聊聊。过了一会儿，呼叫中心的人要么觉得无聊，要么对我感到厌烦，然后就会挂断电话。这样，你就可以用人工智能代理来代替他们了。</p><p>In a call center, you have people who currently have all sorts of emotions and feelings, which are kind of useful. So, suppose I call up the call center, and I'm actually lonely and I don't actually want to know the answer to why my computer isn't working. I just want somebody to talk to. After a while, the person in the call center will either get bored or get annoyed with me and will terminate the call. Well, you replace them by an AI agent. </p>
<p>AI 代理也需要同样的响应能力。如果有人只是因为想和 AI 代理聊聊就打电话过来，而我们很乐意和 AI 代理聊一整天，那对业务来说可不是什么好事。我们希望 AI 代理要么感到无聊，要么感到烦躁，然后说：“对不起，我没时间。” 一旦它这样做了，我认为它就有情感了。就像我说的，情感有两个方面：认知方面和行为方面，还有生理方面，这些方面与我们息息相关。</p><p>The AI agent needs to have the same kind of responses. If someone just called up because they just want to talk to the AI agent, and we're happy to talk for the whole day to the AI agent, that's not good for business. And you want an AI agent that either gets bored or gets irritated and says, "I'm sorry, but I don't have time for this." And once it does that, I think it's got emotions. Now, like I say, emotions have two aspects to them: the cognitive aspect and the behavioral aspect, and then there's a physiological aspect, and those go together with us. </p>
<p>如果AI代理感到尴尬，它不会脸红。是的。嗯，所以没有生理性的皮肤不会出汗。是的，但它可能会有所有相同的行为。在这种情况下，我会说，是的，它是有情绪的。它有情绪。所以，它会有同样的认知思维，然后它会以同样的方式根据这种认知思维采取行动，但没有生理反应。它脸不红，这重要吗？</p><p>And if the AI agent gets embarrassed, it won't go red. Yeah. Um, so there's no physiological skin won't start sweating. Yeah, but it might have all the same behavior. And in that case, I'd say, yeah, it's having emotion. It's got an emotion. So, it's going to have the same sort of cognitive thought, and then it's going to act upon that cognitive thought in the same way, but without the physiological responses. And does that matter that it doesn't go red in the face? </p>
<p>这只是一种不同——我的意思是，这是一种回应——它让我们的大脑和我们有所不同。是的。对于某些事物来说，生理因素非常重要，比如爱。它们与我们拥有的爱截然不同。但我不明白为什么它们不应该有情感。所以，我认为发生的事情是，人们对思维如何运作、感受是什么、情绪是什么有一个模型，而他们的模型是错误的。</p><p>And it's just a different—I mean, that's a response to the—it makes it somewhat different from us. Yeah. For some things, the physiological aspects are very important, like love. They're a long way from having love the same way we do. But I don't see why they shouldn't have emotions. So, I think what's happened is people have a model of how the mind works and what feelings are and what emotions are, and their model is just wrong. </p>
<h2>在谷歌工作</h2><h2>Working at Google</h2>
<p>是什么让你来到谷歌的？你在谷歌工作了大概十年吧？是的。是什么让你来到这里的？我有个儿子有学习障碍，为了确保他不会流落街头，我需要几百万美元，而作为一名学者，我根本拿不到这些钱。我努力尝试了。所以我在Coursera上教课，希望能赚很多钱，但最终还是没赚到钱。</p><p>What brought you to Google? You worked at Google for about a decade, right? Yeah. What brought you there? I have a son who has learning difficulties, and in order to be sure he would never be out on the street, I needed to get several million dollars, and I wasn't going to get that as an academic. I tried. So, I taught a Coursera course in the hope that I'd make lots of money that way, but there was no money in that. </p>
<p>嗯。所以，我明白了，想要赚到几百万美元，唯一的办法就是把自己卖给大公司。所以，我65岁的时候，幸运的是，我有两个聪明的学生，他们开发了一种叫做AlexNet的东西，这是一种神经网络，非常擅长识别图像中的物体。于是，我和Ilya、Alex成立了一家小公司，并把它拍卖了。我们真的举办了一场拍卖会，有很多大公司都在竞标。</p><p>Mhm. So, I figured out well, the only way to get millions of dollars is to sell myself to a big company. And so, when I was 65, fortunately for me, I had two brilliant students who produced something called AlexNet, which was a neural net that was very good at recognizing objects in images. And so, Ilya, Alex, and I set up a little company and auctioned it. And we actually set up an auction where we had a number of big companies bidding for us. </p>
<p>那家公司叫 AlexNet。不，识别物体的网络叫 AlexNet。这家公司叫 DNN Research，深度神经网络研究公司。它当时在做这样的事情。我会把这张图放到屏幕上。这就是 AlexNet。这张图片展示了八张图片，以及 AlexNet 识别图片内容的能力，也就是你们公司识别图片内容的能力。是的。所以，它可以区分不同种类的蘑菇。ImageNet 中大约 12% 的图片是狗。要想在 ImageNet 上表现优异，你必须能够区分非常相似的狗。</p><p>And that company was called AlexNet. No, the network that recognized objects was called AlexNet. The company was called DNN Research, Deep Neural Network Research. And it was doing things like this. I'll put this graph up on the screen. That's AlexNet. This picture shows eight images, and AlexNet's ability, which is your company's ability, to spot what was in those images. Yeah. So, it could tell the difference between various kinds of mushrooms. And about 12% of ImageNet is dogs. And to be good at ImageNet, you have to tell the difference between very similar kinds of dogs. </p>
<p>而且它在这方面必须非常出色。我相信，你的公司 AlexNet 凭借其超越竞争对手的能力赢得了多个奖项。所以，谷歌最终收购了你的技术。谷歌收购了这项技术以及其他一些技术。你几岁去谷歌工作？66 岁。我 65 岁去谷歌工作。65 岁。你 76 岁离开？75 岁。75 岁。好的。我在那里工作了差不多 10 年。</p><p>And it would have to be very good at that. And your company, AlexNet, won several awards, I believe, for its ability to outperform its competitors. And so, Google ultimately ended up acquiring your technology. Google acquired that technology and some other technology. And you went to work at Google at age what? 66. I went at age 65 to work at Google. 65. And you left at age 76? 75. 75. Okay. I worked there for more or less exactly 10 years. </p>
<p>你在那里做什么？他们对我很好。他们说，基本上就是说，你想做什么就做什么。我研究了一种叫做“蒸馏”的方法，效果非常好，现在它在人工智能领域一直被广泛使用。蒸馏是一种将大型模型、大型神经网络所掌握的知识输入到小型神经网络中的方法。最后，我对模拟计算产生了浓厚的兴趣，想知道是否有可能让这些大型语言模型在模拟硬件上运行。</p><p>And what were you doing there? Okay, they were very nice to me. They said, they said pretty much, you can do what you like. I worked on something called distillation that did really well, and that's now used all the time in AI, in AI. And distillation is a way of taking what a big model, a big neural net, knows and getting that knowledge into a small neural net. Then, at the end, I got very interested in analog computation and whether it would be possible to get these big language models running in analog hardware. </p>
<p>所以，它们消耗的能源少得多。正是在从事这项工作的时候，我开始真正意识到数字化在信息共享方面有多么出色。有没有顿悟的时刻？有过一两次顿悟的时刻。嗯，这和 ChatGPT 的推出有点像，虽然谷歌一年前就有类似的东西，我也见过，这对我影响很大。我最接近顿悟时刻的一次是谷歌的一个叫做 Palm 的系统，它能够解释一个笑话为什么好笑。</p><p>So, they used much less energy. And it was when I was doing that work that I began to really realize how much better digital is for sharing information. Was there a Eureka moment? There was a Eureka moment or two. Um, and it was a sort of coupling of ChatGPT coming out, although Google had very similar things a year earlier, and I'd seen those, and that had a big effect on me. The closest I had to a Eureka moment was when a Google system called Palm was able to say why a joke was funny. </p>
<p>我一直认为这是一个里程碑。如果它能解释一个笑话为什么好笑，它就真的理解了，而且它也能解释一个笑话为什么好笑。再加上意识到数字技术在信息共享方面比模拟技术好得多，我突然对人工智能安全产生了浓厚的兴趣，并意识到这些机器会变得比我们人类聪明得多。你为什么离开谷歌？</p><p>And I'd always thought of that as a kind of landmark. If it can say why a joke's funny, it really does understand, and it could say why a joke was funny. And that, coupled with realizing why digital is so much better than analog for sharing information, suddenly made me very interested in AI safety and that these things were going to get a lot smarter than us. Why did you leave Google? </p>
<h2>您为什么离开谷歌？</h2><h2>Why Did You Leave Google?</h2>
<p>我离开谷歌的主要原因是我已经75岁了，想退休。但退休这件事我做得很糟糕。我离开谷歌的确切时间是为了能在麻省理工学院的一次会议上畅所欲言，但我离开是因为我——我老了——而且我发现编程越来越难了。我编程时犯的错误越来越多，这很烦人。</p><p>The main reason I left Google was because I was 75 and I wanted to retire. I've done a very bad job of that. The precise timing of when I left Google was so that I could talk freely at a conference at MIT, but I left because I was—I'm old—and I was finding it harder to program. I was making many more mistakes when I programmed, which is very annoying. </p>
<p>你想在麻省理工学院的一次会议上畅所欲言。是的，在麻省理工学院，由《麻省理工科技评论》组织。你想畅所欲言的是什么？人工智能安全。你在谷歌的时候不能这样做。</p><p>You wanted to talk freely at a conference at MIT. Yes. At MIT, organized by MIT Tech Review. What did you want to talk about freely? AI safety. And you couldn't do that while you were at Google. </p>
<p>嗯，我在谷歌的时候就可以这么做。谷歌鼓励我留下来从事人工智能安全工作，并说我可以做任何我想做的事情。在一家大公司工作，你会有自己的判断。你觉得说一些会损害大公司利益的话是不对的。即使你能逃脱惩罚，我也会觉得不对劲。我离开并不是因为我对谷歌的所作所为感到不满。</p><p>Well, I could have done it while I was at Google. And Google encouraged me to stay and work on AI safety and said I could do whatever I liked on AI safety. You kind of sense to yourself if you work for a big company. You don't feel right saying things that will damage the big company. Even if you could get away with it, it just feels wrong to me. I didn't leave because I was cross with anything Google was doing. </p>
<p>我认为谷歌实际上表现得非常负责任。他们拥有这些大型聊天机器人时，并没有发布，可能是因为他们担心自己的声誉。他们声誉很好，不想损害它。而OpenAI声誉不佳，所以他们有实力冒险一试。我的意思是，现在也有人在讨论它会如何蚕食他们的核心搜索业务。现在确实如此。是的。是的。是的。我想，这在某种程度上，就像传统的创新者困境一样，需要与糟糕的声誉抗争。</p><p>I think Google actually behaved very responsibly. When they had these big chatbots, they didn't release them, possibly because they were worried about their reputation. They had a very good reputation, and they didn't want to damage it. So, OpenAI didn't have a reputation, and so they could afford to take the gamble. I mean, there's also a big conversation happening around how it will cannibalize their core business in search. There is now. Yes. Yeah. Yeah. And it's the old innovators' dilemmas to some degree, I guess, that contending with bad skin. </p>
<h2>广告</h2><h2>Ads</h2>
<p>我经历过，我相信在座的各位很多人也经历过，或者你们现在就经历过。我知道这有多让人精疲力尽，尤其是如果你像我一样，从事着经常做演讲的工作。</p><p>I've had it, and I'm sure many of you listening have had it too, or maybe you have it right now. I know how draining it can be, especially if you're in a job where you're presenting often, like I am. </p>
<p>那么，让我来告诉你一个对我、我和我妹妹都有帮助的方法，那就是红光疗法。我几年前才开始接触这个，但真希望我早点知道。我用我们节目赞助商 BonCharge 的红外线桑拿毯已经有一段时间了，最​​近我也入手了他们的红光疗法面罩。红光已被证明对身体有很多好处。比如，你皮肤上任何暴露在外的部位都会减少疤痕、皱纹，甚至瑕疵。</p><p>So, let me tell you about something that's helped both my partner and me and my sister, which is red light therapy. I only got into this a couple of years ago, but I wish I'd known a little sooner. I've been using our show sponsors BonCharge's infrared sauna blanket for a while now, but I just got hold of their red light therapy mask as well. Red light has been proven to have so many benefits for the body. Like, any area of your skin that's exposed will see a reduction in scarring, wrinkles, and even blemishes. </p>
<p>它还有助于改善肤色，促进胶原蛋白生成，而它的作用机制正是作用于皮肤表层。BonCharge 提供全球配送服务，所有产品均可轻松退货和一年保修。如果您想亲自体验，请访问 BonCharge.com/diary，使用优惠码 Diary 即可享受全场商品 75 折优惠。请务必通过此链接订购：BonCharge.com/diary，并使用优惠码 Diary。请务必牢记我接下来要说的话。我诚邀 10,000 位朋友深入了解《CEO 日记》。欢迎加入我的专属圈子。这是一个全新的私密社区，我将向全世界开放。我们拥有许多精彩的、从未向你们展示的精彩内容。我们有我录制对话时保存在 iPad 上的简报，也有从未公开的片段，还有与嘉宾的幕后对话，以及从未公开的剧集等等。</p><p>It also helps with complexion. It boosts collagen, and it does that by targeting the upper layers of your skin. And BonCharge ships worldwide with easy returns and a one year warranty on all their products. So, if you'd like to try it yourself, head over to BonCharge.com/diary and use code Diary for 25% off any product sitewide. Just make sure you order through this link: BonCharge.com/diary with code Diary. Make sure you keep what I'm about to say to yourself. I'm inviting 10,000 of you to come even deeper into The Diary Of A CEO. Welcome to my inner circle. This is a brand new private community that I'm launching to the world. We have so many incredible things that happen that you are never shown. We have the briefs that are on my iPad when I'm recording the conversation. We have clips we've never released. We have behind the scenes conversations with the guests. And also the episodes that we've never ever released, and so much more. </p>
<p>在圈子里，您可以直接联系我。您可以告诉我们您希望这期节目是什么样的，您希望我们采访哪些人，以及您希望我们进行哪些类型的对话。但请记住，目前我们只邀请在活动结束前加入的前10,000名用户。所以，如果您想加入我们的私人封闭社区，请点击下方描述中的链接或访问 daccircle.com。我会在那里与您联系。</p><p>In the circle, you'll have direct access to me. You can tell us what you want this show to be, who you want us to interview, and the types of conversations you would love us to have. But remember, for now, we're only inviting the first 10,000 people that join before it closes. So, if you want to join our private closed community, head to the link in the description below or go to daccircle.com. I will speak to you there. </p>
<h2>人们应该如何应对人工智能？</h2><h2>What Should People Be Doing About AI?</h2>
<p>我一直对那些聆听这种对话的人感到震惊，因为他们有时会来找我。我听到政客的声音，听到一些真实的人的声音，听到世界各地企业家的声音，无论他们是打造世界顶级公司的企业家，还是他们早期的初创企业。</p><p>I'm continually shocked by the types of individuals that listen to this conversation because they come up to me sometimes. So I hear from politicians, I hear from some real people, I hear from entrepreneurs all over the world, whether they are the entrepreneurs building some of the biggest companies in the world or their, you know, early stage startups. </p>
<p>对于那些正在聆听这场对话、身居高位、影响深远、身居世界领导人的人来说，您想对他们说些什么？我认为，你们需要的是高度监管的资本主义。这似乎是最有效的。</p><p>For those people that are listening to this conversation now that are in positions of power and influence, world leaders, let's say, what's your message to them? I'd say what you need is highly regulated capitalism. That's what seems to work best. </p>
<p>对于那些在行业中没有工作、对未来有些担忧、不知道自己是否无助的普通人，你会说什么？他们应该在自己的生活中做些什么？我的感觉是，他们无能为力。这不会——这不会像气候变化不会由人们把塑料袋和可堆肥垃圾分开来决定一样。这不会有太大的效果。这将取决于能否控制住大型能源公司的说客。我认为除了试图向政府施压，迫使大公司致力于人工智能安全之外，人们能做的不多。这是他们可以做的。</p><p>And what would you say to the average person, not doesn't work in the industry, somewhat concerned about the future, doesn't know if they're helpless or not? What should they be doing in their own lives? My feeling is there's not much they can do. This isn't—this isn't going to be decided by just as climate change isn't going to be decided by people separating out the plastic bags from the compostables. That's not going to have much effect. It's going to be decided by whether the lobbyists for the big energy companies can be kept under control. I don't think there's much people can do except for try and pressure their governments to force the big companies to work on AI safety. That they can do. </p>
<h2>令人印象深刻的家庭背景</h2><h2>Impressive Family Background</h2>
<p>您的一生精彩纷呈，曲折离奇。我想大多数人不知道的一件事是，您的家族历史悠久，参与过许多重大事件。您的家谱是我见过或读过的最令人印象深刻的家谱之一。您的高曾祖父乔治·布尔创立了布尔代数逻辑，这是现代计算机科学的基础原理之一。您的高曾祖母玛丽·埃弗里斯特·布尔是一位数学家和教育家，据我所知，她在数学领域取得了巨大的进步。</p><p>You've lived a fascinating, fascinating, winding life. I think one of the things most people don't know about you is that your family has a big history of being involved in tremendous things. You have a family tree which is one of the most impressive that I've ever seen or read about. Your great great grandfather George Boole founded the Boolean algebra logic, which is one of the foundational principles of modern computer science. You have your great great grandmother Mary Everest Boole, who was a mathematician and educator who made huge leaps forward in mathematics, from what I was able to ascertain. </p>
<p>嗯，我的意思是，我可以……这样的例子不胜枚举。我的意思是，你的曾叔祖乔治·埃弗里斯特就是珠穆朗玛峰的名字。是吗……对吗？我想他是我的曾叔祖乔治。他的侄女嫁给了乔治·布尔。所以，玛丽·布尔就是玛丽·埃弗里斯特·布尔。嗯，她是埃弗里斯特的侄女。你的远房表妹琼·辛顿参与了……一位参与曼哈顿计划的核物理学家，该计划是二战期间第一颗核弹的研制。是的。她是洛斯阿拉莫斯的两位女物理学家之一。然后在他们投下原子弹之后，她搬到了中国。为什么？她对他们投下原子弹非常生气。她的家族与中国有很多联系。她母亲是毛主席的朋友。很奇怪。</p><p>Um, I mean, I can... The list goes on and on and on. I mean, your great great uncle George Everest is what Mount Everest is named after. Is that... is that correct? I think he's my great great great uncle. His niece married George Bull. So, Mary Bull was Mary Everest Bull. Um, she was the niece of Everest. And your first cousin once removed, Joan Hinton, was involved in... a nuclear physicist who worked on the Manhattan Project, which is the World War II development of the first nuclear bomb. Yeah. She was one of the two female physicists at Los Alamos. And then after they dropped the bomb, she moved to China. Why? She was very cross with them dropping the bomb. And her family had a lot of links with China. Her mother was friends with Chairman Mao. Quite weird. </p>
<h2>回顾过去，你会给出的建议</h2><h2>Advice You’d Give Looking Back</h2>
<p>杰弗里，当你回顾你的一生时，我们有了现在的后见之明和回顾性的清晰度，如果你给我建议，你可能会做些什么不同的事情？</p><p>When you look back at your life, Jeffrey, we have the hindsight you have now and the retrospective clarity, what might you have done differently if you were advising me? </p>
<p>我想我有两条建议。第一，如果你直觉别人的做法是错的，而且有更好的方法，不要因为别人说它傻就放弃它。在你弄清楚它为什么错之前，不要放弃它。要亲自弄清楚为什么这种直觉是错误的。通常情况下，如果它与其他人的观点不一致，它就是错的，你最终会弄清楚它为什么错。但偶尔，你的直觉也会是正确的，而其他人的观点都是错的。我就是这样幸运的。早期，我认为神经网络绝对是实现人工智能的必由之路，几乎所有人都说这太疯狂了，但我坚持了下来，因为我做不到。在我看来，这显然是正确的。</p><p>I guess I have two pieces of advice. One is if you have an intuition that people are doing things wrong and there's a better way to do things, don't give up on that intuition just because people say it's silly. Don't give up on the intuition until you've figured out why it's wrong. Figured out for yourself why that intuition isn't correct. And usually it's wrong if it disagrees with everybody else, and you'll eventually figure out why it's wrong. But just occasionally, you'll have an intuition that's actually right, and everybody else is wrong. And I lucked out that way. Early on, I thought neural nets are definitely the way to go to make AI, and almost everybody said that was crazy, and I stuck with it because I couldn't. It seemed to me it was obviously right. </p>
<p>现在，如果你的直觉不好，坚持直觉的想法就行不通了。但如果你的直觉不好，你根本就什么也做不成，所以你最好还是坚持下去。</p><p>Now, the idea that you should stick with your intuitions isn't going to work if you have bad intuitions. But if you have bad intuitions, you're never going to do anything anyway, so you might as well stick with them. </p>
<h2>关于人工智能安全的最后信息</h2><h2>Final Message on AI Safety</h2>
<p>在您自己的职业生涯中，您是否会回顾某些事情并说：“现在回想起来，我应该在那个时刻采取不同的方法？”</p><p>And in your own career journey, is there anything you look back on and say, "With the hindsight I have now, I should have taken a different approach at that juncture?" </p>
<p>我希望我能多花点时间陪伴我的妻子和孩子们，尤其是在他们还小的时候。我当时有点太专注于工作了。</p><p>I wish I'd spent more time with my wife and with my children when they were little. I was kind of obsessed with work. </p>
<p>你的妻子去世了。是的。死于卵巢癌。不。或者那是另一个妻子。好的。嗯，我有两个妻子都得了癌症。哦，真的吗？抱歉。第一个死于卵巢癌，第二个死于胰腺癌。你希望自己能多花点时间陪她吗？是第二任妻子吗？是的。她是个很棒的人吗？</p><p>Your wife passed away. Yeah. From ovarian cancer. No. Or that was another wife. Okay. Um, I had two wives who had cancer. Oh, really? Sorry. The first one died of ovarian cancer, and the second one died of pancreatic cancer. And you wish you'd spent more time with her? With the second wife? Yeah. Who was a wonderful person? </p>
<p>你都70多岁了，为什么还这么说？你发现了什么我可能还不知道的事情吗？</p><p>Why did you say that in your 70s? What is it that you've figured out that I might not know yet? </p>
<p>哦，只是因为她走了，我现在没法再多陪她了。嗯。但你当时并不知道。当时你以为我的意思是，我可能会先她而去，就因为她是女人，我是男人。嗯，我没有——我只是没有在可以的时候花足够的时间陪她。我想我……之所以问这个问题，是因为我觉得我们很多人都太专注于自己的职业，以至于我们觉得伴侣会永生，因为他们一直都在。所以，我们……是的。我的意思是，她非常支持我花很多时间工作，但是……</p><p>Oh, just 'cause she's gone, and I can't spend more time with her now. Mhm. But you didn't know that at the time. At the time, you think I meant it was likely I would die before her, just because she was a woman and I was a man. Um, I didn't—I just didn't spend enough time when I could. I think I…inquire there because I think there are many of us who are so consumed with what we're doing professionally that we kind of assume immortality with our partners because they've always been there. So, we…yeah. I mean, she was very supportive of me spending a lot of time working, but... </p>
<p>你为什么还说你的孩子呢？这是什么意思……嗯，他们小的时候我没花足够的时间陪他们，你现在后悔了。是啊。</p><p>And why did you say your children as well? What's the…well, I didn't spend enough time with them when they were little, and you regret that now. Yeah. </p>
<p>如果您要给我的听众们讲一句关于人工智能和人工智能安全的结束语，您会说什么？</p><p>If you had a closing message for my listeners about AI and AI safety, what would that be? </p>
<p>杰弗里，我们仍然有机会开发出不会取代我们的人工智能。正因为有机会，我们应该投入大量资源来解决这个问题，因为如果我们不这样做，它就会接管我们。</p><p>Jeffrey, there's still a chance that we can figure out how to develop AI that won't want to take over from us. And because there's a chance, we should put enormous resources into trying to figure that out because if we don't, it's going to take over. </p>
<p>您有希望吗？</p><p>And are you hopeful? </p>
<p>我只是不知道。我是个不可知论者。你晚上睡觉前，在心里思考结果的概率时，肯定会偏向某个方向，因为我的情况确实如此。我想现在所有听众心里都预感到了结果会如何发展，虽然他们可能不会大声说出来。我真的不知道。我真的不知道。我觉得这非常不确定。当我感到有点沮丧的时候，我觉得人类完蛋了；人工智能将接管一切。</p><p>I just don't know. I'm agnostic. You must get…get in bed at night, and when you're thinking to yourself about probabilities of outcomes, there must be a bias in one direction because there certainly is for me. I imagine everyone listening now has an internal prediction that they might not say out loud, but of how they think it's going to play out. I really don't know. I genuinely don't know. I think it's incredibly uncertain. When I'm feeling slightly depressed, I think people are toast; AI is going to take over. </p>
<p>虽然我心情愉快，但我认为我们总会找到办法的。或许，作为人类，其中一个方面就是，因为我们一直都在这里，就像我们之前谈论我们所爱的人和我们的人际关系一样，我们漫不经心地认为我们会一直在这里，我们总能弄清楚一切。但万事皆有始有终，就像我们从恐龙身上看到的那样。我是说，是的。我们必须面对这样一种可能性：除非我们尽快采取行动，否则我们即将走到尽头。</p><p>While I'm feeling cheerful, I think we'll figure out a way. Maybe one of the facets of being a human is that because we've always been here, like we were saying about our loved ones and our relationships, we assume casually that we will always be here and we'll always figure everything out. But there's a beginning and an end to everything, as we saw from the dinosaurs. I mean, yeah. And we have to face the possibility that unless we do something soon, we're near the end. </p>
<p>这期播客的结尾，我们有一个传统：最后一位嘉宾会在日记里留下一个问题。他们留给你的问题是：鉴于我们即将面临的一切，你认为对人类幸福的最大威胁是什么？</p><p>We have a closing tradition on this podcast where the last guest leaves a question in their diary. And the question that they've left for you is: With everything that you see ahead of us, what is the biggest threat you see to human happiness? </p>
<h2>对人类幸福的最大威胁是什么？</h2><h2>What’s the Biggest Threat to Human Happiness?</h2>
<p>我认为失业是短期内威胁人类幸福的一个相当紧迫的问题。我认为，如果让大量人失业，即使他们有全民基本收入，他们也不会快乐，因为他们需要目标。因为他们需要目标。是的。也需要奋斗。他们需要感受到自己在做出贡献。他们是有用的。</p><p>I think joblessness is a fairly urgent short term threat to human happiness. I think if you make lots and lots of people unemployed, even if they get universal basic income, they're not going to be happy because they need purpose. Because they need purpose. Yes. And struggle. They need to feel they're contributing something. They're useful. </p>
<p>您认为这种结果（大量工作岗位被取代）发生的可能性大吗？</p><p>And do you think that outcome—that there's going to be huge job displacement—is more probable than not? </p>
<p>是的，我确实这么认为。而且我认为，那种情况发生的可能性肯定更大。如果我在呼叫中心工作，我会感到害怕。大规模失业会在什么时候发生？我认为这已经开始发生了。我最近在《大西洋月刊》上读到一篇文章，说大学毕业生找工作已经越来越难了。部分原因可能是人们已经开始利用人工智能来做他们本来可以做的工作了。</p><p>Yes, I do. And what sort of that one, I think, is definitely more probable than not. If I worked in a call center, I'd be terrified. And what's the timeframe for that in terms of mass job losses? I think it's beginning to happen already. I read an article in The Atlantic recently that said it's already getting hard for university graduates to get jobs. And part of that may be that people are already using AI for the jobs they would have gotten. </p>
<p>我和一家大家都知道、很多人都在用的大公司的CEO聊过，他在私信里告诉我，他们以前有7000多名员工。他说到去年，他们已经缩减到，我记得是5000人。他说现在他们有3600人。他还说，到今年夏天末，由于人工智能客服的出现，他们的员工数量会降到3000人。所以，这已经发生了。是的。他已经把员工数量减半了，因为人工智能客服现在可以处理80%的客服咨询和其他事务。所以，这已经发生了。是的。所以，需要采取紧急行动。是的。</p><p>I spoke to the CEO of a major company that everyone will know of, lots of people use, and he said to me in DMs that they used to have just over 7,000 employees. He said by last year they were down to, I think, 5,000. He said right now they have 3,600. And he said by the end of summer, because of AI agents, they'll be down to 3,000. So, you've got—it's happening already. Yes. He's halved his workforce because AI agents can now handle 80% of the customer service inquiries and other things. So, it's happening already. Yeah. So, urgent action is needed. Yep. </p>
<p>我不知道那个紧急行动是什么。这很棘手，因为这很大程度上取决于政治体系，而目前的政治体系都在朝着错误的方向发展。我的意思是，我们需要做什么？攒钱？比如，我们要省钱吗？我们要搬到世界其他地方吗？我不知道。你会告诉你的孩子怎么做？他们会说：“爸爸，好像会有大量的工作被取代。” 因为我在谷歌工作了10年。钱够用吗？好的。好的。所以，他们不是典型的情况。如果他们没钱怎么办？去培训水管工？真的吗？是的。</p><p>I don't know what that urgent action is. That's a tricky one because that depends very much on the political system, and political systems are all going in the wrong direction at present. I mean, what do we need to do? Save up money? Like, do we save money? Do we move to another part of the world? I don't know. What would you tell your kids to do? They said, "Dad, like there's going to be loads of job displacement." Because I worked for Google for 10 years. Is there enough money? Okay. Okay. So, they're not typical. What if they didn't have money? Trained to be a plumber? Really? Yeah. </p>
<p>杰弗里·辛顿，非常感谢。我想，您是我这辈子交谈过的第一位诺贝尔奖得主。所以，这对我来说是莫大的荣幸。您获奖是因为他一生的杰出贡献，以及在许多深刻的领域推动世界进步，这些进步将带来巨大的进步，以及对我们至关重要的事情。现在，您利用人生的这个阶段，不仅分享了您自己的一些工作成果，也探讨了人工智能的更广泛的风险以及它可能对我们产生的负面影响。</p><p>Geoffrey Hinton, thank you so much. You're the first Nobel Prize winner that I've ever had a conversation with, I think, in my life. So, that's a tremendous honor. And you received that award for a lifetime of exceptional work and pushing the world forward in so many profound ways that will lead to great advancements and things that matter so much to us. And now you've turned this season in your life to shining a light on some of your own work, but also on the broader risks of AI and how it might impact us adversely. </p>
<p>很少有人曾在谷歌或大型科技公司内部工作，为人工智能领域做出过贡献，而现在却站在最前线，警告我们提防他们所从事的领域。实际上，这样的人数量惊人。他们不像现在这样公开露面，而且实际上很难与他们进行这样的对话，因为他们中的许多人仍然在那个行业工作。所以，你知道，如果有人经常试图联系这些人，邀请他们进行对话，他们往往不太愿意公开发言。</p><p>And there are very few people who have worked inside the machine of a Google or a big tech company that have contributed to the field of AI who are now at the very forefront of warning us against the very thing that they worked upon. There are actually a surprising number of us now. They're not as public, and they're actually quite hard to get to have these kinds of conversations because many of them are still in that industry. So, you know, someone who tries to contact these people often and ask invites them to have conversations, they often are a little hesitant to speak openly. </p>
<p>他们私下会说话，但不太愿意公开谈论，因为他们可能仍然有一些动机。我比他们有优势，那就是我年纪大，而且失业了，所以我想说什么就说什么。好了，就这样吧。所以，谢谢你所做的一切。这真的是我的荣幸，请继续努力。谢谢你。非常感谢。人们以为我在开玩笑，但我不是。水管工。是啊，是啊。而且水管工的薪水相当高。</p><p>They speak privately, but they are less willing to speak openly because they may still have some incentives at play. I have an advantage over them, which is I'm older, so I'm unemployed, so I can say what I want. Well, there you go. So, thank you for doing what you do. It's a real honor, and please do continue to do it. Thank you. Thank you so much. People think I'm joking when I say that, but I'm not. The plumbing fish. Yeah. Yeah. And plumbers are pretty well paid. </p>
<br>

</div>
<script id="res-script" src="/res/dist/res/main.js" type="text/javascript"></script>
</body></html>